"""
================================================
File for raw data generating.
From Books to Pretraining Data for LLMs. 
================================================
"""
import os
import json
import pandas as pd
import tiktoken  # ç”¨äºæŒ‰ token é•¿åº¦åˆ‡åˆ†
import nltk
from .utils import *
from .infos import *
from argparse import ArgumentParser


nltk.download('punkt')

if __name__ == "__main__":
    args = ArgumentParser()
    args.add_argument("--bookname", type = str, default = "Koch", help = "The book name for the backward bench.")
    args.add_argument("--max_tokens", type = int, default = 512, help = "The maximum number of tokens for each data.")
    args.parse_args()

    book_name = BOOK_INFO_DICT[args.bookname]["Full Name"]
    # é…ç½®
    txt_folder =f"data/books/pdfs/{book_name}/chapters" 
    output_dir = f"data/books/pretrain"
    check_path(output_dir)
    output_jsonl = output_dir + f"/{book_name}.jsonl"
    output_csv = output_dir + f"/{book_name}.csv"

    # è·å–æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(".txt")])
    # Tokenizerï¼ˆGPT-3/4, LLaMA ç­‰é€‚ç”¨ï¼‰
    tokenizer = tiktoken.get_encoding("cl100k_base")  # OpenAI çš„ tokenizer

    # å­˜å‚¨æ•°æ®
    data = []
    # è¯»å– & å¤„ç†æ–‡æœ¬
    for idx, txt_file in enumerate(txt_files, 1):
        file_path = os.path.join(txt_folder, txt_file)
        
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
        # æŒ‰ "ç©ºè¡Œ" åˆ†å‰²ï¼ˆæ®µè½æ–¹å¼ï¼‰
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
        # è¿›ä¸€æ­¥åˆ†å‰²æˆ token é™åˆ¶çš„ chunks
        chunk = []
        chunk_token_count = 0
        for para in paragraphs:
            tokens = tokenizer.encode(para)  # è®¡ç®—å½“å‰æ®µè½çš„ token æ•°
            if chunk_token_count + len(tokens) > args.max_tokens:
                # å­˜å‚¨å½“å‰ chunk
                if chunk == []:
                    continue
                data.append({"index": len(data) + 1, "text": " ".join(chunk)})
                chunk = []
                chunk_token_count = 0
            chunk.append(para)
            chunk_token_count += len(tokens)
        # å­˜å‚¨æœ€åçš„ chunk
        if chunk:
            data.append({"index": len(data) + 1, "text": " ".join(chunk)})
    # ä¿å­˜ä¸º JSONL
    with open(output_jsonl, "w", encoding="utf-8") as f:
        for entry in data:
            f.write(json.dumps({"text": entry["text"]}, ensure_ascii=False) + "\n")
    # ä¿å­˜ä¸º CSV
    df = pd.DataFrame(data)
    df.to_csv(output_csv, index=False, encoding="utf-8")
    print(f"ğŸ¤–: Dataset saved: {output_jsonl}, {output_csv}")
