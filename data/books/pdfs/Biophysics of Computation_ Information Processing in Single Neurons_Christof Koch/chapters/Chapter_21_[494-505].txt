===== Page 1 =====
COMPUTING WITH NEURONS:
A SUMMARY
We now have arrived at the end of the book. The first 16 chapters dealt with linear and
nonlinear cable theory, voltage-dependent ionic currents, the biophysical origin of spike
initiation and propagation, the statistical properties of spike trains and neural coding,
bursting, dendritic spines, synaptic transmission and plasticity, the types of interactions that
can occur among synaptic inputs in a passive or active dendritic arbor, and the diffusion
and buffering of calcium and other ions. We attempted to weave these disparate threads
into a single tapestry in Chaps. 17-19, demonstrating how these elements interact within
a single neuron. The penultimate chapter dealt with various unconventional biophysical
and biochemical mechanisms that could instantiate computations at the molecular and the
network levels.
It is time to summarize. What have we learned about the way brains do or do not compute?
The brain has frequently been compared to a universal Turing machine (for a very lucid
account of this, see Hofstadter, 1979). A Turing machine is a mathematical abstraction
meant to clarify what is meant by algorithm, computation, and computable. Think of it as a
machine with a finite number of internal states and an infinite tape that can read messages
composed with a finite alphabet, write an output, and store intermediate results as memory.
A universal Turing machine is one that can mimic any arbitrary Turing machine. We are
here not interested in the renewed debate as to whether or not the brain can, in principle, be
treated as such a machine (Lucas, 1964; Penrose, 1989), but whether this is a useful way to
conceptualize nervous systems in this manner.
Because brains have limited precision, only finite amounts of memory and do not live
forever, they cannot possibly be like "real" Turing machines. It is therefore more appropriate
to ask: to what extent can brains be treated as finite state machines or automata! Such a
machine only has finite computational and memory resources (Hopcroft and Ullman, 1979).
The answer has to be an ambiguous "it depends." If one seeks to understand the
nature of computation in general and how operations carried out by nervous systems,
such as associative memory or visual object recognition, can be implemented in machines,
then Boolean circuits or finite state machines will be relevant (although not sufficient to
469
21


===== Page 2 =====
470 
· 
COMPUTING WITH NEURONS: A SUMMARY
understand these operations, since the theory of computation includes no recipe to decide
on the proper level of abstraction and representation).
Yet when the goal is to comprehend any one nervous system and why it has the features it
does, for instance, probabilistic synaptic release, finite state machines and their relatives will
not be very useful for a number of reasons. The most obvious is that these virtual machines
are disembodied entities that fail to account for the fact that any information processing
system existing in the real world has physical extent, takes some time before it comes to
any sort of decision, and requires power to operate. And in the real world, space, time, and
power are in short supply. Build a brain that is too big and the organism will be left behind
in the struggle to survive; if the brain does not react in real time to an approaching threat,
it will be quickly eaten by something that responds more rapidly. And an inefficient brain
requires too much energy to sustain for long. None of these considerations are incorporated
into our current notion of computation. It is only with the advent of portable computers,
cellular phones, and the like that the power consumption associated with each elementary
computation is beginning to be scrutinized (Sarpeshkar, 1998).
Brains differ in some rather obvious ways from present-day computers: memory and
computation are not separated as they are in all of our current machines; the nervous
system operates without any systemwide clock and is built from stochastic elements. Finally,
developing as well as mature brains are constantly reprogramming themselves, up or down
regulating synaptic weights, modulating the degree of adaptation, shifting the character and
frequency of central pattern generators, changing the time constants of integration, and so
on. Conceptually, this amounts to the input changing the transition function governing how
the machine switches from one state to the next. Put differently, a nervous system will act
like a machine that changes its instruction set as a function of its input.
On the positive side, what can we say about the manner in which the nervous system
processes information (Koch, 1997)? Individual neurons convert the incoming streams
of binary pulses into analog, spatially distributed variables, the postsynaptic voltage and
calcium distribution throughout the dendritic tree, soma, and axon. These appear to be the
two primary variables regulating and controlling information flow, each with a distinct
spatio-temporal signature and dynamic bandwidth. The transformation between the input
and the postsynaptic Vm and [Ca
2+],· involves highly dynamic synapses that adapt to
different time scales, from milliseconds to seconds and longer. Information is processed in
the analog domain, using a menu of linear and nonlinear operations. Second-order and higher
multiplication, amplification, sharpening and thresholding, saturation, temporal and spatial
filtering, and coincidence detection are the major operations readily available in the dendritic
cable structure augmented by voltage-dependent membrane and synaptic conductances.
This is a large enough toolkit to satisfy the requirement of the most demanding analog
circuit designer. To see this, take the case of implementing a spatial low-pass or band-pass
filtering operation of the type described in Chap. 10 (e.g., Fig. 10.8). Much of early sensory
processing, for instance, in the retina, involves such linear filtering operations. Convolving
a two-dimensional mxm pixel image by an η χ η pixel size filter requires on the order
of n
2m
2 operations on a digital computer (this can be reduced to 6m
2 log m operations
using the fast fourier transform). Implemented within a dendritic tree or across nonspiking
retinal neurons connected by electrical gap junctions (Sec. 4.10), spatio-temporal filtering
is carried out in parallel within one or two time constants, no matter the size of the image.
The outcome of these analog dendritic computations is once again converted into
asynchronous pulses—possibly supplemented by bursts that could act as high-confidence
symbols—and conveyed to the following neurons. The precision of this firing rate code is in


===== Page 3 =====
21.1 Is a Biophysics of Computation Possible? 
· 471
the 5-10 msec range (and better in some cases). Synchrony or correlations across neurons
are likely to contribute to coding, but the extent remains unclear. Reliability can be achieved
by pooling the responses of a small number of neurons (on the order of 10-1000).
Individual nerve cells respond in a relatively slow and unreliable manner to sub- or
peri-threshold input. However, for rapidly changing suprathreshold input, as expected
during normal operations, neurons can respond in a rapid and reliable manner (Mainen
and Sejnowski, 1995).
It is doubtful whether the effective resolution, that is, the ratio of minimal change in any
one variable, such as Vm or [Ca
2+],·, relative to the noise amplitude associated with this
variable, exceeds a factor of 100. Functionally, this corresponds to between 6 and 7 bits of
resolution, a puny number compared to a standard 32-bit machine architecture.
Memory is everywhere, intermixed with the computational elements, but it cannot be
randomly accessed: in the concentration of free calcium in the presynaptic terminal and
elsewhere, implementing various forms of short-term plasticity, in the density of particular
molecules in the postsynaptic site for plasticity on a longer time scale, in the density and
exact voltage dependency of the various ionic currents at the hour or day time scale, and
ultimately, of course, in the genes at the cell's nucleus for lifetime memories.
21.1 Is a Biophysics of Computation Possible?
Multiplication is both the simplest and one of the most widespread of all nonlinear operations
in the nervous system. It is closely related to the operations of correlation and of squar-
ing. The biophysical implementation of these canonical operations has therefore received
widespread scrutiny. Let us briefly summarize some of the more plausible scenarios.
21.1.1 The Many Ways to Multiply
1. Nonlinear synoptic interaction via shunting inhibition (Torre and Poggio, 1978; Koch,
Poggio, and Torre, 1982; see Sec. 5.1.6).
2. NMDA synapses in a passive or active dendritic tree (Mel, 1992, 1993; see Sees. 5.2.2
and 19.3.4).
3. Log-exp transform. This exploits a technique common to analog electronic circuits. To
multiply χ with y take the logarithm of both, add them, and then invert by computing
an exponential,
(21-1)
A possible example of this can be found in the locust's visual system (Hatsopoulos,
Gabbiani, and Laurent, 1995). The instantaneous firing rate f(t) of a single identified
neuron, the lobula giant movement detector, can be modeled by the algebraic product of
two terms,
(21.2)
where θ is the angle subtended by an approaching object on the insect's retina, θ is its
angular velocity, α and β are appropriate gain factors, and St is the latency (on the order


===== Page 4 =====
472 
. 
COMPUTING WITH NEURONS: A SUMMARY
of 40 msec) of the response. This system has the elegant property that the neuron's peak
firing rate is reached when an approaching object has reached a critical angular size on
the retina.
It has been hypothesized (F. Gabbiani, G. Laurent, and C. Koch, private communica-
tion) that the time-averaged membrane potential (Vm] (Eq. 17.12) at the spike initiation
zone of this cell, when an object is rapidly moving toward the locust, is proportional
to the sum of the synaptic contributions from two sources: the logarithm of θ from the
excitatory input, and —Θ from an inhibitory input. Multiplication could be achieved if
the firing rate is exponentially related to (V m). Because this neuron is readily accessible
to intracellular electrodes while the animal is visually stimulated, it represents an ideal
model system to study the biophysical implementation of multiplication.
4. Product of firing rates (Srinivasan and Bernard, 1976). This mechanism relies on
coincidence detection between two statistically independent spike trains by an integrate-
and-fire unit. Imagine two input streams, carrying spikes at a mean rate of fA and fB, that
converge onto a leaky integrate-and-fire unit (Fig. 21.1). The unitary EPSP of amplitude
VG from either input is large but does not, by itself, exceed the threshold. For this,
two EPSPs are required. In order for near simultaneous inputs from one neuron not to
trigger a spike, their average minimal interspike interval must exceed Δ?, as defined
implicitly by Voe~
At/
T + Vth = Va,. Because the probability of the joint occurrence of
two statistically independent events is the product of the probabilities of the individual
events, it can easily be shown that the average firing rate of the coincidence unit is
(21.3)
This trick only works in the presence of sufficient jitter. If the input spikes are too regular,
the response depends on the exact phase relationship between the two trains. (Srinivasan
and Bernard, 1976, simulated this mechanism using a Cy of the input spike trains of
around 30%.) Poisson distributed input spike trains give rise to a qualitatively similar
result (Fig. 21.1).
5. Linear summation of noisy linear threshold neurons (Suarez and Koch, 1989). A squaring
operation can be implemented using a population of spiking cells. The output of these
neurons is zero up to a threshold Vth and linear in the input for larger values. An input
χ is applied to a population of such cells whose thresholds are drawn from a uniform
probability distribution (the Vth value for any one cell remaining constant in time), and
the output from all cells is summed. This sum approximates the area, between Va, and
some upper limit, under the output curve using a Monte-Carlo sampling procedure.
As the number of neurons goes to infinity (in simulations 50-100 cells suffice; Suarez
and Koch, 1989), the sum converges to a quadratic function of the input (for a linear
input-output curve). Computing χ y requires two populations of such cells with jittered
thresholds, one evaluating (x + y)
2 and one (x — y)
2. Subtracting these two yields the
desired result.
Both this and the previous mechanism exploit random spike times for computational
purposes. In other words, they do not work in spite but because of noise.
For further alternatives involving synaptic disinhibition, the synaptic transfer function at
a graded dendro-dendritic synapse, or the cooperative nature of calcium binding to enzymes
or other molecules (Sec. 11.4.2), see Koch and Poggio (1992).


===== Page 5 =====
21.1 Is a Biophysics of Computation Possible?
473
Fig. 21.1 MULTIPLYING FIRING RATES 
An
elementary fact of probability theory, that in-
dependent probabilities multiply, is exploited
here to achieve multiplication (Srinivasan and
Bernard, 1976). A leaky integrate-and-fire unit
is excited by two presynaptic axons, each with
independent Poisson distributed spikes with
fA = fB = 20 Hz giving rise to two associ-
ated synaptic current sources IA and IB. Each
input gives rise to a 10-mV massive EPSP not
sufficient to exceed threshold Va, = 15 mV
(τ = 10 msec). Occasionally, consecutive EP-
SPs from one fiber are close enough to each other
to exceed the threshold (single arrow in upper
trace). If two inputs from the two axons arrive
"simultaneously," here within about 11 msec,
Vth is exceeded and a postsynaptic spike is trig-
gered (arrows in lower trace). The average out-
put firing rate is proportional to (/A+/a)
2. This
mechanism requires jitter in spike arrival times.
21.1.2 A Large Number of Biophysical Mechanisms for Computation
The integrated electronic circuits at the heart of today's microprocessors are fabricated
using CMOS technology. The physics of these circuits makes logical inverters, NAND and
NOR gates, particularly easy to implement. It is not that AND or AND-NOT gates cannot be
designed in CMOS, but just that they use more transistors and are therefore less efficient
(Fig. 0.1). We posed the question early on in this book whether the biophysics of membranes,
synapses, and neurons equally constrain the types of mathematical operations that can be
implemented efficiently in nervous tissue.
The original intent (Koch and Poggio, 1987) of the "Biophysics of Computation"
program was motivated by the hope that a handful of biophysical computations would
be universal, in the sense that they underlie all computations in the nervous system and that
they can be implemented in a natural and "efficient" (see below) manner. In the earliest
day of neural networks, the spiking threshold (or its continuous equivalent, the sigmoidal
output function) was believed to be the only relevant nonlinear operation. Later on, it was
argued (Torre and Poggio, 1978; Poggio and Torre, 1981) that the repertoire of primitive
nonlinear operations should be expanded to include the nonlinear interactions occurring
between conductance-increasing synapses in a passive dendritic tree.


===== Page 6 =====
474 · COMPUTING WITH NEURONS: A SUMMARY
As we saw in the preceding pages, the list of biophysical mechanisms that have been
tied to specific computations has expanded considerably. Figure 21.2 tabulates these by
borrowing from the summary diagram in Churchland and Sejnowski (1992) and ordering
them along two dimensions, their approximate spatial extent and the time scales at which
they act. And this listing is by no means exhaustive.
It should be kept in mind that many of the biophysical mechanisms discussed here are
likely to have important noncomputational roles. Computers include circuits for managing
power, for compensating parasitic capacities, for speeding up memory access, and so on,
which make the system perform its task more efficiently. Yet these circuits are themselves
neither strictly necessary nor directly related to specific computations. Likewise, neurons
develop their characteristic properties over the lifetime of the animal, have metabolic
requirements, and need to ensure stability in the face of relentless molecular turnover.
Because natural selection acts to ensure the overall optimality of the organism given its
particular ecological niche, it will frequently be difficult to isolate the exact role that any
biophysical mechanism plays in brain-style computation.
21.1.3 Can Different Biophysical Mechanisms Be Selected For?
It is sobering to realize that to implemt a single operation—multiplication—the nervous
system can choose from mechanisms that operate at the level of individual synapses and
spines, to those that require small populations of cells. This raises the unsettling, but
quite plausible scenario in which any one computation is carried out using a plurality
of mechanisms at different spatial and temporal scales.
As just one example consider retinal direction selectivity, the differential response to
the motion of a spot of light in the preferred direction compared to motion in the opposite
direction (Sec. 5.1.7). This asymmetry could arise due to nonlinear synaptic interactions at
the level of the starburst amacrine cells and is reinforced by exploiting temporal dispersion
in the elongated asymmetrical dendrites of these cells and the nonlinear synaptic transfer
function onto the dendrites of the ganglion cells. The response could be further accentuated
by nonlinear synaptic interactions in the dendritic tree of the ganglion cell.
Numerous biophysical mechanisms acting in tandem is a robust way to achieve one's
goal since failure of any of these operations would not imperil the entire computation.
Furthermore, each mechanism most likely works optimally only under a narrow range
of stimulus conditions (in the above example, contrast or speed of the spot of light), so
that invariance to the vagaries of the stimulus requires several mechanisms operating in
parallel.
Indeed, one can argue for an extreme version of this where the exact contributions that
the mechanisms outlined in Fig. 21.2 make toward any one specific computation differ,
depending on the developmental stage of the animal and its own idiosyncratic experience.
In other words, any one computation would be implemented by the linear or nonlinear
superposition of a host of biophysical mechanisms, where the coefficients specifying the
contribution that each mechanism makes vary from one animal to the next.
Once we discovered all of these biophysical operations, it would remain for us to
understand and characterize the learning rules that, given some input stream and various
system level constraints, such as minimizing metabolic costs, would derive the exact
contribution each mechanism makes toward the overall goal of achieving directional
selectivity. Such a learning algorithm would need to take the nonlinear nature of the
biophysical mechanisms into account and the way they interact with each other, whether


===== Page 7 =====
21.1 Is a Biophysics of Computation Possible? 
· 475
Fig. 21.2 BIOPHYSICS OF COMPUTATION: A SUMMARY 
Summary diagram imposing order on
the biophysical mechanisms linked to specific computations by locating them in a space-time
diagram. The degree of uncertainty about the extent to which any one mechanism actually exists
and does what it has been proposed to do—admittedly a dicey and subjective matter—is indicated
by the amount of shading: the darker, the more evidence in favor. The spatial extent of a particular
mechanism that instantiates the computation and the time scale at which the operation occurs are
indicated in logarithmic units (of μτη and msec). This nonexhaustive list includes (with their section
numbers in parentheses): autocatalytic molecular storage elements in single spines (20.1), synapses
that signal deviation from their average input activity (13.5.4), low-, band-, or high-pass spatio-
temporal filters implemented in single neurons or in neurons linked by electrical gap junctions
(Chaps. 2 and 3, and Sees. 4.10 and 10.4), synaptic logic and multiplication on the basis of individual
synapses (5.1.6 and 5.2.3) or groups of synapses in passive or active dendritic trees (5.2, 19.3.2, and
19.3.4), a random event generator via spontaneous generation of spikes by stochastic ionic channels
(8.3.2), coincidence detection using fast dendritic spikes (19.3.3) or whole neurons (21.1.1), gain
normalization and adaptation via the conductance increasing action of chemical synapses (1.5.3,
4.8.1, and 18.3), selective linearization and amplification of distal input via active dendritic currents
(19.3.5), controlling the gain of a cell's discharge curve with /ΑΗΡ (9.3; see also Fig. 21.3), computing
the average activity via intracellular calcium (9.1.6), implementing temporal delay elements using
a potassium current (9.2.1), associative memory via Hebbian synapses (13.5), and reprogramming
networks and the routing of information using peptides and hormones (Sees. 20.4 and 20.5).


===== Page 8 =====
476 
· 
COMPUTING WITH NEURONS: A SUMMARY
cooperatively or competitively. The astute reader will have noticed that the selection
scheme alluded to here has quite a few analogies with the way the vertebrate immune
system works.
The conceptually simpler possibility is that by and large only a single biophysical
operation is responsible for any one specific biological operation. It is to be hoped that
ongoing research will help us decide this issue in the next decade.
It should not be forgotten that a concatenation of nonlinear mechanisms, operating at
different levels in the nervous system, might compensate each other in such a way that the
overall result is difficult to predict by contemplating each mechanism in isolation. A case in
point is shunting inhibition. Within a passive cable, it can act in a very nonlinear, vetolike
manner. Yet as we saw in Sec. 18.5, when treated within the context of a spiking neuron it
affects its firing rate in a subtractive manner.
Sometimes neurons can act in a surprisingly linear fashion, even though we know
that their components, synapses, voltage-dependent currents, and so on include substantial
nonlinearities. A beautiful example of this are brainstem neurons in the medial vestibular
nucleus that help mediate the vestíbulo-ocular reflex. Their discharge curve is remark-
ably linear over the entire physiological firing range (Fig. 21.3). Reducing adaptation by
pharmacological blockage of the underlying calcium-dependent potassium conductance
fails to cause linearity to deteriorate but just leads to an increased gain (as in Fig. 9.13;
du Lac, 1996). The linearity of the cell extends to the temporal domain (du Lac and
Lisberger, 1995). The response to steps of current can be accurately predicted from the
linear superposition of sinusoidal currents that are injected directly into the soma (in the
0.1-10 Hz frequency range). The high degree of linearity requires the active participation
of voltage-gated currents to compensate for the low-pass filtering properties expected from
a passive membrane.
Fig. 21.3 A LINEAR NEURON We have come full circle. The last figure in the book is reminiscent
of one of the first figures (1.4) and underlines that sometimes, despite all the nonlinear mechanisms
known to exist in the nervous system, evolution conspires to construct neurons that act in a perfectly
linear manner: the /-/ curve of a medial vestibular nucleus (brainstem) neuron recorded in a slice
with a slope of 124 Hz per nanoampere of injected current (du Lac, 1996). Circles are the data
with the best linear fit indicated by the line. The neuron fires spontaneously. Apamin, one of the
active components in bee venom, removes adaptation by blocking /ΑΗΡ· As in the model of bullfrog
sympathetic ganglion cells (Fig. 9.13), this causes the gain of the /-/ curve to increase (here by
1.67) without eliminating linearity. Linearity is also observed in the temporal domain and has been
assessed using the superposition theorem. Reprinted by permission from du Lac (1996).


===== Page 9 =====
21.2 Strategic Questions or How to Find a Topic for a Ph.D. Thesis 
· 
477
A further example is the linearization procedure thought to occur in distal dendrites
(Sec. 19.3.5; see also Laurent, 1993). Because of the associated high input impedance,
synaptic input will quickly saturate. This can be counteracted by an inactivating potassium
current such that the transfer function between synaptic input and postsynaptic somatic
current is linear.
It is almost as if there were a deeper principle of "compensating nonlinearities" at
work in these systems. Yet what advantage would the nervous system derive from linear
components that, at best, process no information and, at worst, lose information due to
unavoidable contamination of the signal by noise? Is it possible that linear systems are
easier to evolve and can be more readily incorporated into existing systems without causing
stability problems?
21.2 Strategic Questions or How to Find a Topic for a Ph.D. Thesis
In the early phase of the scientific development of a field, progress occurs as much by
asking the right sort of question as by answering them. Today, in the heat of the battle it is
difficult, of course, to know whether we are asking the right sorts of questions. Undaunted
by this challenge, let us finish the book by enumerating, in no particular order, a small set
of strategic questions.
1. Given the canonical nature of multiplication, we need to understand in detail, in at least
one neurobiological model system, how this nonlinear operation is implemented at the
cellular level.
2. What are the principal, intrinsic noise sources (as compared to external noise sources,
such as the shot nature of individual quanta of light impinging onto the retina) in the
nervous system? Ultimately, they are what limits the accuracy with which signals in
the environment can be detected and computations can be carried out. A catalogue
of noise sources needs to include thermal noise, the ill-defined but ever present I//
noise, the probabilistic nature of synaptic transmission, the flickering of stochastic ionic
channels—in particular if they are present at low density in the dendrites—as well as
the "spontaneous" neuronal background activity.1 Noise sources need to be described in
terms of their amplitudes, power spectra and temporal structure. How do they constrain
particular computations?
3. To what extent do considerations involving the energy metabolism play a role in
constraining the style of neuronal computation?
"On" and "off" cells in the visual pathways are a case in point (Sterling, 1998). The
retina communicates an increase in local spatial contrast (e.g., a bright spot on a dark
background) to the rest of the brain by increasing the firing rate of so-called on ganglion
cells (Kuffler, 1953). Since firing rates cannot be negative, how are decreases in local
contrast coded? One solution would be for the on neurons to have a very high maintained
discharge, say at 100 Hz, and then to signal a decrease in contrast by decreasing this
rate. Yet a million neurons firing at 100 Hz is expensive in terms of the continuous
1. However, this background may just appear as noise from the limited point of view of the observer and may well turn out
to represent a significant signal correlated across many cells.


===== Page 10 =====
478 
· 
COMPUTING WITH NEURONS: A SUMMARY
upkeep of the ionic gradients across the membrane of the ganglion cells. Given that the
retina is already one of the most metabolic active pieces of nervous tissue—as witnessed
by its dense blood supply—this solution appears unfeasible. Instead, the visual system
has adopted an alternative strategy by creating off cells whose discharge increases in
response to a dark spot on a bright background (that is, a local decrease in contrast). By
using two half-wave rectified channels the brain solves the problem of signalling negative
numbers (Schiller, Sandell, and Maunsell, 1986) with the added benefit of doubling
contrast sensitivity (albeit at the price of also doubling the number of neurons needed).
To what extent do metabolic considerations determine whether or not a neuron outputs
graded potentials or spikes (van Hateren and Laughlin, 1990; Haag and Borst, 1997)?
Knowing the amount of sodium and potassium ions that move across the membrane
during the propagation of a spike and the metabolic cost in terms of the universal ATP
energy currency of pumping them back via the Na
+-K
+ exchanger gives us an estimate
of what a single action potential costs. Similar estimates could be made for the cost of
synaptic signaling, although this calculation would be more extensive since the cost of
maintaining and recycling vesicles would have to be factored in (Laughlin, 1994). And
how expensive are dendritic computations?
Coding schemes have been proposed that minimize the firing rate of neurons on
the assumption that the metabolic cost of spiking is substantial and must be kept to a
minimum (Levy and Baxter, 1996). What fraction of the 13-15 watt power consumption
of an adult human brain is necessary for maintaining homeostasis and what fraction
directly supports signaling in general and spiking or dendritic computations in particular,
needs to be determined (Kety, 1957; Sokoloff, 1989).
2
The quiescent power consumption of CMOS integrated electronic circuits—due to
leakage currents—is minuscule. The 30 watt or so required by a modern microprocessor
can be attributed to charging and discharging the parasitic capacitances during every
clock cycle. That is, most of the power goes to computing and switching and very
little to maintenance. (For supercomputers the cost of air-conditioning must be factored
into this estimate.) It would be interesting to "poison" an entire brain with TTX to
block sodium action potentials, and to measure the associated energy metabolism in
the absence of any spiking. It is possible that the fraction of the energy devoted to
homeostasis versus computation/communication is heavily skewed toward the former
while in our microprocessors it is toward the latter?
4. What is the function of the apical dendrite, the most prominent component of many
pyramidal cells in the cortex? Is it a vehicle for rapidly communicating information
perpendicular to the cortical surface, like a copper wire? Can input to the distal apical
tree, frequently originating in higher cortical areas, only weakly modulate the firing rate
of the pyramidal cell or can it, by itself, trigger vigorous spiking activity in the absence
of other input (Sec. 19.3.5)?
Is the total length of the apical dendrite in a neocortical pyramidal cell—and hence
the thickness of the cortical sheet—limited by the distance over which a synaptic signal
from the distal apical tuft to the cell body can travel without being swamped by the
2. Given the 2.4 χ 10
14 synapses in a typical human cortex and an average firing rate of 10 Hz per neuron, we estimate that
about 2.4 X 10 
synaptic switching events occur every second. With a power budget of 15 watts, this amounts to 1.6 X 10
operations per joule of energy (neglecting all computations taking place in the dendrites). This compares very favorably to the
cost of computing on a modern workstation (around 5 to 10 million operations per joule, Sarpeshkar, 1998).


===== Page 11 =====
21.2 Strategic Questions or How to Find a Topic for a Ph.D. Thesis 
· 
479
distributed noise sources in the intervening dendritic membrane (e.g., channel noise)? In
other words, if the apical dendrite were to be any longer, the signal arriving at the soma
from the distal periphery could not be distinguished from the noisy background (this is
an argument having to do with signal-to-noise ratio and not with signal attenuation as
in Sec. 19.3.5).
5. We must derive unsupervised learning algorithms—and compare them against exper-
imental data—that can explain how neurons develop, optimize, and maintain their
complex properties in the face of constantly varying environmental conditions (Bell,
1992; Siegel, Marder, and Abbott, 1994). What type of learning rules could help pattern
dendritic tree morpholgy and the spatial distribution of voltage-dependent membrane
conductances? A simple feedback mechanism—acting via [Ca2+], at the cell body—
has been shown to keep the time-averaged firing rate of a neuron pegged to some
constant even though the maximal conductances are changing (LeMasson, Marder, and
Abbott, 1993).
We need to tackle the challenging problem of "learning" Hodgkin-Huxley-like spiking
systems. Much would be achieved by the discovery of a local learning rule that starts out
with the standard spiking conductances and their correct voltage and time dependencies
but with either zero or random settings of the channel densities and then learns the
associated maximal conductances G^a and GK so that the system generates action
potentials (on the simplifying premise that the nature of the individual sodium and
potassium channels is relatively fixed and that only the number of channels expressed
per area needs to be adjusted to obtain spikes).
6. What is the function of dendritic trees? Why do different cell types possess so strikingly
different morphologies? One possibility was discussed extensively in Chap. 5: the
dendritic tree geometry, coupled with a unique synaptic architecture, implements specific
computations. Although this hypothesis is by now almost two decades old, we still do
not know to what extent individual synapses (as in the AND-NOT logic of Koch, Poggio,
and Torre, 1982) or groups of them (as in Mel, 1993) are involved in such computations
or whether the location of synapses in the tree is pretty much irrelevant.
An alternate hypothesis is that dendrites are a means of maximizing the probability
that diffuse axonal trees contact as many neurons as possible (within a fixed volume),
subject to the constraint that neuronal processes have to have some minimal diameter
to accommodate mitochondria and other intracellular organelles and to assure the
transport of macromolecules. What sort of space-filling, fractal geometry will maximize
the dendritic surface area for contacts with the largest number of axons under these
conditions? Can a simple learning rule give rise to such a geometrical structure during
development?
7. It is important to construct canonical single-cell models that are more faithful to biology
than the linear threshold models in usage today. On the one hand, such models must
be simple enough to be treatable using standard mathematical techniques. On the other
hand, they must be rich enough to capture much of the passive and active dendritic
processing occurring in a real dendritic tree. Polynomial threshold neurons (Sec. 14.4.2)
that include temporal processing in the input and that retain spiking information in their
output are a good starting point. The neural network community needs to come up with
novel theoretical tools that can deal with signal and information processing in cascades
of such digital-to-analog-to-digital computational elements.


===== Page 12 =====
480 
· 
COMPUTING WITH NEURONS: A SUMMARY
Finding answers to any one of these questions, possibly in the form of a Ph.D. thesis,
will ultimately help us better understand the most complex object in the known universe,
the human brain. The author as well as the reader of these pages are fortunate enough to
live in exciting times where much progress in the neurosciences occurs within a relatively
short span of history. Let's get to it: per áspera ad astral


