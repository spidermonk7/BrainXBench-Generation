===== Page 1 =====
SIMPLIFIED MODELS
OF INDIVIDUAL NEURONS
In the previous thirteen chapters, we met and described, sometimes in excruciating detail,
the constitutive elements making up the neuronal hardware: dendrites, synapses, voltage-
dependent conductances, axons, spines and calcium. We saw how, different from electronic
circuits in which only very few levels of organization exist, the nervous systems has many
tightly interlocking levels of organization that codepend on each other in crucial ways. It is
now time to put some of these elements together into a functioning whole, a single nerve
cell. With such a single nerve cell model in hand, we can ask functional questions, such as:
at what time scale does it operate, what sort of operations can it carry out, and how good is
it at encoding information.
We begin this Herculean task by (1) completely neglecting the dendritic tree and (2)
replacing the conductance-based description of the spiking process (e.g., the Hodgkin-
Huxley equations) by one of two canonical descriptions. These two steps dramatically
reduce the complexity of the problem of characterizing the electrical behavior of neurons.
Instead of having to solve coupled, nonlinear partial differential equations, we are left
with a single ordinary differential equation. Such simplifications allow us to formally treat
networks of large numbers of interconnected neurons, as exemplified in the neural network
literature, and to simulate their dynamics. Understanding any complex system always entails
choosing a level of description that retains key properties of the system while removing
those nonessential for the purpose at hand. The study of brains is no exception to this.
Numerous simplified single-cell models have been proposed over the years, yet most
of them can be reduced to just one of two forms. These can be distinguished by the form
of their output: spike or pulse models generate discrete, all-or-none impulses. Their output
over time can be treated as a series of delta functions ^ S(t —1{). Implicitly this assumes
that no information is contained in the spike height or width. The original model of a neuron
in McCulloch and Pitts (1943) as well as the venerable integrate-and-fire unit are instances
of pulse models. In firing rate neurons, the output is a continuous firing rate, assumed to be
a positive, bounded, and stationary function of the input. Examples of these are the units at
the heart of Hopfield's (1984) associative memory network.
330
14


===== Page 2 =====
14.1 Rate Codes, Temporal Coding, and All of That 
• 
331
Yet before we can delve into more detail we need to introduce the deep issue of the
proper output representation of a spiking cell, which relates directly to the question of the
neuronal code used to transmit information among neurons.
14.1 Rate Codes, Temporal Coding, and All of That
How the neuronal output is represented, as a series of discrete pulses or as a continuous
firing rate, relates to the code used by the nervous system to transmit information between
cells. So let us briefly digress and talk about neuronal codes.
In a typical physiological experiment, the same stimulus is presented multiple times to
a neuron and its response is recorded (Fig. 14.1). One immediately notices that the detailed
response of the cell changes from trial to trial. Characterizing and analyzing the stochastic
components of the neuronal response is important enough that we dedicate the following
chapter to it.
Given the pulselike nature of spike trains, the standard procedure to quantify the neuronal
response is to count how many spikes arrived within some sampling window Af and to
divide this number by the number of presentations. This yields the conditional probability
that a spike occurred between t and t + Af given some particular stimulus.
In the limit of very small sampling windows—such that the probability for more than one
spike occurring within At is vanishingly small—and infinitely many trials, the probability
of spiking is given by f(t)8t, where f ( t ) is the instantaneous firing rate of the neuron (in
units of spikes per time). Plotting f ( t ) as a function of the time after onset of a stimulus
gives rise to the poststimulus time histogram (PSTH; Fig. 14.1).
It is important to understand the artificial nature of this construct f ( t ) . The nervous
system has to make a decision based on a single spike train and not on the average of tens or
more spike trains. A visual neuron in the fly does not have the opportunity to see the hand
that is about to swat it approach ten times before it makes the decision to initiate an escape
response! A neuron can only observe £^ <5 (?—f;) from its presynaptic partners, and not /(?).
Under certain conditions this might be different. If a cell, say in the cortex, has access
to the spiking output of many cells with the same receptive field properties, the temporal
average of the single presynaptic neuron can be replaced by an ensemble average over a
population of neurons, thereby approximating f ( t ) . In many cases population rate coding
cannot occur for lack of a sufficient large cell population to average over. In the insect, for
instance, a very small number of clearly identifiable neurons code for particular features of
the sensory input and no ensemble averaging occurs.
Given the stochastic nature of spike trains, a common assumption is that the averaged
firing rate of a neuron constitutes the primary variable relating neuronal response to sensory
experience (Adrian and Zotterman, 1926; Adrian, 1932; Lettvin et al., 1959; Barlow, 1972).
This belief is supported by the existence of a quantitative relationship between the averaged
firing rate of single cortical neurons and psychophysical judgments made by a monkey.
That is, the animal's behavior in a visual discrimination task can be statistically predicted
by counting spikes over a long interval (typically 1 sec or more) in a single neuron in
visual cortex (Werner and Mountcastle, 1963; Barlow et al., 1987; Newsome, Britten, and
Movshon, 1989; Vogels and Orban, 1990; Zohary, Hillman, and Hochstein, 1990; Britten
etal, 1992).
In these experiments, the rate is estimated by averaging over a window that is large
compared to the time in which the sensory stimulus itself changes,


===== Page 3 =====
332
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Fig. 14.1 WHAT Is THE FIRING RATE Definition of the firing rate. The starting point is numerous
trials in which the same stimulus is repeatedly presented to the animal and the spikes generated by
some cell are recorded. These are shown in the raster diagram at the top, taken from a cell in cortical
area V4 in the awake monkey. The stimulus—a grating—is flashed on at 0 and lasts until 1500 msec.
Twenty-three of these trials are averaged, smoothed with a Gaussian of 2-msec standard deviation
a and normalized. This averaging window is so small that it effectively defines the instantaneous
firing rate f ( t ) . These plots are known aspoststimulus time histograms (PSTHs). The two lower plots
illustrate an average firing rate (/(?)} obtained from the raster diagrams using Gaussian smoothing
with CT set to 20 and 200 msec. In many experiments, only the average number of spikes triggered
during each trial, corresponding to a very large value of a (see arrow at 19.5 Hz), is used to relate
the cellular response to the behavior of the animal. It is important to realize that a single neuron only
sees spike trains and not a smoothly varying firing rate. Unpublished data from D. Leopold and N.
Logothetis, printed with permission.
(14.1)
where the first sum j is executed over the n identical trials and the second sum over all rij
spikes at time fy during the jth trial. Notice that the 8 terms have the dimension of spikes


===== Page 4 =====
14.1 Rate Codes, Temporal Coding, and All of That 
• 
333
per St, so that the average {/(?))' has the correct units associated with a rate. Instead of
a rectangular window, a frequent alternative is smoothing the spike train using a Gaussian
convolution kernel with standard deviation a,
(14.2)
(Fig. 14.1). Because of the success in linking (/(/)) with behavior, it has been assumed by
some that only the mean rate, averaged over a significant fraction of a second or longer, is
the relevant code in the nervous system and that the detailed time course of spikes is not.
The past decade has witnessed a revival in the question to what extent an average rate
coding neglects information. (For an expose of these ideas, see the superb textbook by Rieke
et al., 1996.) On the basis of signal-processing and information-theoretical approaches,
we know that individual motion-selective cells in the fly (Bialek et al., 1991), single
afferent axons in the auditory system in the bullfrog (Rieke, Warland, and Bialek, 1993)
and single neurons in the electrosensory system in weakly electric fish (Wessel, Koch,
and Gabbiani, 1996) can carry between 1 and 3 bits of sensory information per spike,
amounting to rates of up to 300 bits per second. This information is encoded using
the instantaneous rate with a resolution of 5 msec or less. And the elegant experiments
of Markram and his colleagues (1997), demonstrating the effect a short delay between
a presynaptic and a postsynaptic spike arriving at a synapse can have on its weight
(Sec. 13.5.4), provide a biophysical rationale for why timing at the 10 msec level is crucial
for synaptic plasticity.
We summarize this vast body of work (see Rieke et al., 1996) by concluding that in many
instances (f(t)}—averaged over a 5-10 msec time frame—appears to be the relevant code
used to transmit information:
More complex neuronal codes do exist and are frequently referred to under the catchall
term of temporal coding. (For an exhaustive listing of possible codes, see Perkel and
Bullock, 1968.) However, because of the implication that rate codes do not preserve detailed
timing information, we prefer the term coined by Larry Abbott (personal communication),
correlation coding.
In an instantaneous firing rate code, the generation of each spike is independent of other
spikes in the trains (neglecting the refractory period and bursting), only a single number,
the rate matters. In a correlation code this assumption is abandoned in favor of coupling
among pairs, triplets, or higher order groupings of spikes.
To give one example of a correlation code, let f ( t ) in response to some stimulus be a
maintained response. We assume that this cell is very noisy with distinct spike patterns on
individual trials. Averaging over them all leads to a flat response of amplitude fc. In a rate
code, fc is the only information available to a postsynaptic neuron. Closer inspection of
the microstructure of spiking reveals that the intervals between consecutive spikes are not
independent of each other, but that two short spike intervals are always followed by a long
one. Any code that fails to exploit these higher order correlations among four consecutive
spikes would miss something. For experimental evidence of such codes see the references
(Segundo et al., 1963; Chung, Raymond, and Lettvin, 1970; Optican and Richmond, 1987;
Eskandar, Richmond, and Optican, 1992; Richmond and Optican, 1992).
A generic problem with the assumption of correlation codes is the question of decoding.
It is unclear what sort of biophysical mechanisms are required to exploit the information
1. Sometimes also written as ( f ( t } ) j - to express its dependency on the size of the averaging window.


===== Page 5 =====
334 
. 
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
hidden in such correlations among spikes. They might be prohibitively complicated to
implement at the membrane level.
So far we have said little about population coding. Once again, one can distinguish two
broad types of codes, correlated ones and noncorrelated ones (Abbott, 1994). The latter are
straightforward: here the information from numerous neurons is combined into a population
code but without taking account of any correlations among neurons (Knight, 1972a,b). There
is plenty of good evidence for such codes in a great variety of different sensory and motor
systems, ranging from the four cricket cereal interneurons encoding the direction the wind is
blowing from (Theunissen and Miller, 1991) to the larger ensembles encoding the direction
of sound in the barn owl (Knudsen, du Lac, and Esterly, 1987; Konishi, 1992) and eye
movements in the mammalian superior colliculus (Lee, Rohrer, and Sparks, 1988) to the
posterior parietal cortex in the monkey encoding our representation of space (Pouget and
Sejnowski, 1997).
Correlation population codes exploit the exact temporal relationships among streams of
action potentials. One way to discover such codes is to record from two or more neurons
simultaneously and to measure their cross-correlation function. For instance, it may be that
two presynaptic neurons always generate spikes within 1 or 2 msec of each other. Much
technological advance has occurred in this area in recent years, so that multi-unit recordings
have now become routine.
In a variety of sensory systems in both invertebrates and vertebrates, physiological
evidence indicates that cross correlations among groups of cells appear to encode various
stimulus features (Freeman, 1975; Abeles, 1982a, 1990; Strehler and Lestienne, 1986;
Eckhorn et al., 1988; Gray et al., 1989; Bialek et al., 1991; Eskandar, Richmond, and
Optican, 1992; Konishi, 1992; Singer and Gray, 1995; Decharms and Merzenich, 1996;
Wehr and Laurent, 1996). The best evidence to date linking neuronal synchronization
directly to behavior comes from the bee's olfactory system (Stopfer et al., 1997). When
pharmacological agents were used to block cellular synchronization—without interrupting
neuronal activity per se—fine olfactory discrimination was disrupted.
Much theoretical work using pulse-coded neural networks has focused on the idea that
spike coincidence across neurons encodes information (Sejnowski, 1977a; Abeles, 1982a,
1990; Amit and Tsodyks, 1991; Koch and Schuster, 1992; Griniasty, Tsodyks, and Amit,
1993; Abbott and van Vreeswijk, 1993; Zipser et al., 1993; Softky, 1995; Hopfield, 1995;
Maass, 1996; van Vreeswijk and Sompolinsky, 1996). Indeed, it has been proposed that
the precise temporal correlation among groups of neurons is a crucial signal for a number
of perceptual processes, including figure-ground segregation and the binding of different
attributes of an object into a single coherent percept (Milner, 1974; von der Malsburg,
1981; von der Malsburg and Schneider, 1986), selective visual attention (Niebur and Koch,
1994), and even the neuronal basis of awareness (Crick and Koch, 1990; for a review see
Koch, 1993).
From our point of view as biophysicists, a correlation population code based on coinci-
dence detection has the significant advantage that it is straightforward to implement at the
membrane level (witness Fig. 21.2).
The question of rate versus correlation coding remains with us. Yet this stark either-or
dichotomy is not very useful. Clearly, the timing of spikes, at least at the 10 msec level, is
important. And in some systems, detailed information across groups of cells will also prove
to be of relevance, although for which properties and at what time scale is an open question.
It therefore behooves us to study how accurately and reliably neurons can generate
individual action potentials and how robust these are to noise. We here lay the groundwork by


===== Page 6 =====
14.2 Integrate-and-Fire Models 
• 
335
introducing pulse neurons as well as firing rate models and describing their basic properties.
We believe that such single-cell models represent the most reasonable tradeoff between
simplicity and faithfulness to key neuronal attributes. The following chapter will deepen our
discussion of stochastic aspects of neuronal firing. We will also discuss firing rate models.
14.2 Integrate-and-Fire Models
We turn to a very simple, but quite powerful model of a spiking cell with a long and distin-
guished history, first investigated by Lapicque (1907, 1926) before anything specific was
known about the mechanisms underlying impulse generation. In its vanilla flavored version,
it is known as the integrate-and-fire model (Stein, 1967a,b; Knight, 1972a; Jack, Noble, and
Tsien, 1975; Tuckwell, 1988b; frequently also referred to as voltage threshold or Lapicque's
model in the older literature). Its simplicity rivals physics' linear oscillator model, yet it does
encapture the two key aspects of neuronal excitability: a passive, integrating subthreshold
domain and the generation of stereotypical impulses once a threshold has been exceeded.
In the world of high speed electronics, integrate-and-fire models have their counter-
part in the class of one-bit analog-digital converters known as oversampled Delta-Sigma
modulators (Wong and Gray, 1990; Aziz, Sorensen, and van der Spiegel, 1996).2
The nonleaky or perfect integrate-and-fire unit consists but of a single capacitance for
integrating the charge delivered by synaptic input in addition to a fixed and stationary
voltage threshold Vth for spike initiation (Fig. 14.2). The leaky or forgetful integrate-and-
fire model includes a resistance, accounting for leakage currents through the membrane.
While integrate-and-fire models do not incorporate the detailed time course of the action
potential, the effect of adaptation can be included. Indeed, the current-frequency relationship
of such an integrate-and-fire cell with a handful of parameters can be very close to that of
a much more complex, conductance-based cell model.
14,2.1 Perfect or Nonleaky Integrate-and-Fire Unit
We will be considering a number of variants of integrate-and-fire "units." All are char-
acterized by a subthreshold domain of operation and a voltage threshold Vtn for spike
generation. The perfect integrate-and-fire unit deals with subthreshold integration via a
single capacitance C. While unphysiological, it is mathematically tractable, which is why it
is frequently invoked for pedagogical purposes. For the sake of mathematical convenience
we assume the input to be a current I(t), arising either from synaptic input or from an
intracellular electrode. The generalization to a conductance-based input is straightforward.
The voltage trajectory of the perfect integrator is governed by the first-order differential
equation
(14.3)
Together with an initial condition Eq. 14.3 specifies the subthreshold time course of the
membrane potential.
Once the potential reaches Vtb, a pulse is triggered and the charge that has accumulated
on the capacitance is shunted to zero (through the open switch in Fig. 14.2A). This would
normally be accomplished by the various conductances underlying spiking. Sweeping the
2. A significant body of mathematics has sprung up around these AS modulators that should be explored for its relevance
to neuroscience; see, for example Norsworthy, Schreier, and Temes (1996).


===== Page 7 =====
336 • 
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Fig. 14.2 INTEGRATE-AND-FIRE MODELS Three basic variants of integrate-and-fire units. Com-
mon to all are passive integration within a single compartment for the subthreshold domain and
spike generation accomplished with a voltage threshold Vih- Whenever the membrane potential V(t)
reaches Vth, a pulse is generated and the unit is short-circuited. For a duration tlef following spike
generation, any input I ( t ) is shunted to ground (corresponding to an absolute refractory period).
(A) The perfect or nonleaky integrate-and-fire model contains but a capacitance. (B) The leaky or
forgetful integrate-and-fire unit accounts for the decay of the membrane potential by an additional
component, a leak resistance R. (C) The adapting integrate-and-fire unit with six free parameters
(Eqs. 14.12 and 14.13) shows firing rate adaptation via the introduction of gadapt» corresponding
to a calcium-dependent potassium conductance (in addition to the absolute refractory period).
Each spike increments the amplitude of the conductance; its value decays exponentially to zero
between spikes.
charge to "ground" has the effect of instantaneously resetting V(t) to zero. Because the
model has no pretense of mimicking the currents involved in shaping the action potential,
spike generation itself is not part of the model. Formally, we model the action potential
by assuming that at the instant t' at which V(t') — Fth (or the first time V(t) exceeds
Vfa for models with instantaneously rising EPSPs) an output pulse, described by the delta
function 8 (t — t'), is generated. The successive times t{ of spike occurrence are determined
recursively from the equation
(14.4)


===== Page 8 =====
14.2 Integrate-and-Fire Models
337
As discussed in section 6.4, a canonical way in which experimentalists characterize a
cell's behavior is by determining its discharge or /-/ 
curve, the relationship between
the amplitude of an injected current step and the average firing frequency (defined over an
interval longer than the interspike interval; as in Eq. 14.1, {/) is computed as the inverse
of the interspike interval).
In response to a sustained current, the membrane potential will charge up the capacitance
until Vih is reached and V is reset to zero. The larger the current, the smaller the intervals
between spikes and the higher the firing rate, according to
(14.5)
Three features are worthwhile here. (1) The firing rate is linearly related to the input current
(Fig. 14.3B). (2) Arbitrarily small input currents will eventually lead to a spike, since no
input is forgotten. (3) The output spike train is perfectly regular. Of course, real neurons
rarely, if ever, respond to a sustained current injection with a regular discharge of spikes
but instead show substantial variability in the exact timing of the spikes. This is particularly
true of neurons recorded in vivo (Holt et al., 1996). The following chapter will deal with
this situation.
Fig. 14.3 SPIKING IN A LEAKY INTEGRATE-AND-
FIRE MODEL 
Average firing frequency, deter-
mined as the inverse of the interspike interval,
as a function of the amplitude of a maintained
current input, for a leaky integrate-and-fire unit
(Fig. 14.2B). (A) Exemplar trace of such a unit
receiving a current step input with / = 0.5 nA.
Before the membrane potential has time to reach
equilibrium, the unit spikes. Vth = 16.4 mV, C =
0.207 nF, R = 38.3 Mfl, and fref = 2.68 msec.
(B) /-/ or discharge curve for the same leaky unit
with refractory period (Eq. 14.11). The slope is
infinite at threshold (/th = Vth/R). The firing rate
saturates at l/fref. For comparison, the /-/ curve
of the nonleaky unit without refractory period with
constant slope l/(VthC) is superimposed. (C) An
adapting conductance (with Ginc = 20.4 nS and
*adapt = 52.3 msec) is added to the leaky integrate-
and-fire unit (see Fig. 14.2C) and the resulting /-
/ curve is compared against the discharge curve
of the biophysical detailed compartmental model
of the layer 5 pyramidal cell (Fig. 17.10). The
degree of matching between simple and very com-
plex models is quite remarkable and supports our
contention that suitably modified integrate-and-fire
models do mimic numerous aspects of the behavior
of neurons. Adaptation is already evident when
considering the first interspike interval (between
the first and second spikes). Adaptation linearizes
the very steep /-/ curve around /^ (compare with
B).


===== Page 9 =====
338 • 
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
The dynamic firing range of nerve cells is limited by the fact that the sodium current
responsible for spiking must recover from inactivation. Potassium currents furthermore
limit the peak firing range. The effect of the absolute refractory period is mimicked by
postulating that following spike generation, the membrane potential is set to zero for a fixed
duration tref; any current arriving within this window is shunted away. This introduces a
nonlinear saturation into the /-/ curve of the perfect integrator,
(14.6)
The output of such an integrator to an arbitrary input current consists of a series of
impulses, £\ S(t — ti), all of which are spaced at least ?ref apart.
14.2.2 Forgetful or Leaky Integrate-and-Fire Unit
The model considered so far will sum linearly multiple subthreshold inputs irrespective of
their temporal relationship because no account is made of a leak. A more realistic behavior
is obtained by incorporating a leak resistance into the subthreshold domain (Fig. 14.2B),
(14.7)
Having first met this equation in Chap. 1 (Eq. 1.5), we know that the evolution of the
subthreshold voltage is completely characterized by convolving / (t) with the associated
Green's function, e~^r (with r = RC). The time course of the membrane potential of
the leaky integrate-and-fire unit to a step of constant current /, switched on at t — 0 and
remaining on, can be obtained by solving Eq. 14.7,
(14.8)
The membrane charges exponentially up to its stationary value V = I R.
The integrator model will only follow this equation as long as the voltage remains below
Vth, since upon reaching the threshold a spike is initiated and the voltage is reset to zero
(Fig. 14.3A). The minimal sustained current necessary to trigger an action potential, that
is, the threshold current, is
(14.9)
For any current / larger than !&, an output impulse will be generated at time T&, such that
IR(1 — e~T*lr) — Vth holds. Inverting this relationship yields the time to spike as
(14.10)
Solving this equation for the minimal duration needed for a sustained current of a fixed
amplitude to generate a spike generates what is known as the strength-duration curve
(Noble and Stein, 1966; Jack, Noble, and Tsien, 1975). Since the voltage is reset following
an impulse and if we assume that the input current persists, the membrane will again charge
up to the threshold, triggering the next spike 7^, + fref later (Fig. 14.3A).
If we take proper account of the refractory period by assuming that for fref following
each spike all input current is simply lost (due to the shunting effect of the conductances
underlying the afterhyperpolarization), the continuous firing rate as a function of the injected
current will be (Fig. 14.3B)


===== Page 10 =====
14.2 Integrate-and-Fire Models 
• 
339
(14.11)
For currents below I& no spike is triggered and at / = /a,, the slope of the /-/ 
curve is
infinite. For large currents, the firing rate saturates at the inverse of the refractory period
(Fig. 14.3B). In the absence of a refractory period, the slope of the /-/ 
curve levels off to
a constant value of l/(VthC), identical to the slope of the nonleaky unit.3 Its steepness can
be increased by reducing the threshold voltage or by decreasing the membrane capacitance.
Due to the refractory period, the /-/ curve gently bends over to level off at /max = l/fref
for (unphysiologically) high current levels.
14.2.3 Other Variants
Besides the generic version of the integrate-and-fire model discussed above, a number of
variants are in use.
1. In order to better account for the 50-100 msec time course of adaptation, Wehmeier
and colleagues (1989) introduced a purely time-dependent shunting conductance gadapt
(with a reversal potential equal to the resting potential, here assumed zero). Each spike
increases this conductance by a fixed amount G-mc; between spikes, gadapt decreases
exponentially with a time constant Tadapt • Such an effective calcium-dependent potassium
conductance imitates both the absolute and the relative refractory period following
spike initiation. We will refer to such a unit as an adapting integrate-and-fire model
(Fig. 14.2C). Note that a refractory period tref is still necessary in order to mimic the
very short-term aspect of adaptation. In the subthreshold domain, this unit is described by
(14.12)
(14.13)
If V reaches V^ at time t', a spike is generated at this point in time and gadapt(O
is incremented by Ginc- This model is completely characterized by six parameters:
VJh, C, R, tK{, Gjnc and Tadapt.
2. An alternative to this output-dependent membrane conductance is to increase the voltage
threshold following each spike in a deterministic manner, for instance, using the rule
(14.14)
where t — t' is the time from the last impulse, Vih.o the threshold in the absence of any
adaptation, and a the maximal normalized voltage threshold (Calvin and Stevens, 1968;
Holden, 1976).
3. In order to account for those neurons that do not show any profound afterhyperpolar-
ization following spiking, the membrane potential can be reset to a value closer to Vih
(e.g., 20% of VJh) instead of zero. This is equivalent to resetting the potential to zero but
adding a constant current and can have a considerable effect on the jitter in pulse timing
(Troyer and Miller, 1997).
3. This can be seen upon developing the In term in Eq. 14.11 into a Taylor series, with £«(!+ x) <*> x — Jc2/2for|jt| <g 1.


===== Page 11 =====
340 
. 
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
4. In a strategy to imitate the seemingly random nature of spike times, some authors resort to
drawing the voltage threshold from some probability density distribution (Holden, 1976;
Gestri, Masterbroek, and Zaagman, 1980). Yet in real neurons, the spiking mechanism
itself appears to be quite reliable (Calvin and Stevens, 1968; Mainen and Sejnowski,
1995). In the case of the perfect integrator, a random threshold and a constant input can
be shown to be equivalent to a random input and a constant threshold. Frequently, the
former situation is both mathematically and computationally easier to deal with than the
latter (for more details, see Gabbiani and Koch, 1998).
What all of these models share is a mechanism for passive integration of synaptic inputs,
a voltage threshold for spike initiation, and a lack of specific spiking currents.
In Chap. 17 we will learn that an action potential in a full-blown model of a pyramidal cell
(with eight voltage-dependent conductances) is, indeed, generated whenever the somatic
membrane potential exceeds —49 mV. This is because the synaptic current flowing into the
soma—caused by rapid EPSPs on the time scale of milliseconds—primarily charges up the
capacitance. Relative to this rapid charging current, the ionic currents in the subthreshold
regime change on a much slower time scale.
How well does this /-/ curve compare against curves obtained from much more detailed
and sophisticated models? Presaging Sec. 17.5, we plot the /-/ 
curve of the layer 5
pyramidal cell, including the effect of firing rate adaptation, in Fig. 14.3C. Notice the
very low slope of the /-/ 
curve around threshold, in contrast to the infinite slope of the
/-/ 
curve of the leaky integrate-and-fire unit. If an adapting conductance is incorporated
into the leaky integrator (Eqs. 14.12 and 14.13 and Fig. 14.2C), it is surprising how well
this single-cell model (with just six degrees of freedom) resembles the much more detailed
compartmental model based on membrane conductances.
Due to the presence of the leak term, integrate-and-fire models have been difficult to fully
characterize analytically but have also been surprisingly successful in describing neuronal
excitability. They have been applied to model the firing behavior of numerous cell types:
neurons in the limulus eye (Knight, 1972b), a motoneurons (Calvin and Stevens, 1968),
neurons in the visual system of the housefly (Gestri, Masterbroek and Zaagman, 1980),
cortical cells (Softky and Koch, 1993; Troyer and Miller, 1997), and others.
While singing the praise of integrator models, it must be pointed out that many cells
do not behave like integrate-and-fire units. For instance, cerebellar Purkinje cells (Jaeger,
DeSchutter, and Bower, 1997) or the many types of oscillating neurons that constitute the
central pattern generators found throughout the animal kingdom (Marder and Calabrese,
1996) have such strong inherent nonlinearities, generated by powerful intrinsic currents, that
any attempts to directly map their behavior onto this class of models would fail miserably.
Approximations are possible, though. For instance, bursting (Chap. 16) could be treated by
letting the rapid Na+ spikes be handled by the integrate-and-fire threshold mechanism. The
slow dynamics governing at what instant the burst is triggered are generated by incorporating
voltage-dependent conductances into the unit.
14.2.4 Response Time of Integrate-and-Fire Units
When a spiking, nonadapting membrane (such as the squid axon) receives a sustained
suprathreshold current input, its membrane potential never reaches an equilibrium but moves
along a limit cycle. That is, it undergoes periodic changes in its state variables (Chap. 7).
When the integrate-and-fire neuron spikes, and the state variable is reset, it loses memory of
the previous input current and begins to respond to the new current by charging toward the


===== Page 12 =====
14.3 Firing Rate Models
341
threshold. It follows from these considerations that if there is a step change in current, the
integrate-and-fire neuron must converge to its limit cycle by the end of the first interspike
interval after the change, because everything during this first interval is exactly the same as
during subsequent intervals.
Figure 14.4 compares the step response of an integrate-and-fire unit, a compartmental
model of a cortical pyramidal neuron, an experimental record derived from a neuron in cat
visual cortex, and a mean-rate neuron. The first interspike interval already reflects the new
firing rate—the convergence occurs on as short a time interval as can be defined (that is, the
interspike interval). The inclusion of adaptation currents (Fig. 14.3B) does not substantially
affect this analysis. Cortical cells (Fig. 14.4C) also reach their maximum firing rate by the
first interspike interval. Thereafter, the firing rate decreases slowly due to the temporal
dynamics of adaptation.
Just because the subthreshold dynamics are governed by r does not imply that the
neuron must respond to a suprathreshold input with the same dynamics. Returning to
expression 14.10 for the time to spike Tth, we notice that the larger the injected current,
the sooner the cell spikes (Fig. 14.5A). Furthermore, Tit, actually decreases as the input
resistance, and therefore T increases (Fig. 14.5B). This can easily be explained by recalling
that 7jh is the time it takes for the membrane potential to reach the fixed threshold Vth-
Increasing the input resistance will shorten this time, even if overall it would have taken
the membrane longer to reach its ultimate steady-state value RI (which is never reached
since a spike is triggered and the membrane potential reset once V hits Vth).
14.3 Firing Rate Models
The potential in a continuous firing rate unit, such as those at the heart of most neural
networks, has the same dynamics as that in the leaky integrate-and-fire unit,
(14.15)
A subtle but far-reaching difference is that the instantaneous output of this unit f ( t ) is a
continuous but nonlinear function of V(t):
Fig. 14.4 SPIKING CELLS Sample spike rasters in
response to a step current injection. (A) Leaky integrate-
and-fire unit with refractory period spiking in response
to a current step of 1.6 nA (for parameters, see legend to
Fig. 14.3A). The arrows indicate the time at which the
current injection commenced. (B) Somatic membrane
potential in the layer 5 pyramidal cell model in response
to a 1.5-nA current input. (C) Response of a cell in the
primary visual cortex of the anesthetized adult cat to a
0.6-nA current injection (from Ahmed et al., 1993). The
firing rate does not increase gradually; the effect of the
change in current is fully visible in the first interspike
interval. (D) Output of a nonadapting firing rate model
with T = 20 msec. In the linear regime of the cell's
/-/ 
curve, the firing rate can be considered to be a
low-pass-filtered version of the step input.


===== Page 13 =====
342
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Fig. 14.5 INTEGRATE-AND-FIRE UNITS CAN RESPOND MUCH FASTER THAN T Membrane time
constant r = RC of a leaky integrate-and-fire unit and the time T^ it takes such a unit, starting at
V = 0, to reach Vt|, and spike (Eq. 14.10). (A) As the amplitude of the injected current increases, the
unit can spike very rapidly. The true time to spike is < 7*th, since the unit's initial state is usually V > 0.
The parameters are as in Fig. 14.3A. (B) Input resistance R is varied over two orders of magnitude.
As r diverges, TH, converges to C Va,//, the time it takes for the voltage across a capacitance to reach
Vth- While this may be counterintuitive, it follows from the fact that Tit, is the time it takes to reach
a fixed threshold value, while T dictates the approach toward a steady-state value beyond Vih- The
injected current / = 4.3 nA. All else as in the upper panel. For small values of R, the current fails to
reach /a, and 7"th diverges.
(14.16)
Most commonly g is a sigmoidal function, that is, a monotonically increasing, positive, and
saturating function. A popular choice is
(14.17)
where ft is some positive constant (Fig. 14.6). Other functions have also been used (e.g.,
tanh V or V2). V is sometimes identified with the so-called generator potential, the
somatic membrane potential when the spikes are disabled (e.g., by blocking the fast sodium
conductance). Under certain conditions, the generator potential correlates well with the
firing rate when spikes are blocked (Katz, 1950; Granit, Kernell, and Shortess, 1963; Stein,
1967a; Dodge, Knight, and Toyoda, 1968). If / is thought of as the firing rate of the unit,


===== Page 14 =====
14.3 Firing Rate Models
343
then the function g can be identified with the /-/ 
curve of the cell under investigation and
can be directly fitted against experimental data. Given the continuous and smooth nature of
g, no true current threshold exists.
A circuit implementation of a firing rate neuron is shown in Fig. 14.6. Here the ideal
operational amplifier is assumed to draw no input current (that is, it has an infinite input
impedance), converting the difference between V and ground into the potential / = g(V).
In this class of models the firing rate changes smoothly in response to a rapid change in
I. For small steps in input current g(V) is approximately linear. In the linear regime, the
firing rate is given by convolving the input current with a first-order low-pass filter with
time constant r = RC, and is therefore a smoothed version of the input.
14.3.1 Comparing the Dynamics of a Spiking Cell with a Firing Rate Cell
As we mentioned above, the voltage in an integrate-and-fire unit in response to a sustained
suprathreshold input moves along a limit cycle. This is not true for the potential in a firing
rate model. V reaches its equilibrium value in a time dictated by T ; the firing rate follows
V without further delay.
If T is increased in either model by increasing the neuron input resistance /?, the rate of
change of the subthreshold voltage also increases. This allows the spiking neuron to reach
threshold more quickly (Fig. 14.7). In contrast, it will take the firing rate unit longer to reach
its steady-state voltage because the equilibrium voltage is also increased, implying that the
dynamics in the firing rate model slow down as T increases (Fig. 14.7). In the extreme case,
when T -» oo and the leaky integrate-and-fire model converges to a perfect integrator
model, the firing rate model does not even asymptotically approach an equilibrium, while
the spiking unit settles into its steady-state limit cycle faster than it does for a finite T.
Similarly, decreasing T will not make a spiking neuron respond faster.
A spiking mechanism can therefore speed up a neuron's response to a step change in the
input. In a population of spiking cells with uniformly distributed initial conditions, there
will be cells whose T& will be arbitrarily close to zero. Knight (1972a) made this point,
proving rigorously that an infinite population of either perfect or leaky integrate-and-fire
units uniformly distributed in phase will respond instantaneously to any suprathreshold
stimulus. This is true for increases or decreases in input. In theory, how fast a neuron that
receives input from such a population could detect a change in its inputs is limited only
Fig. 14.6 CONTINUOUS FIRING RATE SINGLE-CELL
MODEL 
As in the integrate-and-fire models (Fig.
14.2), the input current in a firing rate neuron charges up
an R C element. The instantaneous firing rate is obtained
by passing the membrane potential V through a smooth,
stationary nonlinear!ty g. In the circuit idiom used here,
this nonlinearity is implemented by an ideal operational
amplifier. The associated adapted /-/ curve, specified
by Eq. 14.17, g(V) = g(RI), is a continuous, positive,
and saturating function with no threshold.


===== Page 15 =====
344
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Fig. 14.7 RESPONSE TIME IN SPIKING
AND FIRING RATE MODELS Effect of
changing the membrane time constant on
(A) a nonspiking (or firing rate) neuron and
(B) an integrate-and-fire neuron. The input
current to the cell changes from zero to
0.85 nA at time 0. Subthreshold parameters
were the same in both panels (C = 1 nF; R
was varied from 20 M£2 to infinity; no re-
fractory period). The subthreshold voltage
of the integrate-and-fire model in B is ex-
actly the same as for the nonspiking model
in A until the threshold Vth (dashed line in
A) is crossed. Increasing R increases the
rate of change of voltage, but also increases
the equilibrium voltage. Nonspiking neu-
rons therefore converge more slowly as the
time constant increases. As r -> oo (when
the leaky integrate-and-fire unit turns into
a perfect one) the integrate-and-fire model
responds earlier.
by the number of statistically independent inputs (Panzeri et al., 1996). This is unlike the
response of one or even a population of firing-rate neurons. Because of the low-pass filtering
stage, their firing rate cannot change instantaneously.
It has been argued that the temporal dynamics of neurons embedded within a network are
primarily determined by the time course of the synaptic currents. This has as the consequence
that the subthreshold RC time constant should more properly be replaced by one or more
synaptic time constants (e.g., Amit and Tsodyks, 1991; Amit and Brunei, 1993; Burkitt,
1994; Suarez, Koch, and Douglas, 1995; Brunei, 1996). A very simple, yet physiologically
correct way to express the firing rate is
(14.18)
where hss is the steady-state /-/ discharge curve of the cell, and / (t) is the total current
flowing into the soma. This equation dispenses with the subthreshold voltage since it only
plays a very limited role for suprathreshold currents. The subthreshold domain will come
into play for inputs hovering just around threshold (e.g., the lower trace in Fig. 14.7B).
In order to turn this into a complete single-cell model, two additional ingredients are
needed (Abbott, 1994).
1. How to obtain the current / (t) 1 The simplest manner is to use the standard neural network
"linear sum over all inputs" formulation (see Eq. 14.20). We can be more accurate and
incorporate the nonlinear effects occurring in the dendritic tree (synaptic interaction,


===== Page 16 =====
14.4 Neural Networks 
• 
345
synaptic saturation, and so on). This is discussed at length in Sec. 18.4. Almost any
degree of biophysical complexity can be accounted for. What cannot readily be included
are back-propagating action potentials and the like (Chap. 19) since we assume that the
coupling between dendrites and the spike-initiation zone is one-way only.
2. What is the relationship between the instantaneous firing rate f(t) and the steady-state
rate? The net effects of synaptic time constants and adaptation currents can all be lumped
into a single, low-pass temporal filter, which from a phenomenological point of view,
can be described by a first-order differential equation,
(14.19)
where Teg is the above-mentioned effective time constant (in the 20-30 msec range, in
particular if one wants to account for both NMDA and adaptation currents), not related
to the passive membrane time constant rm. This constitutes an alternative procedure to
Eqs. 14.15 and 14.16 to define a firing rate model (Abbott, 1994). It gives rise to the
same phenomenological equation but with a different physiological interpretation.
We conclude that because the discharge rate is often quite linear over the relevant range
of firing rates (Granit, Kernell, and Shortess, 1963; Ahmed et al., 1993), a linear threshold
unit with no threshold may often be a satisfactory approximation for the firing rate of a real
neuron.
14.4 Neural Networks
Neural networks consist of a large number of neurons connected using a scalar synaptic
weight Wij. Almost always, the single-cell model used is a mean rate one, from the earliest
publications of Steinbuch (1961) to Wilson and Cowan (1972), Hopfield (1984), and more
recent work (summarized in Arbib, 1995).
14.4.1 Linear Synaptic Interactions Are
Common to Almost All Neural Networks
The evolution of the network, often termed neurodynamics, is governed by a coupled system
of single-cell equations. For any one cell i (out of n such cells) it takes the form
(14.20)
where _/} is the firing rate of the jth neuron and /, corresponds to the external current
injected into the cell. /) is related to V; via Eq. 14.16, that is, via a stationary nonlinearity
(14.21)
In our usual circuit idiom (Fig. 14.8) in which both /) and Vj are voltages, the synaptic
coupling between the two neurons, that is, to/,-, has the dimension of a conductance.
Negative, hyperpolarizing synapses are implemented by inverting the output of the amplifier.
The evolution of the circuit in Fig. 14.8 can be expressed by an equation of the form shown
in Eq. 14.20.


===== Page 17 =====
346
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Fig. 14.8 Two INTERACTING FIRING CELLS IN A
NEURAL NETWORK 
Circuit model of two inter-
acting mean rate neurons. Such continuous output
units constitute the standard working horse of neural
networks. Common to all is that the coupling among
neurons is characterized by a scalar Wjj that can take
on any real value, depending on whether the synapse
is inhibitory or excitatory. The interaction among
synaptic inputs is strictly linear. Local learning rules
of the type discussed in Sec. 13.5 are used for deter-
mining the amplitude of the u),-/s. A qualitatively
very similar model of linear synaptic interactions
has been used in the neural network community for
studying the computational power of networks of
spiking units.
Different from the biophysics of synaptic conductance inputs (Sec. 4.5), a change Sfj
in the firing activity of the j'th presynaptic neuron leads to a change uijjSfj 
in the current
delivered to the operational amplifier. In effect, the synaptic inputs act as current sources,
and no nonlinear interaction among synaptic inputs exists.
Firing rate models incorporating a low-pass filter to capture the passive properties of the
underlying membrane have been applied widely in abstract neural network analysis (Wilson
and Cowan, 1972; Cohen and Grossberg, 1983; Hopfield, 1984; Arbib, 1995; an excellent
textbook covering this area in a relatively intuitive manner is the one by Hertz, Krogh, and
Palmer, 1991) and in the analysis of the dynamics of real neurobiological networks (Knight,
Toyoda, and Dodge, 1970; Abbott, 1991; Traub and Miles, 1991; Amit andTsodyks, 1991;
Carandini and Heeger, 1994).
The study of the evolution of networks of spiking neurons—usually either of the
integrate-and-fire or of the Hodgkin-Huxley variety—has only began of late because the
discontinuous nature of their output and the attendant mathematical difficulties (Hansel
and Mato, 1993; van Vreeswijk and Abbott, 1993; Usher et al., 1994; van Vreeswisk and
Sompolinsky, 1996; Maass, 1996). Given the revival of interest in the role of spike timing in
computation, more progress is likely to be just around the corner. The equations of motion
of such a network remain governed by linear synaptic interactions that usually take the form
(14.22)
with an auxiliary equation for the voltage to deal with the refractory period (or adaptation)
following spike generation (Usher et al., 1994). The second sum on the right-hand side
includes all times ^ at which the j\h neuron generated a spike. Each time this happens
all of the postsynaptic targets of j receive a current pulse of amplitude Wij. (Propagation
delays can easily be incorporated into this notation.)
14.4.2 Multiplicative Interactions and Neural Networks
The bottom line of the previous section is that the vast majority of neural networks have
been built on the assumption of linearity of synaptic interactions. The nonlinearity that is
necessary for any true information processing to occur resides solely in the firing mechanism
at the output of the cell (Vth for spiking models and g for rate neurons).


===== Page 18 =====
14.4 Neural Networks · 347
In view of the many different types of nonlinear interactions that can occur in the
dendritic tree, including AND-NOT interactions (Sec. 5.1), NMDA synapses (Sec. 5.2), and
the voltage-dependent calcium and sodium membrane conductances found in the dendritic
tree as discussed as length in Chap. 19, it appears wise to cast around for some canonical
single-cell model that captures some of these nonlinearities yet is simple enough to be still
amenable to analysis.
From a computational point of view, the simplest nonlinearity is multiplication, as in
(14.23)
(with α φ 0). A substantial body of evidence supports the presence of multiplicative-like
operations in the nervous system (Koch and Poggio, 1992). Physiological and behavioral
data strongly suggest that the optomotor response of insects to moving stimuli is medi-
ated by a correlation-like operation. Psychophysical work in humans from a number of
independent groups strongly supports models of the correlation type, albeit with spatio-
temporal filters different from those in insects. Simple cells recorded in the primary
visual cortex of the cat (Emerson, Bergen, and Adelson, 1992) appear to encode some
of the stages in the most popular of these algorithms, the spatio-temporal energy model
of motion perception (Adelson and Bergen, 1985). From a mathematical point of view,
all these motion algorithms can be implemented by the multiplication of two variables
(Buchner, 1984; Hildreth and Koch, 1987; Poggio, Yang, and Torre 1989; Koch and
Poggio, 1992).
Another instance of a multiplication-like operation in the nervous system is the mod-
ulation of the receptive field location of neurons in the posterior parietal cortex by the
eye and head positions of the monkey (Andersen, Essick, and Siegel, 1985; Zipser and
Andersen, 1988; Van Opstal and Hepp, 1995; Brotchie et al., 1995). This operation serves
to transform the image from a retinal coordinate system into one that takes eye and body
positions into account. A final example is the output of an identified neuron—the descending
contralateral movement detector—in the visual pathway of the locust that signals rapidly
approaching objects. Its firing rate can be accurately described as the product of the angular
image velocity and a term that depends exponentially on the angular size of the approaching
object on the animal's retina (Hatsopoulos, Gabbiani, and Laurent, 1995).
The generalization of multiplicative algorithms are polynomial ones. The output of a
polynomial cell consists of the sum of contributions from a set of products,
(14.24)
where χ = (x\, x%,...) represents the synaptic input and the scalar T(x) the output of the
unit. Single-cell models that implement such a function are known as sigma-pi or higher
order
4 units (Feldman and Ballard, 1982; Volper and Hampson, 1987; Mel, 1992). If the
highest order product in Eq. 14.24 is quadratic, that is, only terms such as x¡Xj and xf are
represented and d\ and all higher coefficients are all identical to zero, the neuron is known
as a second-order or multiplicative unit.
The evolution of the membrane potential is identical to the one used in the linear threshold
ones. For a second-order multiplicative unit it is
(14.25)
4. Order refers to the maximal number of variables multiplied in each term (3 in the case of Eq. 14.24).


===== Page 19 =====
348 
· 
SIMPLIFIED MODELS OF INDIVIDUAL NEURONS
Associated with this single-cell model are second-order "synapses" w'¡jk. Such a synapse
only contributes to the postsynaptic potential if both inputs j and k are simultaneously
active. In principle, up to n2 synapses exist per neuron. For real neurons with dendritic
trees, the connectivity matrices w¡j and w'¡jk are not independent of each other but are
functions of the cable properties and of the specific synaptic architecture used. In general,
they are also functions of time.
As an illustrative example, let us reinterpret the interaction occurring between an
excitatory and a silent inhibitory synapse in a passive dendritic tree of a direction-selective
cell—assuming that the conductance inputs ge and g, are small—in terms of a multiplicative
unit (Sec. 5.1). We assume that the excitatory postsynaptic conductance change ge is
proportional to the presynaptic firing frequency fe of the neuron e and that the amplitude
of the silent inhibition g, is proportional to the firing frequency /; of an inhibitory unit
synapsing onto the cell. With E¡ = 0, we can reexpress the steady-state potential of the
direction-selective cell j in Eq. 5.13 as
(14.26)
The synaptic strengths are specified in terms of the input and transfer resistances and batter-
ies as Wje — EeKes, Wjee = EeKesKee and Wjei — EeKisKie. Note the unconventional
nature of the dimensions of / and w. Equation 14.26 directly illustrates the connection
between the nonlinear interactions among synapses and multiplicative neural network units.
Mel (1992, 1993, 1994) argues persuasively that NMDA synaptic input in combination
with voltage-dependent sodium and calcium conductances distributed throughout the den-
dritic tree implements something akin to Eq. 14.24 (Sec. 5.2). Synaptic input distributed in
spatial clusters throughout the dendritic tree approximates a polynomial, in the sense that
simultaneous excitation of m neighboring synapses (where m can be large) causes a larger
somatic response than activation of a similar number of synapses distributed in a diffuse
fashion throughout the tree (Figs. 5.7-5.9). Different from Eq. 14.26, this operation is both
more robust and less specific, since the absence of any one particular input will have little
effect on the overall output (as indicated by the broad peak in Fig. 5.8B).
It is easy to see the power of polynomial units in the case of binary Boolean functions and
circuits. For instance, a single such unit can implement an exclusive-or function, something
that a single-layer neural network of linear threshold units is unable to do. (For a rigorous
investigation of this, see Brack, 1990; Brack and Smolensky, 1992; for background material
see Koch and Poggio, 1992). Of course, neurons always have the sigmoidal /-/ nonlinearity
to fall back upon (g in Eq. 14.16 or hss in Eq. 14.18).
14.5 Recapitulation
We started off this chapter by defining the instantaneous firing frequency f ( t ) . It is a fictive
variable that can be obtained by averaging the spiking response of a single neuron to multiple
presentations of the same stimulus. Due to the stochastic nature of the neurona! response
the exact microstructures of spike trains are rarely reproducible from trial to trial. This is
why the temporal average (/(i)} of the firing rate is the most common variable measured
during neurophysiological experiments. In a handful of experiments {/), evaluated over
fractions of a second or longer, has been directly related to the behavior of the animal.
There is no question that firing rate codes that preserve temporal information at the 5-
10 msec level are used in the nervous system. To what extent more complex correlation


===== Page 20 =====
14.5 Recapitulation · 349
codes—exploiting information encoded among η-tuples of spikes from one neuron or spik-
ing information across multiple neurons—exist remains a subject of considerable debate.
What is the simplest model of a single neuron that captures some of its key operations?
Two families of models are in common use today: integrate-and-fire and continuous firing
rate models. While the former retains the timing information of individual action potentials,
the latter assumes that it is only the average or mean firing rate of a neuron that matters to
its postsynaptic targets. Both neglect the dendritic tree and both eliminate the complex time
course of the sodium and potassium membrane conductances underlying spiking.
The key insight behind the various guises of the integrate-and-fire model is that from
a phenomenological point of view the neuron possesses two domains of operation, a
subthreshold and a suprathreshold one. In the subthreshold domain, synaptic inputs are
integrated and decay away; their temporal evolution is governed by the time constant τ.
Once the voltage threshold is reached, a pulse is generated and the membrane potential is
reset. Different versions of integrate-and-fire models, incorporating various mechanisms
to account for adaptation, can be well fitted to the discharge curves of cortical and other
cells. It will be argued in Sec. 17.3 that firing in response to fast synaptic input in complex,
conductance-based single-cell models is, indeed, initiated whenever a voltage threshold is
exceeded.
In response to a suprathreshold stimulus, these units, in accordance with their biological
counterparts, can spike in a time T^ <K τ. A network of integrate-and-fire units can respond
almost instantaneously to a stimulus. The take home lesson is that the dynamics of the
subthreshold domain do not carry over into the suprathreshold domain.
In firing rate neurons the continuous output variable is an instantaneous function of the
voltage. Since the evolution of the voltage is dictated by a time constant, the firing rate
will always be low-pass filtered with respect to the input current, distinct from the response
of real neurons, and different from integrate-and-fire units. If one would like to retain the
continuous nature of the firing rate model, a more physiologically correct way to achieve
this would be to make the steady-state firing rate a function of the total current (synaptic,
dendritic, or otherwise) at the cell body. The output of such a neuron can be interpreted as
the firing rate associated with a population of spiking cells.
At the heart of the vast majority of neural networks lies the assumption that synaptic
inputs interact in a linear manner. The nonlinearity that is necessary for computation is
relegated to the firing mechanism at the output. A biophysically more faithful and more
complex model that incorporates multiplicative interactions among synaptic inputs is the
polynomial or sigma-pi unit.
Multiplication is a key operation underlying many neuronal operations. Chapter 5 treated
the evidence in favor of the view that a dendritic tree endowed with NMDA synapses and
voltage-dependent membrane conductances (see Chap. 19 as well) can implement a robust
version of such a polynomial unit. The nonlinear operations underlying the polynomial
interactions do not depend on the threshold occurring at the cell body but precede it. The
computational power of such neurons is considerably beyond that of their feeble-minded
linear threshold counterparts.


