===== Page 1 =====
1372    Part VIII / Learning, Memory, and Cognition
Figure 55–1  Formant frequencies. For-
mants are systematic variations in the 
concentration of energy at various sound 
frequencies and represent resonances of 
the vocal tract. They are shown here as a 
function of time in a spectrographic analy-
sis of speech. The formant patterns for 
two simple vowels (/a/ and /ae/) spoken 
in isolation are distinguished by differ-
ences in formant 2 (F2). Formant patterns 
for the sentence “Did you hit it to Tom?” 
spoken slowly and clearly illustrate 
the rapid changes that underlie normal 
speech. (Data from Patricia Kuhl.)
to distinguish semantically different sounds and thus 
understand speech. Whereas in written language, 
spaces are customarily inserted between words, in 
speech, there are no acoustic breaks between words. 
Thus, speech requires a process that can detect words 
on the basis of something other than sounds bracketed 
by silence. Computers have a great deal of trouble rec-
ognizing words in the normal flow of speech.
Phonotactic rules specify how phonemes can be 
combined to form words. Both English and Polish use 
the phonemes /z/ and /b/, for example, but the com-
bination /zb/ is not allowed in English, whereas in 
Polish, it is common (as in the name Zbigniew).
Morphemes are the smallest structural units of a 
language, best illustrated by prefixes and suffixes. In 
English, for example, the prefix un (meaning not) can 
be added to many adjectives to convey the opposite 
meaning (eg, unimportant). Suffixes often signal the 
tense or number of a word. For example, in English, we 
add s or es to indicate more than one of something (pot 
becomes pots, bug becomes bugs, or box becomes boxes). 
To indicate the tense of a regular verb, we add an end-
ing to the word (eg, play can become plays, playing, and 
played). Irregular verbs do not follow the rule (eg, go 
becomes went rather than goed and break becomes broke 
rather than breaked). Every language has a different set 
of rules for altering the tense and number of a word.
Finally, to create language, words have to be 
strung together. Syntax specifies word and phrase 
order for a given language. In English, for example, 
sentences typically conform to a subject-verb-object 
order (eg, He eats cake), whereas in Japanese, it is typi-
cally subject-object-verb (eg, Karewa keeki o tabenzasu, 
literally He cake eats). Languages have systematic dif-
ferences in the order of larger elements (noun phrases 
and verb phrases) of a sentence, and in the order of 
words within phrases, as illustrated by the difference 
between English and French noun phrases. In English, 
adjectives precede the noun (eg, a very intelligent man), 
whereas in French, most follow the noun (eg, un homme 
tres intelligent).
Language Acquisition in Children  
Follows a Universal Pattern
Regardless of culture, all children initially exhibit uni-
versal patterns of speech perception and production 
that do not depend on the specific language children 
hear (Figure 55–2). By the end of the first year, infants 
have learned through exposure to a specific language 
which phonetic units convey meaning in that language 
and to recognize likely words, even though they do 
not yet understand those words. By 12 months of age, 
 Frequency (kHz)
 Frequency (kHz)
0
1
2
3
4
5
0
1
2
3
4
5
6
7
/a/ in “hot”
/ae/ in “hat”
d i
d
y
ou
h
i t
i
t
t
o
t
o
m
 Time (s)
0
1
2
 Time (s)
 Time (s)
0
0.2
0.4
0
0.2
0.4
F3
F3
F2
F1
F2
F1


===== Page 2 =====
Chapter 55 / Language    1373
infants understand approximately 50 words and have 
begun to produce speech that resembles the native lan-
guage. By the age of 3 years, children know approxi-
mately 1,000 words (by adulthood 70,000), create long 
adult-like sentences, and can carry on a conversation. 
Between 36 and 48 months, children respond to the 
differences between grammatical and ungrammatical 
sentences in an adult-like way, although tests using 
the most complex sentences indicate that the intrica-
cies of grammar are not mastered until late childhood, 
between 7 and 10 years of age.
In the last half of the 20th century, debate on the 
nature and acquisition of language was ignited by a 
highly publicized exchange between a strong learning 
theorist and a strong nativist. In 1957, the behavioral 
psychologist B. F. Skinner proposed that language was 
acquired through learning. In his book Verbal Behavior, 
Skinner argued that language, like all animal behavior, 
was a learned behavior that developed in children as a 
function of external reinforcement and careful parental 
shaping. By Skinner’s account, infants learn language 
as a rat learns to press a bar—through monitoring and 
management of reward contingencies. The nativist 
Noam Chomsky, writing a review of Verbal Behavior, 
took a very different position. Chomsky argued that 
traditional reinforcement learning has little to do with 
the ability of humans to acquire language. Instead, he 
proposed that every individual has an innate “language 
faculty” that includes a universal grammar and a uni-
versal phonetics; exposure to a specific language trig-
gers a “selection” process for one language.
More recent studies of language acquisition in 
infants and children have clearly demonstrated that 
the kind of learning going on in infancy does not 
resemble that described by Skinner with its reliance on 
external shaping and reinforcement. At the same time, 
a nativist account such as Chomsky’s, in which the lan-
guage the infant hears triggers selection of one of sev-
eral innate options, also does not capture the process.
The “Universalist” Infant Becomes Linguistically 
Specialized by Age 1
In the early 1970s, psychologist Peter Eimas showed 
that infants were especially good at hearing the acous-
tic changes that distinguish phonetic units in the 
world’s languages. When speech sounds were acousti-
cally varied in small equal steps to form a series rang-
ing from one phonetic unit to another, say from /ba/ 
to /pa/, Eimas showed that infants could discern very 
slight acoustic changes at the locations in the series 
(the “boundary”) where adults heard an abrupt change 
between the two phonetic categories, a phenomenon 
called categorical perception. Eimas demonstrated that 
infants could detect these slight acoustic changes at the 
phonetic boundary between two categories for pho-
netic units in languages they had never experienced, 
whereas adults have this ability only for phonetic units 
in languages in which they are fluent. Japanese people, 
for example, find it very difficult to hear the acoustic 
differences between the American English /r/ and /l/ 
sounds. Both are perceived as Japanese /r/, and as we 
have seen, Japanese speakers use the two sounds inter-
changeably when producing words.
Categorical perception was originally thought to 
occur only in humans, but in 1975, cognitive neurosci-
entists showed that it exists in nonhuman mammals 
such as chinchillas and monkeys. Since then, many 
studies have confirmed this result (as well as iden-
tifying species differences between mammals and 
birds). These studies suggest that the evolution of 
phonetic units was strongly influenced by preexist-
ing auditory structures and capacities. Infants’ abil-
ity to hear all possible differences in speech prepares 
them to learn any language; at birth, they are linguis-
tic “universalists.”
Speech production develops simultaneously with 
speech perception (Figure 55–2). All infants, regardless 
of culture, produce sounds that are universal. Infants 
“coo” with vowel-like sounds at 3 months of age and 
“babble” using consonant–vowel combinations at 
about 7 months of age. Toward the end of the first year, 
language-specific patterns of speech production begin 
to emerge in infants’ spontaneous utterances. As chil-
dren approach the age of 2 years, they begin to mimic 
the sound patterns of their native language. Chinese 
toddlers’ utterances reflect the pitch, rhythm, and 
phonetic structure of Mandarin, and the utterances 
of British toddlers sound distinctly British. Infants 
develop an ability to imitate the sounds they hear oth-
ers produce as early as 20 weeks of age. Very early in 
development, infants begin to master the subtle motor 
patterns required to produce their “mother tongue.” 
Speech-motor patterns acquired in the earliest stages 
of language learning persist throughout life and influ-
ence the sounds, tempo, and rhythm of a second lan-
guage learned later.
Right before the onset of first words, infants’ abilities 
to discriminate native and nonnative phonetic units 
show a dramatic shift. At 6 months of age, infants can 
discriminate all phonetic units used in all languages, 
but by the end of the first year, they fail to discriminate 
phonetic changes that they successfully recognized  
6 months earlier. At the same time, infants become 
significantly more adept at hearing native-language 
phonetic differences. For example, when American 


===== Page 3 =====
1374    Part VIII / Learning, Memory, and Cognition
and Japanese infants were tested between 6 and  
12 months of age on the discrimination of the American 
English /r/ and /l/, American infants improved 
significantly between 8 and 10 months, whereas 
Japanese infants declined, suggesting that this is a sen-
sitive period for phonetic learning. Moreover, infants’ 
native-language discrimination ability at 7.5 months 
of age predicts the rate at which known words, sen-
tence complexity, and mean length of utterance grow 
between 14 and 30 months.
If the second half of the first year is a sensitive 
period for speech learning, what happens when infants 
are exposed to a new language during this time? Do 
they learn? When American infants were exposed to 
Mandarin Chinese in the laboratory between 9 and 
10 months of age, the infants learned if exposure 
occurred through interaction with a human being; 
infants exposed to the exact same material through 
television or audiotape with no live human interac-
tion do not learn (Figure 55–3). When tested, the per-
formance of the group exposed to live speakers was 
statistically indistinguishable from that of infants 
raised in Taiwan who had listened to Mandarin for  
10 months. These results established that, at 9 months 
of age, the right kind of exposure to a foreign language 
permits phonetic learning, supporting the view that 
this is a sensitive period for such learning. The study 
also demonstrated, however, that social interaction 
plays a more significant role in learning than previ-
ously thought.
Further work showed that the degree to which 
infants track the eye movements of the tutor—watching 
what she is looking at as she names objects in the for-
eign language—correlates strongly with neural meas-
ures of phonetic and word learning after exposure to 
the new language, again implicating social brain areas 
in language learning.
An infant’s ability to pick up social cues is essen-
tial to language learning, but what other skills promote 
learning during this critical period? Studies suggest 
that early exposure to speech induces an implicit learn-
ing process that increases native-language discrimi-
nation and reduces the infant’s innate ability to hear 
distinctions between the phonetic units of all other lan-
guages. Infants are sensitive to the statistical proper-
ties of the language they hear. Distributional frequency 
patterns of sounds affect infants’ speech learning by  
6 months of age. Infants begin to organize speech sounds 
into categories based on phonetic prototypes, the most fre-
quently occurring phonetic units in their language.
Six-month-old infants in the United States and Swe-
den were tested with prototypical English and Swed-
ish vowels to examine whether infants discriminated 
acoustic variations in the vowels, like those that occur 
when different talkers produce them. By 6 months 
of age, the American and Swedish infants ignored 
Figure 55–2  Language development progresses through 
a standard sequence in all children. Speech perception 
and production in children in various cultures initially  
follow a language-universal pattern. By the end of the first 
year of life, language-specific patterns emerge. Speech  
perception becomes language-specific before speech  
production. (Adapted, with permission, from Doupe and 
Kuhl 1999.)
Infants discriminate phonetic
contrasts of all languages
Recognition of
language-speciﬁc vowels
Statistical learning
(transitional
probabilities)
Detection of typical
stress patterns in words
Recognition of
language-speciﬁc
sound combinations
Decline in
foreign-language
phonetic perception
Increase in
native-language
phonetic perception
Language-speciﬁc
speech production
First words
produced
Understands
60–90 words
Universal speech perception
Period of overlap
Time
(months)
Language-speciﬁc speech perception
Universal speech production
Language-speciﬁc 
speech production
Infants produce
nonspeech sounds
Infants produce
vowel-like sounds
“Canonical babbling”
Speech production
Speech perception
3
0
4
5
6
7
8
9
10
11
12        


===== Page 4 =====
Chapter 55 / Language    1375
Figure 55–3  Infants can learn the pho-
nemes of a nonnative language at  
9 months of age.  Three groups of American 
infants were exposed for the first time to a 
new language (Mandarin Chinese) in  
12 25-minute sessions between the ages 
of 9 and 10.5 months. One group interacted 
with live native speakers of Mandarin; a 
second group was exposed to the identi-
cal material through television; and a third 
group heard tape recordings only. A control 
group had similar language sessions but 
heard only English. Performance on dis-
crimination of Mandarin phonemes was 
tested in all groups after exposure (age 11 
months). (Reproduced, with permission, 
from Kuhl, Tsao, and Liu 2003.)
Left. Only infants exposed to live Manda-
rin speakers discriminated the Mandarin 
phonemes. Infants exposed through TV or 
tapes showed no learning, and their perfor-
mance was indistinguishable from that of 
control infants (who heard only English).
Right. The performance of American infants 
exposed to live Mandarin speakers was 
equivalent to that of monolingual Taiwanese 
infants of the same age who had experi-
enced Mandarin from birth.
Chance
45
Percent correct
50
55
60
65
70
American infants exposed to Chinese language 
Monolingually raised infants
Audiovisual exposure
Live exposure
Live person
Audio-only
Audio-visual
Control
(English only)
Chinese
English
Language-speciﬁc speech perception
Time
(months)
Language-speciﬁc speech production
Understands two-word
combinations
(e.g., “wash baby”)
Understands
170–230 words
Produces
50 words
2 word utterances
(18–26 months)
200 to 300 words
Future tense
Plural
1,000
words
Past tense
regular
Adult-like sentence
construction
Past tense
irregular
Understands more complex sentence
(e.g., “Look, Cookie Monster is helping Big Bird.”) 
Understands basic word order
(e.g., “mommy kiss Big Bird?”)
Language
production
Language
 comprehension
18
16
15
12
24
28
29
30
34
36


===== Page 5 =====
1376    Part VIII / Learning, Memory, and Cognition
acoustic variations around native language prototypes 
but not with nonnative prototypes. Paul Iverson has 
shown that language experience alters the acoustic fea-
tures to which speakers of different languages attend 
and distorts perception around category prototypes. 
This makes stimuli perceptually more similar to the 
prototype, which helps explain why 11-month-old Jap-
anese infants fail to discriminate English /r/ and /1/ 
after experience with Japanese.
The Visual System Is Engaged in Language 
Production and Perception
Language is ordinarily communicated through an 
auditory-vocal channel, but deaf individuals com-
municate through a visual-manual channel. Natural 
signed languages, such as American Sign Language 
(Ameslan or ASL), are those invented by the deaf and 
vary across countries. Deaf infants “babble” with their 
hands at approximately the same time in development 
as hearing infants babble orally. Other developmental 
milestones, such as first words and two-word combi-
nations, also occur on the developmental timetable of 
hearing infants.
Additional studies indicate that visual informa-
tion of another kind, the face of the talker, is not only 
very helpful for communication but also affects the 
everyday perception of speech. We all experience the 
benefits of “lip reading” at noisy parties—watching 
speakers’ mouths move helps us understand speech 
in a noisy environment. The most compelling labora-
tory demonstration that vision plays a role in every-
day speech perception is the illusion that results when 
discrepant speech information is sent to the visual and 
auditory modalities. When subjects hear the syllable 
“ba” while watching a person pronounce “ga” they 
report hearing an intermediate articulation “da.” Such 
demonstrations support the idea that speech catego-
ries are defined both auditorily and visually and that 
perception is governed by both sight and sound.
Prosodic Cues Are Learned as Early as In Utero
Long before infants recognize that things and events 
in the world have names, they memorize the global 
sound patterns typical in their language. Infants 
learn such prosodic cues as pitch, duration, and loud-
ness changes. In English, for example, a strong/weak 
pattern of stress is typical—as in the words “BAby,” 
“MOMmy,” “TAble,” and “BASEball”—whereas in 
some languages, a weak/strong pattern predominates. 
Six- and 9-month-old infants given a listening choice 
between words in English or Dutch show a listening 
preference for native-language words at the age of 
9 months (but not at 6 months).
Prosodic cues can convey both linguistic informa-
tion (differences in intonation and tone in languages 
such as Chinese) and paralinguistic information, such as 
the emotional state of the speaker. Even in utero fetuses 
learn prosodic cues by listening to their mother’s speech. 
Certain sounds are transmitted through bone conduc-
tion to the womb; these are typically intense (above 
80 dB), low-frequency sounds (particularly below 
300 Hz, but as high as 1,000 Hz with some attenuation). 
Thus, the prosodic patterns of speech, including voice 
pitch and the stress and intonation patterns characteris-
tic of a particular language and speaker, are transmitted 
to the fetus, while the sound patterns that convey pho-
netic units and words are greatly attenuated. At birth, 
infants demonstrate having learned this prosodic infor-
mation by their preference for (1) the language spoken 
by their mothers during pregnancy, (2) their mother’s 
voice over that of another female, and (3) stories with a 
distinct tempo and rhythm read out loud by the mother 
during the last 10 weeks of pregnancy.
Transitional Probabilities Help Distinguish Words 
in Continuous Speech
Seven- to 8-month-old infants learn to recognize 
words using the probability that one syllable will fol-
low another. Such transitional probabilities between 
syllables within a word are high because the sequen-
tial order remains fixed. In the word potato, for exam-
ple, the syllable “ta” always follows the syllable “po” 
(probability of 1.0). Between words, on the other hand, 
as between “hot” and “po” in the string hot potato, are 
much lower transitional probabilities.
Psychologist Jenny Saffran showed that infants treat 
phonetic units and syllables with high transitional prob-
abilities as word-like units. In one experiment, infants 
heard 2-minute strings of pseudo-words, such as tibudo, 
pabiku, golatu, and daropi, without any acoustic breaks 
between them. They were then tested for recognition 
of these pseudo-words as well as new ones formed by 
combining the last syllable of one word with the two 
initial syllables of another word (such as tudaro formed 
from golatu and daropi). Infants recognized the original 
pseudo-words but not the new combinations they had 
not been previously exposed to, indicating that they 
used transitional probabilities to identify words.
These forms of learning clearly do not involve 
Skinnerian reinforcement. Caretakers do not man-
age the contingencies and gradually shape through 
reinforcement the statistical analyses performed by 
infants. Conversely, language learning by infants also 


===== Page 6 =====
Chapter 55 / Language    1377
does not appear to reflect a process in which innately 
provided options are chosen based on language experi-
ence. Rather, infants learn language implicitly through 
detailed analysis of the patterns of statistical variation 
in the natural speech they hear and sophisticated anal-
ysis of information provided through social interaction 
(eg, eye gaze). The learning of these patterns in turn 
alters perception to favor the native language. In sum-
mary, both the statistical properties of language and 
the social cues provided during language interactions 
help infants learn. Language evolved to capitalize on 
the kinds of cues that infants are innately able to recog-
nize. This mirrors the argument that the development 
of phonetic units was significantly influenced by the 
features of mammalian hearing, ensuring that infants 
would find it easy to discriminate phonemes, the fun-
damental units of meaning in language.
There Is a Critical Period for Language Learning
Children learn language more naturally and efficiently 
than adults, a paradox given that the cognitive skills of 
adults are superior. Why should this be the case?
Many consider language acquisition to be an 
example of a skill that is learned best during a criti-
cal period in development. Eric Lenneberg proposed 
that maturational factors at puberty cause a change in 
the neural mechanisms that control language acquisi-
tion. Evidence supporting this view comes from clas-
sic studies of Chinese and Korean immigrants to the 
United States who had been immersed in English at 
ages ranging from 3 to 39 years. When asked to iden-
tify errors in sentences containing grammatical mis-
takes, an easy task for native speakers, the responses 
of second-language learners declined with the age of 
arrival in the United States. A similar trend emerges 
when one compares individuals exposed to ASL from 
birth to those exposed between 5 and 12 years of age. 
Those exposed from birth were best at identifying 
errors in ASL, those exposed at age 5 were slightly 
poorer, and those exposed after the age of 12 years 
were substantially poorer.
What restricts our ability to learn a new language 
after puberty? Developmental studies suggest that 
prior learning plays a role. Learning a native language 
produces a neural commitment to detection of the acous-
tic patterns of that language, and this commitment 
interferes with later learning of a second language. 
Early exposure to language results in neural circuitry 
that is “tuned” to detect the phonetic units and pro-
sodic patterns of that language. Neural commitment 
to native language enhances the ability to detect pat-
terns based on those already learned (eg, phonetic 
learning supports word learning) but reduces the 
ability to detect patterns that do not conform. Learn-
ing the motor patterns required to speak a language 
also results in neural commitment. The motor patterns 
learned for one language (eg, lip rounding in French) 
can interfere with those required for pronunciation of 
a second language (eg, English) and thus can hinder 
efforts to pronounce the second language without an 
accent. Early in life, two or more languages can be eas-
ily learned because interference effects are minimal 
until neural patterns are well established.
Neurobiologist Takao Hensch has been working 
on identifying the chemical switches that open and 
close neurodevelopmental critical periods in learning, 
including those in animals and humans. Hensch has 
found that the neurotransmitter γ-aminobutyric acid 
(GABA) opens the critical period by inhibiting the fir-
ing of excitatory neurons, bringing them into balance 
with the firing of inhibitory neurons so as to create 
an excitatory–inhibitory (EI) balance. Studies testing 
this hypothesis in humans are difficult to conduct, but 
investigations on the infants of mothers who altered 
the EI balance of the fetus during pregnancy by taking 
psychotropic medications (serotonin reuptake inhibi-
tors [SRIs]) for depression support the EI hypothesis. 
One of fluoxetine’s off-target effects is to increase the 
sensitivity of some GABA receptors to GABA. When 
compared to infants of depressed mothers who were 
not exposed prenatally to SRIs and control mothers 
without depression or SRIs, infants exposed prenatally 
to SRIs showed an accelerated phonetic learning pro-
cess, indicating that the well-established timing of the 
early transition in infants’ phonetic perception can be 
altered.
We do not completely lose the ability later in life 
to learn a new language, but it is far more difficult. 
Regardless of the age at which learning begins, second-
language learning is improved by a training regimen 
that mimics critical components of early learning—
long periods of listening in a social context (immer-
sion), the use of both auditory and visual information, 
and exposure to simplified and exaggerated speech 
resembling “parentese.”
The “Parentese” Speaking Style Enhances  
Language Learning
Everyone agrees that when adults talk to their chil-
dren they sound unusual. Discovered by linguists and 
anthropologists in the early 1960s as they listened to 
languages spoken around the world, “motherese” (or 
“parentese,” as fathers produce it as well) is a spe-
cial speaking style used when addressing infants and 


===== Page 7 =====
1378    Part VIII / Learning, Memory, and Cognition
young children. Parentese has a higher pitch, slower 
tempo, and exaggerated intonation contours, and is 
easily recognized. Compared to adult-directed speech, 
the pitch of the voice is increased on average by an 
octave both in males and in females. Phonetic units are 
spoken more clearly and are acoustically exaggerated, 
thus increasing the acoustic distinctiveness of phonetic 
units. Adults speaking to infants exaggerate just those 
features of speech that are critical to their native lan-
guage. For example, when talking to their infants, 
Chinese mothers exaggerate the four tones in Manda-
rin that are critical to word meaning.
When given a choice, infants prefer listening to 
infant-directed rather than adult-directed speech. 
When infants are allowed to activate recordings of 
infant-directed or adult-directed speech by turning 
their head left or right, they will turn in whatever 
direction is required to turn on infant-directed speech.
Recent research by psychologists Nairan Ramirez-
Esparza and Adrian Garcia-Sierra shows that the 
degree to which parentese is used in language spoken 
to infants at 11 and 14 months of age at home is strongly 
correlated with a child’s language development by the 
age of 24 months and remains strongly correlated at 
the age of 36 months. This relationship holds for both 
monolingual and bilingual children. However, in bilin-
gual children, early advances in the two languages dif-
fer depending on the language spoken in parentese. 
For example, Spanish-language parentese enhances a 
child’s behavioral and neural responses to Spanish, but 
not English, and vice versa. Children raised in families 
in which the amount of language exposure and the use 
of parentese are low often show deficits in language 
and literacy by the time they enter school, and these 
deficits correlate with decreased functional activation 
in brain areas related to language.
Successful Bilingual Learning Depends on the Age 
at Which the Second Language Is Learned
How does the brain handle two languages? Behavioral 
data show that if exposure to two languages begins at 
birth, children reach the milestones of language at the 
same age as their monolingual peers—they coo, bab-
ble, and produce words at the benchmark ages seen 
in monolinguals. The idea that bilingual experience 
produces “confusion” has been debunked by studies 
that measure “conceptual” vocabulary, that is, word 
knowledge regardless of the language the child uses 
to express that knowledge. Older studies measured 
words in only one of the infants’ two languages, and 
such word counts often showed decreased vocabu-
lary when compared to monolinguals. Conceptual 
vocabulary scores show that bilingual children’s 
vocabulary counts meet or exceed those of their mono-
lingual peers.
Exposure to a second language after puberty 
shows limitations in the degree to which the new lan-
guage can be learned. Whether subjects are tested on 
phonological rules, morphological endings, or syntax, 
the ability to learn a new language appears to decline 
every 2 years after the age of 7 years, indicating that 
acquisition of a second language after puberty is quite 
difficult.
Brain measures on bilingual infants reflect these 
behavioral data. Psychologist Naja Ferjan Ramirez 
used MEG to show that activation of the superior tem-
poral area in 11-month-old infants exposed to two lan-
guages (English and Spanish) from birth is the same for 
the sounds of both languages and that brain responses 
to English sounds are equivalent to those of age-
matched monolingual infants for English. Bilingual 
infants listening to speech also exhibit greater activa-
tion in the prefrontal cortex, a region mediating atten-
tion, when compared to monolingual infants; this 
finding is consistent with the fact that bilingual chil-
dren (adults as well) demonstrate superior cognitive 
skills related to attention. Arguably, listening to two 
languages requires multiple shifts in attention to acti-
vate one language over another.
If a second language is acquired later in develop-
ment, the age at which exposure occurs and the degree 
of eventual proficiency affect how the brain processes 
both languages. In “late” bilinguals (those who learned 
a second language after puberty), the second language 
and native language are processed in spatially sepa-
rated areas in the language-sensitive left frontal region. 
In “early” bilinguals (those who acquired both lan-
guages as children), the two languages are processed 
in the same left frontal area.
A New Model for the Neural Basis of  
Language Has Emerged
Numerous Specialized Cortical Regions Contribute 
to Language Processing
The classical Wernicke-Geschwind neural model of 
language was based on the works of Broca (1861), 
Wernicke (1874), Lichtheim (1885), and Geschwind 
(1970). In the Wernicke-Geschwind model, acoustic 
cues contained in spoken words were processed in 
auditory pathways and relayed to Wernicke’s area, 
where the meaning of a word was conveyed to higher 
brain structures. The arcuate fasciculus was assumed to 


===== Page 8 =====
Chapter 55 / Language    1379
Table 55–1  Differential Diagnosis of the Main Types of Aphasia
Type of 
aphasia
Speech
Comprehension
Capacity for 
repetition
Other signs
Region affected
Broca
Nonfluent,  
effortful
Largely preserved 
for single words 
and grammatically 
simple sentences
Impaired
Right hemiparesis (arm > 
leg); patient aware of defect 
and can be depressed
Left posterior 
frontal cortex 
and underlying 
structures
Wernicke
Fluent, abundant, 
well articulated, 
melodic
Impaired
Impaired
No motor signs; patient 
can be anxious, agitated, 
euphoric, or paranoid
Left posterior supe-
rior and middle 
temporal cortex
Conduction
Fluent with some 
articulatory  
defects
Intact or largely 
preserved
Impaired
Often none; patient can 
have cortical sensory loss  
or weakness in right arm
Left superior tem-
poral and supra-
marginal gyri
Global
Scant, nonfluent
Impaired
Impaired
Right hemiplegia
Massive left peri-
sylvian lesion
Transcortical 
motor
Nonfluent, 
explosive
Intact or largely 
preserved
Intact or largely 
preserved
Sometimes right-sided 
weakness
Anterior or supe-
rior to Broca’s area
Transcortical 
sensory
Fluent, scant
Impaired
Intact or largely 
preserved
No motor signs
Posterior or inferior 
to Wernicke’s area
be a unidirectional pathway that brought information 
from Wernicke’s area to Broca’s area to enable speech 
production. Both Wernicke’s and Broca’s areas inter-
acted with association areas. The Wernicke-Geschwind 
model formed the basis for a practical classification of 
the aphasias that clinical neurologists still use today 
(Table 55–1).
Advancements in basic and clinical neuroscience, 
the advent of more sophisticated functional brain 
imaging tools, advanced methods for structural brain 
imaging, and an increasing number of studies that 
combine brain and behavioral measures have resulted 
in the development of a new “dual-stream” model. In 
the dual-stream model, the processing of language is 
thought to involve large-scale networks that are com-
posed of different brain areas, each with a specialized 
function, and the white matter tracts that connect them.
This dual-stream model of language processing 
is similar to the well-established “what” and “where” 
dual-stream model of the visual system. The existence 
of two cortical streams of auditory information process-
ing was first postulated by Josef Rauschecker. Gregory 
Hickok and David Poeppel further elaborated the 
dual-stream model, and it has since been even further 
expanded upon by Angela Friederici as well as others 
studying the neurobiology of language. Figure 55–4 
shows the basic components of the dual-stream model.
Compared to the classic Wernicke-Geschwind 
model, the dual-stream model comprises a larger 
number of cortical areas that are more widely distrib-
uted in the brain and adds critical connecting bidirec-
tional pathways between specialized brain regions. 
These improvements in the model for language pro-
cessing are due to advances in structural brain imag-
ing techniques, such as DTI and diffusion-weighted 
imaging, which provide quantitative measures on 
a microscopic scale of the white matter in fascicles 
that connect various cortical areas and allow for the 
detailed delineation of neural tracts throughout the 
brain (tractography).
In the dual-stream model, initial spectrotemporal 
processing of auditory speech sounds is performed 
bilaterally in the auditory cortex. This information is 
then communicated to the posterior superior temporal 
gyrus bilaterally, where phonological-level processing 
occurs. Language processing then diverges into a dor-
sal “sensorimotor stream,” which maps sound to artic-
ulation, and a ventral “sensory-conceptual” stream, 
which maps sound to meaning.
The bidirectional dorsal stream connects auditory 
speech information with motor plans that produce 
speech. The dorsal stream passes above the lateral ven-
tricles and maps sounds onto articulatory representa-
tions, connecting regions of the inferior frontal lobe, 
premotor cortex, and insula (all involved in speech 
articulation) to the region that is classically recognized 
as Wernicke’s area. It is considered to comprise two 
pathways: Dorsal pathway 1 connects the posterior 


===== Page 9 =====
1380    Part VIII / Learning, Memory, and Cognition
Figure 55–4  Dual-stream model of language 
processing.  Temporal and spectral analyses of 
speech signals occur bilaterally in the auditory 
cortex followed by phonological analysis in the 
posterior superior temporal gyri (yellow arrow). 
Processing then diverges into two separate 
pathways: a dorsal stream that maps speech 
sounds to motor programs and a ventral stream 
that maps speech sounds to meaning. The dorsal 
pathway is strongly left hemisphere dominant 
and has segments that extend to the premotor 
cortex (dorsal pathway 1) and to the posterior 
inferior frontal cortex (dorsal pathway 2). The 
ventral pathway occurs bilaterally and extends 
to the anterior temporal lobe and the posterior 
inferior frontal cortex. (Adapted, with permission, 
from Hickok and Poeppel 2007, and Skeide and 
Friederici 2016.)
superior temporal gyrus to the premotor cortex, and 
dorsal pathway 2 connects the posterior superior tem-
poral gyrus to Broca’s area. Pathway 2 is involved in 
higher-order analysis of speech, such as discriminat-
ing subtle differences in meaning based on grammar 
and interpreting language using more complex con-
cepts. The dorsal stream is strongly left hemisphere 
dominant. The arcuate fasciculus and the superior lon-
gitudinal fasciculus are white matter fiber tracts that 
mediate communication along the dorsal stream.
The ventral stream passes below the Sylvian fissure 
and is composed of regions of the superior and mid-
dle temporal lobes as well as regions of the posterior 
inferior frontal lobe. This stream conveys information 
for auditory comprehension, which requires transfor-
mation of the auditory signal to representations in a 
mental lexicon, a “brain-based dictionary” that links 
individual word forms to their semantic meaning. This 
stream comprises the inferior fronto-occipital fascicu-
lus, the uncinate fasciculus, and the extreme fiber cap-
sule system and is largely bilaterally represented.
The cortical brain regions included in the dual-
stream model also interact with spatially distributed 
regions throughout both hemispheres of the brain that 
provide additional information crucial for language 
processing. These regions include the prefrontal cortex 
and cingulate cortices, which exert executive control and 
mediate attentional processes, respectively, as well as 
regions in the medial temporal, frontal, and parietal 
areas involved in memory retrieval.
The Neural Architecture for Language Develops 
Rapidly During Infancy
The study of language development in infancy requires 
a methodology that documents significant changes in 
behavior and links those changes to changes in brain 
function and morphology over time. Neuroimaging 
methods for the infant brain have improved substan-
tially over the past decade, allowing for a detailed 
assessment of the progression of development of 
the specialized regions and structural connections 
required by the language network. For example, devel-
opmental neuroscientists have created models of the 
average infant brain and brain atlases for the infant 
brain at 3 and 6 months of age. These models indicate 
that brain structures essential to language processing 
in adulthood, such as the inferior frontal cortex, pre-
motor cortex, and superior temporal gyrus, support 
speech processing in early infancy. Studies using DTI 
and tractography indicate that the arcuate fasciculus 
and the uncinate fasciculus connect language regions 
by 3 months of age.
The development of the neural substrates for lan-
guage in 1- to 3-day-old infants has been studied in 
depth by Daniela Perani using fMRI and DTI. Perani’s 
fMRI work reveals that listening to speech activates 
the infant superior temporal gyrus bilaterally and 
that in the left hemisphere this activation extends to 
the planum temporale, inferior frontal gyrus, and 
inferior parietal lobe. Perani’s DTI studies of the same 
newborn infants demonstrate weak intrahemispheric 
connections, but strong connections between the hemi-
spheres. Nevertheless, the ventral fiber tract connect-
ing the ventral portion of the inferior frontal gyrus via 
the extreme fiber capsule system to the temporal cortex 
is evident in newborns and in both hemispheres. The 
dorsal pathway connecting the temporal cortex to 
the premotor cortex is also present in the newborns, 
although the dorsal tract that connects the temporal 
cortex to Broca’s area in adults is not detectable in new-
borns. These early connections between sensory areas 
Inferior
frontal gyrus
(Broca’s area)
Anterior middle
and superior
temporal gyri
Premotor
cortex
Ventral pathway
Dorsal pathway 1
Dorsal pathway 2
Posterior middle
temporal gyrus
Auditory
cortex
Posterior superior
temporal gyrus
(Wernicke’s area)


