===== Page 1 =====
668    Part IV / Perception
were puzzled by how difficult it was to drive neurons 
in the auditory cortex of awake cats. Now we know it 
was because they were probably recording from highly 
selective neurons and using nonpreferred stimuli. The 
availability of digital technology since then has made 
it possible to create and test a large battery of acoustic 
stimuli in search of the preferred stimulus of a highly 
selective neuron in auditory cortex. The overall picture 
elucidated by experimenters is that when a sound is 
heard, the auditory cortex first responds with tran-
sient discharges (encoding the onset of a sound) across 
a relatively large population of neurons. As the time 
passes, the activation becomes restricted to a smaller 
population of neurons that are preferentially driven by 
the sound (Figure 28–9C), which results in a selective 
representation of the sound within the neuronal popu-
lation and over time. Because each neuron has its own 
preferred stimulus that differs from preferred stimuli 
of other neurons, neurons in the auditory cortex col-
lectively cover the entire acoustic space with their sus-
tained firing regions. Therefore, any particular sound 
can evoke sustained firing throughout its duration in a 
particular population of neurons in the auditory cortex. 
In other words, the region of auditory cortex activated 
by acoustic stimulation in whole-brain imaging (eg, 
functional magnetic resonance imaging [fMRI], posi-
tron emission tomography [PET]) comprises neurons 
that are preferentially driven by the acoustic stimulus.
The Auditory Cortex Maps Numerous  
Aspects of Sound
The auditory cortex includes multiple distinct func-
tional areas on the dorsal surface of the temporal lobe. 
The most prominent projection is from the ventral divi-
sion of the medial geniculate nucleus to the primary 
auditory cortex (A1, or Brodmann’s area 41). As in the 
subcortical structures, the neurons in this cytoarchitec-
tonically distinct region are arranged tonotopically. In 
monkeys, neurons tuned to low frequencies are found 
at the rostral end of A1, while those responsive to high 
frequencies are in the caudal region (Figure 28–10). 
Thus, like the visual and somatosensory cortices, A1 
contains a map reflecting the sensory periphery.
Because the cochlea encodes discrete frequen-
cies at different points along the basilar membrane, 
however, a one-dimensional frequency map from the 
periphery is spread across the two-dimensional sur-
face of the cortex, with a smooth frequency gradient 
in one direction and isofrequency contours along the 
other direction. In many species, subregions of the 
auditory cortex that represent biologically significant 
frequencies are larger than others because of extensive 
1
5
15
20
Higher-order 
auditory 
cortex 
Primary
auditory 
cortex 
Characteristic
frequency 
(kHz) 
Figure 28–10  The auditory cortex of primates has multiple 
primary and secondary areas.  The expanded figure of the 
primary auditory cortex shows its tonotopic organization.  
The primary areas are surrounded by higher-order areas (see 
Figure 28–11).
inputs, similar to the large area in the primary visual 
cortex devoted to inputs from the fovea.
In addition to frequency, other features of auditory 
stimuli are mapped in the primary auditory cortex, 
although the overall organization is less clear and pre-
cise than for vision. Auditory neurons in A1 are excited 
either by input from both ears (EE neurons), with the 
contralateral input usually stronger than the ipsilateral 
contribution, or by a unilateral input (EI). The EI neu-
rons are inhibited by stimulation of the opposite ear.
Certain neurons in A1 also seem to be organ-
ized according to bandwidth, that is, according to 
their responsiveness to a narrow or broad range of 
frequencies. Neurons near the center of the isofre-
quency contours are tuned more narrowly to band-
width or frequency than those located away from the 
center. Distinct subregions of A1 form clusters of cells 
with narrow or broadband tuning within individual 
isofrequency contours. Within intracortical circuits, 
neurons receive input primarily from neurons with 
similar bandwidths and characteristic frequencies. 


===== Page 2 =====
Chapter 28 / Auditory Processing by the Central Nervous System    669
This modular organization of bandwidth selectivity 
may allow redundant processing of incoming signals 
through neuronal filters of varying bandwidths as well 
as center frequencies, which could be useful for the 
analysis of spectrally complex sounds such as species-
specific vocalizations, including speech.
Several other parameters are represented in A1. 
These include neuronal response latency, loudness, 
modulation of loudness, and the rate and direction of 
frequency modulation. Although it remains to be seen 
how these various maps intersect, this array of param-
eters clearly endows each neuron and each location 
in A1 with the ability to represent many independent 
variables of sound and thus allows for a great diversity 
of neuronal selectivity.
As is true for visual and somatosensory areas of 
the cortex, sensory representation in A1 can change 
in response to alterations in input pathways. After 
peripheral hearing loss, tonotopic mapping in A1 can 
be altered so that neurons that were previously respon-
sive to sounds within the lost range of hearing will 
begin to respond to adjacent frequencies. The work of 
Michael Merzenich and others has shown that behav-
ioral training of adult animals can also result in large-
scale reorganization of the auditory cortex, so that the 
most behaviorally relevant frequencies—those spe-
cifically associated with attention or reinforcement—
come to be overrepresented.
The auditory areas of young animals are particu-
larly plastic. In rodents, the frequency organization 
of A1 emerges gradually during development from 
an early, crude frequency map. Raising animals in 
acoustic environments in which they are exposed to 
repeated tone pulses of a particular frequency results 
in a persistent expansion of cortical areas devoted 
to that frequency, accompanied by a general dete-
rioration and broadening of the tonotopic map. This 
result not only suggests that the development of A1 
is experience-dependent but also raises the possibil-
ity that early exposure to abnormal sound environ-
ments can create long-term disruptions of high-level 
sensory processing. A greater understanding of how 
this happens and whether it is also true for human 
fetuses and infants may provide insights into the 
origin and remediation of disorders in which central 
auditory processing is impaired, such as many forms 
of dyslexia. Moreover, the ability to induce plasticity 
in the auditory cortex of adults by engaging attention 
or reward raises new hopes for brain repair even in 
adulthood.
The primary auditory area of mammals is sur-
rounded by multiple distinct regions, some of 
which are tonotopic. Adjacent tonotopic fields have 
mirror-image tonotopy: The direction of tonotopy 
reverses at the boundary between fields. In monkeys, 
as many as 7 to 10 secondary (belt) areas surround 
the three or four primary or primary-like (core) areas 
(see Figure 28–11). The secondary areas receive input 
from the core areas of the auditory cortex and, in some 
cases, from thalamic nuclei. Electrophysiological and 
imaging studies have confirmed that A1 in humans 
lies on Heschl’s gyrus, in the temporal lobe, medial 
to the Sylvian fissure. In addition, recent fMRI stud-
ies have revealed that in humans, just as in monkeys, 
pure tones activate primarily core areas, whereas the 
neurons of belt areas prefer complex sounds such as 
narrowband noise bursts.
A Second Sound-Localization Pathway From the 
Inferior Colliculus Involves the Cerebral Cortex in 
Gaze Control
Many neurons in the auditory cortex have broad spa-
tial tuning, but neurons with narrow spatial tuning are 
also found when studied in awake animals. In mon-
keys, auditory cortex neurons are tuned to both frontal 
space and rear space (outside the coverage of vision), 
as well as the space above and below the horizontal 
plane. In contrast to the auditory midbrain, however, 
there is yet no evidence for a spatially organized map 
of sound in any of the cortical areas sensitive to sound 
location.
The sound-localization pathways in the cortex 
originate in the central nucleus of the inferior collic-
ulus and ascend through the auditory thalamus and 
the primary and secondary cortical areas, eventually 
reaching the frontal eye fields involved in gaze con-
trol. Eye or head movements can be elicited by stimu-
lating the frontal eye fields, which connect directly to 
brain stem tegmentum premotor nuclei that mediate 
gaze changes as well as to the superior colliculus. But 
why should there be this second sound-localization 
pathway connected to gaze control circuitry when the 
midbrain pathway from location-sensitive neurons in 
the inferior colliculus to the superior colliculus to gaze 
control circuitry directly controls orientation move-
ments of the head, eyes, and ears?
Behavioral experiments shed light on this ques-
tion. Although lesions of A1 can result in profound 
sound-localization deficits, no deficiency is seen when 
the task is simply to indicate the side of the sound 
source by pushing a lever. The deficit becomes appar-
ent only when the animal must approach the location 
of a brief sound source; that is, when the task is the 
more complex one of forming an image of the source, 
remembering it, and moving toward it.


===== Page 3 =====
670    Part IV / Perception
Experiments in barn owls have produced particu-
larly compelling evidence. The ability of owls to ori-
ent to sounds in space is unaffected by inactivation of 
the avian equivalent of the frontal eye fields. Similarly, 
when the midbrain sound localization pathway is dis-
rupted by pharmacological inactivation of the superior 
colliculus, the probability of an accurate head turn is 
decreased, but animals still respond correctly more 
than half of the time. In contrast, when both structures 
are inactivated, animals are completely unable to ori-
ent accurately to acoustic stimuli on the contralateral 
side. Thus, cortical and subcortical sound-localization 
pathways have parallel access to gaze control centers, 
perhaps providing some redundancy. Moreover, when 
only the frontal eye fields are inactivated, birds lose 
their ability to orient their gaze toward a target that 
has been extinguished and must be remembered, just 
as is seen with mammalian A1 lesions. Thus, in both 
mammals and birds, cortical pathways are required for 
more complex sound-localization tasks.
This appears to be a general difference between 
cortical and subcortical pathways. Subcortical cir-
cuits are important for rapid and reliable performance 
of behaviors that are critical to survival. Cortical cir-
cuitry allows for working memory, complex recogni-
tion tasks, and selection of stimuli and evaluation of 
their significance, resulting in slower but more differ-
entiated performance. Examples of this also exist in 
auditory pathways not involved in localization. Con-
ditioned fear responses to simple auditory stimuli are 
mediated by direct rapid pathways from the auditory 
thalamus to the amygdala; they can still be elicited 
after cortical inactivation. However, fear responses 
that require more complex discrimination of auditory 
stimuli require pathways through the cortex and are 
accordingly slower but more specific.
Auditory Circuits in the Cerebral Cortex Are 
Segregated Into Separate Processing Streams
In the visual system, the output from the primary vis-
ual cortex is segregated into separate dorsal and ven-
tral streams concerned respectively with object location 
in space and object identification. A similar division of 
labor is thought to exist in the somatosensory cortex, 
and recent evidence suggests that the auditory cortex 
also follows this plan.
Anatomical tracing studies of the three most acces-
sible belt areas in monkeys show that the more rostral 
and ventral areas connect primarily to the more rostral 
and ventral areas of the temporal lobe, whereas the 
more caudal area projects to the dorsal and caudal 
temporal lobe. In addition, these belt areas and their 
temporal lobe targets both project to largely different 
areas of the frontal lobes (Figure 28–11).
The frontal areas receiving anterior auditory 
projections are generally implicated in nonspatial 
functions, whereas those that are targets of posterior 
auditory areas are implicated in spatial processing. 
Electrophysiological and imaging studies provide 
support for this. Caudal and parietal areas are more 
active when a stimulus must be localized or moves, 
and ventral areas are more active during identifica-
tion of the same stimulus or analysis of its pitch. Thus 
anterior-ventral pathways may identify auditory 
objects by analyzing spectral and temporal character-
istics of sounds, whereas the more dorsal-posterior 
pathways may specialize in sound-source location, 
detection of sound-source motion, and spatial segre-
gation of sources.
Although the idea that all sensory areas of the 
cerebral cortex initially segregate object identification 
and location is attractive, it is likely an oversimplifica-
tion. It is clear that the medial-belt areas of the audi-
tory cortex project to both dorsal and ventral frontal 
cortices, and neurons with broad spatial responsive-
ness are distributed throughout caudal and anterior 
areas. Nonetheless, although the details may differ 
between systems, the basic concept holds that sensory 
systems deconstruct stimuli into features and analyze 
each type in discrete pathways.
The Cerebral Cortex Modulates Sensory Processing 
in Subcortical Auditory Areas
An intriguing feature of all mammalian cortical areas, 
and one shared by the auditory system, is the massive 
projection from the cortex back to lower areas. There 
are almost 10 times as many corticofugal fibers enter-
ing the sensory thalamus as there are axons projecting 
from the thalamus to the cortex. Projections from the 
auditory cortex also innervate the inferior colliculus, 
olivocochlear neurons, some basal ganglionic struc-
tures, and even the dorsal cochlear nucleus.
Insights into possible functions of this feedback 
have come from the bat’s auditory system. Silencing 
of frequency-specific cortical areas leads to decreased 
responses in thalamus and inferior colliculus in the 
corresponding frequency-specific areas, whereas acti-
vation of cortical projections increases and sharpens 
the responses of some neurons. The auditory cortex 
can therefore actively adjust and improve auditory 
signal processing in subcortical structures. A variety of 
evidence suggests that cortical feedback also occurs in 
other mammals. This challenges the view of ascending 


===== Page 4 =====
Chapter 28 / Auditory Processing by the Central Nervous System    671
PP
PFC
T2/T3
Where
What
Core
MGB
Cortex
PB
PB
PP
T2/T3
PFC
Belt
Belt
Core (A1 and
association
cortex)
Thalamus
Figure 28–11  The “what” and “where” 
streams in the auditory cortical system of 
primates.  The ventral “what” stream and dorsal 
“where” stream originate in different parts of 
primary and belt cortex and ultimately project to 
distinct regions of prefrontal cortex through inde-
pendent paths. (MGB, medial geniculate body of 
the thalamus; PB, parabelt cortex; PFC, prefron-
tal cortex; PP, posterior parietal cortex; T2/T3, 
areas of temporal cortex.) (Adapted, with permis-
sion, from Rauschecker and Tian 2000. Copyright 
2000 National Academy of Sciences; adapted 
from Romanski and Averbeck 2009.)
sensory pathways as purely feedforward circuits and 
suggests that we should regard the thalamus and cor-
tex as reciprocally and highly interconnected circuits 
in which the cortex exercises some top-down control 
of perception.
The Cerebral Cortex Forms Complex  
Sound Representations
The Auditory Cortex Uses Temporal and Rate Codes 
to Represent Time-Varying Sounds
An important function of the auditory system is to 
represent time-varying sounds across multiple time 
scales, from a few milliseconds to tens and hundreds 
of milliseconds or even longer. In the auditory nerve, 
firing patterns largely mirror the temporal structure of 
sounds, firing in phase with sounds to the limit of the 
phase-locking. The precision of this temporally based 
neural representation gradually decreases as informa-
tion ascends toward the auditory cortex due to synap-
tic integration at the soma and dendrites.
The upper limit of the phase-locking to periodic 
sounds progressively decreases along the ascend-
ing auditory pathway from approximately 3,000 Hz 
in the auditory nerve to less than approximately  
300 Hz in the medial geniculate body in the thala-
mus and less than 100 Hz in A1. The upper limit of 
the phase-locking in A1 is similar to that found in the 
primary visual and somatosensory areas of cortex. 
In the auditory cortex, the temporal firing pattern 
alone is inadequate to represent the entire range of 
time-varying sounds that are perceived by humans 
and animals.
Cortical neurons use an alternative method to rep-
resent time-varying sounds that change more rapidly 
than the upper limit of the phase-locking in A1. When 
an animal listens to a sequence of periodic clicks, two 
types of neural responses are observed in A1. One 
population of neurons displays phase-locked peri-
odic firing in response to click trains with long inter-
vals between clicks or slowly varying sounds, but not 
to click trains with short intervals between clicks or 
rapidly varying sounds (Figure 28–12A). The second 
population of neurons does not respond to click trains 


===== Page 5 =====
672    Part IV / Perception
Figure 28–12  Temporal and rate coding of time-varying 
sounds.
A. Stimulus-synchronized responses of a neuron to periodic 
click trains recorded from A1 of an awake marmoset. The hori-
zontal bar below the x-axis indicates the duration of the stimu-
lus. (Adapted, with permission, from Lu, Liang, and Wang 2001. 
Copyright © 2001 Springer Nature.)
B. Nonsynchronized responses of a neuron to periodic click 
trains recorded from A1 in an awake marmoset. (Adapted, with 
permission, from Lu, Liang, and Wang 2001. Copyright © 2001 
Springer Nature.)
C. Comparison of temporal response properties between 
primary auditory cortex (A1) and medial geniculate body of 
the thalamus (MGB). Stimulus-synchronized responses are 
quantified by vector strength, a measure of the strength of 
phase-locking. Nonsynchronized responses are quantified by 
the normalized firing rate (data curves identified as A1 rate and 
MGB rate). Error bars represent standard error of the mean. 
(Adapted, with permission, from Bartlett and Wang 2007.)
0.2
0.4
0.6
Vector strength
0
0.2
0.4
0.6
0.8
1
Normalized ﬁring rate
1
100
10
Interclick interval (ms)
0
3
10
20
30
40
50
60
70
100
Interclick interval (ms)
Time (ms)
0
500
1,500
Time (ms)
0
500
1,500
A
C  Transformation from MGB to A1
Synchronized responses
(responsive to long intervals)
B Nonsynchronized responses
(responsive to short intervals)
MGB
A1
MGB
A1
at long interclick intervals, but instead fires increas-
ingly rapidly as the interclick interval becomes shorter 
(Figure 28–12B). These two populations of A1 neu-
rons, referred to as synchronized and nonsynchronized, 
respectively, have complementary response proper-
ties. Neurons of the synchronized population explicitly 
represent slowly occurring sound events by synchro-
nized neural firing (a temporal code), whereas neurons 
of the nonsynchronized population implicitly represent 
rapidly changing sound events by changes in average 
firing rates (a rate code).
The nonsynchronized neurons have been observed 
in the auditory cortex of awake primates and rodents. 
In A1, neural representation changes from a temporal 
code to a rate code at the interclick interval of about 25 
ms, corresponding to a repetition rate of approximately 
40 Hz (Figure 28–12A,B). This is near the boundary 
where our perception of a periodic click train changes 
from being “discrete” to “continuous.”
The combination of temporal and rate codes to 
represent the whole range of time-varying sounds 
is the consequence of a progressive transformation 
beginning in the auditory nerve, where only a tem-
poral code (phase-locking) is available. The progres-
sive reduction in the upper limit of the phase-locking 
along the ascending auditory pathway is accompanied 
by the emergence of firing-rate-based representations.  
In the medial geniculate body of the thalamus, the 
intersection between temporal and rate codes is at a 
shorter interclick interval than in A1 (Figure 28–12C). 
This indicates that neurons in the medial geniculate 
body can phase-lock to more rapidly time-varying 
sounds than A1 neurons, but still utilize a rate code 
to represent rapidly changing sounds beyond their 
phase-locking limit.
The prevalence of rate-coding neurons in A1 has 
important functional implications. It shows that a 
considerable transition from temporal to rate coding 
has taken place by the time auditory signals reach the 
auditory cortex. The importance of the nonsynchro-
nized neural responses is that they represent trans-
formed instead of preserved temporal information. 
It suggests that cortical processing of sound streams 
operates on a segment-by-segment basis rather than 
on a moment-by-moment basis, as found in the audi-
tory nerve. This is necessary for complex integration 
because higher-level processing tasks require tempo-
ral integration over a time window. The reduction in 
A1 of the upper limit of phase-locking is a prerequi-
site for multisensory integration in the cerebral cortex. 
Auditory information is encoded at the periphery at a 
much faster temporal modulation rate than visual or 
tactile information, but phase-locking is similar across 


===== Page 6 =====
Chapter 28 / Auditory Processing by the Central Nervous System    673
primary sensory areas of the cortex. The slowing of 
the phase-locking limit along the ascending auditory 
pathway and accompanying transition from a tem-
poral code to a rate code are necessary for auditory 
information to be integrated in the cerebral cortex 
with information from other sensory modalities that 
are intrinsically slower.
Primates Have Specialized Cortical Neurons That 
Encode Pitch and Harmonics
Pitch perception is crucial for perceiving speech and 
music and for recognizing auditory objects in a com-
plex acoustic environment. Pitch is the percept that 
allows harmonically structured periodic sounds to be 
perceived and ordered on a musical scale. Pitch carries 
crucial linguistic information in tonal languages such 
as Chinese and prosodic information in European 
languages. We use pitch to identify a particular voice 
from a noisy background in a cocktail party. When lis-
tening to an orchestra, we hear the melody of the solo-
ist over the background of accompanying instruments.
An important phenomenon for understanding 
pitch is the perception of “missing fundamental,” also 
referred to as the residue pitch. When the harmonics 
of a fundamental frequency are played together, the 
pitch is perceived as the fundamental frequency even 
if the fundamental frequency is missing. For example, 
the harmonics of the fundamental frequency of 200 Hz 
are at 400, 600, 800 Hz, and so on. Playing the frequen-
cies 400, 600, and 800 Hz together will generate a pitch 
perception of 200 Hz, even though a distinct frequency 
component of 200 Hz is not physically present in the 
sound. We encounter this phenomenon routinely when 
we listen to music over speakers that are too small to 
generate sounds at low frequencies.
Many combinations of frequencies can give rise to 
a common fundamental frequency or pitch, making it 
a particularly valuable auditory cue. This is especially 
useful when pitch conveys behaviorally important 
information, as in the case of human speech or ani-
mal vocalizations. Sounds propagated through the 
environment can become spectrally degraded, losing 
high or low frequencies. While such spectral filtering 
distorts spectral information, the perception of the 
missing fundamental is robust despite the loss of some 
harmonic components.
The ability to perceive pitch is not unique to 
humans; birds, cats, and monkeys can also pick out 
pitch. Monkeys are capable of spectral pitch discrimi-
nation, melody recognition, and octave generalization, 
each of which requires the perception of pitch. Mar-
moset monkeys (Callithrix jacchus), a highly vocal New 
World primate species whose hearing range is similar 
to that of humans, exhibit human-like pitch percep-
tion. Marmosets are able to discriminate a missing 
fundamental in harmonic sounds with a precision 
as small as one semitone for the periodicity above  
440 Hz.
Given that both humans and some animals 
experience a pitch that generalizes across a vari-
ety of sounds with the same periodicity (including 
harmonic sounds with a missing fundamental), it is 
reasonable to expect that some neurons extract pitch 
from complex sounds. Xiaoqin Wang and his col-
leagues discovered a decade ago that a small region 
in the auditory cortex of marmoset monkeys contains 
“pitch-selective neurons.” These neurons are tuned to 
pure tones with a best frequency and respond to har-
monic complexes with a fundamental frequency near 
its best frequency even when the harmonics lay out-
side the neuron’s excitatory-frequency response area 
(Figure 28–13A).
A pitch-selective neuron responds to pitch-evoking 
sounds (eg, harmonic sounds, click trains) when the 
pitch is near the neuron’s preferred best frequency. 
Pitch-selective neurons increase their firing rates as 
the behavioral salience of pitch increases and prefer 
sounds with periodicity over aperiodic sounds. It is 
important to note that the pitch-selective neurons in 
marmoset monkeys, which extract and code for pitch 
embedded in harmonic sounds (a highly nonlinear 
computation), are distinctly different from neurons in 
subcortical areas or A1 that merely “reflect” informa-
tion on pitch in their firing patterns.
The region containing the pitch-selective neurons 
in marmoset monkeys is confined to the low-frequency 
border of A1, the rostral auditory cortex (area R), and 
lateral belt areas (Figure 28–13B). Human imaging 
studies have identified a restricted region at the lateral 
end of Heschl’s gyrus anterolateral to A1 that extracts 
pitch of harmonic complex sounds and is sensitive to 
changes in pitch salience. The location of this region 
mirrors the location of the pitch center in marmoset 
monkeys (Figure 28–13B).
The core regions of auditory cortex in marmosets 
also contain a class of harmonic template neurons 
that respond weakly or not at all to pure tones or two-
tone combinations but respond strongly to particular 
combinations of multiple harmonics. The harmonic 
template neurons show stronger responses to har-
monic sounds than inharmonic sounds and selectiv-
ity for particular harmonic structures. In contrast to 
the pitch-selective neurons that are localized within 
a small cortical region lateral to the low-frequency 
border between A1 and R and have best frequencies 


===== Page 7 =====
674    Part IV / Perception
500
1,000
1,500
2,000
2,500
12–14
10–12
8–10
6–8
4–6
1–3
Harmonic composition
Frequency (Hz)
0
300 
600 
900 
1,200
Time (ms)
f0
1 mm 
32
16
8
4
2
1
0.5
0.25
0.125
M
L
C
R
Recording site (519 total units)
Pitch neuron site (19 pitch units) 
AI
R
RT
BF (kHz)
A
B
Common-pitch stimuli
Neuronal response
Pitch center
Stimulus
Figure 28–13  Pitch is encoded by specialized neurons in 
primate auditory cortex.
A. An example of a pitch-selective neuron recorded from 
marmoset auditory cortex. Left: Frequency spectra of a series 
of harmonic stimuli that share the same fundamental fre-
quency (f0). Right: Peristimulus time histogram of the neuron’s 
response to the stimuli (stimulus duration indicated by the 
shaded region). (Adapted, with permission, from Bendor and 
Wang 2005. Copyright © 2005 Springer Nature.)
B. Anatomical organization of the marmoset auditory cortex 
and the location of a pitch center. Top: Side view of the mar-
moset brain. Bottom: Tonotopic map of the left auditory cortex 
characterized in one marmoset. Pitch-selective neurons (black 
squares) are clustered near the low-frequency border between 
A1 and area R (rostral auditory cortex). Frequency reversals 
indicate the borders between A1/R and R/RT (rostrotemporal 
auditory cortex). (BF: best frequency.) (Adapted from Bendor 
and Wang 2005. Copyright © 2005 Springer Nature.)
less than 1,000 Hz, the harmonic template neurons are 
distributed across A1 and R and have best frequencies 
ranging from approximately 1 kHz to approximately 
32 kHz, a range that covers the entire hearing range 
of marmosets.
Whereas in the periphery single auditory nerve 
fibers encode individual components of harmonic 
sounds, the properties of the harmonic template neu-
rons reveal harmonically structured receptive fields 
for extracting harmonic patterns. The change in neu-
ral representation of harmonic sounds from auditory 
nerve fibers to the auditory cortex reflects a principle 
of neural coding in sensory systems. Neurons in sen-
sory pathways transform the representation of physi-
cal features, such as the frequency of sounds in hearing 
or luminance of images in vision, into a representation 
of perceptual features, such as pitch in hearing or cur-
vature in vision. Such features lead to the formation 
of auditory or visual percepts. The harmonic template 
neurons in the auditory cortex are key to processing 
sounds with harmonic structures such as animal vocal-
izations, human speech, and music.


===== Page 8 =====
Chapter 28 / Auditory Processing by the Central Nervous System    675
Insectivorous Bats Have Cortical Areas Specialized 
for Behaviorally Relevant Features of Sound
Although it is generally assumed that upstream audi-
tory areas perform increasingly specialized functions 
related to hearing, much less is known about the 
functions of serial relays in the auditory system com-
pared to the visual system. In humans, one of the most 
important aspects of audition is its role in processing 
language, but we know relatively little about how 
speech sounds are analyzed by neural circuits. New 
techniques for imaging the human brain are gradually 
providing insights into the functional specialization of 
cortical areas associated with language (Chapter 55).
Evidence for specialized analysis of complex audi-
tory signals in the cerebral cortex comes from stud-
ies of insectivorous bats. These animals find their 
prey almost entirely through echolocation, emitting 
ultrasonic pulses of sound that are reflected by flying 
insects. Bats analyze the timing and structure of the 
echoes to help locate and identify the targets, and dis-
crete auditory areas are devoted to processing different 
aspects of the echoes.
Many bats, such as the mustached bat studied by 
Nobuo Suga and his collaborators, emit echolocating 
pulses with two components. An initial constant-frequency 
(CF) component consists of several harmonically 
related sounds. These harmonics are emitted stably for 
tens to hundreds of milliseconds, akin to human vowel 
sounds. The constant-frequency component is fol-
lowed by a sound that decreases steeply in frequency, 
the frequency-modulated (FM) component, which resem-
bles the rapidly changing frequency of human conso-
nants (Figure 28–14A).
The FM sounds are used to determine the distance 
to the target. The bat measures the interval between 
the emitted sound and the returning echo, which corre-
sponds to a particular distance, based on the relatively 
constant speed of sound. Neurons in the FM-FM area 
of auditory cortex (Figure 28–14B) respond preferen-
tially to pulse-echo pairs separated by a specific delay. 
Moreover, these neurons respond better to particular 
combinations of sounds than to the individual sounds 
in isolation; such neurons are called feature detectors 
(Figure 28–14C). The FM-FM area contains an array of 
such detectors, with preferred delays systematically 
ranging from 0.4 to 18 ms, corresponding to target 
ranges of 7 to 280 cm (Figure 28–14B). These neurons 
are organized in columns, each of which is responsive 
to a particular combination of stimulus frequency and 
delay. In this way, the bat, like the barn owl in its infe-
rior colliculus, is able to represent an acoustic feature 
that is not directly represented by sensory receptors.
The CF components of bat calls are used to deter-
mine both the speed of the target relative to the bat and 
the acoustic image of the target. When an echolocat-
ing bat is flying toward an insect, the sounds reflected 
from the insect are Doppler-shifted to a higher fre-
quency at the bat’s ear, for the bat is moving toward 
the returning sound waves from the target, causing 
a relative speeding up of these waves at its ear. Simi-
larly, a receding insect yields reflections of lowered 
frequency at the bat’s ear. Neurons in the CF-CF area 
(Figure 28–14B) are sharply tuned to a combination of 
frequencies close to the emitted frequency or its har-
monics. Each neuron responds best to a combination 
of a pulse of a particular fundamental frequency with 
an echo corresponding to the first or second harmonic 
of the pulse, Doppler-shifted to a specific extent. As in 
the FM-FM area, neurons do not respond to the pulse 
or echo alone, but rather to the combination of the two 
CF signals.
CF-CF neurons are arranged in columns, each 
encoding a particular combination of frequencies. 
These columns are arranged regularly along the corti-
cal surface, with the fundamental frequency along one 
axis and the echo harmonics along a perpendicular 
axis. This dual-frequency coordinate system creates a 
map wherein a specific location corresponds to a par-
ticular Doppler shift and thus a particular target veloc-
ity, ranging systematically from –2 m/s to 9 m/s.
The CF components of returning echoes are also 
used for detailed frequency analysis of the acoustic 
image, presumably important in its identification. The 
Doppler-shifted constant-frequency area (DSCF) of 
the mustached bat is a dramatic expansion of the pri-
mary auditory cortex’s representation of frequencies 
between 60 kHz and 62 kHz, corresponding well to the 
set of returning echoes from the major CF component 
of the bat’s call (Figure 28–14B). Within the DSCF area, 
individual neurons are extremely sharply tuned to fre-
quency, so that the tiny changes in frequency created 
by fluttering moth wings are easily detected.
Transient inactivation of some of these specialized 
cortical areas while the bat performs a discrimination 
task strikingly supports the importance of their func-
tional specialization in behavior. Silencing of the DSCF 
selectively impairs fine frequency discrimination while 
leaving time perception intact. Conversely, inactivation 
of the FM-FM area impairs the bat’s ability to detect 
small differences in the time of arrival of two echoes, 
while leaving frequency perception unchanged.
Investigation of this auditory system was greatly 
facilitated by knowledge of the stimuli relevant to bats. 
It remains to be seen whether these cortical areas are 
functionally or anatomically analogous to particular 


===== Page 9 =====
676    Part IV / Perception
Figure 28–14  The auditory system of the bat has special-
ized areas for locating sounds.
A. A sonogram of an animal’s calls (solid lines) and the result-
ant echoes (dashed lines) illustrates the two components 
of the call: the protracted, harmonically related constant-
frequency (CF) signal and the briefer frequency-modulated (FM) 
signal. The duration of the calls declines as the animal approaches 
its target. (Adapted, with permission, from Suga 1984.)
B. A view of the cerebral hemisphere of the mustached bat 
shows three of the functional areas within the auditory cor-
tex. The FM area is where the distance from the target is 
computed; the CF area is where the velocity of the target is 
computed; and the Doppler-shifted CF area is specialized for 
the identification of small fluttering objects. The expanded 
cortical representation of Doppler-shifted CF signals near the 
second harmonic of the call frequency (60–62 kHz) forms the 
acoustic “fovea.” (Adapted, with permission, from Suga 1984.)
C. The FM-FM combination-sensitive neuron shown does not 
respond significantly to either pulses or echoes alone, but 
responds very strongly to a closely paired pulse-echo. However, 
the neuron is also sensitive to the time difference between the 
pulse and echo, as seen in the record on the right, where the 
neuron fails to respond to a pulse-echo combination that is not 
closely paired. (Adapted, with permission, from Suga et al. 1983.)
100
95
50
60-62
Frequency axis
40
30
24 20 10
92
91
A
B
C
120
90
60
30
0
0
20
FM1 FM3
FM1 FM4
FM1
CF1/CF2
CF1/CF3
FM2
40
Time (ms)
Doppler shifted constant 
frequency area
Frequency (kHz)
100
50
0
Frequency (kHz)
60
80
100
Pulse
Echo
FM1
FM2
FM3
FM4
CF1
CF2
CF3
CF4
2 mm
Delay
axis
Pulse
Echo
Pulse and echo
closely paired
Pulse and echo
not closely paired
FM area


