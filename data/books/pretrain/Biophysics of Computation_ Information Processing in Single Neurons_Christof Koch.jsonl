{"text": "===== Page 1 =====\nLINEARIZING\nVOLTAGE-DEPENDENT\nCURRENTS\nWe hinted several times at the fact that a small excitatory synaptic input in the pres-\nence of voltage-dependent channels will lead to a local depolarization, followed by a\nhyperpolarization (Fig. 10.1). Those of us who built our own radios will recognize such\nan overshooting response as being indicative of so-called RLC circuits which include\nresistances, capacitances as well as inductances. As a reminder, a linear inductance is\ndefined as a circuit element whose instantaneous I—V relationship is,\n(10.1)\nwhere L is the inductance measured in units of henry (abbreviated as H). Although\nneurobiology does not possess any coils or coillike elements whose voltage is proportional\nto the current change, membranes with certain types of voltage- and time-dependent\nconductances can behave as (/they contained inductances. We talk of a phenomenological\ninductance, a phenomenon first described by Cole (1941) and Cole and Baker (1941) in the\nsquid axon (see Cole, 1972). Under certain circumstances, discussed further below, such\ndamped oscillations can become quite prominent.\nThis behavior can be obtained in an entirely linear system, as can be observed when\nreducing (in numerical simulations) the amplitude of the synaptic input (or step current):\neven though the voltage excursion around steady-state becomes smaller and smaller, the\novershoot persists (Fig. 10.1). It is not due to any amplification inherent in such a membrane\nbut is caused by its time- and voltage-dependent nature. Such a linear membrane, whose\nconstitutive elements do not depend on either voltage or time, and which behaves like a\nbandpass element, has been called quasi-active (Koch, 1984) to distinguish it from a truly\nactive, that is, nonlinear membrane.\nIn this chapter, we will explain in considerable detail how an inductance-like behavior can\narise from these membranes by linearizing the Hodgkin-Huxley equations. Experimentally,\nthis can be done by considering the small-signal response of the squid giant axon and\n232\n10"}
{"text": "===== Page 2 =====\n10.1 Linearization of the Potassium Current\n233\nFig. 10.1 A QUASI-ACTIVE MEMBRANE A small (gpeak = 1 nS) excitatory synaptic input close\nto the soma of a simulated pyramidal cell (Fig. 18.IB) gives rise to an EPSP followed by a small\nhyperpolarization due to activation of somatic potassium currents. This overshoot is not due to any\nnonlinearity in the system since it can be observed for arbitrarily small inputs. (If the synaptic input\nis reduced by a factor of 100 to gp^ = 0.01 nS and the associated EPSP rescaled by 100 and\nsuperimposed onto the larger amplitude response, the curves cannot be distinguished.) The system\nbehaves like a linear electrical circuit containing an inductance-like element and the associated\nmembrane is called quasi-active.\ncomparing it against the theoretical predicted value, a further test of the veracity of the\nHodgkin-Huxley equations, which they passed with flying colors. For more details on\nthe linearization procedure we will be using, consult the original references (Chandler,\nFitzHugh, and Cole, 1962; Sabah and Leibovic, 1969; and the very clear Mauro et al.,\n1970). Although such a linearization has been applied to membranes other than the squid\naxon (Clapham and DeFelice, 1976, 1982) we will primarily discuss this well-explored\nmodel system.\n10.1 Linearization of the Potassium Current\nIn order to demonstrate the principle behind this linearization, we will describe the procedure\nin detail for the potassium current of the Hodgkin-Huxley equation (Eq. 6.4 and Fig. 10.2A):\n(10.2)\nLet us consider small variations of this current around some fixed potential Vr. We can\nexpress such a variation as\n(10.3)\nwhere both derivatives are evaluated at the potential around which the system is linearized,\nhence the subscript r. Retaining only the first-order terms and neglecting all higher order\nones (such as (Sn)2 or SnS V) is at the heart of the linearization procedure. We have\n(10.4)"}
{"text": "===== Page 3 =====\n234 \n• \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.2 PHENOMENOLOGICAL INDUCTANCE ASSOCIATED WITH THE LINEARIZED POTASSIUM\nCURRENT \n(A) Equivalent electrical circuit for a noninactivating potassium current. The conduc-\ntance gK is given by H^n4, with n the voltage- and time-dependent activation constant. (B) Conduc-\ntance g in series with an inductance L. The voltage across both components is SV and the current\nthrough them 57. VL is the voltage at the intermediate location. (C) Equivalent circuit when the\npotassium current (in panel A) is linearized around some fixed membrane potential. The response\nto this circuit mimics the small-signal response of the potassium current /K around some potential\nVr. It includes a pure conductance, GK = lie\"4' m parallel witn a conductance gn in series with an\ninductance Ln. For Vm > EK, all three components are positive.\n(see Fig. 10.2C) and\n(10.5)\nWe eliminate the variation in the rate constant Sn by recalling Eq. 6.6 governing its dynamics,\n(10.6)\nFor small variations we can expand this to\n(10.7)\nSince an and /?„ only depend on the membrane potential, we can express their variations as\n(10.8)\nand\n(10.9)\nReplacing these two expression into Eq. 10.7 leads to\n(10.10)"}
{"text": "===== Page 4 =====\n10.1 Linearization of the Potassium Current \n• \n235\nThis can be formally expressed as\n(10.11)\nWhen replacing this result in Eq. 10.3 and exploiting Eqs. 10.4 and 10.5, we arrive at the\nfollowing equation for the first-order variation of the potassium current\n(10.12)\nwith two parameters that need to be evaluated at the membrane potential at which the current\nis linearized,\n(10.13)\nand\n(10.14)\nIn order to understand Eq. 10.12 let us apply Kirchhoff's voltage law to the one branch\ncircuit shown in Fig. 10.2B. With SI the current through both components we can write\n(10.15)\nEliminating the voltage VL, we are left with\n(10.16)\nComparing this result with the second component on the right-hand side of Eq. 10.12 we\ncan see that they are very similar. Indeed, the voltage S V across the electrical circuit with\ntwo branches drawn in Fig. 10.2C is the same as 8V in Eq. 10.12 if\n(10.17)\nand if we set the inductance to\n(10.18)\nFor a small perturbation SV around Vr, the potassium current responds as though the\nconductance GK (in units of S-cm2) is shunted by the conductance gn (also in S-cm2) in\nseries with the inductance Ln (in units of sec-cm2/S, that is, in H-cm2). By examining the\nslopes of the on and off rates an and fin (Eqs. 6.10 and 6.11), we see that for the entire range\nof relevant voltages, V > EK, all three electrical components are always positive.\nMauro et al., (1970) captured the key point of the previous analysis in a graphical\nmanner; see Fig. 10.3. As before, we assume that we are only considering the small-signal\nresponse (this is, after all, what any linearization procedure amounts to) of the potassium\ncurrent around some fixed potential Vr (positive to EK). Injecting a current step /jnj > 0\nat this potential will turn on the potassium current 7K (Fig. 10.3A). Because the underlying\npotassium conductance requires some time to increase to its final value, the initial change\nin membrane potential 5V is given by the injected current divided by the conductance GK.\nIn the absence of any capacitance, this causes the membrane to depolarize instantaneously"}
{"text": "===== Page 5 =====\n236 \n• \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nby /inj / GK, moving in the direction of the dotted slope (the vector labeled to in Fig. 10.3 A).\nHowever, this depolarization will eventually led to an increase in the potassium conductance,\nwhich causes the voltage change S V to decrease with time, as indicated by the trajectory\nin Fig. 10.3C, to finally approach the new steady-state value of the membrane potential Vr,\n(around t2). In other words, as expected of an inductance—inducing a voltage proportional\nto the change in current—8 V rises instantaneously and then falls off gradually. This analysis\nalso makes obvious the fact that this inductive behavior is caused by both the time and the\nvoltage dependency of the membrane conductance.\nWhen the current /inj terminates, the inverse occurs (Fig. 10.3B). 8 V first undershoots Vr,\nrelaxing with a time lag back to its earlier steady-state value Vr. The response is inductive\nfor both on and off steps (provided that Vr > EK). If the capacitive nature of the neuronal\nmembrane is included, the phenomenological inductance combines with the capacitance to\ngive a damped oscillatory response (Fig. 10.3C).\n10.2 Linearization of the Sodium Current\nIn a similar manner, we can compute the small-signal response of the sodium current,\n(10.19)\nBy taking variations and retaining only the first-order terms we arrive at\n(10.20)\nthat is,\n(10.21)\nFollowing the same procedure to eliminate the variational variables 5m and Sh as for the\npotassium current leaves us with an equation for the small-signal change in the sodium\ncurrent:\n(10.22)\nwith\n(10.23)\n(10.24)\nand\n(10.25)\nall evaluated at V — Vr. As illustrated in Fig. 10.4A, this describes the current flow in a\nthree-branch electrical circuit with inductances\n(10.26)"}
{"text": "===== Page 6 =====\n10.2 Linearization of the Sodium Current \n• \n237\nFig. 10.3 GRAPHICAL INTERPRETATION OF THE LINEARIZATION PROCEDURE Following the\nlead of Mauro et al., (1970), we here illustrate the linearization procedure for the potassium current\n/K. (A) Steady-state f-V relationship of a potassium current is indicated schematically and we are\nconsidering the small-signal response around the potential V,. Not shown is the time dependency of\nthe underlying potassium conductance. Injecting a positive current /;nj leads to an instantaneous jump\nof the membrane potential to a new value (which lies along the dotted line going through the reversal\npotential £\"K) given by /JHJ/GK (the vector labeled to). As the potassium conductance gradually\nadapts to its new value and increases, the voltage change 8 V becomes smaller, until it converges to\nits new value Vr, (from to to t\\ to ti\\ left panel). This inductive behavior would not be evident in a\nstationary nonlinear membrane. (B) Upon termination of the current step /inj, the voltage change SV\nfirst overshoots before settling to its old value. (C) Complete voltage response to the current injection\nin the absence of any membrane capacitance (left panel). When the capacitance is added to the circuit,\na damped oscillatory response results (right panel).\nand\n(10.27)\nThe two inductive branches reflect the separate contributions from the activating and the\ninactivating components of the sodium conductance."}
{"text": "===== Page 7 =====\n238 \n• \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.4 RLC CIRCUIT FOR THE LINEARIZED SODIUM CURRENT (A) Electrical RLC circuit\nobtained by linearizing the Hodgkin-Huxley sodium current at some potential below £NS- Both\nelements associated with the activation variable are negative. (B) These negative elements can be\ntransformed into a positive conductance g'm in series with a positive phenomenological capacitance\nC'm, with a shunt conductance G^a that is negative.\nAt this point, a problem arises. For the voltage range of interest, that is, EK < V < E^a,\nthe resistive and inductive components of the inactivating branch of the circuit, that is, gf,\nand Lf,, are positive, but the corresponding circuit elements of the m branch are negative:\ngm < 0 and, therefore, Lm < 0. The negative conductance arises as a consequence of the\nnegative slope of the iNa-V curve."}
{"text": "===== Page 8 =====\n10.3 Linearized Membrane Impedance of a Patch of Squid Axon \n• \n239\nWe can fix this problem by transforming the negative components into an equivalent\nRC system (Fig. 10.4B) with\n(10.28)\n(10.29)\nand a positive capacitance, instead of a negative inductance,\n(10.30)\nThis capacitance is also a phenomenological one, in the sense that activating the sodium\ncurrent leads to similar behavior (for small perturbation) as would the existence of an\nadditional capacitive branch.\nFor V < ENa, the conductance g'm and the capacitance C'm are positive, but the shunting\nconductance G'Na is negative. This is to be expected if we only treat /Na in isolation, with\nno counteracting current. Indeed, in order for the circuit to be stable, the total steady-state\nconductance must be positive (Koch, 1984). This can be assured if the linearized potassium\ncurrent as well as the leak current are all jointly considered, as we will do now.\n10.3 Linearized Membrane Impedance of a Patch of Squid Axon\nThe final RLC circuit, mimicking the small-signal response of a patch of squid axon\nmembrane, includes five branches (Fig. 10.5A): a pure capacitance due to the physical\ncapacitance of the bilipid membrane, a single positive conductance lumping the steady-\nstate contributions of the leak, sodium, and potassium currents, the two phenomenological\ninductive branches and the one phenomenological capacitive branch. The values of these\nelectrical elements vary with Vr, as discussed in Mauro et al., (1970) and in the legend to\nFig. 10.5.\nWe know from electrical circuit theory that RLC circuits can show oscillatory behavior\nwith the presence of one or more resonances. This is best investigated by computing the\ncomplex impedance K (/) of the circuit in Fig. 10.5A. Writing down an expression for the\ntotal current (see Eqs. 10.12 and 10.22) directly in Fourier space, we have\n(10.31)\nWith dSI(t)/dt transforming into if • <$/(/), we obtain an expression for the impedance\nas the ratio of S V(f) and 81'(/):\n(10.32)\nThe a, 's and ft's are constants, which depend on the values of the electrical components\n(Koch, 1984). Figure 10.5B shows the calculated amplitude of the impedance if the mem-\nbrane is linearized around the resting potential, and Fig. 10.5C shows the experimentally\nrecorded amplitude. Both sets of curves have the same characteristic bandpass response.\nFor the low-temperature curves, sinusoidal inputs in the neighborhood of 50-70 Hz are\nmore attenuated than slower or more rapidly varying current inputs. The linearized squid\naxon membrane behaves similar to a passive membrane for sinusoidal inputs with 200 Hz\nor faster components."}
{"text": "===== Page 9 =====\n240\nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.5 LINEARIZED SQUID AXON\nMEMBRANE (A) Electrical RLC cir-\ncuit mimicking the small-signal re-\nsponse of a patch of squid axon\nobtained by adding <5/K> \n<5^Na. as\nwell as the contribution from the leak\ncurrent <5/ieak- Note that G is posi-\ntive. If the Hodgkin-Huxley equations\nwith standard parameters (at 6.3° C;\nsee Chap. 6) are linearized around\nrest, the following numerical values\nare obtained (for details, see the ap-\npendix in Koch, 1984): Cm = 1,\nG = 0.246, gn = 0.849, gh = 0.072,\ng'm = 0.432, Ln = 6.43, Lh = 119.0\nand C'm = 0.102 (units are ^F/cm2\nfor the capacitances, H • cm2 for the\ninductances, and mS/cm2 for the con-\nductances). The combined steady-state\nconductance of the system at rest is\n1.167 mS/cm2. (B) Amplitude of the\nmembrane impedance of this circuit,\ncorresponding to the impedance of the\nsmall-signal response of the squid axon\nat two different temperatures. As ex-\npected of a quasi-active membrane,\nthe transversal membrane impedance\ndisplays a resonance around 67 Hz.\nReprinted in modified form by permis-\nsion from Mauro et al., (1970). (C) Am-\nplitude of the small-signal impedance\nmeasured in the space-clamped squid\naxon at two different temperatures.\nReprinted in modified form by permis-\nsion from Mauro et al., (1970).\nThe range of validity of the linearized membrane is a few millivolts around the resting\npotential (for more details, see Sabah and Leibovic, 1969; Koch, 1984). This is evident in\nthe experimental record from the squid axon displayed in Fig. 10.6A. Here, a current step\nof increasing magnitude is injected into the space-clamped squid giant axon (see Fig. 6.2A\nfor the experimental arrangement). As is apparent, the voltage has a characteristic ringing\nresponse as well as displaying undershooting at the offset of the current step. This ringing\nbecomes more pronounced for larger, but still subthreshold, current stimuli, revealing\nnonlinear components. A similar experiment is repeated in Fig. 10.6B, where an action\npotential is elicited at the top of the oscillation.\nIt makes sense to associate a resonant frequency with the membrane impedance, defined"}
{"text": "===== Page 10 =====\n10.3 Linearized Membrane Impedance of a Patch of Squid Axon \n• \n241\nFig. 10.6 SMALL-SIGNAL \nVOLTAGE \nRE-\nSPONSE IN SQUID AXON Membrane poten-\ntial in a space-clamped giant axonal fiber (of\n0.4-mm diameter). (A) Membrane potential in\nresponse to depolarizing current steps of differ-\nent amplitudes. Notice the damped oscillations\nduring and at the offset of the current step. The\nmaximum current strength applied is barely sub-\nthreshold in the last case. Reprinted in modified\nform by permission from Mauro et al., (1970).\n(B) A suprathreshold current of sufficient am-\nplitude is applied to evoke a second action po-\ntential that arises from the second oscillation.\nAt higher current amplitudes, a train of spikes\ncan be generated with a moderate increase in\nrepetition rate relative to the subthreshold oscil-\nlation frequency. Reprinted in modified form by\npermission from Mauro et al., (1970).\nhere as the frequency of the peak response of K ( f ) . If the standard Hodgkin-Huxley\nequations at 6.3° C are linearized around rest, /max — 67 Hz (Fig. 10.6B). This frequency\nis relatively stable for larger current steps, as is evident in Fig. 10.5B, varying no more than\n15% and roughly coinciding with the frequency of the oscillatory components seen in the\nexperimental records. Furthermore, /max is only marginally higher than the natural spiking\nfrequency of an \"infinite\" train of action potentials (53 Hz; see Fig. 6.9B).\nSo far, we only considered a patch of squid membrane (or, equivalently, a space-clamped\nax on). On the basis of the propagation factor y (/) defined in Sec. 23.1, the input and trans-\nfer impedances as well as the frequency-dependent space constant of the linearized squid\naxon can be computed. It is possible to prove (Koch, 1984) that if the membrane impedance\nis a bandpass, then so will these three functions for an infinite cable. For instance, a 1 £tm thin\ninfinite cable with the quasi-active membrane shown in Fig. 10.5A has a dc space constant\nof 175 jU,m. In any passive cable structure, the space constant at higher frequency will always\nbe less than the dc space constant (Fig. 2.8), due to the unavoidable charge leakage via the\nmembrane capacitance. While this capacitance is also present, the inductances \"present\"\nin the linearized squid axon can boost A(/) for an intermediate frequency range (for the\ndefinition of the frequency-dependent space constant in an infinite cable see Eq. 2.36). Thus,\nA,(/) has a pronounced peak (345 /urn) at 74 Hz. The sharpness of the tuning curve (as\nexpressed by its half width at half height) as well as the resonant frequency /max increase\nmonotonically with the channel density of both potassium and sodium channels as well as\nwith the distance between the injection and the recording sites (Koch, 1984).\nFigure 10.7A shows the computed amplitude of the somatic input impedance of our\nstandard model of the layer 5 pyramidal cell as a function of the frequency of the applied\nsinusoidal current injection. While its dendritic tree is passive, the soma contains seven"}
{"text": "===== Page 11 =====\n242 • \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.7 SMALL-SIGNAL SOMATIC INPUT IMPEDANCE OF PYRAMIDAL CELL MODEL \nSmall-\nsignal response of the voltage-dependent model of the layer 5 pyramidal cell. (A) Amplitude of\nthe somatic input impedance \\Kss(f)\\ \ndefined as the peak somatic voltage change relative to the\nresting potential (at —65 mV) in response to a small sinusoidal current (peak amplitude of 10 pA) of\nfrequency / injected into the soma. The peak occurs around 12 Hz. The roll-off at high frequency is\ndue to the distributed capacitance. (B) The range of this linear approximation is illustrated by plotting\nthe normalized membrane change in response to current steps /damp- For currents less then 0.1 nA,\nthe responses superimpose and the depolarizing response is a mirror image of the hyperpolarizing\nresponse. Larger currents increasingly deviate from this linear behavior. For current steps of 0.30 nA,\nthe membrane potential slowly drifts upward, eventually initiating an action potential.\nvoltage-, time-, and calcium-dependent membrane conductances. K is computed using a\nsmall-signal response. The amplitude of K is 16.5 M£2 at dc and peaks at 24.1 M£2 around\n12 Hz. At these frequencies, the transient potassium current /A is relatively inactivated\n(the inactivation rate constant has a time constant of 100 msec). The decay evident at high"}
{"text": "===== Page 12 =====\n10.4 Functional Implications of Quasi-Active Membranes \n• \n243\nfrequencies is due to the membrane capacitance and has a dependency close to 1/V/,\nas expected in an infinite cable. (At these high frequencies, the neuron can be considered\nto have a practical infinite extent relative to A(/); Fig. 2.9.) The range of validity of this\nsmall-signal approximation to the neuronal response can be directly read off Fig. 10.7B.\nFor current steps up to 0.1 nA, this relatively large cell is in its linear range. Larger currents,\nin particular if they are depolarizing, will trigger a sufficient amount of sodium current to\nbring the cell out of its linear range. For a step of 0.295 nA amplitude, the potential will\nslowly creep upward until an action potential is initiated (not shown; see Sec. 17.3).\n10.4 Functional Implications of Quasi-Active Membranes\nWhat are some of the computational implications for neurons with quasi-active membranes?\nThe fact that the input and transfer impedances and the frequency-dependent space constant\nhave peaks at nonzero frequencies has interesting consequences for information processing.\nLet us allude to three examples.\n10.4.1 Spaf/o-Temporal Filtering\nIn a dendritic tree endowed with a purely passive membrane, Kij and A. are monotonically\ndecreasing functions of the frequency of the applied current. As illustrated with the help of\nthe dynamic morphoelectrotonic transforms in Fig. 3.8, voltage attenuation is substantially\nmore pronounced at higher than at lower frequencies. For a neuron in the visual pathway,\nsay in the retina, this effect will partly determine its receptive field, since a visual stimulus\nwith a high frequency content can only influence the cell's firing if it is closer to the cell body\nthan more slowly varying synaptic input of the same amplitude. This effect is caricaturized\nin Fig. 10.8A. In a sort of precursor to METs, Koch (1984) determined the fraction of the\ndendritic tree of a cat retinal ganglion cell whose voltage attenuation to the soma, Ais,\nfor two different frequencies is less than 4. This amounts to a functional definition of the\nreceptive field of the cell (assuming that the synaptic inervation density is constant across\nits dendrites). If the dendritic tree is entirely passive, the \"receptive field\" of this cell is\nlarge for dc synaptic input, shrinking to a much smaller size for 100 Hz input.\nIf, to the contrary, the cell is endowed with the quasi-active membrane shown in\nFig. 10.5A, the opposite behavior occurs. Because the membrane impedance is higher for\nhigh-frequency inputs, the fraction of the dendritic tree for which Ats < 4 is large for 100 Hz\ninput, and small for either dc or 200 Hz input (Fig. 10.8B). Assuming that the processing\nbetween the photoreceptor and the ganglion cell is linear for low-contrast visual stimuli,\nthis neuron would be tuned to transient input and would respond less to sustained stimuli, a\nform of spatio-temporal tuning (Derrington and Lennie, 1982; Enroth-Cugell et al., 1983).\n70.4.2 Temporal Differentiation\nComputing the temporal derivative of some function V(t) is equivalent to multiplying its\nFourier transform V ( f ) by if. Such a differentiation operation can be implemented by\nfiltering V(t) with a function whose Fourier transform is proportional to /.\nWhile the transfer impedance of any neuronal membrane will ultimately decay with\nincreasing frequency (due to the presence of the capacitance), the transfer impedance in a\ncable endowed with a quasi-active membrane shows a high-pass behavior over a limited"}
{"text": "===== Page 13 =====\n244 \n• \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.8 SPATIO-TEMPORAL FILTERING IN RETINAL GANGLION CELLS In a cell endowed with\na quasi-active membrane, high-frequency synaptic input can propagate further than sustained input.\nThis effect is shown here graphically by determining the fraction of the dendritic tree whose voltage\nattenuation to the soma is less than some threshold, here Ajs < 4 in a cat retinal ganglion cell\nreconstructed from Boycott and Wassle (1974). This can be thought of as the receptive field of the\ncell. (A) The membrane is entirely passive. The large, heavily dotted area indicates the region within\nwhich the voltage attenuation for stationary inputs to the soma is less than 4. The smaller, lightly\ndotted area illustrates the same concept for a sinusoidal input of 100 Hz. (B) If the quasi-active\nmembrane obtained from the linearized Hodgkin-Huxley equations (Fig. 10.5A) is added to the\npassive component in panel A, the previous situation is reversed. Now a sinusoidal current input\nat 100 Hz (lightly dotted region) attenuates much less than sustained input (heavily dotted region).\nFor 200-Hz input the region of small attenuation will be less than for the region for sustained input.\nSuch a cell would be more responsive to transient than to dc signals. Reprinted by permission from\nKoch (1984).\nfrequency range. If the signal is restricted to this part of the spectrum, then such a membrane\ncan implement a temporal derivative operation.\nKoch (1984) simulated this behavior in an extended, unbranched cable with the quasi-\nactive membrane of Fig. 10.5A. Injecting a current that peaks at 5 msec and decays within\n20 msec (whose spectrum is monotonically decreasing and has decayed to 50% of its dc\nvalue at 70 Hz, the peak frequency of the transfer) the voltage recorded 350 /u,m away is\nqualitatively very similar to the temporal derivative of the current injected into the cable.\nThe approximation is worse for faster synaptic input (since a larger fraction of its spectrum\nwill lie beyond the peak at /max) and better for slowly changing input. What is appealing\nabout this linear operation is that it can be implemented in a very compact manner in a\nsingle cable.\n10.4.3 Electrical Tuning in Hair Cells\nReceptor cells in auditory and electroreceptive systems of many species show electrical\nresonance: upon appropriate sensory stimulation, their membrane potential oscillates. This\nhas been explored in detail for hair cells in the amphibian auditory system. Hair cells, whose\nfunction is to translate the acoustic stimuli into electrical ones (for an incisive review of\ntheir biophysical properties, see Hudspeth, 1985), are organized tonotopically along the\nbasilar membrane in the cochlea according to their characteristic frequency (the frequency"}
{"text": "===== Page 14 =====\n10.5 Recapitulation \n• 245\nof the acoustic stimuli the cell optimally responds to). Upon stimulation with a current step,\nthe membrane potential responds with exponentially damped oscillations (Crawford and\nFettiplace, 1981; Lewis and Hudspeth, 1983; see Fig. 10.9A). The oscillation frequency is\na nonlinear function of the membrane potential. The frequency around the resting potential\nindicates the cell's natural frequency, that is, the frequency of sound or vibration to which\nthe cell is most sensitive. In the bullfrog, this ranges from 80 to 160 Hz and in the turtle\nfrom 100 to about 700 Hz.\nHudspeth and Lewis (1988a,b) used the patch-clamp technique to identify the biophysical\nmechanism responsible for this resonance. They characterized two key currents, a noninac-\ntivating calcium current /ca, and a calcium- and voltage-dependent potassium current /K(Ca)\n(which they modeled using a linear, five-state kinetic scheme that requires two intracellular\nCa2+ ions for the channel to open, and a third Ca2+ ion that can prolong the channel\nopening; Hudspeth and Lewis, 1988a). Using a Hodgkin-Huxley-like model, which also\nincorporates the capacitive and leak currents, they showed that electrical tuning could be\nexplained in its majority by the interdependence of these currents (Hudspeth and Lewis,\n1988b). Injection of a step current depolarizes the membrane, activating /ca. Calcium ions\nrush in and rapidly turn on /K(Ca) • This outward current then brings the membrane potential\ndown again, turning itself off in the process. If the current step persists, the cycle can now\nbegin anew. Varying the kinetics and the density of the calcium-dependent calcium channels\ncontrols the frequency of resonance.\nThe input to hair cells is provided by activation of the mechanically sensitive cilia\n(hairs) that modulate a conductance in their membrane. If this conductance is included in\nthe model and stimulated using sinusoidally varying acoustical input, the resulting tuning\ncurve (Fig. 10.9B) closely mimics the electrical resonance frequency around the resting\npotential. We conclude that in the amphibian cochlea, hair cells behave like small electrical\nresonant elements—bandpass filters— whose tuning properties are due to the interplay of\nan inward and an outward current. Note that Hudspeth and Lewis (1988b) used a nonlinear\nmodel to explain the electrical resonance. Previous efforts to model hair cells using RLC\ncircuits as described above (Crawford and Fettiplace, 1981; Art and Fettiplace, 1987) did,\nhowever, capture most of the qualitative aspects of these oscillations.\nHave such resonating neuronal elements been found anywhere else? Llinas, Grace, and\nYarom (1991), recording in layer 4 neurons in the cortical slice preparation, found that a\nconstant current step evoked subthreshold oscillations in the 10-50 Hz frequency range.\nBecause these cells are small, with smooth and aspiny dendrites, Llinas and coworkers\nargue that they are inhibitory interneurons. In contradistinction, other neurons in the cortex,\ntermed chattering cells, show an oscillatory firing pattern in the 20-70 Hz band in response\nto visual stimuli or suprathreshold current injection, but no such oscillatory changes in the\nmembrane potential in response to subthreshold current injections (Gray and McCormick,\n1996). The function of these oscillations is at present not known, although much has been\nspeculated on this topic (for an overview, see Koch, 1993).\n10.5 Recapitulation\nAlthough it can be argued that a linear analysis of a nonlinear phenomenon does not do justice\nto it, it will certainly help us to understand certain aspects of the mechanism underlying\nthe phenomenon. This is true when considering certain resonant or oscillatory behaviors\nevident in nerve cells."}
{"text": "===== Page 15 =====\n246 \n• \nLINEARIZING VOLTAGE-DEPENDENT CURRENTS\nFig. 10.9 ELECTRICAL RESONANCE IN HAIR CELLS \nHair cells in the cochlea of amphibians\nbehave like electrical band-pass elements, preferentially responding to an acoustic stimulus of a\nparticular frequency. (A) Membrane potential recorded in a solitary hair cell of the bullfrog (Hudspeth\nand Lewis, 1988b). Upon injection of a 50-msec-long current step, the membrane responds by\ngenerating exponentially damped oscillations whose frequency varies with the membrane potential.\nThe frequency (93 Hz) around the resting potential (—49 mV) determines the frequency of sound\nor vibration the cell is most sensitive to. As the average membrane potential increases to —41 mV\n(by using larger current steps, whose amplitude is indicated to the right of each trace), the frequency\nof the damped oscillations increases to 240 Hz. Reprinted by permission from Hudspeth and Lewis\n(1988b). (B) The acoustic stimulus couples to a mechanically triggered membrane conductance. If\nthis conductance is added to a model that includes a calcium and a calcium-dependent potassium\ncurrent, it is sufficient to explain the resonant property of isolated hair cells. Here the peak-to-peak\namplitude of the receptor potential in response to a sinusoidal ±20-nm displacement of the hair bundle\nis plotted as a function of the stimulus frequency. Thus, each individual cell acts like a bandpass\nelement, here optimally sensitive to input around 112 Hz. Reprinted by permission from Hudspeth\nand Lewis (1988b)."}
{"text": "===== Page 16 =====\n10.5 Recapitulation \n• 247\nAs Detwiler, Hodgkin and McNaughton (1980) pointed out, a time- and voltage-\ndependent potassium current that is activated by depolarization or an inward current that is\nactivated by hyperpolarization (as the h part of the fast sodium current) does behave, under\ncertain restricted conditions, like an inductance in series with a conductance would. That\nis, one can mimic the small-signal behavior of such a current by an electrical circuit that\nincludes such an inductance. This explains why EPSPs or the depolarization in response to\neven very small current injections will be followed by a hyperpolarizing overshoot (provided\nthe experimental setup is sensitive enough to record such small voltages superimposed onto\nthe background noise). The membrane impedance of a membrane that includes resistances,\ncapacitances, as well as phenomenological inductances (RLC circuit) peaks at some\nnonzero resonant frequency /max- Such a quasi-active membrane with a bandpass response\nwill cause the membrane potential to show damped oscillations at or close to the resonant\nfrequency /max in response to a current step. This has been confirmed empirically when\nrecording the response of the squid giant axon to small, subthreshold current steps.\nOne consequence of a bandpass-like membrane impedance is that the transfer impedance\nand the frequency-dependent space constant associated with an infinite cable covered by\nsuch a quasi-active membrane can also show resonant behavior. This implies that input\nsignals with frequencies around /max are treated preferentially in terms of a smaller voltage\nattenuation to the soma (or other measures of synaptic efficiency) than inputs at faster or\nslower frequencies: the cable is spatio-temporally tuned to some frequency range. If the\ntransfer impedance of a cable with a quasi-active membrane can be approximated by a\nbandpass, injecting a current whose dominant signal content lies in the spectrum between\ndc and /max induces a voltage change that approximates the continuous temporal derivative\nof the injected current. In other words, a small piece of quasi-active dendritic cable can\nimplement a temporal derivative operation.\nHudspeth and Lewis (1988b) used a nonlinear, squid axonlike membrane description of\na calcium and a calcium-dependent potassium current (in series with leak and capacitive\ncurrents) to model the electrical behavior of bullfrog hair cells in the cochlea. Confirming\nearlier linear RLC analyses, each hair cell by itself acts as an electrical resonant element\nbest tuned to respond to sounds in the /max frequency band. The kinetics and density of\n^K(Ca) control /max as well as the tuning of the bandpass. The morale is that individual\nneurons can implement a variety of different linear and nonlinear computational operations\non the basis of the cornucopia of known membrane currents."}
{"text": "===== Page 3 =====\n250 \n• \nDIFFUSION, BUFFERING, AND BINDING\n(11.1)\nSubtracting p(x, t — Af) from both sides as well as dividing by Af yields\n(11.2)\nThe left-hand side of this equation is nothing but the first-order approximation of the first\ntemporal derivative of p, while the term on the right-hand side corresponds to the second-\norder approximation of the second spatial derivative of p(x, t — A?). In the limit as Ax\nand A? approach zero in such a way that\n(11.3)\nconverges toward a finite constant (with units of cm2/sec), we end up with the partial\ndifferential diffusion equation\n(11-4)\nIt describes how the probability function p(x, t) evolves over time and is also known under\nthe generic name of Fokker-Planck equation. We will reencounter this class of equations\nin Chap. 15.\nThe simplest interpretation of diffusion can be given if the single particle is replaced by\na cloud of N independently moving particles that all start off at the origin, x = 0. What\nis the expected mean position of this cloud of particles and the variance around this mean\nposition? According to our simple model, Xj(0 for particle i at time t = nAf can only\ndiffer from the previous position by ± AJC,\n(11.5)\nThe average position of the cloud, (x(n A?)), at time n Af is given by summing over all the\nindividual particles,\n(11.6)\nSince we assumed earlier that each particle has an equal chance of moving to the left\nor to the right (and that no boundaries impede the motion of the particles), the last term\nin brackets averages zero for large numbers of particles. This equation then tells us that\nthe mean position of the cloud at time n Af does not change from its previous position,\n(x(nAf)) = (x((n — l)Af))- If the cloud starts off at the origin, we conclude that the\naverage location of the cloud will never move,\n(11.7)\nIf we incorporate a systematic drift into the motion of the particles, for instance, by applying\nan electric field that imposes a preferred direction of motion (assuming that the moving"}
{"text": "===== Page 4 =====\n11.1 Diffusion Equation \n• \n251\nparticle is charged), this result will have to be modified. However, in the absence of such a\nbias, the cloud will remain centered at the origin.\nHow much does the cloud spread out over time? As anybody can observe when placing\na drop of ink into a water glass, at first a small part of the water is stained an intense blue.\nLater on, the spot of blue becomes less intense and larger, dispersing eventually throughout\nthe entire glass. One measure of this spreading is the variance of the mean position of the\ncloud. This is defined as\n(11.8)\n(we here used Eq. 11.5). Because the random walk is unbiased, the second term in this\nequation will be zero on average, resulting in\n(11.9)\nWe can, of course, recursively apply the same decomposition for (x((n — 1) At)2), ending\nup in n steps with (jc(0)2). Since the entire population started out at the origin, this last\nterm in zero. Setting t = « A? and remembering that (A.x)2/(2AO = D, we conclude\n(11.10)\nThus, while the mean position of the cloud does not move, the variance or width of the\ncloud increases linearly with time and its standard deviation as the square root of time.\nInterpreted in terms of single particles, Eq. 11.7 implies that the mean position of a single\nparticle does not change while the probability of finding it at larger and larger distances\nfrom the origin increases as the square root of time (Eq. 11.10).\n//. 1.2 Diffusion in Two or Three Dimensions\nThe same concept can be applied to the motion of a particle in more than one dimension. As\nlong as the motion along the x direction is independent of the motion in the y direction, the\nvariance of motion in the plane is the sum of the variances of motion in the two independent\ndirections,\n(11.11)\nA computer simulation of such a random walk is illustrated in Fig. 11.1. Given the square-\nroot relationship between time and distance traversed, it is possible for the particle to explore\nshort distances much more thoroughly than large distances. Thus, the typical pattern evident\nin Fig. 11.1: the particle tends to return to the same region many times before eventually\nwandering away. When it does move away, it blindly chooses another region to explore,\nwith no regard for whether or not it had previously visited that area. This reflects the fact\nthat the particle does not move down any gradient in response to a \"diffusive\" force as\npostulated by Fick (1855), but acts in a probabilistic manner to spread over the available\nspace; its tracks do not uniformly fill up the available space. For a particle diffusing in three\ndimensions, exactly the same principle applies, except that now"}
{"text": "===== Page 5 =====\n252\nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.1 DIFFUSION OF A PARTICLE IN THE PLANE Computer simulation of a single particle\nexecuting a random walk on a rectangular grid. At each point in time, it has a probability of 0.25 of\nmoving in any one of four directions. In the limit of an infinitesimally fine grid, this approximates\ndiffusion in a plane. The particle started at the center and after 5000 iterations ended up at the lower\nleft corner. The radii of the four circles correspond to the variance in the locations of the particle after\n500, 1000, 2000, and 4000 iterations. The expected time-averaged location of the particle is always\nat the center of the bull's eye.\n(11.12)\nThe random walk approach to diffusion has been very fruitful in mathematical physics. It\nis straightforward to derive equations corresponding to particles moving down a gradient\ncaused, for instance, by an applied electric field, or if the space the particle moves in\nhas absorbing or reflecting boundaries such as a membrane. For more details on the\nmathematics of random walk see the idiosyncratic, but immensely informative, monograph\nby Mandelbrot (1977); for a delightful text on the use of random walk techniques to describe\nthe motion of bacteria and other small organisms see Berg (1983).\n11.1.3 Diffusion Coefficient\nIn the above, we assumed—see Eq. 11.3—that D is purely dictated by the temporal and\nspatial discretization steps. However, for real particles in solution—where gravity can be\nneglected—D must clearly depend on certain physical attributes of the particles themselves\nas well as on properties of the solution. What precisely determines D and how large is it\nfor real ions, such as Ca2+, and big molecules, such as calmodulin which binds free Ca2+\nin neurons?\nEinstein (1905), arguing that diffusion is thermal agitation of particles opposed by\nfriction, derived a general relationship between diffusion and friction. For particles that\ncan be approximated as spheres much larger than the size of individual water molecules\n(which are on the order of 1 A), D is determined by the Einstein-Stokes relation as"}
{"text": "===== Page 6 =====\n11.2 Solutions to the Diffusion Equation \n• \n253\n(11.13)\nwhere r^ is the radius of the diffusing particle and r\\ the viscosity of water. The proportion-\nality of D to the absolute temperature T simply reflects the thermal origin of diffusion. For\nroom temperature (20° C), this equation reduces to D = a/rs, with a = 2.15 ^tm2/msec\nand the radius rs specified in units of angstrom (A). (For a very thorough discussion of\nthis, see Hille, 1992.) The diffusion coefficient is usually specified in cm2/sec; however,\nwe will adopt units more convenient to neuronal structures and measure D in units of\n/im2/msec.\nThe Einstein-Stokes relationship—based on classical hydrodynamics— is accurate for\nlarge particles, but works less well for small ions. For instance, for the 0.99 A radius calcium\nion, D should be 2.14 /im2/msec but is, in fact, 0.6 /im2/msec (Blaustein and Hodgkin,\n1969). This mismatch is partially caused by the formation of a hydration shell around the\nion, that is, water molecules that lie in direct contact with each ion in solution. For larger\nand heavier molecules, this is less of a consideration and their radius can be thought of as\nbeing approximately proportional to the cubic root of their molecular weight. Table 11.1\nlists the diffusion coefficients of a number of ions and molecules for diffusion in an aqueous\nmedium characteristic of the intracellular cytoplasm.\n11.2 Solutions to the Diffusion Equation\nBefore we discuss the impulse response or Green's function of the diffusion equation,\nlet us sketch out its macroscopic phenomenological derivation. The mathematical theory\nof diffusion in an isotropic milieu is based on the simplest phenomenological expression\npossible—also called Pick's first law of diffusion (Pick, 1855)—that the rate of transfer\nS(x, t) of a diffusing substance across a surface of unit area (also called the flux; Eq. 9.2)\nis proportional to the concentration gradient measured normal to the surface,\n(11.14)"}
{"text": "{\n  \"table_name\": \"TABLE 11.1\",\n  \"description\": \"Diffusion Coefficients\",\n  \"columns\": [\"Ion or molecule\", \"Diffusion coefficient\"],\n  \"rows\": [\n    {\"Ion or molecule\": \"H+\", \"Diffusion coefficient\": 9.3},\n    {\"Ion or molecule\": \"NO\", \"Diffusion coefficient\": 3.82},\n    {\"Ion or molecule\": \"Na+\", \"Diffusion coefficient\": 1.331},\n    {\"Ion or molecule\": \"K+\", \"Diffusion coefficient\": 1.961},\n    {\"Ion or molecule\": \"Ca2+\", \"Diffusion coefficient\": 0.63},\n    {\"Ion or molecule\": \"IP3\", \"Diffusion coefficient\": 0.24},\n    {\"Ion or molecule\": \"Calmodulin\", \"Diffusion coefficient\": 0.13},\n    {\"Ion or molecule\": \"CaM kinase II\", \"Diffusion coefficient\": 0.034}\n  ]\n} Diffusion coefficients in an aqueous environment for different ions and second\nmessenger molecules in units of 10~5 cm2/sec, that is /!*m2/msec.\n'Table 10.1 in Hille (1992).\n2 In rat cortex; Meulemans (1994); see also Wise and Houghton (1968).\n3 Blaustein and Hodgkin (1969).\n4 Allbritton, Meyer, and Stryer (1992)."}
{"text": "===== Page 7 =====\n254 \n• \nDIFFUSION, BUFFERING, AND BINDING\nwhere D is the constant of proportionality and the minus sign arises because diffusion\noccurs against a concentration increase. Since the amount of substance diffusing across the\nboundary is proportional to the area of the boundary, the diffusion coefficient has dimensions\nof jU-m2/msec.\nBecause this amount corresponds to a particular number of molecules diffusing, we refer\nto it using the colloquial, but entirely appropriate, term stuff for lack of a more specific term\nand express it in units of gram molecules or moles (mol) while the concentration C is given\nin molars (M), that is moles of molecules per liter.' The extracellular concentration of Ca2+\nions is about 2 mM, while its intracellular concentration at rest is around 10 to 20 nM, a\ndifference of five orders of magnitude.\nConsider a cylindrical cable where the concentration C only varies along the x dimension,\nreducing the problem of solving the diffusion equation from three dimensions to one. The\njustification of this is identical to the one we made for one-dimensional cable theory in\nChap. 2, namely, that the radius of the cylinder is much shorter than its length and that the\nequilibrium time for radial diffusion is short compared to longitudinal diffusion.\nIf the concentration inside a compartment centered at x and with boundaries at x+A x and\nx — A* (Fig. 11.2) varies by dC/dt, the change in the amount of stuff in this compartment\nis given by\n(11.15)\nThis change should be identical to the net rate of transfer across one boundary minus the\nrate of transfer across the other,\n(11.16)\nSetting these two expressions equal leads to\n(11.17)\nIn the limit of an infinitesimally small interval (Ax —> 0), the expression S(x + Ax, t) —\nS(x — Ax, t) converges to 2Ax 8S(x, t)/dx. Applying Eq. 11.14 we arrive once again at\nthe diffusion equation\nFig. 11.2 DIFFUSION IN A CYLINDER Diffusion of some substance into and out of a cylindrical\ncompartment with boundaries at x — Ax and x + Ax. S(x, t) corresponds to the rate of transfer across\nthe cross section of area nd2/4, and C(x, t) corresponds to the concentration. As in one-dimensional\ncable theory, under certain conditions the three-dimensional diffusion equation can be reduced to a\none-dimensional one.\n1. To remind the reader, one mole of water, that is, 18 grams molecules, corresponds to 6.023 X 1023 HjO molecules. Its\nconcentration is 1000 grams per liter divided by 18 grams per mole, that is 55 M."}
{"text": "===== Page 8 =====\n11.2 Solutions to the Diffusion Equation\n255\n(11.18)\nThis is a linear partial differential equation of the parabolic type, with a unique solution if\nboth an initial and a boundary condition are specified. We wish to point out already here,\nalthough we will not exploit this similarity until Sec. 11.7, that Eq. 11.18 is isomorphic to\nthe linear cable equation (Eq. 2.7) if rm -» oo (Fig. 11.3).\n11.2.1 Steady-State Solution for an Infinite Cable\nWhat is the steady-state behavior of the diffusion equation? If, in analogy with the cable\nequation, we \"clamp\" the concentration at the origin C(x = 0, t) for all times to a fixed\nvalue CQ, Eq. 11.18 reduces to an ordinary differential equation,\n(11.19)\nIts solution for an infinite cable is C{x) = Co for all values of x. After enough time has\npassed, the concentration in the entire cable rises to the concentration at the origin. This\nbehavior is in marked contrast to the exponential decay of the potential in response to a\ncurrent step (Eq. 2.12) and is a consequence of the fact that none of the diffusing substance\n\"leaks\" out across the walls of the cylinder.\n11.2.2 Time-Dependent Solution for an Infinite Cable\nThe simplest time-dependent solution is the one for the concentration change along an\ninfinite one-dimensional cable if an amout So of calcium ions is injected instantaneously\nFig. 11.3 EQUIVALENT ELECTRICAL CIRCUIT ASSOCIATED WITH THE DIFFUSION EQUATION\nLumped electrical circuit representation associated with the diffusion of a substance in an elongated\nfiber. The input here is defined as Ilci/Fd, where d is the diameter of the process, F is Faraday's\nconstant, and /ca is the calcium current flowing across the membrane. With the membrane capacity\ncm set to 1 and ra = l/D, this circuit can be mapped onto the lumped electrical circuit approximating\nthe cable equation (Fig. 2.3) in the absence of any membrane conductance (that is, rm -> oo). As we\nwill show in Sec. 11.7, this analogy to the cable equation can be extended to the presence of a fast\nbuffer and ionic pumps in the membrane. In the limit of infinitely small mesh size, the solution to this\ncircuit approximates the continuous diffusion equation (Eq. 11.18)."}
{"text": "===== Page 9 =====\n256 \n• \nDIFFUSION, BUFFERING, AND BINDING\ninto the cylinder at x — 0. We can show by simple differentiation that the resulting spatio-\ntemporal evolution of C is given by\n(11.20)\nThe associated concentration profile is plotted in Fig. 11.4 for different values of t and\nx. In comparing this function with the impulse response function of the infinite, passive\ncable (Eq. 2.31 and Fig. 2.6), we notice the absence of an e~' term (caused by the lack\nof a finite, membrane leak conductance for the diffusion equation). As a consequence, the\ntotal amount of substance is conserved, / Cj(x, t)dx = S$, at all times t. The absence\nof this exponential causes the Green's function for the diffusion equation to decay less\nrapidly than the corresponding Green's function for the cable equation. In particular, and\nnot surprisingly, given our earlier result concerning the variance in the mean position of a\ncloud of randomly moving particles, the variance of the impulse response function increases\nlinearly with time. We expand upon this point in the following section.\n11.2.3 Square-Root Relationship of Diffusion\nFor any fixed time t, Eq. 11.20 can be expressed as a Gaussian, as can be seen upon\ninspection of Fig. 11.4A,\n(11.21)\nwith\n(11.22)\nThe variance of the Gaussian increases linearly with t, or the standard deviation increases\nwith the square root of time. This has important consequences.\nLet us consider the concentration in a semi-infinite cylinder, where the concentration at\none end is held fixed: C(x = 0, f) = Co for all times. In the case of the cable equation,\nthis would be equivalent to clamping the voltage at one end of the cable. We can solve for\nC(x, t) directly by using either Fourier or Laplace transforms (Crank, 1975),\n(11.23)\nwhere erfc(jt) is the complementary error function defined as\n(11.24)\nwith erfc(O) = 1. The propagation of this concentration increase along the cylinder—\nillustrated in Fig. 11.5—involves only the dimensionless parameter x/(a\\/2). Due to the\nsquare-root relationship between a and t, it follows that the time required for any location\nto reach a given concentration is proportional to the square of the distance. If we ask at what\ntime t\\/2(x) the concentration at x reaches half of the source concentration, that is, Co/2,\nwe solve for t in Eq. 11.23, with its left-hand side set to Co/2. By consulting tables for the\nerror function (such as in Crank, 1975), we see that erfc(0.5) ~ 1/2 within a few percent.\nIn other words, \nor\n(11.25)"}
{"text": "===== Page 10 =====\n11.2 Solutions to the Diffusion Equation \n• \n257\nFig. 11.4 IMPULSE RESPONSE OF THE DIFFUSION EQUATION IN AN INFINITE CABLE \nConcen-\ntration Cj (x, t) in an infinite cylinder in response to an instantaneous injection of substance at t = 0\nat the origin x — 0 as a function of space (A) or time (B; see Eq. 11.20). This Green's function decays\nmore slowly than the Green's function of the linear cable equation (Eq. 2.31). As is clear from panel\nA, for any fixed time t the impulse response function can be described as a Gaussian, whose variance\nincreases linearly with time.\nNote that the approximation only involves: the numerical factor in front of the square. We\ncan reformulate this equation by stating thjat for a given duration t from the onset of the\nconcentration step, the distance x\\/2 at whiih the concentration has reached half of its peak\nvalue is given by\n(11.26)"}
{"text": "===== Page 11 =====\n258 \n• \nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.5 THE SQUARE-ROOT LAW\nOF DIFFUSION At t = 0, the cal-\ncium concentration at the origin of\na semi-infinite cable is clamped to\nCQ. (A) Evolving concentration pro-\nfile along the cable. (B) As time goes\nby, the concentration throughout the\ncable slowly rises to Co (different\nfrom the solution of the cable equa-\ntion). At any one instance, we can\nask at what location x\\/2 along the\ncable the concentration first reaches\nCo/2 (dashed line). This relationship,\nshown in (C), is a square-root one.\nIt takes four times as long to diffuse\ntwice the distance (Eq. 11.26). This\nimposes a fundamental physical con-\nstraint on how fast any substance can\ndiffuse in one or more spatial dimen-\nsions.\nIn three-dimensional space, the right-hand side needs to be multiplied by \\/3 (as in\nEq. 11.12). If considering diffusion of ions within the porous and highly restricted ex-\ntracellular space, further adjustments are necessary (see Sec. 20.2).\nAs an example, let us assume that the concentration of calcium at one end of a one-\ndimensional cylinder is clamped to 1 \\iM. Assuming that no buffer impedes the diffusion of\nthe calcium ions along the cable (and that £>ca = 0.6 /i,m2/msec), a calcium front—defined\nhere as the time at which the concentration at any one location first reaches 0.5 fiM—moving\ndown the cylinder takes about 1 msec to cover the first 0.77 //,m, 10 msec to propagate"}
{"text": "===== Page 12 =====\n11.3 Electrodiffusibn and the Nernst-Planck Equation \n• \n259\n2.5 /J,m away from the origin and has traveled only 7.7 /^m after 100 msec (Fig. 11.5). In\nneuronal tissues, these times are considerably further reduced due to the binding of calcium\nto intracellular buffers (Sec. 11.4).\nThe linear diffusion equation, as a member of the family of parabolic differential\nequations which also includes the linear cable equation (Sec. 2.3), does not admit to any\nsolution C(x, t) = C(x — vt) that propagates with fixed velocity v along the cable. This\nbehavior is in stark contrast to the linear relationship between distance and time for action\npotentials propagating down the axon or for the wave of calcium release occurring in eggs\nfollowing fertilization, making the cells impervious to the entry of additional sperms.\nThe square-root behavior imposes a fundamental limitation on the time required for\nconcentration changes of calcium or any other intracellular messenger to affect distant sites\n(in the absence of active transport systems) and is apparent everywhere. For instance, if\nwe keep the concentration of calcium in the shell just below the membrane of a spherical\ncell constant, the center of the sphere will have reached one half the shell concentration\nafter about 0.04J2/D msec, where d is the diameter of the sphere (in micrometers). For\nadditional solutions to the diffusion equation under various boundary conditions consult\nCrank (1975) and the monograph by Cussler (1984).\n11.3 Electrodiffusion and the Nernst-Planck Equation\nIn the derivation of the cable equation in the second chapter, we had assumed that the\nconcentration of ions does not vary along the longitudinal direction of the cable. Thus, ions\nare only propelled along the cable by the voltage gradient, giving rise to the 32 V/dx2 term.\nIn the previous section, we discussed the effective motion of ions due to diffusion, down\nthe concentration gradient.\nIn general, of course, we need to include the movement of ions caused by concentration\ndifferences as well as by drift along the electric field. Even if a dendrite is initially at\nequilibrium with respect to the spatial distribution of sodium, potassium, calcium, and\nchloride ions, which are most important for fast signaling, the influx and efflux of ions across\nthe membrane disturb this equilibrium. This is particularly true for very small volumes,\nsuch as thin dendrites or spines where the influx of even a moderate amount of calcium\nsignificantly increases [Ca2+],-, creating concentration gradients that propel calcium ions\ndown this gradient (in addition to any existing voltage gradient).\nIn order to account for these effects, we need to combine Pick's law (Eq. 11.14) with\nOhm's law, something done by Nernst (1888, 1889) as well as Planck (1890). Assuming\nthat the longitudinal current and ionic concentrations are uniform across the cross section\nof the cylindrical dendrite or axon and that the random, diffusional motion superimposes\nlinearly onto the electrotonic motions of ions driven by a true \"force,\" we can write the\none-dimensional Nernst-Planck electrodiffusion equation (Hille, 1992),\n(11.27)\nwhere /,,/tOe, r) is the axial current for the ionic species being considered (here labeled k),\nZk is its valence, D^ its diffusion coefficient, R the gas constant, and F Faraday's constant.\nThe ZkFDk term in front of the concentration gradient converts \"stuff\" into a current,\nwhile the constants in front of the potential gradient express implicitly how the electrical\nconductance relates to the concentration and the mobility of the ions (see also Eq. 11.13)."}
{"text": "===== Page 13 =====\n260 \n• \nDIFFUSION, BUFFERING, AND BINDING\nThus, ions drift down the potential gradient, while simultaneously spreading due to diffu-\nsion. Hodgkin has remarked that while diffusion is like a hopping flea, electrodiffusion is\nlike a flee that is hopping in a breeze.\nAs an aside, let us note that application of this kinetic equation in the direction perpen-\ndicular to the membrane directly yields Eq. 4.3 for the synaptic reversal or Nernst potential\nof an ionic current at equilibrium. Since the net current across the membrane must be zero,\nwe can rearrange Eq. 11.27,\n(11.28)\nwith r the spatial variable across the membrane. Integrating this across the membrane yields\nthe expression for the Nernst potential associated with each ionic species k.\nLet us return to our main argument. Due to reasons of conservation, the change in\nconcentration of the ions in an infinitesimally small cylindrical segment of length Sx and\ndiameter d must be balanced by the sum of the transmembrane current im^(x, t) per unit\nlength (suitably weighted by the surface-to-volume ratio 4/d; Fig. 11.2) and the difference\nbetween the ingoing and outgoing axial currents, or\n(11.29)\nTaking the spatial derivative of both sides of the electrodiffusion equation and extracting\nthe expression for the change in concentration from Eq. 11.29 leads to\n(11.30)\nThe terms on the right-hand side of this nonlinear coupled partial equation correspond to the\ntransmembrane current (given either by Ohm's law with the appropriate reversal potential\nor by the GHK current equation), the familiar diffusional term, and a voltage gradient\nterm. In general, this equation must be solved for each species of ions that is present at\nrelevant concentrations (assuming that different ionic species move independent of each\nother, something that may not always be true).\nSince Eq. 11.30 is a single equation in two unknowns C;,(x, t) and Vm(x, t), it needs\nto be supplemented by an equation specifying the membrane potential, here expressing the\nfact that Vm (t) is determined by the change in the total charge—added over all ionic species\nk weighted by their valence—divided by the membrane capacitance Cm in addition to an\noffset term, or\n(11.31)\nwhere Ck,rest is the resting concentration of the fcth ionic species.\nIn principle we need to solve Eqs. 11.30 and 11.31 to describe the dynamics of the\nmembrane potential in extended cable structures properly (supplemented by an additional\nconstraint at branching points; Qian and Sejnowski, 1989). However, this does not come\ncheaply. When linearly coupling two differential equations, the temporal discretization step"}
{"text": "===== Page 14 =====\n11.3 Electrodiffusion and the Nernst-Planck Equation \n• \n261\nAf required for an accurate evolution of the system is, in general, the smaller of the two time\nsteps associated with the individual equations. In other words, the discretization necessary\nto solve Eq. 11.27 has to be much finer in both time and space than the discretization\nrequired for solving the cable equation with constant concentrations, resulting in much\nlonger running times for the numerical algorithm.\n11.3.1 Relationship between the Electrodiffusion\nEquation and the Cable Equation\nIn \"large\" neuronal processes, the intracellular concentration of various ions changes by\nrelatively small amounts, implying that the diffusional contributions are negligible and that\nthe longitudinal currents are purely resistive. It is straightforward to obtain the familiar\ncable equation as a special case of Eq. 11.30 by assuming that the axial ionic concentration\ngradients can be neglected, dCk(x, t)/dx « 0. In combination with Eq. 11.29, Eq. 11.30\nreduces to\n(11.32)\nThis corresponds to the cable equation if we identify the intracellular resistance /?,• with\n(11.33)\nQian and Sejnowski (1989) compare numerical simulations of the propagating action\npotential in the squid giant axon (with a 0.476 mm diameter) using the electrodiffusion\nequation against the cable model solution of Cooley and Dodge (1966). With the diffusion\ncoefficients for sodium and potassium reported in Table 11.1, they calculate /?,• from\nEq. 11.33, obtaining 33.4 Q-cm for the potassium resistance and 267 Q-cm for the sodium\nresistance, for a total value of /?,- = 29.7 £2-cm. This last value is reasonably close to the\ninternal resistance of 35.4 fi-cm used by Hodgkin and Huxley (1952d). The final solutions\nare indistinguishable from those of Cooley and Dodge (1966). Reducing the diameter several\nhundredfold to 1 /u.m only leads to a maximum relative concentration change of 1.4%, too\nsmall to have any significant effect. Thus, as in a metal wire, charged carriers2 move under\nthe influence of Ohm's law and diffusion does not play any (significant) role.\nThe concentration change due to synaptic input scales as 1/r, implying that for submi-\ncrometer dimensions, diffusional transport of ions will contribute substantially to the total\ncurrent. Chapter 12 deals with one such case, dendritic spines, where a single excitatory or\ninhibitory synaptic input can change the concentration of calcium or chloride significantly\non a very rapid time scale. Here the predictions of the electrodiffusion model can deviate\nsubstantially from those of the cable model.\n11.3.2 An Approximation to the Electrodiffusion Equation\nFrom a physical point of view, the electrodiffusion equation constitutes a better description\nof current flow in a neuronal process than the cable equation. Thus, it behooves us to\nroutinely use the former, rather than the latter, for simulating events in dendrites. Yet, because\nof the additional computational load imposed by solving Eq. 11.30 using a finer spatio-\ntemporal discretization grid than the one required for solving the cable equation, almost\n2. Electrons in one case and Na^, K+, and other ions in the other."}
{"text": "===== Page 15 =====\n262\nDIFFUSION, BUFFERING, AND BINDING\nnobody has done this. Qian and Sejnowski (1989) offer a remedy for this by describing a\nfast approximation for solving the electrodiffusion equation in extended cable structures. It\ninvolves replacing the constant intracellular resistance RI by batteries and resistances for\neach ionic species being considered (Fig. 11.6). At each time step, this algorithm\n1. Calculates the intracellular concentration of each ionic species in each compartment by\nintegrating over both the transmembrane and the intraaxial currents between compart-\nments.\n2. Computes the new value of the Nernst reversal potential for each ionic species for each\ncompartment across the membrane (E^a and E& in Fig. 11.6).\n3. Replaces the single intracellular resistance Rj of the cable equation with individual\nlongitudinal resistances,\n(11.34)\nFig. 11.6 APPROXIMATING THE ELECTRODIFFUSION EQUATION Solving the electrodiffusion\nequation, a better description of the transport of charged ions than the cable equation, is compu-\ntationally more demanding than solving the cable equation (due to the higher spatial and temporal\ndiscretization required). Qian and Sejnowski (1989) advocate the usage of an approximation to\nEq. 11.30, illustrated here. The crucial step is to replace (at each time step and for each compartment)\nthe single axial resistance /?; of the cable equation (Fig. 2.3) with a resistance /?,-_* and battery £,,*\nfor each ionic species being considered. The transmembrane reversal batteries £i\\ja and EK are given\nby the Nernst equation (Eq. 4.3). This procedure is illustrated for the thick squid giant axon using a\nsymmetrical discretization procedure with R, Na = 267 Q-cm and R,^ = 33.4 Q-cm. Given the tiny\nchanges in intracellular sodium and potassium concentration, Ej,Na ^ £>,K ^ 0- Even for a much\nthinner axon of 1 /urn-diameter, the maximal value of these batteries is 0.4 mV. Only for very thin\ndistal dendrites or spines will qualitative differences to the cable model appear. Reprinted in modified\nform by permission from Qian and Sejnowski (1989)."}
{"text": "===== Page 16 =====\n11.4 Buffering of Calcium \n• \n263\nand batteries between compartment j and j + 1,\n(11.35)\nfor each ionic species k.\nFor most situations that Qian and Sejnowski considered, this algorithm agrees well\nwith the solutions to the full electrodiffusion equation, only doubling execution times in\ncomparison to solving the cable equation. When in doubt whether or not diffusion of ions\nwill affect the solution of the cable equation, we would recommend this algorithm to the\nreader (for more details, see Qian and Sejnowski, 1989).\n11.4 Buffering of Calcium\nIn a classical experiment, Hodgkin and Keynes (1957) used radioactive 45Ca to track the\ndiffusion of calcium in squid axon fibers. From the observed broadening of the radioactive\npatches the effective diffusion constant was estimated to be about one-tenth of the diffusion\ncoefficient Dca in aqueous solution. This is in contrast to the behavior of potassium ions un-\nder similar circumstances. This showed that once calcium enters the intracellular cytoplasm\nit is not free to diffuse. Indeed, 95% and more of the entering calcium is quickly bound by\na host of systems, ranging from a set of different protein buffers to cellular organelles, such\nas mitochondria and the smooth endoplasmic reticulum. Mitochondria have a considerable\nability to accumulate (and release) Ca2+. The endoplasmic reticulum in neurons—related to\nthe sarcoplasmic reticulum in skeletal muscle cells, which is responsible for the release and\nsubsequent reuptake of Ca2+ during muscle contractions—also accumulates calcium, yet at\na relatively slow rate. Since the calcium uptake of these organelles takes place on a time scale\nof seconds and longer (Rasgado-Flores and Blaustein, 1987), we focus our discussion on the\ndynamics of calcium binding to protein buffers. (For an overview of neuronal calcium home-\nostasis, see Carafoli, 1987; McBurney and Neering, 1987; Blaustein, 1988; Clapham, 1995.)\nA large number of Ca2+-binding proteins, such as calmodulin, calbindin, and parval-\nbumin, are present at high concentrations in nerve cells. The most important neuronal\ncalcium buffer, calmodulin, a 15,000 dalton regulatory protein, is present in brain tissue at\na concentration of 30-50 /j,M and acts as an internal calcium sensor (Manalan and Klee,\n1984; see Fig. 11.7). It is located in the cell bodies, dendrites, and postsynaptic densities\nof most neurons in the central nervous system, but not in axons. Each calmodulin (CaM)\nmolecule has four Ca2+-binding sites. At resting levels of calcium concentration, none or\nonly one of these sites is occupied. As the concentration of free calcium rises to micromolar\nlevels, the four binding sites are occupied successively. The fully bound calcium-calmodulin\ncomplex in turn can bind to a large number of regulatory proteins to alter their function,\nsuch as calmodulin-dependent protein kinases (CaM kinases), protein phosphatases, and\nadenylate cyclases.\nThese enzymes, some of which have been implicated in the induction of long-term\npotentiation (Miller and Kennedy, 1986; Kennedy, 1989, 1992; Ghosh and Greenberg,\n1995), can trigger the modification of other synaptic proteins locally at the synapse or can\nmediate more general cellular responses by activating molecules involved in the regulation\nof gene expression. As the concentration of calcium in the cytoplasm drops, the calcium\nions are progressively released from the buffer and are free to wander about. The resulting"}
{"text": "===== Page 17 =====\n264 \n. \nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.7 STRUCTURE OF CALMODULIN Calmodulin is the most important calcium receptor\nprotein. It is ubiquitous in eukaryotic cells and is found in the brain at concentrations of 30-50 iiM.\nThe 15,000-dalton protein can bind four Ca2+ ions in a cooperative manner with dissociation constants\nin the /j,M range. In the absence of calcium, the protein has some structure. Binding of Ca2+ to all\nfour sites induces a conformational change, leading to a dumbbell-shaped molecule. Two heads, each\nof which contains the two bound calcium ions, are interconnected by a long a chain. In this form,\nthe calcium-calmodulin complex can interact with a large number of other proteins, giving rise to a\ncomplex regulatory network. Reprinted by permission from Babu et al., (1985).\nchanges, which were triggered by the initial binding of Ca2+ ions to calmodulin, can outlast,\nby far, the calcium transient (Sec. 20.1). Before we come to this, let us first quantify the\ndynamics of various types of binding.\n11.4,1 Second-Order Buffering\nIn one of the simplest chemical reactions possible, a single calcium ion binds to a single\nbuffer molecule B, resulting in a bound buffer-calcium complex B -Ca. If theforward binding\nrate with which this reaction proceeds is / (in units of bindings per second per molar) and\nthe backward rate is b (in units of bindings per second), we have\n(11.36)\nIn terms of associated kinetic equations, we can write\n(11.37)\nwhere [B] and [B-Ca] denote the concentrations of the buffer and the buffer-calcium\ncomplex. The last equation expresses the fact that only a fixed amount TB of buffer molecules\nexists, where TB corresponds to the total amount of the buffer. / and b tell us something\nabout the speed with which the buffering reaction occurs. The above type of reaction is\nknown as a second-order reaction, since two substances participate, each with 1 mol, re-\nsulting in a quadratic term in the associated kinetic Eq. (11.37) (Pauling and Pauling, 1975)."}
{"text": "===== Page 18 =====\n11.4 Buffering of Calcium \n• \n265\nSetting the temporal derivative to zero yields the steady-state distribution of the buffer-\ncalcium complex,\n(11.38)\nwhere Kj = b/f is the dissociation constant of the buffer expressed in molars. If the\ncalcium concentration has reached Kj, exactly half the available buffer is bound to calcium.\nThe lower Kd, the lower the calcium concentration at which the buffer begins to bind Ca2+\nions. Kd therefore tells us something about the affinity of the buffer. The Kj of most calcium\nbinding proteins is in the low to mid micromolar range.\nIf the calcium concentration is much less than Kj, Eq. 11.38 can be approximated by\n(11.39)\nAt these low calcium concentrations [B • Ca] is proportional to [Ca \n]. We will explore\nthe consequences of this in Sec. 11.7.\nFor [Ca2+] < K&, the Ca2+ binding ratio, that is the ratio of bound calcium to free\ncalcium, is given by\n(11.40)\nThis number is usually large, upward of 20 (Allbritton, Meyer, and Stryer, 1992; Neher,\n1995; Gabso, Neher, and Spira, 1997), implying that 95% or more of all calcium is bound\nto intracellular buffers. Clearly the cell cares a great deal about regulating the amount of\nfree calcium ions sloshing around the neuron.\n11.4.2 Higher Order Buffering\nAs mentioned above, the ubiquitous calcium buffer protein calmodulin has four calcium\nbinding sites on each molecule. A general four-site system is defined by more than a dozen\nrate constants. In particular, binding calcium to one site can induce a conformational change\nof the molecule, affecting the binding of calcium to the remaining sites. Calmodulin appears\nto display positive cooperativity, such that binding of calcium to the first two binding sites\nincreases the affinity for the remaining two sites. Furthermore, the binding constants for the\nfour sites differ significantly from each other. For the sake of simplicity, we will assume\nthat all four sites are independent of each other and have identical binding constants. The\nKd of calmodulin is on the order of 10 /u.M (Manalan and Klee, 1984; Klee, 1988), such\nthat at 40-50 [iM levels of Ca2+, most of the calcium-binding sites will be occupied. For\nn binding sites, we can write down a series of n second-order equations of the from\n(11.41)\nwith m = 1, 2, • • •, n. By repetitively applying the associated kinetic equations, we can\nexpress the steady-state concentration of all intermediate species and of the fully bound\nbuffer-calcium complex as\n(11.42)\nwhere TB is the total concentration of the buffer in its various guises.\nIn the case of calmodulin, the forward rate constant / is about 50 per second and per\nmicromolar, the backward rate b is about 500 per second, implying a dissociation constant"}
{"text": "===== Page 19 =====\n266 \n• \nDIFFUSION, BUFFERING, AND BINDING\nKj = 10 fiM. Therefore, as long as [Ca2+] is less than a few micromolar, the denominator\nin Eq. 11.42 is close to 1 and the steady-state concentration of the fully bound calmodulin\ncomplex will be proportional to the fourth power of calcium. Each additional binding step\nto another protein or enzyme will potentiate or sharpen the polynomial relationship between\n[Ca2+] and the buffer, since n increases by 1 in the above equation.\nComputationally, higher order binding processes can be thought of as a squaring (for\nn — 2) or an nth-order polynomial operation (Koch and Poggio, 1992). Gamble and Koch\n(1987) showed numerically that a fast burst of action potentials (10 spikes at 333 Hz) to a\nsynapse located on a spine elevates intracellular calcium in the spine head by about a factor\nof 5 in comparison to the peak calcium evoked after 10 spikes at 50 Hz (see also Chap. 12).\nThis difference in levels of free calcium in the low- versus high-frequency input situation is\namplified 1000-fold if the concentration of fully bound calmodulin is considered; in the first\ncase 1000 times more [CaM • Ca4] is evoked than in the latter case (roughly about a factor\nof 54). If a certain level of concentration of some critical substance is required to initiate\nsome reaction, multiple binding steps will tend to lead to an all-or-none threshold behavior.\n11.5 Reaction-Diffusion Equations\nLet us now combine the diffusion equation for calcium with the kinetic equations expressing\nthe binding of calcium to a single buffer via a second-order reaction. We assume that\nthe buffer itself is stationary and does not diffuse and denote the intracellular calcium\nconcentration as [Ca2+](.x, t). The equations governing the evolution of the calcium and\nthe bound buffer concentration, frequently referred to as reaction-diffusion equations, are\nof the form\n(11.43)\nAs the reader can see, these coupled partial differential equations are nonlinear, due to the\nfact that we assumed cooperative binding between calcium and buffer (expressed in the\n[B] • [Ca2+] = (TE - [B • Ca]) x [Ca2+] term), making their analytical solution difficult\n(Chap. 14 in Crank, 1975, catalogues the major known solutions). If additional buffers\nare included or calcium binds to calmodulin or other proteins with multiple binding sites,\nfurther equations are required.\nCoupled reaction-diffusion equations are well known in developmental biology. They\nwere first invoked by the computer science pioneer Turing (1952) to explain the regular\nstructures in biological systems and have been used to explain the stripes of the zebra and\nof certain fish (Kondo and Asai, 1995) and aspects of embryonic development in frogs and\nfruit flies (Kauffman, 1993; Meinhardt, 1994).\nA particularly simple and insightful solution to Eq. 11.43 can be obtained if the con-\ncentration of the buffer-calcium complex is proportional to the concentration of calcium.\nThis occurs if the binding of calcium to the buffer is very fast compared to the time\nscale of diffusion and if the calcium concentration is much less than the Kj of the buffer\n(instantaneous buffer model)."}
{"text": "===== Page 20 =====\n11.5 Reaction-Diffusion Equations \n• \n267\nThe first condition implies that the buffer is always at equilibrium relative to the\ndiffusional time scale, while the second condition implies that [B • Ca] = /8[Ca2+]\n(Eq. 11.39, where ft is defined in Eq. 11.40). Both assumptions allow us to reduce the\nnonlinear reaction-diffusions Eqs. 11.43 to a single linear equation with\n(11.44)\nand, therefore,\n(11.45)\nIn other words, in the presence of fast buffering dynamics and relatively small amounts\nof calcium, the spatio-temporal dynamics of calcium ions are governed by the canonical\ndiffusion equation, except that the original diffusion coefficient is replaced by a smaller one,\n(11.46)\nDiffusion is slowed down, since the Ca2+ ions bind to the buffer and are therefore\nnot available to diffuse. In the case of intracellular cytoplasm containing calmodulin at\nTB = 100 /JiM (with Kj in the 5-10 [iM range), the effective diffusion coefficient Z)eff\nfor calcium is at least 10 times slower than the coefficient measured in aqueous solution, in\nagreement with the Hodgkin and Keynes (1957) experiment mentioned. Due to the square-\nroot relationship between time and distance, this translates into a substantially reduced\nability of calcium to act as a fast intracellular messenger if distances larger than a few\nmicrometers are involved.\nWe conclude that a nondiffusible buffer will always slow down the diffusive spread of\ncalcium.\n11.5.1 Experimental Visualization of Calcium\nTransients in Diffusion-Buffered Systems\nOne system where the observed calcium dynamics have been compared with numerical\ncalculations are bullfrog sympathetic ganglion cells. These relatively large, spherical cells\ndevoid of dendrites (described in Sec. 9.5) are an ideal test bed for applying confocal laser-\nscanned microscopy to record the dynamics of intracellular calcium using the fluorescent\nCa2+ indicator fluo-3. In a pioneering application of this technology, Hernandez-Cruz,\nSala, and Adams (1990) measured [Ca2+],- in these cells after application of a voltage-\nclamp pulse to briefly activate voltage-dependent calcium channels. Because of tradeoffs\nbetween temporal and spatial resolution of this method, they recorded the free calcium\nconcentration across a narrow one-dimensional slot that extends across the width of the\ncell, thereby achieving a 5 msec temporal resolution (Fig. 11.8).\nThe influx of calcium across the membrane leads to a wave of calcium (Fig. 11.8A and\nE). Although much of this calcium becomes bound to the resident buffers as well as to the\ncalcium indicator dye, enough remains for the calcium wave to reach the center of the cell\nafter about 300 msec. [Ca2+], then equilibrates across the cell (Fig. 11.8F, G, and H), slowly\nreturning to its resting state after 6-8 sec. These basic features were qualitatively reproduced\nby a radial reaction-diffusion model (Sala and Hernandez-Cruz, 1990). It is similar to the\nmodel of Yamada, Koch, and Adams (Sec. 9.5) except that it makes no attempt to simulate\nthe voltage dynamics and that it includes mobile buffers, that is, buffers that—with or"}
{"text": "===== Page 21 =====\n268\nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.8 COMPARING EXPERIMENTAL CALCIUM TRANSIENTS AGAINST A MODEL Experimental\nmeasurements of intracellular calcium dynamics in bullfrog sympathetic ganglion cells (Hernandez-\nCruz, Sala, and Adams, 1990) compared to numerical solutions of the associated one-dimensional\nradial reaction-diffusion equations (Sala and Hernandez-Cruz, 1990). The data were obtained by\nmonitoring the calcium dye fluo-3 along a thin strip across the diameter of these spherical cells with\nthe aid of confocal microscopy. Measured (A) and simulated (B) radial spread of Ca2+. Time extends\ndownward and space across. (C) A 100-msec-long voltage-clamp step to +10 mV is used to trigger\nan influx of Ca2+ via voltage-dependent calcium channels (Im; see also the vertical bars in A and B).\nCalcium concentration profiles as a function of space and time extracted from the data in A (dotted\ntraces) or the model in B (solid traces). (D) Resting calcium concentration. [Ca2+],- is reduced below\nthe membrane due to the presence of calcium pumps in the model. Spatial profile 100 msec (E) and\n1 sec (F) after the stimulus onset. Temporal dynamics of free calcium 2.5 /xm below the membrane\n(G) and at the center (H) of the 40-^tm-diameter cell. Reprinted by permission from Hernandez-Cruz,\nSala, and Adams, (1990)."}
{"text": "===== Page 22 =====\n11.6 Ionic Pumps \n• \n269\nwithout bound calcium—can diffuse inside the cell (with diffusion coefficients of 0.25 and\n0.1 /zm2/msec). This causes the decay of calcium below the shell to occur at two different\ntime scales (Fig. 11.8G): a fast one, over within 300 msec, which is due to the diffusional\nredistribution of Ca2+ throughout the cell, and a much slower phase, lasting for seconds,\nwhich reflects the slow buffering, the diffusion of the buffer, and the extrusion of calcium\nacross the membrane by the calcium pumps (see the following section).\nThe qualitative match between observation and model evident in Fig. 11.8 underscores\nthat the evolution of calcium in these simple structures can be understood in terms of\ndiffusion and buffering.\n11.6 Ionic Pumps\nThe crucial component of the system controlling the homeostasis of sodium, potassium,\nchloride, and calcium are specialized membrane-bound molecules that act as ionic pumps.\nAlso known as ion transporters, they maintain the ionic gradients across the membrane that\nenable neurons to signal and to generate and propagate action potentials. In conjunction with\nbuffers and other uptake systems, they also provide exquisite regulation of the intracellular\nconcentration of free Ca2+.\nThe single most important ion transporter is probably the Na+-K+ pump (Hille, 1992).\nIt is driven by the energy derived from hydrolysis of ATP. Three Na+ ions are pumped out\nof the cell for every two K+ ions moved into the cell. This results in a net accumulation\nof charge, that is, in a small but measurable current. The pump is therefore known as\nelectrogenic and is ubiquitous in the membranes of all cells, neuronal or not. Given the fact\nthat most cells have stable resting potentials that have to be continuously maintained in the\nface of EPSPs and IPSPs, action potentials, and so on, the Na+-K+ pump consumes a lot of\npower. Roughly half the metabolic energy used in the retina of a rabbit (Ames et al., 1992)\nand in the mammalian brain in general (Ames, 1997) has been attributed to it.\nTwo major transport systems are responsible for the net outward movement of Ca2+\nacross the neuronal membrane against the large concentration gradient (Dipolo and Beauge,\n1983; Blaustein, 1988; McBurney and Neering, 1987). One system, the Na+-Ca2+ ex-\nchanger, exploits the energy gained when moving three Na+ ions inward by moving one\ncalcium ion out of the cell, thereby generating one excess charge for each Ca2+ ion that is\nremoved. This pump has a maximal rate of Ca2+ removal of 2-3 nmol per square centimeter\nof membrane area per second and a Kj in the low micromolar range (that is, the pump is\noperating at half its maximum if [Ca2+],- = Kd). DiFrancesco and Noble (1985) have\ndeveloped a model of the cardiac Na+-Ca2+ exchanger, which has also been applied to\nneurons (Gabbiani, Midtgaard, and Knopfel, 1994).\nA second pump system requires energy in the form of one ATP molecule for each\ncalcium ion pumped out. This ATP-driven calcium pump can be considered to be a pump\nwith a higher affinity (Kd = 0.2 /uM) yet a lower capacity compared to the Na+-Ca2+\nexchanger, with a maximal rate of removal of about 0.2 nmol/cm2/sec. Its Ca2+ dependence\nis frequently approximated by a Michaelis-Menten equation (see below; Garrahan and Rega,\n1990). While both systems operate continuously, the ATP-driven pump can quickly turn on\nfollowing Ca2+ influx subsequent to an action potential, while the lower affinity but much\nhigher capacity system is primarily responsible for maintaining resting levels of Ca2+ over\nlonger times."}
{"text": "===== Page 23 =====\n270 \n• \nDIFFUSION, BUFFERING, AND BINDING\nFrom the point of view of charge entering or leaving the cell, both ionic pumps have to\nbe treated as ionic currents, the ATP-driven pump acting as an outward current (calcium is\nremoved from the cytoplasm) and the sodium-calcium exchanger acting as an inward current\n(since three positive charges are moved inside for every two charges being removed). In\ngeneral, their contributions will be small (but see DeSchutter and Smolen, 1998).\nOne simple way to model the dynamics of these pumps is via saturable first-order\nMichaelis-Menten kinetics,\n(11.47)\nwhere Pm is given by the number of calcium ions that can be pumped out per square\nmicrometer of neuronal membrane divided by Arf_pump. The 4/d factor takes account of\nthe fact that the molecules acting as the pump are inserted into the membrane area (length\nx?rd) that encloses the volume (length x nd2/4) containing the calcium ions (surface-\nto-volume ratio). The decrease in calcium concentration due to the action of the pump is\ndirectly proportional to the calcium concentration if [Ca +] <sC Kj.\n11.7 Analogy between the Cable Equation\nand the Reaction-Diffusion Equation\nAs witnessed in previous chapters, a great deal of knowledge and intuition has accumulated\nabout the behavior of the membrane potential in one-dimensional cables and dendritic trees.\nCan we transfer any of this to the solutions of the reaction-diffusion equations? In particular,\ncan we define appropriate space and time constants to characterize the spatio-temporal\ndynamics of calcium—or any other substance—in response to synaptic input? Drawing\nupon the study by Zador and Koch (1994), we show how the techniques developed for\none-dimensional cable theory can be applied to reaction-diffusion equations.\nOur starting point is the distribution of calcium ions in a cylinder following the influx of\na calcium current /ca(*> 0 across the membrane. This current can flow through voltage-\nor ligand-activated channels. As in one-dimensional cable theory, we neglect the radial\ncomponents of diffusion, assuming that their associated time constants are much, much\nfaster than the longitudional ones (e.g., Rail, 1969b).\nThe inflowing calcium ions diffuse to neighboring locations, bind to various buffers, and\ncan be pumped back out of the cable. The buffer itself can also diffuse with a diffusion\ncoefficient DB. The expressions governing the resulting change in the concentration of\ncalcium [Ca2+](x, t) and bound calcium-buffer [B • Ca](x, t) is\n(11.48)"}
{"text": "===== Page 24 =====\n11.7 Analogy between the Cable Equation and the Reaction-Diffusion Equation \n• \n271\nIn the first equation, the first term on the right-hand side corresponds to the diffusive\ncontribution to the change in calcium concentration, the second and third terms are caused\nby the removal of calcium due to its binding with the buffer, the fourth corresponds to the\nreduction in [Ca2+], due to the action of the calcium pump, and the last term converts\nthe inward (that is, negative) calcium current (carrying 2e charge per ion) into a calcium\nconcentration (the 4/d factor accounts for the area-to-volume ratio). The second equation\nspecifies the change in bound calcium-buffer concentration as a function of diffusion and\nbuffer binding and unbinding. The last equation stipulates that in the absence of any buffer\nsources and sinks and assuming that the buffer diffuses at the same pace as the bound\ncalcium-buffer complex, the total buffer concentration T-& is constant.\nNote the nonlinear coupling between the two variables (the [Ca2+] x [B] term), rendering\nthe solution to these partial differential equations difficult.\n11.7.1 Linearization\nAs we will show now, under certain limiting conditions, Eqs. 11.48 can be reduced to a\nsingle, linear partial differential equation, formally equivalent to the cable equation. This\nreduction is based on the instantaneous buffer assumption; that is, the kinetics of buffering\nare much faster than diffusion. Since the former occurs on a microsecond to millisecond\ntime scale (Falke et al., 1994) and the latter requires 10-100 msec, this is a very valid\nassumption and implies that\n(11.49)\nholds everywhere.\nFrom a mathematical point of view, Eqs. 11.48 constitute a singularly perturbed system,\nin which one variable evolves much faster than the others. Other instances of such systems\nare the Hodgkin-Huxley and the FitzHugh-Nagumo equations (Keener, 1988; Wagner and\nKeizer, 1994). The concentration of the bound buffer at any instant can be approximated\nby its steady-state distribution (Eq. 11.38),\n(11.50)\nAssuming furthermore that the calcium concentration is less than the Kj of the binding\nprocess (see Eq. 11.39) and that the pump is not saturated (that is, [Ca2+](;t, t) < Kd-pump)^\nEqs. 11.48 can be reduced (Zador and Koch, 1994; Wagner and Keizer, 1994) to\n(11.51)\nEquation 11.51 should be very familiar to us, since it is the cable equation in disguise. To\nrecall, the cable equation in an infinite cylinder in response to an injected current density\nAnj(*, t) is (see Eq. 2.7),\n(11.52)\nIf the following identifications are made, these two linear equations are identical:"}
{"text": "===== Page 25 =====\n272 . \nDIFFUSION, BUFFERING, AND BINDING\n(11.53)\nThat the pump acts like a membrane conductance R~l is straightforward enough to\nunderstand: the more pump molecules are present, the more calcium will \"leak\" out of the\ncell. Equation 11.51 was derived under a low calcium constraint. For higher concentrations,\nthe pump saturates (Eq. 11.47) and acts like a constant hyperpolarizing current, removing\ncalcium ions at a constant rate.\nJust as the axial resistance determines the spread of the voltage along the longitudinal\naxis, so does the diffusion constant determine the rate of calcium flux along the longitudinal\naxis (see also Fig. 11.3). The effect of a diffusible buffer is to increase the effective diffusion\nconstant by an additive term /3D% = T^D^/K^. There are now two sources of calcium\nmobility: direct diffusion of Ca2+ ions and diffusion of bound calcium riding \"piggyback\"\nalong with the buffer. The net effect of buffering and diffusion of the bound buffer can be\nexpressed by a revised effective diffusion \ncoefficient\n(11.54)\n(For a generalization, see Wagner and Keizer, 1994.) For the observed large binding ratio\nft, diffusion is dominated by the diffusion of the calcium-buffer complex.\nBecause calcium is measured using calcium-dependent fluorescent dyes, which them-\nselves act as buffers for calcium ions, the perturbation of the Ca2+ signals by the measure-\nment act must be taken into account (Neher, 1995).\nAs illustrated by Fig. 11.3, the basic diffusion equation includes an effective membrane\ncapacitance of unity. The effect of a fast buffer is to boost this capacitance by an additive\nterm given by the binding ratio ft. Similar to a capacitance, the buffer acts to slow down\nchanges in the calcium concentration. Note that the buffer does not affect the steady-state\ndistribution of calcium in response to a sustained calcium current injection /ca.\n11.7.2 Chemical Dynamics and Space and\nTime Constants of the Diffusion Equation\nFurther exploiting the analogy between the two equations (Zador and Koch, 1994; see also\nKasai and Petersen, 1994), we know that the response of the reaction-diffusion equation to\na stationary calcium current /ca in an infinite cable will be a decaying exponential, allowing\nus to define a space constant,\n(11.55)\nHere r-d stands for \"reaction-diffusion.\" We can also define a time constant associated with\nthe linearized reaction-diffusion equation (Eq. 11.51) as"}
{"text": "===== Page 26 =====\n11.7 Analogy between the Cable Equation and the Reaction-Diffusion Equation \n• \n273\n(see Table 11.2), with\n(11.56)\n(11.57)\nThis similarity allows us to apply the results we derived in Chaps. 2 and 3 directly to write\ndown equivalent expressions for the linearized reaction-diffusion equation. In particular, we\ncan introduce the transfer \"resistance\" Ktj, defined as the ratio of the sustained change in\ncalcium concentration at location j in response to the sustained calcium current /ca injected\nat location i. If locations ;' and j are a distance Xij apart in an infinite cylinder, we have\n(see Eq. 3.23),\n(11.58)\nwith the steady-state input resistance defined as (Eq. 3.24; see also Carnevale and Rosenthal,\n1992),\n(11.59)\nThe unit of the chemical input resistance is M/A (injecting so many amperes of current\nincreases the concentration by so many molar). From all of this we can infer a number of\ninteresting facts.\n1. As pointed out above, the buffering scheme will only affect the transient behavior, not\nthe sustained response, acting like a capacitance. The more buffer that is present, the\nlarger TB and therefore fi and thus the longer r,._d.\n2. The scaling behavior of the sustained response in a cable of diameter d is identical to that\nof A and KH for the cable equation (Fig. 11.9). Because the constant in front of the pump\nterm scales with the ratio of surface area to volume, that is, as 4/d, and D, DB and ft are\nindependent of d, Xr^d scales as \\fd and KH as d~3/2. Thus, injecting a calcium current\ninto a small volume will give rise to a much larger change in calcium concentration than\ninjecting the identical current into a larger volume.\n(11.60)\n3. What does scale differently is the time constant. While rm is independent of the radius of\nthe neuronal process, T,._</ increases linearly with d (Eq. 11.56 and Fig. 11.9). Calcium"}
{"text": "Definition of space and (ime constants as well as the steady-state input resistance for the cable and the linearized reaction-diffusion\nequation for an infinite cylinder. j8 = TB/KJ characterizes the buffer and Pm the ionic membrane pump. See Zador and Koch\n(1994). ===== Page 27 =====\n274 . \nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.9 SPACE AND TIME CONSTANTS AS A FUNCTION OF CABLE DIAMETER \n(A) Space and\n(B) time constants for the linearized reaction-diffusion equation (Eq. 11.51) (solid line) and the cable\nequation (Eq. 2.7) (dashed line) as a function of the diameter of the infinite cable. Notice that the\nelectrical space constant is much larger than the chemical space constant, with important functional\nconsequences. While this is also true for the time constants for thin fibers, the time constant of the\nreaction-diffusion equation scales with the diameter and can therefore exceed im in large structures.\nThe parameters are chosen to mimic the binding of calcium to calmodulin. Numerical values are\ndefined in Table 11.3. Reprinted by permission from Zador and Koch (1994).\ndynamics will be slower in thicker cables than in thinner ones. (We are not, of course,\naccounting for radial diffusion within the cylinder, since we are only considering the\none-dimensional diffusion equation.)\n4. Table 11.3 lists some typical values for the space and time constants and the input\nresistance in an infinite cable for both Eqs. 2.7 and 11.51. What is immediately apparent\nis the substantial difference between the space constant of the cable equation and that of\nthe reaction-diffusion equation,\nIn other words, while voltage can act over substantial distances, the effect of the\nconcentration of calcium (or other second messengers) is much more local. Reducing\nthe density of calcium pumps by one, two, or even three orders of magnitude does not\naffect this difference dramatically. Ultimately this is due to the fact that the membrane\nconductances dominating the resting levels of the membrane potential (mainly K+"}
{"text": "===== Page 28 =====\n11.7 Analogy between the Cable Equation and the Reaction-Diffusion Equation\n275\n{\n  \"table_name\": \"TABLE 11.3\",\n  \"description\": \"Numerical Values of X, r, and KU for the Linearized Reaction-Diffusion and Cable Equations\",\n  \"columns\": [\"Diameter\", \"Xr_rf\", \"rr_</\", \"KU (r — d)\", \"X\", \"rm\", \"KU\"],\n  \"rows\": [\n    [0.1, 0.49, 1.38, 84.6, 224, 20, 14200],\n    [1.0, 1.54, 13.8, 26.8, 707, 20, 450],\n    [10.0, 4.88, 137.5, 0.085, 2236, 20, 14.2]\n  ]\n}"}
{"text": "Space and time constants and the sustained input resistance of the reaction-diffusion and the cable equations in an infinite cable\nof indicated diameter (in /j,m) for calcium (with D = 0.6 ^,m2/msec) binding to diffusible calmodulin (with TB = 100 p.M,\nKj = 10 \\iM, ft = 10, and £>B = 0.13 jitni2/msec) and in the presence of a high-affinity calcium pump (Pm = 0.2 ;im/msec).\nThis is compared against a standard dendritic cable with R, = 100 £2-cm, Rm = 20,000 Q-cm2 and Cm = 1 pf/cm2. Notice the\ndramatic difference between k,-d and A. and the different scaling behavior of rr_<; and rm. The space constants are in units of pm\nand the time constants in msec. The chemical input resistance is in units of nM/f A and the electrical input resistance in Mfi.\nconductances) are small relative to the interaxial resistance, while relatively more Ca2+\nions pass through the membrane via the pumps than flow longitudinally.\n5. The dynamics of the two processes are comparable for small cylinders, but can differ\ngreatly—due to the dependency of Tr~d on the geometry—for thicker dendrites (Ta-\nble 11.3 and Fig. 11.9). Due to the smaller surface-to-volume ratio, the time constant of\nthe reaction-diffusion equation can be quite slow, in the hundreds of milliseconds.\nHow well does the linearized reaction-diffusion equation hold up in practice? This was\nevaluated by observing the spread of calcium along the axon of an Aplysia neuron (Gabso,\nNeher, and Spira, 1997). Following the injection of calcium from a micropipette into the\naxon, calcium was tracked with the help of another fluorescent dye, fura-2, for tens of\nseconds as the calcium ions diffused away from the injection site for a few hundreds of\nmicrometers.\nAs expressed by Eq. 2.31 and illustrated in Fig. 2.7A, the impulse response function of\nthe cable equation in an infinite cylinder for any fixed point in time is a Gaussian. This also\nholds true for the linearized reaction-diffusion system we are considering,\n(11.61)\nwhere the effective diffusion coefficient L>eff is defined in Eq. 11.57. Gabso, Neher, and\nSpira (1997) fit a Gaussian through the spatial profile of the calcium signal at different times\n(Fig. 11.10A). If the spatio-temporal dynamics of [Ca2+] follow Eq. 11.51, then the square\nof the standard deviation of the Gaussian should increase linearly in time, which it does\n(Fig. 11.1 OB). The slope of the curve gives the effective diffusion coefficient of calcium\nand the calcium bound to any buffers intrinsic to the axon as well as to the fura-2. Note that\nin this study, the calcium signal was purposely kept below 0.5 [iM.\nWhile the analogy between the cable equation and the reaction-diffusion equation breaks\ndown for large values of calcium concentration and for more complex buffering schemes,\nthe behavior of [Ca2+] will not deviate qualitatively from that described by the linear\nEq. 11.51.\nThis is demonstrated in Fig. 11.11, which plots the normalized calcium concentration\nat two locations in an infinite cable for a series of calcium injections. For small currents,\nthe system operates in the low calcium limit, and the dynamics of [Ca2+], obtained by\nsolving the nonlinear coupled partial differential Eqs. 11.48, are fitted well by the linear"}
{"text": "===== Page 29 =====\n276\nDIFFUSION, BUFFERING, AND BINDING\nFig. 11.10 CALCIUM SPREAD ALONG AN AXON\nExperimental determination of the calcium signal\n(recorded using the calcium-dependent fluorescent\ndye fura-2) following a brief intracellular injection\nof calcium at one point into the axon of a cultured\nmetacerebral Apfysia neuron by Gabso, Neher, and\nSpira (1997). (A) Spatial profile of the [Ca2+];\nsignal for different times. A baseline value was\nsubtracted from each curve so that they all approach\nzero for large distances. Images were acquired ev-\nery 1.8 sec, with the second, third, fifth, and seventh\nmeasurements displayed in the enlarged inset. A\nGaussian was fitted through each curve (smooth\nlines in the inset). (B) The square of the standard\ndeviation of the Gaussian was plotted as a function\nof time. If the spatio-temporal calcium dynamics\nfollows the linearized reaction-diffusion equation\n(Eq. 11.51) these points should fall on a straight\nline whose slope is the effective diffusion coeffi-\ncient Deff of free and bound calcium, here equal\nto 0.112 jiinWmsec. Reprinted by permission from\nGabso, Neher, and Spira (1997).\napproximation of Eq. 11.51. As the current is made larger, the buffer saturates and becomes\nineffective, since it no longer absorbs any of the inflowing calcium ions. Indeed, following\nEqs. 11.37 and 11.50, at high calcium concentrations the entire buffer concentration is taken\nup by the bound calcium-buffer complex. Formally, this corresponds to f) = 0 in Eq. 11.51,\nwhich explains the greatly spedup chemical dynamics (Tr~d decreases by about one order\nof magnitude). However, the important point to note is that the behavior of the solutions to\nthe full equations is bracketed by the solutions to the linear equation with f) = 0 and 10,\nwithout deviating in any significant way from them (for instance, they are all monotonic,\nsaturating functions).\nThe morale is that without further significant nonlinearites, our conclusions regarding\nthe space and time constants associated with the reaction-diffusion equation do not change\ndramatically.\n11.8 Calcium Nonlinearities\nIt is known that a number of different cell types exhibit all-or-none calcium events that occur\nover and over again (Berridge, 1990; for an excellent review of this topic see Meyer and\nStryer, 1991). Such oscillations in the calcium concentration, whose duration lasts on the\norder of seconds or longer, can be observed in response to hormones. They have very sharp\nonsets and vary in frequency in a monotonic manner with the concentration of the hormone.\nAlthough these calcium spikes are on the order of three to four orders of magnitude slower\nthan voltage spikes, they do share a number of features with their faster cousins, in particular\nbeing caused by a highly nonlinear positive feedback mechanism. Indeed, equations similar\nto the Hodgkin-Huxley equations have been used to replicate these experimental findings\nin a computer model (Meyer and Stryer, 1991; Ogden, 1996)."}
{"text": "===== Page 30 =====\n11.9 Recapitulation\n277\nFig. 11.11 EFFECT OF BUFFER SATURATION ON CALCIUM DIFFUSION \nEffect of saturating the\nbuffer in an infinite cable. Plotted is the normalized calcium concentration (A) at the origin of the\ncurrent input as well as (B) one space constant kr-d away. The dashed curves represent solutions\nto the linearized reaction-diffusion equation (Eq. 11.51) for /8 = 10 (lower dashed curve) and for\nthe fully saturated buffer (corresponding to ft = 0 in Eq. 11.51). The solid curves correspond to the\nsolution of the nonlinear coupled Eqs. 11.48 for a peak calcium current of 10~4, 10~3, 10~2, and\n10~' nA amplitude (from bottom to top). The behavior of the full system is bracketed by the two\nlinear solutions; no qualitatively new behavior appears. Reprinted by permission from Zador and\nKoch (1994).\nIn some model systems, waves of increased [Ca2+],- have been observed (Cornell-Bell\net al., 1990; Dupont and Goldbeter, 1992; Sanderson, 1996). Applying the excitatory neu-\nrotransmitter glutamate to hippocampal astrocytes—non-neuronal supporting cells, which\noccur in great numbers throughout the brain—triggers such calcium waves, propagating at\nconstant speeds of about 20 /urn/sec throughout the cytoplasm of the astrocyte. Frequently,\nthese waves propagate across adjacent astrocytes. The possible significance of such waves\nfor neuronal signaling is not known.\nWhile such nonlinear calcium events are quite intriguing, their long time scale makes\nit unlikely that they play a significant role in the rapid computations that we are primarily\ninterested in.\n11.9 Recapitulation\nDiffusion is a fundamental fact of life for molecules in the intracellular or extracellular\ncytoplasm. Through its random action, it acts to move substances throughout the cell."}
{"text": "===== Page 31 =====\n278 \n• \nDIFFUSION, BUFFERING, AND BINDING\nFrom a computational point of view, the most important fact about diffusion is that it\nplaces strong constraints on how rapid calcium or other second messenger molecules can\naffect things far away. The distance over which some concentration increase diffuses is\nproportional to the square root of the time that has passed. In the absence of any calcium\nnonlinearities and active transport processes, this square-root law fundamentally limits the\nability of the calcium signal to implement the fast type of information processing operations\nrequired for many perceptual, cognitive, or motor tasks. Recognizing a friend's face, shifting\nvisual attention from one location to a neighboring one, or raising one's hand to catch a ball\ncan all be accomplished within a few hundred milliseconds.\nThe calcium that rushes into the cell via ionic channels is tightly regulated. The vast\nmajority is bound to a host of intracellular buffers, such that only one out of 20 Ca2+ ions\nis free to interact with other molecules, severely limiting the effective diffusion coefficient\nof calcium.\nWhile calcium ions diffuse along some process, their concentration rapidly decreases.\nThis is especially true for a substance that diffuses in the three-dimensional extracellular\ntissue; its spatial concentration profile decreases sharply with distance from the source (as\ne~r ). A relevant case are certain unconventional neuroactive substances, such as nitric\noxide, that can diffuse across the membrane cytoskeleton (Sees. 20.2 and 20.3).\nOf course, these constraints do not argue against the use of the local intracellular calcium\nconcentration for computing and for short-term memory storage (Sobel and Tank, 1994).\nWhen calcium and its protein targets are in close spatial proximity, the rate at which calcium\ncan bind to this protein limits the speed of the computational operation being implemented\n(Sec. 20.1). This allows chemical switching to proceed in the submillisecond domain. Using\nconcentration changes for implementing rapid operations does impose stringent conditions\non a fast local input and a fast local read-out mechanism.\nIn general, movements of ions due to the inhomogeneous distribution of the various\nrelevant ions (Ca2+, Na+, K+ and Cl~) must be incorporated into the cable equation,\nleading to the Nernst-Planck electrodiffusion equation. However, as long as the diameter of\nthe neuronal process is above a fraction of a micrometer, this equation is well approximated\nby the cable equation. Only when studying very small processes, such as dendritic spines\nor very thin dendrites, does the longitudinal diffusion of the carriers need to be taken into\naccount.\nIf the buffering reaction is substantially faster than diffusion and if the calcium con-\ncentration is small (technically, if [Ca2+],- < Kj of the pump and of the buffer), the\ncoupled system of reaction-diffusion equations can be reduced to a single linear partial\ndifferential equation, which is formally equivalent to the cable equation. This allows us\nto define space and time constants and input resistances in analogy to these parameters in\npassive dendrites. One important insight is that A.r_,/ <g; A., implying that from the point\nof view of spatial compartmentalization, the presence of reasonable amounts of calcium\npumps and buffers in the dendritic tree will fractionate the tree into a series of small\nand relatively independent compartments. In each of these subunits, independent calcium-\ninitiated chemical computations could be carried out. This is in contrast to the relatively\nsmaller attenuation experienced by the membrane potential in a dendritic tree. It may well\nbe possible that the architecture and morphology of the dendritic tree reflects less the need\nfor electrical computations but more its role in isolating and amplifying chemical signals.\nWe will study a beautiful instance of this in the following chapter on dendritic spines.\nBecause of the mathematical equivalence between electrical, chemical, and even bio-\nchemical networks (Busse and Hess, 1973; Eigen, 1974; Hjelmfelt and Ross, 1992; Barkai"}
{"text": "===== Page 32 =====\n11.9 Recapitulation \n• 279\nand Leibler, 1997) that derives from their common underlying mathematical structure,\nappropriate sets of reaction-diffusion systems can be devised that emulate specific electrical\ncircuits. In principle, computations can be carried out using either membrane potential as\nthe crucial variable—controlled by the cable equation—or concentration of calcium or\nsome other substances—controlled by reaction-diffusion equations (for examples of this,\nsee Poggio and Koch, 1985). The principal differences are the relevant spatial and temporal\nscales, dictated by the different physical parameters, as well as the dynamical range of the\ntwo sets of variables. Given neuronal noise levels, the membrane potential can be considered\nto vary by a factor of hundred or less, while the concentration of calcium or other substances\ncan vary by three or more orders of magnitude during physiological events."}
{"text": "===== Page 4 =====\n12.1 Natural History of Spines\n283\nFig. 12.2 STEREO VIEW OF A SPINY DENDRITE \nStereoscopic view of a portion of a spiny dendrite\n(of the type shown in Fig. 12.1) in the rat neostriatum. The large number of spines springing from\nthe dendrite can be viewed in depth by focusing the eyes at a point at infinity (that is, by looking\nthrough the image). The reconstruction has been performed using high-voltage electron-microscopic\naxial tomography from a single 3-^tm-thick section. The cube has a dimension of 0.5 p,m on each\nside. Reprinted by permission from Wilson et al., (1992).\nFig. 12.3 DENDRITIC SPINES IN THE HIPPOCAMPUS\nThree-dimensional reconstruction of a piece of dendrite\nfrom a CA1 pyramidal cell of the rat hippocampus. There\nare between one and three spines per micrometer of den-\ndrite. The diversity in the morphologies and dimensions\nof spines is striking. Reprinted by permission from Harris\nand Stevens (1989).\net al., 1995). Larger spine heads are associated with larger synapses—as measured by the\nsize of the associated postsynaptic density—and more vesicles in the presynaptic axonal\nvaricosity.\nThese dimensions indicate how spines bridge the gap between molecular and cellular\nscales: at a resting concentration of 80 nM and for the average spine head volume of\n0.05 //,m3, only about two unbuffered Ca2+ ions are expected to be found in a spine head.\nDendritic spines appear to be filled with complex cellular machinery (for an overview, see\nHarris and Kater, 1994), most notably a specialized form of smooth endoplasmic reticulum\ntermed the spine apparatus. The membranes making up the spine apparatus are closely\napposed to the plasma membrane of the spine neck and appear to sequester calcium (Fifkova,\nMarkham, and Delay, 1983; Burgoyne, Gray, and Barron, 1983)."}
{"text": "===== Page 5 =====\n284 . \nDENDRITIC SPINES\nType of Spine \ndn (nm) \nL (jirn) \nArea (jim2) Volume (urn3)\nThin \n0.10 ±0.03 0.98 ±0.42 0.59 ±0.29 \n0.04 + 0.02\nMushroom \n0.20 ±0.07 1.50 + 0.25 2.70 ±0.93 \n0.29 ±0.13\nStubby \n0.32 + 0.13 0.44 + 0.15 0.45 ±0.14 \n0.03 + 0.01\nFig. 12.4 DIVERSITY OF SPINE SHAPES Three most frequent types of spine shapes in the cortex with\nsome of their dimensions: spine neck diameter dn, total length L, surface area, and volume. The length\nof the spine neck /„ is 0.51 ± 0.34 /zm for thin spines and 0.43 ± 0.21 /urn for mushroom-type spines.\nAll values are averages and standard deviations. Black areas indicate the postsynaptic densities (PSD)\nof the asymmetric excitatory synapses on the spines. Frequently more than one synapse is located on\na spine. Stubby spines are also known as \"sessile.\" The data are measured from electron-microscopic\nreconstructions of spines on adult rat CA1 hippocampal pyramidal cells. Reprinted in modified form\nby permission from Harris, Jensen, and Tsao (1992).\nOther cellular organs, such as mitochondria, microtubules, or ribosomes, are usually\nabsent. An exception are the spines on granule cell dendrites, which make reciprocal dendro-\ndendritic synapses with projection neurons in the mammalian olfactory bulb (Rail et al.,\n1966; Cameron, Kaliszewski, and Greer, 1991; Sec. 5.3). Here the mitochondria are most\nlikely needed to provide energy to subserve presynaptic functions. Although spines lack\nneurofilaments, spine heads contain a dense network of actin filaments (Fischer et al., 1998).\nIn the neck, actin filaments are oriented lengthwise along the spine apparatus (Fifkova,\n1985). A number of proteins known to be involved in actin-mediated activities, such as\nneuronal myosin (Drenckhahn and Kaiser, 1983), fodrin (Carlin, Bartelt, and Siekevitz,\n1983) and calmodulin (Caceres et al., 1983), have been found in dendritic spines.\nThe close association of spines with the terminal boutons of axons prompted early specu-\nlation that spines might conduct impulses between neurons (Ramon y Cajal, 1909). Electron\nmicroscopic studies have since confirmed that spines are indeed the major postsynaptic\ntarget of excitatory (asymmetric, type I) synaptic input (Gray, 1959). For instance, in cat\nvisual cortex, 93% of the afferent geniculate terminals in layer 4 is located on spines (LeVay,\n1986), while a similarly high percentage is observed in the Schaffer projection from CA3\ncells onto CA1 pyramidal cells in the hippocampus. Note, however, that not all excitatory\npathways terminate on spines. For instance, about 70% of the excitatory synapses from\nlayer 6 pyramidal cells terminates directly on the dendrites of layer 4 spiny stellate cells,\nand only a bit less than one-third of layer 6 synapses chooses spines as their termination\nzones (Ahmed et al., 1994). As mentioned before, excitatory projections onto inhibitory\ncells always make a synapse directly onto the dendrites since these cells lack spines.\nInterestingly, the association between spines and synapses is limited to excitatory traffic.\nInhibitory (symmetric, type II) profiles are only observed at a small fraction (between 5\nand 20%) of spines in the cortex, but usually in conjunction with excitatory synapses and"}
{"text": "===== Page 6 =====\n12.2 Spines only Connect \n• \n285\nnever by themselves (Jones and Powell, 1969; Dehay et al., 1991; Fifkova, Eason, and\nSchaner, 1992).\n12.1.3 Induced Changes in Spine Morphology\nBoth the absolute number and the shape of spines can change—sometimes quite drastic-\nally—in the young and in the mature animal, and also as a function of external stimulation.\nDuring development, absolute spine density can more than double relative to densities\nin the adult. Yet this increase does not occur homogeneously across all spines. Rather,\nthin, branched, and certain types of mushroom spines increase fourfold in density (between\n2 weeks postnatal and adulthood in the rat hippocampus), while stubby spines decrease\nby more than half (Harris and Stevens, 1989; Harris, Jensen, and Tsao, 1992; see also\nSchiiz, 1986; Papa et al, 1995; and Fig. 12.4). In adult female rats, the dendritic spine\ndensity on CA1 hippocampal pyramidal cells varies by 30% or more over the five-day\nestrus cycle, while pyramidal cells in the CAS region show little statistically significant\nvariation (Woolley et al., 1990). It is not known whether the number of synapses also varies\nby this fraction, or whether it is just the spines that wax and wane, but this certainly makes\nfor a very dynamic environment.\nOther studies have shown that the shape of spines—in particular the length and diameter\nof the neck—can change in response to behavioral or environmental cues such as light,\nsocial interaction, or one-shot learning or exploratory motor activity (Purpura, 1974; Coss\nand Globus, 1978; Bradley and Horn, 1979; Brandon and Coss, 1982; Rausch and Scheich,\n1982; Lowndes and Stewart, 1994; Moser, Trommald, and Andersen, 1994).\nAs discussed in the following chapter, long-term potentiation (LTP), the best studied\nform of synaptic plasticity, is induced by brief high-frequency electrical stimulation. In\nthe hippocampus, stimulating cells in this manner causes alterations in the spine structure\n(Van Harrefeld and Fifkova, 1975; Lee et al., 1980; Greenough and Chang, 1985; Desmond\nand Levy, 1990; Calverly and Jones, 1990; Harris, Jensen and Tsao, 1992). Some of the\nreported changes include larger spine heads, distortions in the shape of the spine stem,\nan increased incidence of concave spine heads, and an increase in the number of shaft\nsynapses. It is not known what role—if any—the changes in spine shape play in the\nincrease in synaptic efficacy (see also Desmond and Levy, 1988). Remarkably, some of\nthese changes in spine shape occur within seconds, mediated by actin filaments (Fischer et\nal., 1998).\n12.2 Spines only Connect\nMost of the early hypotheses considered the establishment of physical contact with presy-\nnaptic terminals as the main function of spines. (For a modern view of this, see Swindale,\n1981.) It was argued that because dendritic space is scarce, spines provide additional\nmembrane areas for synapses to make contact with. This idea has been largely dismissed,\nhowever, because electron microscopic views of spiny dendrites show that the dendritic\nmembrane between spines often lacks synapses.\nOn the basis of their three-dimensional electron-microscopic reconstructions, Harris\nand Stevens (1988a) estimate that 29-45% of the dendritic membrane area of Purkinje cells\nwould have been covered by synapses if all spines had been deleted and the associated\nsynapses moved onto the dendrites. For CA1 pyramidal cell dendrites, only 5-9% of the\ntotal dendritic surface area would have been covered by the spine synapses (Harris and"}
{"text": "===== Page 7 =====\n286 \n• \nDENDRITIC SPINES\nStevens, 1988a), arguing against the hypothesis that spines are necessary to supplement\nthe dendritic membrane area available for synaptic contacts. This is corroborated by the\nobservation (Schiiz and Dortenmann, 1987) that the density of excitatory synapses on\nnonspiny dendrites in the cortex exceeds the spine density of pyramidal cell dendrites.\nThese results do not, however, address whether spines play a role in streamlining the\nlayout of axonal and dendritic processes in the three-dimensional neuropil. It is certainly\ntrue that for a fixed dendritic radius, spiny dendrites sample a larger brain volume than\ndendrites devoid of spines (indeed, the latter tend to be thicker than the former; Harris and\nKater, 1994).\nIn order to understand the functional theories that have been proposed to explain\nthe existence of spines, we need to study their electrical properties and their ability to\ncompartmentalize various important biological molecules.\n12.3 Passive Electrical Properties of Single Spines\nRail (1974, 1978) was the first researcher to analyze the properties of passive spines,\nfollowed by many others (Wilson, 1984; Turner, 1984; Koch and Poggio, 1983a,b; Brown\net al., 1988). We here follow the derivation of Koch and Poggio (1983b).\n12.3.1 Current Injection into a Spine\nThe principal idea can be understood in terms of the simplified electrical circuit of a passive\nspine attached to a dendrite (Fig. 12.5). Any current injected into the spine head must flow\neither across the spine head resistance R/, and capacitance Cf, or down through the spine\nneck impedance and into the dendrite. Neglecting capacitive and cable properties of the\nneck—due to its tiny size—we model the spine neck resistance as that of a cylinder of\nlength /„, diameter dn, and resistivity /?,-,\n(12.1)\nThe input impedance of the spine head membrane can be described by the complex function\n(12.2)\nwhere rm = RhCh = RmCm. Because RH is inversely proportional to the spine head area,\nfor average spine dimensions, Rh > 1000 GQ.\nThe input impedance at the spine head Ksp<sp(f) is determined by current flowing either\nthrough the spine head or through the spine neck. Remembering the way parallel resistances\nadd, we have\n(12.3)\nwhere Kdd(f) is the dendritic input impedance. Because for the entire relevant frequency\nrange (up to hundreds of kilohertz) the spine head impedance is so much bigger than the\nsum of the spine neck resistance and the dendritic input impedance, we obtain the simple\nadditive relationship between spine and dendritic input impedance,\n(12.4)"}
{"text": "===== Page 8 =====\n12.3 Passive Electrical Properties of Single Spines \n• \n287\nFig. 12.5 ELECTRICAL MODEL OF A PASSIVE SPINE \n(A) Schema of a spine with a single synaptic\ninput on a passive dendrite. Drawn to scale, the dendrite is 0.63 /u.m thick, the spine head has a diameter\nof 0.7 ^m, and the spine neck has dimensions of dn =0.1 fj,m and /„ = 1 /xm. With /?; = 200 £2-cm,\nthe spine neck resistance is 254 MS (Eq. 12.1). With a spine neck half as thick, Rn quadruples to just\nbeyond 1 G£2. (B) Lumped electrical model of such a spine, described by an RC \"head\" compartment\nattached through an ohmic neck resistance Rn to the parent dendrite. Due to its small size, the cable\nand capacitive properties of the neck can be neglected.\nSolving directly for the transfer impedance from the spine to the parent dendrite on the basis\nof Fig. 12.5 while assuming that no current loss occurs across the spine head membrane up\nto very high frequencies, leads to\n(12.5)\nNumerically, the two last equations hold to within a small fraction of a percent (Koch and\nPoggio, 1983b). By exploiting transitivity (Eq. 3.29), we can write an expression for the\ntransfer impedance from the spine to any point i in the dendritic tree as\n(12.6)"}
{"text": "===== Page 9 =====\n288 \n• \nDENDRITIC SPINES\nIn other words, the depolarization at any point i in a passive dendritic tree due to a spine\ninput is identical to the EPSP caused by the same current injected into the dendrite at the\nbase of the spine. Or, put more succinctly, electrically speaking, \"spines don't matter in the\nlinear case\" (Koch and Poggio, 1985c). The reason why any current injected into the spine\nwill reach the dendrite is that practically no current is lost across the membrane of the spine\nhead and neck.\nHowever, it should be noted that the membrane potential in the spine neck might need to\nexceed some critical threshold value to be able to initiate some biochemical event (such as\nthose leading to long-term potentiation, as described in Chap. 13). If a synapse were made\ndirectly on the dendrite, it would require a far larger conductance change to achieve the\nsame EPSP amplitude than a synapse on a spine. Thus, spines might matter for essentially\n\"local\" reasons (see below and Segev et al., 1995).\n12.3.2 Excitatory Synaptic Input to a Spine\nAs discussed above, spines always appear to carry at least one excitatory synapse (Fig. 12.5).\nWe can express the current flowing across this synapse as\n(12.7)\nwhere Vsp is the EPSP at the spine head. Applying Ohm's law, we arrive at\n(12.8)\nDealing first with stationary (or slowly varying) synaptic input, we can express the\nsteady-state amplitude of the spine EPSP as\n(12.9)\nwhere Ksp^sp corresponds to the amplitude of the steady-state spine input resistance. By the\njudicious use of Ohm's and Kirchhoff 's laws, we can derive a similar expression for V^,\nthe EPSP in the dendrite just below the spine,\n(12.10)\nLet us consider these equations in the case of very small and very large synaptic input.\nIf the product of the synaptic conductance change gsyn and the spine input resistance\nKsp<sp is much less than 1, the numerator in Eqs. 12.9 and 12.10 can be approximated by 1\nand the spine and dendritic EPSPs can be expressed as\n(12.11)\nand\n(12.12)\nIn other words, if the synaptic-induced conductance change gsyn is small relative to the\nspine input conductance ]/KspiSp, the action of the synapse can be approximated by a\nconstant current source of amplitude gsynEsyn. As we saw above, spines do not matter in\nthis limit. More specifically, changes in spine morphology have no effect on the dendritic\npotential just below the spine. Note, however, that this does not imply that the spine EPSP is\nidentical to the dendritic EPSP; they are not. Indeed, the dendritic EPSP will be attenuated"}
{"text": "===== Page 10 =====\n12.3 Passive Electrical Properties of Single Spines \n• \n289\nwith respect to the spine EPSP by a factor of Vj/Vsp = Kddl(f-dd + Rn) (Kawato and\nTsukahara, 1983; Turner, 1984; Koch and Poggio, 1985c).\nAt the other extreme, if gsyn is large (that is, if gsyn KspiSp 3> 1), the voltage at the spine\nhead saturates and the spine potential converges toward the synaptic reversal potential\n(12.13)\nwhile the dendritic EPSP converges to\n(12.14)\nBecause in this regime the synapse-spine complex acts as a voltage—rather than a current—\nsource, that is, like a battery, changes in the geometry of the spine neck will—via changes in\n/?„—affect the amount of synaptic current entering the spine head. Under these conditions,\nchanges in spine morphology will change the effective \"weight\" of the synapse.\nFigure 12.6A indicates graphically how stretching or squishing the spine neck, keeping\nits membrane area constant, affects the somatic EPSP for a sustained synaptic input of\ngpeak = 1 nS. Spine geometry only plays a role for very thin and elongated spines, when\ngpeak > l/Rn (the curve bends around dn = 0.05 /^m, where gpeak & !//?„). For transient\ninputs, very similar considerations apply (Fig. 12.6A and B). Owing to the very small\nmembrane area of the spine head, the total spine capacity is exceedingly small, leading to\nvery large impedance values even at high frequencies. Thus, relative independence of the\ndendritic membrane potential for small conductance inputs and saturation for large inputs\nalso hold for very rapid inputs (Fig. 12.6). Spine neck geometry can affect the weight of\nthe synapse significantly if the spine starts out very thin and elongated and becomes very\nshort and chubby. (However, a factor of 5 change of the somatic EPSP requires an order of\nmagnitude change in both /„ and dn.)\nDepending on the product of the synaptic conductance change and the spine input\nresistance, gSynKSp,sp = gsyn(Kdd + ^«)» the spine-synapse complex will thus tend to\nact either more as a current or as a voltage source, constraining the extent to which the\nmorphology of the spine can alter the synaptic efficiency of the synapse. An additional\nrequirement for the spine shape to influence the synaptic weight is that Rn needs to be large\ncompared to the dendritic input resistance. Otherwise, modulating /?„ will have little or no\neffect on Ksp,sp (Eq. 12.10).\nWe can now pose the critical question: are spines elongated and thin enough to be able to\neffectively modulate synaptic weight? Technical advances in the last decade have provided\nsufficient data to estimate the spine operating regime. Probably the most complete data are\navailable at the Schaffer collateral input to region CA1 pyramidal cells in the hippocampus.\nHere, the experimental estimates of gsyn are 0.05-0.2 nS for the fast voltage-independent\nAMPA synaptic component (Bekkers, Richerson, and Stevens, 1990; Malinow and Tsien,\n1990) and less than 0.5 nS for the NMDA component (Bashir et al., 1991).\nBased on their reconstruction of spines in the same region, Harris and Stevens (1988b,\n1989) estimate the spine neck conductance Gn — !//?„ to lie mainly between 18 and\n138 nS, making the critical ratio gsyn/Gn very small. Even taking account of the spine\napparatus, the smooth endoplasmic reticulum that partly occludes the spine neck, does not\nchange this conclusion appreciably. Svoboda, Tank, and Denk (1996), on the basis of high-\nresolution two-photon microscopy, directly estimated the diffusive and resistive coupling\nbetween the spine head and its parent dendrite, concluding with a lower bound on Gn\nof7nS."}
{"text": "===== Page 11 =====\n290\nDENDRITIC SPINES\nFig. 12.6 How Do CHANGES IN SPINE NECK GEOMETRY AFFECT SYNAPTIC WEIGHT? \nNeck\nlength ln of a single spine is varied in the presence of a single fast ((peak = 0.5 msec) voltage-\nindependent excitatory synapse (gpeak = 1 nS) on a spine along the upper portion of the apical tree in\nthe layer 5 pyramidal cell (around location a 1 in Fig. 3.7). We here assume that the total surface area\nof the spine neck remains constant (at 0.157 /xm2) as dn (upper part of label on x axis) is changed,\nimplying that the spine neck length /„ (lower part of label on x axis) changes inversely. It follows from\nEq. 12.1 that the spine neck resistance scales as d~3. The resultant EPSP is computed at the spine head\nitself (in A) as well as at the soma (in B). Experimental estimates of the spine neck resistance Rn are\nbelow 150 M£2, corresponding here to values of dn > 0.15 /xm, falling on the portion of the curve that\nis relatively flat. Given the large spine neck conductance relative to the peak synaptic conductance\nchange (estimated to be about 0.2 nS for the AMPA component), changes in spine geometry are\nlikely to have only a minor effect on the size of the associated EPSP. In other words, postsynaptic\nchanges in spine geometry are unlikely to implement synaptic \"weight\" changes. The dotted curve in A\ncorresponds to the steady-state change in membrane potential in response to a sustained conductance\ninput of the same amplitude as the transient input. When the spine is shielded from any dendritic\ncapacitance, that is, for dn -> 0, the capacitance of the spine itself can be neglected."}
{"text": "===== Page 12 =====\n12.3 Passive Electrical Properties of Single Spines \n• \n291\nIn agreement with earlier estimates of spine dimensions (Wilson, 1984; Turner, 1984;\nBrown et al., 1988), we conclude that the hippocampal projection onto spines on CA1\npyramidal cells acts as a current source and that changing the morphology of these spines\nmost likely does not affect the associated dendritic EPSP. A similar conclusion has also\nbeen reached for a reconstructed spiny stellate cell in somatosensory cortex (Segev et al.,\n1995).\nA different way to understand this is to ask: What effective synaptic conductance does an\nexcitatory synapse directly on the dendrite need to possess in order to inject the same current\nas an excitatory synapse of amplitude gsyn on a spine? Neglecting all capacitive effects and\nany other synaptic input, the single spine will inject the current /„ = gsyn^syn/O + ^ngsyn)\ninto the dendrite. Thus, a synapse on a spine can be mimicked by a dendritic synapse of\namplitude\n(12.15)\nWe see that unless the product of the spine neck resistance with the synaptic conductance\nchange is on the order of unity (or larger), the effect of the spine can be neglected.\nThe observation that spine necks probably do not contribute toward modulating synap-\ntic weight agrees with experimental evidence regarding the mechanisms underlying the\nexpression of LTP in the hippocampus. While the specific pathways and sites of action\nremain controversial, there is little to suggest that a postsynaptic change in the electrical\nimpedance of the spine is involved (see the next chapter). This conclusion is also likely\nto hold in the neocortex, unless synaptic conductance changes are substantially larger and\nspines thinner and longer than in hippocampus.\n12.3.3 Joint Excitatory and Inhibitory Input to a Spine\nAs mentioned above, it has been a consistent observation that between 5 and 15% of all\nspines in the cortex carry symmetrical, GABAergic synaptic terminals. Almost always,\nthese same spines are also contacted by excitatory synapses. Such a pair of excitatory and\ninhibitory synapses localized on a single spine would be the ultimate in highly specific\nmicrocircuitry.\nKoch and Poggio (1983a,b; see also Diamond, Gray, and Yasargil, 1970) proposed that\nsuch a dual synaptic arrangement can implement a temporally and spatially very specific\nveto operation on the basis of the nonlinear interaction between excitation and inhibition of\nthe silent or shunting type (see Sec. 5.1). Indeed, they found that fast GABA^ inhibition can\nreduce the EPSP in the spine to very small levels provided that it arrived within a very tight\ntime window around the onset of excitation (Fig. 12.7A). According to these numerical\nsolutions to the linear cable equation, such a microcircuit implements an AND-NOT gate\nwith temporal discrimination at the 0.1 msec level.\nThe degree of temporal specificity, caused by the exceedingly small capacitance in the\nspine head, can be quantified by the use of the input delay in the spine Dsp,sp (Sees. 2.4\nand 3.6.2). Following Agmon-Snir and Segev (1993), this corresponds to the delay between\nthe centroid of the synaptic current and the centroid of the local EPSP. On the basis of the\nelectrical model of the spine illustrated in Fig. 12.5 and neglecting the tiny current loss\nacross the spine head membrane, we have\n(12.16)"}
{"text": "===== Page 13 =====\n292\nDENDRITIC SPINES\nFig. 12.7 AND-NOT GATING AT THE LEVEL OF A SINGLE SPINE Nonlinear interaction of the AND-\nNOT type between a fast excitatory synaptic input (of the AMPA type) and inhibition at a single spine.\n(A) The effectiveness of silent GABA/i inhibition in reducing excitation when both are colocalized\neither on the spine (solid curve) or on the dendrite at the base of the spine (dotted curve). The curve\nshows the F factor (Eq. 5.14), that is, the ratio of EPSP without inhibition to the mixed excitatory-\ninhibitory potential, as a function of the onset of inhibition relative to excitation. The half-widths of the\ncurves are 0.12 and 0.22 msec for the spine and dendrite case, respectively. The linear cable equation\nfor a spine located on a pyramidal cell was solved with 1^^. = 0.25 msec for both excitation and\ninhibition, Et = 0 and Ee = 80 mV. Reprinted in modified form by permission from Koch and Poggio\n(1983b). (B) Time course of the spine membrane potential (relative to rest) for the nonlinear Nernst-\nPlanck electrodiffusion model. Here a small EPSP (solid line; (peai; = 1 msec, G^ = 0.1 nS) can be\neffectively vetoed by a hyperpolarizing potassium conductance increase (due to a GAB AS inhibition\nwith ?peak = 1 msec, GK = 1 nS; lower dotted curve), while the same or a hundred times stronger\nconductance change associated with silent inhibition of the GABA/i type (intermediate dashed curves)\nhad almost no effect. Reprinted in modified form by permission from Qian and Sejnowski (1990).\nwhere D^ is the local dendritic delay at the base of the dendrite. If the spine neck is\nsufficiently thin and elongated, the time window during which inhibition can effectively\nveto excitation can be a fraction of the corresponding dendritic time window. This effect\nis particularly strong for spines close to the soma—providing an isolated environment for\nexcitation and inhibition to interact on a very small time scale—while the local spine\ndelay for spines in the distal dendritic tree will be close to the local dendritic delay\n(Kdd/Rn -> oo).\nHowever, as described in Sec. 11.3, for very thin processes it is important to account"}
{"text": "===== Page 14 =====\n12.3 Passive Electrical Properties of Single Spines \n• \n293\nfor the change in the relevant ionic concentrations. In the case of a spine, even small\nsynaptic inputs can significantly alter the reversal potentials for chloride and potassium,\nboth across the membrane and between the spine head, neck, and the dendrite. This\nrequires solving the nonlinear Nernst-Planck equation as advocated by Qian and Sejnowski\n(1989, 1990). They studied the interaction between excitation and inhibition on a spine\nand found that for reasonable inhibitory conductance changes associated with chloride ions\n(such as would be caused by activation of GAB A^ receptors), the intracellular concentra-\ntion of chloride rapidly increases, bringing the synaptic reversal potential of inhibition\nsubstantially above the resting potential, thereby rendering it ineffective (Fig. 12.7B).\nIndeed, massive activation of GABA^ receptors in thin processes will lead to a depo-\nlarization as the chloride concentration gradient decreases, shifting ECI to progressively\nmore depolarized values. Such \"paradoxical\" EPSPs have indeed been observed in distal\ndendrites of pyramidal cells following intense GABAA activation (Staley, Soldo, and\nProctor, 1995).\nWhile shifts in the potassium reversal battery will also occur, rendering GABA^ inhi-\nbition equally ineffective for large conductance changes, the situation is quite different for\nsmall inputs. Here £K moves toward the resting potential of the spine, causing a profound\nreduction in the amplitude of the EPSP without leading to a hyperpolarizing response\n(Fig. 12.7B). This leads Qian and Sejnowski (1990) to argue that should inhibition play a\nrole in selectively vetoing excitation on a spine or on thin dendrites, that inhibition must be\nof the GABAfi type (for large dendrites, the advantage reverts to silent GABA^ inhibition).\nWe find no fault in their arguments.\nMust dual-input cortical spines necessarily be functional? Dehay et al., (1991) throw the\nentire idea into doubt by serially reconstructing selected spines that receive geniculate\ninput in cat primary visual cortex. Here, as elsewhere, about 8% of all spines in the\ninput layer (layer 4) carry both an excitatory as well as an inhibitory synaptic input.\nBecause Emerson et al., (1985) as well as Koch and Poggio (1985b) hypothesized that\ndirection and orientation selectivity in visual cortex neurons is mediated by very specific\ninteractions among excitation and shunting inhibition possibly localized on spines, Dehay\nand colleagues investigated whether geniculo-cortical synapses on spines are a preferred\ntarget for inhibition. Using a laborious electron-microscopic reconstruction technique, they\nshowed that these synapses do not have a higher likelihood of being paired with an inhibitory\nsynapse than the majority of synapses originated among cortical cells. Furthermore, in\ncertain cases Dehay et al., (1991) observed an axon from an inhibitory cell making both a\nsynapse on a spine and, a few micrometers away, another synapse on the parent dendrite,\nrevealing a lack of spatial specificity.\nAt least in cat visual cortex, it appears that dual-input spines could simply be the logical\nconsequence of an imprecise developmental rule that specifies that inhibitory synapses\nshould primarily innervate dendrites and that a small fraction fails to do so.\n12.3.4 Geniculate Spine Triad\nIn other parts of the brain, however, the association between dual synaptic input and spines is\nmuch stronger. We encountered already one such microcircu.it (Rail et al., 1966; Shepherd,\n1978) in the mammalian olfactory bulb (Sec. 5.3), which mediates self and lateral inhibition.\nWe will here discuss another instance of a synaptic microcircuit, involving dual input to a\nspinelike structure."}
{"text": "===== Page 15 =====\n294\nDENDRITIC SPINES\nThe mammalian lateral geniculate nucleus (LGN), the midway station for visual input\nbetween the retina and the visual cortex, is the site of a very complex synaptic structure\nknown as glomerulus (Fig. 12.8). At the heart of each glomerulus lies a terminal from a\nretinal axon. It is surrounded by the complex intertwining of very thin dendritic processes\nof a geniculate interneuron with those of a geniculate relay cell (Harnori et al., 1974; Hamos\net al., 1985). This circuit is also termed a triad because of the intimate association among\nthree different synaptic terminals: the retinal input makes a synapse both on the spine of a\ngeniculate relay cell and on the interneuron. The GABAergic interneuron, in turn, makes a\nsynapse on the geniculate spine.\nRetinal output is organized in multiple parallel channels of information. One of these,\nthe X cells, which are particularly sensitive to high spatial frequencies, tends to make the\nmajority of synaptic contacts onto geniculate relay cells and interneurons in association with\nthese spine triads (Wilson, Friedlander, and Sherman, 1984). At least in cat, the Y cells, the\nother major retinal output pathway, tend to make their synapses directly onto the dendrites\nof their geniculate target relay cells, bypassing the inhibitory interneurons (Sherman and\nGuillery, 1996).\nPassive cable modeling of geniculate relay cells with the spine-triad local circuit using\nexcitation and shunting inhibition (Koch, 1985) shows that if the size of the inhibitory\nFig. 12.8 SYNAPTIC SPINE TRIADS IN THE LGN\nThese drawings, based on laborious electron-\nmicroscopic reconstructions of cells in the cat\nLGN (Hamos et al., 1985), illustrate the very\nintricate and baroque synaptic microcircuits that\ncan be found in the nervous system. The arrange-\nment shown, known as a glomerulus, includes\n(A) a dendrite (marked d) of a local inhibitory in-\nterneuron from which a dozen very thin processes\nspring forth. The full circles indicate the location\nof excitatory synapses from a retinal axon. The\ninterneuron in turn inhibits the geniculate relay cell\nat the nine sites indicated with full arrows. (B) This\ninterneuron (in black) is shown grappling with five\nspinelike appendages on a dendrite of a geniculate\nrelay cell (in white), which are shown more fully in\n(C). The full circles indicate nine retinal synapses,\nand the filled triangles 40 inhibitory synapses from\nother interneurons. The triad is a synaptic arrange-\nment in which an excitatory input (here, an optic\nnerve axon) excites both an interneuron as well as\nthe geniculate relay cell. The interneuron, in turn,\ndirectly inhibits this site. Reprinted by permission\nfrom Hamos et al., (1985)."}
{"text": "===== Page 16 =====\n12.4 Active Electrical Properties of Single Spines \n• \n295\nconductance change due to the GABA^ receptors is neither too small nor too large, it can\neffectively and specifically veto the retinally induced EPSP without inhibiting EPSPs from\nneighboring synapses. Thus, although the inhibition is postsynaptic, from a functional point\nof view it acts similar to presynaptic inhibition.\nBecause of the expected 1 or 2 msec delay between retinal input triggering an EPSP in\nthe interneuron and the opening of postsynaptic GABA^ receptors on the spine in response\nto this input, inhibition is likely to be not very effective at low firing rates (caused, for\ninstance, by a low contrast stimulus). Inhibition would be strongly activated for sustained,\nhigh-contrast input. Because the spine-triad circuit is essentially limited to X cells, Y cells\nshould not show any comparable localized and specific type of inhibition. Experimentally, it\nis known that geniculate cells are more phasic than their retinal counterparts (Cleland, Dubin,\nand Levick, 1971) and that inhibition is independent of stimulus contrasts for Y cells but\nincreases with increasing stimulus contrast in X cells (Berardi and Morrone, 1984). Finally,\nit is clear that the level of activity in the interneuron itself will be crucial in determining\nwhether or not it can inhibit the retinal input (Bloomfield and Sherman, 1989). Such context-\ndependent inhibition could be important, for instance, during saccadic suppression.\nWe conclude that the spine-triad microcircuit on geniculate X cells appears to implement\na specific type of activity-dependent inhibition (Hamori et al., 1974; Koch, 1985).\n12.4 Active Electrical Properties of Single Spines\nIn the mid-1980s, several independent groups worked on the idea of synaptic amplification in\ndendritic spines endowed with active, regenerative electrical properties (Perkel and Perkel,\n1985; Miller, Rail and Rinzel, 1985; Shepherd et al., 1985; Pongracz, 1985; Segev and\nRail, 1988). At first viewed with skepticism by experimentalists, recent calcium-imaging\nexperiments have shown that spines on hippocampal pyramidal as well as cerebellar\nPurkinje cells can show all-or-nothing responses (Yuste and Denk, 1995; Denk, Sugimore\nand Llinas, 1995; see Sec. 12.6.2). It remains to be seen whether these spikes are actually\nrestricted to the spines or whether they invade the neighboring dendrites.\nThe basic idea is simple and will be explained with reference to the painstakingly\nexecuted Segev and Rail (1988; see also Rail and Segev, 1987) study. They endowed the\nhead of a single spine—on an otherwise passive dendrite—with the fast sodium and delayed-\nrectifier potassium conductances found in the squid axon (adjusted for 20° C and with a\ntenfold increase in peak conductances; see Fig. 12.9). With a single-channel conductance\nof about 15 pS (Table 8.1) this corresponds to 1050 sodium channels per spine. Using a\nvery fast synaptic input (fpeak = 0.035 msec), Segev and Rail found that the amplitude of\nthe synaptic input gpeak nad to exceed a very sharp threshold in order to generate an all-\nor-nothing action potential in the spine (Fig. 12.10). Such a tongue-twisting \"spine spike\"\nwill be initiated once Vsp exceeds a critical voltage threshold V& (see the discussion in\nSees. 6.3.1 and 19.2). At Vth, the sum of the synaptic and active spine currents exceeds the\ncurrent /„ flowing through the spine neck and onward into the dendrite.\nIn the suprathreshold domain, the amplification that can be obtained with a small number\nof strategically placed active channels on a spine is evident in the lower graph of Fig. 12.10.\nThe 90 mV spine action potential is attenuated across the 1000 M£2 neck but still leads to a\n5 mV EPSP at the base of the spine. Contrariwise, a passive spine would have depolarized\nto 27 mV, of which only about 1 mV would have survived the attenuation through the spine\nneck (dotted lines in Fig. 12.10). Thus, a tiny 1 /zm2 patch of active channels leads to a\nfivefold voltage amplification."}
{"text": "===== Page 17 =====\n296 \n• \nDENDRITIC SPINES\nFig. 12.9 MODEL OF AN ACTIVE SPINE Lumped electrical model of an active spine, as used in the\nSegev and Rail (1988) study. The spine head is endowed with squid axon membrane (at 20° C, but with\na tenfold increase in the maximal conductances G^ and GK). With Cm = 1 ju,F/cm2, the passive time\nconstant in the spine at rest is 1.4 msec. The synaptic input is very fast, with fpeak = 0.035 msec and\n£syn = 100 mV. A tenfold slower synaptic input is expected to yield a similar behavior in the presence\nof a tenfold slower membrane time constant. The input resistance at the base spine is 262 M£2.\nThe amplitude (as well as the integrated area) of the dendritic EPSP is independent\nof variations in gpeak (as long as gpeak is above threshold; Fig. 12.10), since the maximal\nvalue of Vsp is determined by the height of the action potential, which is rather fixed.\nGiven the large variability in the postsynaptic amplitude of individual synaptic inputs at\ncortical synapses (see, Fig. 4.4), active spines can thereby eliminate a source of uncertainty\nassociated with passive spines (as well as amplify their responses).\nFigure 12.11 illustrates the dependency of Vsp and Vd on the spine neck resistance.\nFor small values of Rn (here below 620 M£2), the spine input impedance KsptSp is not\nsufficiently high in order for the local EPSP to exceed V±: the peak value of both Vsp and\nVd in the presence of sodium and potassium channels is little different from the peak value\nof the potential in the passive situation (right panels in Fig. 12.11). For /?„ = 630 M£2,\na spike is generated after a short delay, while for the high resistance neck (curve b),\nan action potential is established in a very secure manner and with a larger amplitude.\nSeemingly paradoxically, however, the amplitudes of the resulting EPSPs at the dendrite\nare reversed. While a larger spine neck resistance increases the spine input impedance\n(witness Eq. 12.4), thereby facilitating spike generation, it also causes a larger voltage\nattenuation across the spine neck. In the limit of an infinitely thin spine neck, the smallest\ninput will cause V,/ to \"lock up\" at £syn; yet none of that will be visible at the base of\nthe spine. (For all the subtleties of this dependency, see Segev and Rail, 1988.) As the\nlower right panel in Fig. 12.11 best summarizes, amplification over the passive spine is\nobtained within a narrow range of Rn. Given the inverse quadratic dependency of Rn\non dn (Eq. 12.1), this translates into a very strong dependency of Vj on the spine neck\ngeometry."}
{"text": "===== Page 18 =====\n12.4 Active Electrical Properties of Single Spines\n297\nFig. 12.10 ACTIVE SPINES AS A FUNCTION or THE INPUT AMPLITUDE Time course and peak\ndepolarization in the spine and dendrite shown in Fig. 12.9 as a function of the amplitude of the fast\n('peak = 0.035 msec) voltage-independent synaptic conductance change gpeak- The two left panels\nillustrate Vsp(t) and Vj(t) for two values of gpeak (0.2 nS for a and 0.4 nS for V) while the right\npanels plot the peak values of Vsp and Vj as a function of the input amplitude. Rn is always set to\n1 GR. Dotted curves are for a passive spine, whereas the solid curves are for an active spine. Close\ninspection of these plots reveals some minimal evidence of active channels for the smaller inputs of\nthe two. If the input is doubled, a very robust all-or-none spike is initiated. Further increases of gpeak\nhave no further effect on the peak of either Vsp or Vj. The dashed line in the lower right panel shows\nthe peak of V^ when the same synaptic input is applied directly to the passive dendrite, rather than to\nthe spine. Active spines can be used to amplify synaptic input. Reprinted by permission from Segev\nand Rail (1988).\nAt present, we do not know whether sodium channels are present on dendritic spines.\nHowever, phenomena similar to those discussed here could be instantiated with slower\ncalcium-mediated all-or-none spine spikes. Two-photon microscopy experiments using\ncalcium dyes have provided tantalizing preliminary evidence for all-or-none calcium events\nfollowing synaptic input onto spines of hippocampal and Purkinje cells (see Sec. 12.6.2).\nWhat Segev and Rail (1988; see also Perkel and Perkel, 1985; Miller, Rail, and Rinzel,\n1985; Shepherd et al., 1985) demonstrate is that active spines can significantly amplify\n(two to tenfold) synaptic input at minimal metabolic costs (only requiring the insertion of\nvoltage-dependent channels into a very small membrane patch the size of a spine head). We\nconclude that active spines could provide the biophysical substrate for information storage\nas well as information processing.\nThe strong dependency of Vj on parameters associated with spine neck geometry\nprovides the cell with a sensitive mechanism to modulate the synaptic weight of individual\ninputs. Only within a narrow range of Rn will the synaptic input be high; outside of this range,\nthe dendritic EPSP will be much reduced. At the same time, this sensitivity also represents\nthe Achilles heel of the hypothesis: as evidenced by the large degree of variability in the"}
{"text": "===== Page 19 =====\n298\nDENDRITIC SPINES\nFig. 12.11 ACTIVE SPINES AS A FUNCTION OF THE SPINE NECK RESISTANCE Time course and\npeak depolarization in the spine and dendrite shown in Fig. 12.9 as a function of the spine neck\nresistance (for gp^ = 0.37 nS and fpeak = 0.035 msec). The two left panels illustrate Vsp(t) and\nVd(t) for Rn = 630 Mfl (curve a) and Rn = 1000 Mfi (curve b) while the right panels plot the peak\nvalues of Vsp and Vj as a function of R„. Dotted curves are for a passive spine, whereas the solid\ncurves are for an active spine. Large amplification of the dendritic EPSP with respect to a passive\nspine only occurs in a relatively narrow range of Rn values: if the neck is too short or too thick, the\nelectrical decoupling of excitable channels at the spine head from the rest of the cell is insufficient to\ninitiate an action potential, while for very thin and long spine necks (implying large values of Rn),\nthe attenuation across the spine neck is excessive. This dependency on spine neck geometry could be\nexploited in synaptic plasticity. Reprinted by permission from Segev and Rail (1988).\nspine neck geometry (Fig. 12.4), the brain might not be able to control spine neck geometry\nwith sufficient accuracy to exploit it in a consistent manner.\nWhat remains unclear from an experimental point of view is whether action potentials\nare restricted to the spines themselves or whether they spread to the parent dendrite. As\nwitnessed by Fig. 12.11, a true spiking spine requires a very thin or elongated neck to\nachieve the necessary isolation from the parent dendrite.\nMore interesting from the point of view of neuronal computation are the combinatorial\npossibilities for nonlinear interactions among active spines on distal dendrites. Under\ncertain circumstances, one or a small number of simultaneous inputs to active spines can\ntrigger action potentials in neighboring spines. We will pick up the threads of this story in\nSec. 19.3.2.\n12.5 Effect of Spines on Cables\nGiven the large spine density (up to 10 spines per micrometer of dendrites), 50% or more of\nthe total neuronal membrane area resides in spines. It is therefore important to account for"}
{"text": "===== Page 20 =====\n12.5 Effect of Spines on Cables \n• \n299\nthis additional area to properly analyze the propagation of electrical signals in the dendritic\ntree. Although these effects provide no insight into the function of spines, they may be\nironically the only ones that we can infer with confidence to be important.\nAccounting for the effect that spines have on cable propagation is based on the assumption\nthat for current flow from the dendrite back into the spine the membrane area of the spine\ncan be incorporated into the membrane of the parent dendrite. As we expect from basic\ncable theory and as discussed more fully below, there exists almost no voltage attenuation\nfrom the dendrite back into the spine (contrary to the voltage attenuation from the spine\ninto the dendrite). This isopotentiality is the reason that the spine area can be lumped into\nthe overall area of the dendrite. Several different methods to deal with this problem have\nbeen proposed, based on the intuition that in the presence of a large number of spines, the\ntotal capacitance will increase while the membrane resistance will decrease.\nOne method (Stratford et al., 1989) transforms the spiny dendrite (with diameter d and\nlength /) into a single \"equivalent\" (and smooth) cylinder, whose diameter d' and length /'\nare larger than those of the parent dendrite by some fraction y, with\n(12.17)\nwhere Adend is the total membrane area of the \"stem\" dendrite without any spines, and\n^spine is the total membrane area of all spines (spine necks and spine heads) on the dendrite.\nThe dimensions of this equivalent cable are\n(12.18)\nand\n(12.19)\nSolving for the membrane potential in a cable studded by n spines requires the solution of\n2n + 1 coupled equations (assuming that each spine is modeled using two compartments,\none for the neck and one for the head). The transformation described here reduces this to\nsolving the cable equation in a single, unbranched cylinder. The effective area of the cable\nis scaled by J7, its infinite input resistance by JF~1//2, and its electrotonic length of the\nequivalent cable by JF1/2. The time constant remains unchanged. The transformed segment\nbehaves like the spiny original under most circumstances (Stratford et al., 1989).\nAn alternative transformation method (Shelton, 1985; Holmes, 1989; Segev et al., 1992)\npreserves the dimensions of the dendrite but scales Cm and Rm appropriately by\n(12.20)\nand\n(12.21)\nBoth methods are mathematically equivalent.\nA novel analytical method to incorporate spines into the continuous cable equation was\ndeveloped by Baer and Rinzel (1991). It is powerful, since it can deal with passive as well\nas active spines. Baer and Rinzel replace the cable equation for a single passive cable by\ntwo equations, one for the voltage along the dendrite Vd(x, t) and one for the voltage in\nthe spines Vsp(x, t). The density of spines per unit electrotonic length of cable is s(x). A\nsingle spine receives synaptic input current Isya(x, 0 (which can vary with the position of"}
{"text": "===== Page 21 =====\n300 • \nDENDRITIC SPINES\nthe spine along the cable), generating a local current across the spine head /ionic that can\ninclude Hodgkin-Huxley or other voltage-dependent currents. Spines are independent of\neach other, only interacting with the dendrites via the neck current /„ that flows through the\nspine neck. The associated cable equation can be expressed as\n(12.22)\n(12.23)\nwith an additional equation for the neck current,\n(12.24)\nHere T, A. and 7?oo are the time and space constants and the input resistance of a semi-infinite\ncable in the absence of any spines, and C/, is the tiny spine head capacitance (Fig. 12.5).\nBaer and Rinzel (1991) go on to solve these equations for active as well as passive spines.\nIn the former case, wavelike propagation can occur along the passive cable, supported by\nthe amplifying spines. In the latter case, both equations are linear and /ionjc = Vsp/Rh-\nAssuming a constant density of spines, s (x) = s, and steady-state conditions, they compute\nthe effective electrotonic length of a spiny dendrite as\n(12.25)\nand the infinite input resistance as\n(12.26)\nSince under almost all circumstances Rh ^> /?„, these reduce to\n(12.27)\nand\n(12.28)\nas in the more heuristic methods discussed at the beginning of this section.\nIn summary, all of the spine-replacement methods discussed here conclude that the\ncollective effect of dendritic spines is to increase the effective membrane capacitance and\ndecrease the membrane resistance, leading to a lower input resistance and an increased\nelectrotonic length compared to a smooth dendrite of equal diameter and physical length\n(Wilson, 1988). Indeed, Jaslove (1992) argues that these population effects—influencing\nthe degree of spatio-temporal integration occurring in the dendrites—are the primary\nfunctional reason for why some cells are studded with spines while others are devoid\nof them.\n12.6 Diffusion in Dendritic Spines\nSo far, we have focused exclusively on the electrical properties of spines. Yet, over the\nlast decade, an alternative view of spines has emerged which emphasizes their effect on\nchemical, rather than electrical, signaling. As discussed in Chap. 13, chemical dynamics of"}
{"text": "===== Page 22 =====\n12.6 Diffusion in Dendritic Spines \n• \n301\nintracellular calcium and other second messengers in the spine are of particular importance\nin the induction of long-term potentiation.\n12.6.1 Solutions of the Reaction-Diffusion Equation for Spines\nCompartmental modeling of calcium diffusion and binding following synaptic input to the\nspine (Robinson and Koch, 1984; Coss and Perkel, 1985; Gamble and Koch, 1987; Wickens,\n1988) played a trail-blazing role here, followed a few years later by the experimental study\nof calcium dynamics in single spines (Miiller and Connor, 1991; Guthrie, Segal and Kater,\n1991; Jaffe, Fisher, and Brown, 1994; Eilers, Augustine, and Konnerth, 1995; Yuste and\nDenk, 1995; Denk, Sugimore, and Llinas, 1995; Svoboda, Tank, and Denk, 1996). More\nrecent modeling studies have, of course, become much more sophisticated and use a very\nfine grained spatial resolution to keep track of substances in the spine head and neck (Holmes\nand Levy, 1990; Zador, Koch, and Brown, 1990; Koch, Zador, and Brown, 1992;DeSchutter\nand Bower, 1993; Gold and Bear, 1994; Zador and Koch, 1994; Woolf and Greer, 1994;\nJaffe, Fisher and Brown, 1994).\nThe principal ideas are really very simple, in particular in light of the linearized reaction-\ndiffusion equation discussed in Sec. 11.7. Using the geometry of Fig. 12.12A, and in the\npresence of low calcium concentration, a linear nonsaturable calcium pump in the spine neck\nand head membrane, and a fast second-order nondiffusible buffer, the reaction-diffusion\nequations (Eqs. 11.48) in the presence of calcium pumps can be reduced to the linear partial\ndifferential equation (Eq. 11.51), formally identical to the cable equation.\nThis allows us to define the \"chemical input resistance\" (Carnevale and Rosenthal, 1992;\nZador and Koch, 1994), in analogy to the standard electrical input resistance, as the change\nin calcium concentration in response to a current of calcium ions (Table 11.2). Both the\nchemical and the electrical spine input resistances are much larger than the associated\ndendritic input resistances. At the spine head, the input resistance is 4.2 x 10~2 /zM/fA\nwhile the dendritic input resistance is 4.8 x 10~3 /u-M/fA. In other words, a sustained\ncalcium current injected into the spine head leads to a tenfold larger increase in calcium\nconcentration than the same current applied at the dendritic shaft.\nA key difference between the electrical and the chemical properties of spines arises from\nthe effect of the spine neck. In our analysis, as well as in almost all published studies of the\nelectrical properties of spines, the electrical cable properties associated with the spine neck\nare neglected, since no significant current will cross the membrane of the 1 -/zm long \"cable,\"\na direct consequence of the large value of A.. In order to estimate the loss of calcium current\nthrough the spine neck membrane, we compute the space constant of Eq. 11.55 associated\nwith the linearized reaction-diffusion equation. Reading off from Fig. 11.9, \\r-d for a\n0.1 /xm thin cable is 0.46 /urn (0.27 /urn without diffusible buffer). Because the dendritic\nshaft is more than 1kr-d away from the spine head, the calcium concentration at the shaft\nis expected to be tenfold lower than at the head. This is in dramatic contrast to the almost\ncomplete lack of electrical current attenuation experienced between the spine head and the\nbase (A. is close to three orders of magnitude larger than Ar__rf; Fig. 11.9).\nThese principles are illustrated in simulations of the fully nonlinear calcium dynamics\nthought to underly the induction of LTP (Zador, Koch, and Brown, 1990; Brown et al.,\n1991b). In the model, synaptic input activates a fast non-NMDA conductance (Eq. 4.5) as\nwell as an NMDA voltage-gated conductance (Eq. 4.6; Fig. 12.12A). 2% of the NMDA\ncurrent is assumed to be carried by Ca2+ ions. Subsequent to entry into the intracellular\ncytoplasm, these ions can bind to one of the four calmodulin binding sites (Fig. 11.7),"}
{"text": "===== Page 23 =====\n302\nDENDRITIC SPINES\nFig. 12.12 SPATIO-TEMPORAL DYNAMICS OF CALCIUM IN A SPINE \n(A) Compartmental represen-\ntation for solving the reaction-diffusion equation associated with the system controlling intracellular\ncalcium in response to a mixed excitatory voltage-independent non-NMDA and a voltage-dependent\nNMDA synaptic input. Opening of the NMDA channels causes an influx of calcium ions. These\nare removed by the action of membrane-bound pumps in the spine head and neck, by binding to\ncytoplasmic calcium buffers, as well as by diffusion into the dendrite. The drawing is to scale, with\ndn = 0.1 fj,m. The individual compartments are shown by thin lines. (B) Spatio-temporal dynamics\nof Ca2+ in response to a train of three presynaptic stimuli (at 100 Hz) while the membrane potential\nin the spine was simultaneously clamped to —40 mV (mimicking a somatic voltage-clamp protocol).\nChanges in [Ca2+],-—induced by the calcium influx through the NMDA channel—are restricted\nmainly to the spine head, x indicates the distance from the dendritic shaft (with 0 corresponding to\nthe shaft and x = 1.3 /zm to the subsynaptic cleft). Reprinted by permission from Zador, Koch, and\nBrown (1990).\nthey can be pumped out of the cell via two different pumps, and they can diffuse along\nthe spine neck and out into the dendrite (assumed to be an infinite capacity sink with a\nresting calcium level of 50 nM). Simulated calcium dynamics following a train of synaptic\nstimuli are shown in Fig. 12.12B. Due to the tiny volume of the spine, the small Ca2+\ninflux following synaptic stimulation leads to a large transient increase in the spine calcium\nconcentration (here, [Ca +]sp = 10/u,M corresponds to about 350 Ca2+ions). We conclude\nthat spines can amplify the small incoming calcium signal dramatically. Yet due to the large\nmismatch in volumes and the associated small chemical input impedance at the dendrite, the"}
{"text": "===== Page 24 =====\n12.6 Diffusion in Dendritic Spines \n• \n303\nlarge peak changes in [Ca2+](- at the spine cause the dendritic level of [Ca2+],- to fluctuate\nonly by several tens of nanomolars.\nDue to the difference in input impedances between the spine and the dendrite, both\ncalcium and voltage will be severely attenuated when going from the spine to the dendrite,\neven though no current is lost across the spine neck membrane in the case of the cable\nequation. This is illustrated in a different manner in Fig. 12.13, showing both voltage and\ncalcium attenuation along the spine, that is, the ratio of the voltage (or calcium concentration)\nat one location to the voltage (or calcium concentration) at another location.\nQuite a dramatic difference emerges when considering antidromic attenuation. When\nvoltage clamping the dendrite to any particular value, the voltage at the spine head will be\nattenuated by the factor (Eq. 3.34)\n(12.29)\nSince Ksp^ «* K^d (Eq. 12.5) to a pretty good approximation, no voltage attenuation is\nexpected into the spine, as confirmed by the upper dashed line in Fig. 12.13. However,\ndue to the sustained loss of calcium ions as they are being pumped through the spine neck\nmembrane into the extracellular cytoplasm, the effective value of Ksp^ for the reaction-\nFig. 12.13 VOLTAGE AND CALCIUM ATTENUATION IN AND OUT OF A SPINE \nVoltage and calcium\nattenuation for two different simulations as a function of the distance from the dendrite (at 0; the spine\nneck ends at 1 /u.m and the spine head extends from 1 to 1.3 /zm). The upward diagonal dashed and\nsolid curves illustrate voltage and calcium attenuation from the spine into the dendrite. These curves\nare taken from the simulation shown in Fig. 12.12B and indicate peak calcium and voltages following\nthree presynaptic stimuli to the spine synapse. Both variables attenuate about tenfold. In a different\nsimulation illustrating antidromic steady-state behavior, either the calcium (downward going solid\ncurve) or the voltage (constant dashed line) is clamped to a fixed value in the dendrite. Due to the\npresence of calcium pumps in the spine neck, calcium rapidly attenuates and reaches baseline levels\nat the spine head. The membrane potential, however, does not attenuate across the spine neck due to\nthe negligible current loss across the spine neck and head membranes. Reprinted by permission from\nKoch and Zador( 1993)."}
{"text": "===== Page 25 =====\n304 . \nDENDRITIC SPINES\ndiffusion system is very low. As a consequence, when the calcium concentration in the\ndendrite is \"clamped\" to 1 /xM, the calcium attenuation into the spine head is very large:\nit remains protected from the high dendritic calcium values by the presence of calcium\npumps in the membrane of the spine neck (Zador, Koch, and Brown, 1990; Koch, Zador,\nand Brown, 1992), providing a graphic illustration of the tiny size of the associated space\nconstant. Given the square-root dependency of Ar_^ on the density of pump molecules in\nthe membrane (Table 11.3), this density would have to change substantially before any\nsignificant effect of dendrite calcium on the spine head would be seen. Or, short and sweet,\n\"spines are electrically coupled but chemically isolated from changes occurring in the\ndendrite.\" Of course, the presence of the spine apparatus or other organelles would further\nimpede the flow of calcium ions into the spine head. What applies to calcium will, of course,\nalso apply to other second messenger systems (see also Woolf and Greer, 1994).\nThe value of many of the relevant biophysical and biochemical parameters can only be\nspecified within a factor of 2, 5, or even 10. For instance, relatively small changes in the\nspine neck geometry or in the buffer concentration can cause the peak calcium concentration\nin the spine head to vary quite a bit. Whether this sensitivity of peak [Ca2+]sp subserves a\nfunction, as contended by Gold and Bear (1994) (Fig. 13.7) is not known.\nOne obvious effect that should be emphasized is that the electrical as well as the chemical\ninput impedance increases with increasingly longer and thinner spines. If the establishment\nof LTP were to depend on exceeding either a voltage threshold or a critical concentration\nof [Ca2+],, the thin and elongated spines would be in a better position than the short,\nstubby ones to achieve this. Conversely, since the synaptic weight depends little on the\nspine neck—for the range of geometries reported in hippocampus and for passive spines—\nstubby spines could be considered to be permanently modified, while elongated or thin\nspines would constitute a reservoir that can be recruited for long-term synaptic potentiation\nas well as for long-term depression. Indeed, several reports have emphasized that high-\nfrequency synaptic stimulation that causes LTP leads to an excess of short or thick spines\n(Lee et al., 1980; Chang and Greenough, 1984).\nAlthough we mainly use arguments from linear cable theory, the simulations in Figs.\n12.12 and 12.13, based as they are on the peak transient calcium concentration changes in\nthe full nonlinear model, confirm these expectations.\n12.6.2 Imaging Calcium Dynamics in Single Dendritic Spines\nWhile it remains technically impossible to record spine EPSPs directly, a number of groups\nare \"pushing the technological envelope\" by imaging calcium activity in dendrites and in\nindividual spines via calcium-sensitive fluorescent dyes (Miiller and Connor, 1991; Guthrie,\nSegal, and Kater, 1991;RegehrandTank, 1992; Jaffe, Fisher, and Brown, 1994; Yuste and\nDenk, 1995; Segal, 1995a; Eilers, Augustine, and Konnerth, 1995; Denk, Sugimore, and\nLlinas, 1995; for a review see Denk et al., 1996). The more recent studies make use of\nconfocal laser scanning or two-photon fluorescence microscopy. They confirm that in the\nabsence of calcium entry through voltage-dependent calcium channels, spines do isolate in\nboth directions: a high calcium concentration in the dendrite is frequently not paralleled by\nan equally high concentration change in the spine (Fig. 12.14C). Control experiments with\ninjected cobalt indicate that the lag is not due to a physical diffusion barrier between the\ndendrite and the spine (Guthrie, Segal, and Kater, 1991), supporting the idea that calcium-\ndependent processes, such as calcium pumps or other uptake systems, were responsible\nfor isolating the spine head. Conversely, synaptic stimulation leads to restricted calcium"}
{"text": "===== Page 26 =====\n12.6 Diffusion in Dendritic Spines \n• \n305\nFig. 12.14 IMAGING CALCIUM IN A SINGLE SPINE Fluorescence signal from a single spine on a\nCA1 pyramidal cell in a rat hippocampus slice. (A) Using two-photon microscopy, Yuste and Denk\n(1995) record the increase in fluorescence of a calcium indicator, due to an increase in free intracellular\ncalcium, evoked either by spontaneous synaptic activity (sy) or by the antidromic invasion of a\npostsynaptic action potential (ap). Following synaptic input, calcium rises within 2 msec, the temporal\nresolution of the measurement. The influx of calcium in response to a postsynaptic spike is mediated\nby voltage-dependent calcium currents in the dendrites and spines. (B) Stochastic failure of synaptic\ntransmission at the same spine can also be visualized. Left column: fluorescence measurements in\nresponse to subthreshold synaptic stimulation. Right column: simultaneous current recording at the\nsoma. In only three cases is a clear synaptic response evident at the single spine. Only in about 20\nto 40% of the cases will a presynaptic spike cause release of a synaptic vesicle and a postsynaptic\nresponse. (C) Spontaneous synaptic activity causes an increase in [Ca2+], at a single spine (1) and\nthe adjacent dendritic shaft (2), while a neighboring spine (3) or dendrite (4) shows no such increase.\nLater during the same recording session, a synaptic-evoked calcium accumulation was restricted to\nthe spine (3). Reprinted by permission from Yuste and Denk (1995).\nincrease in individual spines but not in the dendrite, confirming the basic aspects of the\nmodels discussed above. Indeed, the resolution of these methods is such that stochastic\nfailure of synaptic transmission can be observed in the calcium response at a single spine\n(Fig. 12.14B).\nIn real life, things become more interesting, but also more complicated, by the presence of\nhigh-threshold noninactivating voltage-dependent calcium channels in dendrites and spines:\nboth somatic depolarization via an intracellular electrode and antidromic spike invasion\nlead to calcium increases in dendrites and in spines throughout the cell (Fig. 12.14A;\nMarkram, Helm, and Sakmann, 1995; Sec. 19.1). With the exception of Robinson and\nKoch (1984) and Gamble and Koch (1987), previous compartmental models have assumed\nthat all calcium entry occurs via the activated NMDA synaptic receptors at the stimulated"}
{"text": "===== Page 27 =====\n306 \n• \nDENDRITIC SPINES\nspine. Since antidromic spike invasion does occur under physiological conditions and may\nindeed be facilitated by the presence of sodium channels on the apical dendrites (Stuart and\nSakmann, 1994; Sees. 19.1 and 19.2.2), it is unclear what this implies about the specificity\nof LTP, which is generally believed to be restricted to the stimulated synapse (see the\nfollowing chapter). It might well be that under physiological conditions the critical calcium\nconcentration required to initiate the biochemical cascade that leads to the enhancement of\nthe synapse cannot be achieved unless calcium accumulation from synaptic (calcium entry\nviaNMDA and non-NMDA receptors; Schneggenburger et al., 1993) as well as nonsynaptic\n(voltage-dependent calcium channels and calcium-triggered calcium release; Regehr and\nTank, 1992) sources cooperate.\n12.7 Recapitulation\nThe small extent of dendritic spines, somewhat smaller than the average-sized Escherichia\ncoli colon bacterium, precluded until very recently direct experimental access, making the\nfunction and properties of dendritic spines a favorite subject among modelers. Hypotheses\nconcerning their function can be divided into two major categories: spines as devices for the\ninduction and/or the expression of synaptic plasticity and spines as devices subserving spe-\ncific computations. Other proposals have been advanced, such as that spines primarily serve\nto connect axons with dendrites (Swindale, 1981), that spines serve to increase the effective\nmembrane capacity of a dendrite (Jaslove, 1992), or that spines serve to protect the dendrite\nfrom high, and therefore possibly toxic, overdoses of calcium during synaptic activation\n(Segal, 1995b). While each of these ideas may contain some grain of truth, we here empha-\nsize those properties of spines of direct relevance to information processing and storage.\nThat changes in the electrical resistance of the spine neck can modulate the EPSP\namplitude of a synapse on this spine has been a popular idea since it was discussed in detail\nby Rail (1970, 1974). It now appears, at least in the case of hippocampal CA1 pyramidal\ncells (but most likely also for neocortical spines), that the spine neck conductance is too\nlarge relative to the synaptic-induced conductance change to be able to effectively modulate\nthe dendritic EPSP for passive spines. Furthermore, as covered in the following chapter,\nthe weight of the evidence leans in favor of a pre- rather than a postsynaptic site as the\nlocus where the long-term changes in synaptic weight are affected. It is most likely in\nthe induction phase of Hebbian long-term changes in synaptic plasticity that spines play a\ncritical role.\nBoth theoretical and experimental evidence is accumulating that spines may create an\nisolated biochemical microenvironment for inducing changes in synaptic strength. In the\ninduction of associative (or Hebbian) LTP, for example, the spine could restrict changes\nin postsynaptic calcium concentration to precisely those synapses that met the criteria\nfor potentiation. Furthermore, changes in spine shape could control the peak calcium\nconcentration induced by synaptic input: all other factors being equal, long and skinny spines\nwould have higher peak calcium levels than short and stubby spines. This could provide\nan alternative explanation for some of the environmental effects on spine morphology. The\nbiochemical compartmentalization provided by dendritic spines could, of course, be equally\ncrucial for a number of other diffusible second messengers, such as I?3, calmodulin, cyclic\nAMP, and others. We conclude that while spines may not play any role in the expression\nof synaptic plasticity, they may be crucial in the induction phase by offering a protected\nmicroenvironment for calcium and other messenger molecules."}
{"text": "===== Page 28 =====\n12.7 Recapitulation \n• \n307\nSpines can provide a very rich substrate for computation if they are endowed with\nregenerative, all-or-none electrical properties as experimental data are now suggesting.\nThe elevated spine input resistance, as compared to the input resistance of the parent\ndendrite, and the partial electrical isolation of the spine from the rest of the cell, could\nmake spines a favorable site for the initiation of action potentials. The proposal that spines\ncontain voltage-dependent membrane conductances, first advanced by theoreticians and\nnow experimentally supported by evidence for calcium spikes, implies that a dendritic\nspine is a basic computational gate with two states, on or off. Modeling studies, treated\nin Sec. 19.3.2, demonstrate that a population of strategically placed active spines can\ninstantiate various classes of quasi-Boolean logic. The combination of excitatory and\ninhibitory synapses on a single spine—whether passive or active—observed in certain brain\nareas can only enhance this computational power. With up to 200,000 spines on certain cell\ntypes, it remains to be seen whether the nervous system makes use of this vast computational\ncapacity."}
{"text": "===== Page 3 =====\n310 . \nSYNAPTIC PLASTICITY\nFig. 13.1 SYNAPTIC RESPONSE DEPENDS ON THE HISTORY OF PRIOR USAGE Excitatory postsy-\nnaptic currents (EPSCs) recorded from a CA1 pyramidal neuron in a hippocampal slice in response\nto stimulation of the Schaffer collateral input. The stimulus is a spike train recorded in vivo from the\nhippocampus of an awake behaving rat, and \"played-back\" at a reduced speed in vitro. The presynaptic\nspikes have an average interspike interval of 1950 msec, which varies from a low at 35 msec to a\nmaximum at 35 sec. The normalized strength of the EPSC varies in a deterministic manner depending\non the prior usage of the synapse. For a constant synaptic weight, the normalized amplitudes should\nall fall on the dashed line. (A) EPSC as a function of time. The mean and the standard deviation\n(4 repetitions) are shown. Note that the response amplitude varies rapidly by more than twofold.\n(B) Excerpt at a high temporal resolution. Unpublished data from L. E. Dobrunz and C. F. Stevens,\nprinted with permission.\n(LTP and LTD). Subsequently, we treat the computational implications of these various\nforms of synaptic plasticity. Finally, we close with a brief digression about nonsynaptic\nplasticity. We will not discuss developmental plasticity, or plasticity in subcortical areas\nor in nonmammalian preparations. For more information on these topics, or more in depth\nreviews of topics considered in this chapter, the following reviews provide good starting\npoints (Madison, Malenka, and Nicoll, 1991; Ito, 1991; Hawkins, Kandel, and Siegelbaum,\n1993; Zola-Morgan and Squire, 1993; Bliss and Collingridge, 1993; Miller, 1994; Schuman\nand Madison, 1994a; Malenka, 1994; Bear and Malenka, 1994; Linden and Connor, 1995;\nCarew, 1996).\n13.1 Quantal Release\nRecall the simple form of the quantal hypothesis described in Sec. 4.2.2. The time-averaged\nresponse size R is given by"}
{"text": "===== Page 4 =====\n13.1 Quantal Release \n• \n311\n{\n  \"table_name\": \"TABLE 13.1\",\n  \"description\": \"Different Forms of Synaptic Plasticity\",\n  \"columns\": [\"Phenomenon\", \"Duration\", \"Locus of Induction\"],\n  \"rows\": [\n    {\"Phenomenon\": \"Paired-pulse facilitation (PPF)\", \"Duration\": \"100 msec\", \"Locus of Induction\": \"Pre\"},\n    {\"Phenomenon\": \"Augmentation\", \"Duration\": \"10 sec\", \"Locus of Induction\": \"Pre\"},\n    {\"Phenomenon\": \"Posttetanic potentiation (PTP)\", \"Duration\": \"1 min\", \"Locus of Induction\": \"Pre\"},\n    {\"Phenomenon\": \"Short-term potentiation (STP)\", \"Duration\": \"15 min\", \"Locus of Induction\": \"Post\"},\n    {\"Phenomenon\": \"Long-term potentiation (LTP)\", \"Duration\": \"> 30 min\", \"Locus of Induction\": \"Pre and Post\"},\n    {\"Phenomenon\": \"Paired-pulse depression (PPD)\", \"Duration\": \"100 msec\", \"Locus of Induction\": \"Pre\"},\n    {\"Phenomenon\": \"Depletion\", \"Duration\": \"10 sec\", \"Locus of Induction\": \"Pre\"},\n    {\"Phenomenon\": \"Long-term depression (LTD)\", \"Duration\": \"> 30 min\", \"Locus of Induction\": \"Pre and Post\"}\n  ],\n  \"notes\": \"Synaptic plasticity occurs across many time scales. This table lists some of the better studied forms of plasticity together with a very approximate estimate of their associated decay constants, and whether the conditions required for induction depend on pre- or postsynaptic activity, or both. This distinction is crucial from a computational point of view, since Hebbian learning rules require a postsynaptic locus for the induction of plasticity. Note that for LTP and LTD, we are referring specifically to the form found at the Schaffer collateral input to neurons in the CA1 region of the rodent hippocampus; other forms have different requirements.\"\n}"}
{"text": "This model, with modifications, has been applied with remarkable success to chemical\nsynapses throughout the nervous system.\nThe number n of release sites, and their anatomical correlate, are different at different\nsynapses. At one extreme, an axon can make a single anatomical synapse with one\nindependent site of vesicular release onto the postsynaptic target (corresponding ton = 1;\nFig. 13.2A). Single synapses are common in the hippocampus: an axon from a CAS\nhippocampus pyramidal cell usually only forms a single synapse with a CA1 neuron\n(Sorra and Harris, 1993). Frequently, the same axon makes several, independent anatomical\nsynapses with the dendrites of the same cell (Markram, Helm, and Sakmann, 1995; see the\nn = 3 case in Fig. 13.2B). From the point of view of network connectivity, all three\nsynapses correspond to a single functional connection, since stimulation of the presynaptic\naxon excites all three synaptic terminals equally. Some pathways implement a fail-safe\nstrategy by having a very large number of release sites n. Well-known examples of\nsuch \"supercharged\" (and presumably highly reliable) connections include retinal axons\nsynapsing onto geniculate relay cells (Sherman and Koch, 1998) and the climbing fiber to\nPurkinje cell synapse (Llinas and Walton, 1998). In the latter case, a single climbing fiber\ninnervates the soma and main dendrite of a cerebellar Purkinje cell by making up to 200\nsynaptic contacts. Finally, in a number of pathways a single axon terminal forms multiple\nindependent release sites within a single synaptic contact (Fig. 13.2C). The best example\nof such a synapse is the neuromuscular junction, where the terminal axon field of a motor\naxon forms on the order of 1000 release sites with the muscle (Katz, 1966).\nAs emphasized repeatedly in this book, the number of functional contacts made by\ncortical and hippocampal neurons can be quite small (from one to about a dozen). What\nis the significance of small «? Recall that release from each site is probabilistic. At a\nsynapse with n sites of release probability p, the probability that the synapse fails to release\ntransmitters following a stimulus is"}
{"text": "===== Page 5 =====\n312 \n• \nSYNAPTIC PLASTICITY\nFig. 13.2 QUANTAL MODEL OF SYNAPTIC RELEASE \nBasic unit of synaptic physiology, the\nsynoptic release site. In response to a presynaptic spike, the vesicle fuses with the presynaptic\nmembrane and releases its content, the neurotransmitter molecules, into the synaptic cleft. At each\nsuch release site, only a single vesicle is released (or fails to release) in response to a presynaptic spike.\nSynaptic transmission is both quantal and probabilistic, since the probability of release p is typically\nlow (30% or less). If a vesicle is released, it induces a mean postsynaptic response q. (A) A common\nform of excitatory synaptic connections between a pair of central neurons: one anatomical synapse\nmakes a single release site (n = 1) on its postsynaptic target. (B) Frequently, a single axon makes a\nsmall number of independent anatomical synapses on the dendrite of another cell (here n = 3). (C) At\nsome synapses, such as the neuromuscular junction, thousands of release zones act independent of\neach other. Other examples include the calyx synapse between the cochlear nerve and one of the\nbrainstem auditory nuclei. Reprinted in modified form by permission from Korn and Faber (1991).\n(13-2)\nThat is, the failure probability falls exponentially as the number of release sites increases.\nMoreover, fluctuations in the response size R are inversely proportional to */n.\nFor synapses such as the neuromuscular junction, with thousands of release sites, the\nprobability of failure under physiological conditions is very low, and fluctuations are small.\nAt central synapses, by contrast, failure is a very real possibility. For a synapse with a single\nrelease site and p = 0.3, failures occur 70% of the time. Hence transmission becomes\nunreliable (as illustrated in Fig. 4.3).\n13.2 Short-Term Synaptic Enhancement\nAs suggested by Table 13.1, any distinction between short- and long-term forms of en-\nhancement is somewhat arbitrary. It is nevertheless useful to distinguish forms that operate\non a time scale from about a millisecond to about a minute, that is, facilitation and FTP,\nfrom longer lasting forms like LTP.\n13.2.1 Facilitation Is an Increase in Release Probability\nSynaptic facilitation was first described at the frog neuromuscular junction (Feng, 1941;\nKatz, 1966; Mallart and Martin, 1968;Magleby, 1987), but has subsequently been observed\nat nearly all synapses studied. Figure 13.3 illustrates one form of short-term facilitation in\nthe mammalian central nervous system known as paired-pulse facilitation (PPF), since it\nis observed after a single pair of stimuli is delivered to a synapse. In the experiment shown\nhere, an extracellular stimulus activated a relatively large (but undetermined) number of\nfibers, and the response was recorded in a single region CA1 pyramidal neuron. Because a"}
{"text": "===== Page 6 =====\n13.2 Short-Term Synaptic Enhancement\n313\nFig. 13.3 PAIRED-PULSE FACILITATION Paired-pulse facilitation (PPF) at the CA3 to CA1 synapse\n(made by the Schaffer collaterals; see Fig 13.4) in the hippocampus slice. (A) Demonstration of PPF\nat the population level. Excitatory synaptic current recorded in a hippocampal region CA1 pyramidal\ncell following the response to a pair of extracellular stimuli (40 msec apart) delivered to the Schaffer\ncollateral inputs. With this technique, many synapses are simultaneously activated. Note that the\nsecond response is larger than the first. This increase in synaptic response decays away with a time\nconstant of about 100 msec. (B) PPF is demonstrated at an individual synapse via 21 consecutive\ntrials under conditions of minimal stimulation. In each trial, two stimuli were delivered, separated by\n40 msec. Five out of 21 trials lead to a signal for the first stimulus, but eight out of 21 on the second.\nEven though the variability in the amplitude of the postsynaptic response is high, the mean amplitudes\nof the responses in the first and second pulses are the same. The notches around 10 and 50 msec are\nstimulus artifacts. Unpublished data from L. E. Dobrunz, printed with permission.\ntypical CAS Schaffer collateral makes only about one or two synapses onto its CA1 target\ncell, this electrical stimulus will activate a comparable number of release sites. (For an\noverview of the hippocampal circuitry, see Brown and Zador, 1990 and also Fig. 13.4.)\nConsider a hypothetical example in which there are n release sites, each of which has the\nsame initial probability of release PQ. q corresponds to the average excitatory postsynaptic\ncurrent (EPSC) under voltage clamp. Under these conditions the mean response size to the\nextracellular stimulus is"}
{"text": "===== Page 7 =====\n314 \n• \nSYNAPTIC PLASTICITY\nFig. 13.4 HIPPOCAMPAL CIRCUITRY \nNeuronal elements of the hippocampal formation in rodents\nas drawn by Ramon y Cajal at the turn of the century (when it was called Ammon's horn). This\ncortical structure is implicated in the transfer from short- to long-term memory. Granule cells in the\ndentate gyrus send their output axons, so-called mossy fibers, to pyramidal cells in the CA3 region.\nThese pyramidal cells in turn project, with so-called Schaffer collaterals, onto pyramidal cells in the\nCA1 region. The majority of LTP and LTD research has been carried out at either the mossy fiber to\nCA3 synapse or the Schaffer collateral to CA1 synapse. Reprinted by permission from Brown and\nZador(1990).\n(13.3)\n(Fig. 13.3A). If a brief high-frequency (tetanic) stimulus is now delivered, the probability\nof release jumps to p\\, and the mean response to the same test stimulus jumps to np\\q.\nSubsequently, the probability of release—and therefore the response size—slowly decays\nto baseline with a characteristic time constant on the order of hundreds of milliseconds,\nmuch slower than the onset time constant.\nThat the higher response magnitude is due to an increase in the release p and not to an\nincrease in n or q is illustrated clearly by Fig. 13.3B. Here a minimal stimulation protocol"}
{"text": "===== Page 8 =====\n13.2 Short-Term Synaptic Enhancement \n• \n315\n(Raastad, Storm, and Andersen, 1992; Allen and Stevens, 1994; Dobrunz and Stevens,\n1997) is employed, so that only a single release site is activated. As seen in Fig. 13.3B,\nthe first response at low stimulus rates frequently does not lead to a postsynaptic response;\nit fails to cause the release of even a single vesicle, and is therefore termed a failure. The\nfailure rate / depends on p; if we assume one release site per synapse, / = 1 — p. In\nthis particular experiment 15 out of 21 trials produced no response (/ = 0.71) on the first\nstimulus, so on the assumption of a single release site, p = 1 — 0.71 = 0.29, that is, only\nthree out of ten presynaptic spikes will cause the release of a synaptic vesicle. Note that\neven if release does occur, the amplitude of the average postsynaptic response R is itself\nquite variable (see Fig. 4.4), because of the underlying variability in q (Bekkers, Richerson,\nand Stevens, 1990).\nIf a pair of stimuli are now delivered in rapid succession (here 40 msec), the probability\nof release on the second trial p\\ is higher than that on the first (in Fig. 13.3B, p increased\nfrom 0.29 to 0.38). This increase when a second pulse follows soon after a first, is the\nsingle-synapse correlate of the paired-pulse facilitation shown at the population level in\nFig. 13.3A. The release probability decays back to the initial probability with the same time\nconstant as does the population amplitude.\nThe onset of facilitation is nearly as rapid as can possibly be resolved: it appears after a\nsingle stimulus (but see Dobrunz, Huang, and Stevens, 1997). The decay of PPF is slower,\non the order of hundreds of milliseconds, and can be described by a simple exponential,\n(13.4)\nwhere PQ and pf are the probabilities of release before and after facilitation, respectively,\nand Tf is the characteristic decay time.\nThe implications of facilitation for neuronal computation remain unclear. One might\nspeculate that paired-pulse facilitation acts as a kind of \"burst-filter\": the probability of at\nleast one successful release will be much higher during a burst of action potentials, that is,\na handful of spikes within 10-30 msec, than if the same number of action potentials were\nuniformly distributed. In other words, a burst of spikes can be interpreted as a high-fidelity\nsignal (see also Sec. 16.2).\n13.2.2 Augmentation and Posttetanic Potentiation\nWhile some facilitation is induced after a single stimulus, the degree of facilitation increases\nwith the number of stimuli. As the number and frequency of stimuli increase, another form\nof potentiation, augmentation, is induced. Further stimulation brings into play a third form,\ntermed posttetanic potentiation (FTP). These different forms of short-term potentiation\nhave been best characterized at the neuromuscular junction (Magleby, 1987; see also Hirst,\nRedman, and Wong, 1981; Langdon, Johnson, and Barrionuevo, 1995). They differ from\none another most notably by their characteristic time constant. For instance, the time course\nof augmentation can be expressed in the same form as the dynamics of facilitation above,\n(13.5)\nwhere pa is the probability of release after augmentation and ra the characteristic decay\ntime (on the order of seconds). The formulation for FTP is identical, except that the time\nconstant is on the order of a minute or so.\nAlthough facilitation, augmentation, and PTP all operate by increasing the probability\nof release, they do so by distinct mechanisms distinguishable not only by their kinetics,"}
{"text": "===== Page 9 =====\n316 \n• \nSYNAPTIC PLASTICITY\nbut also by their pharmacology (see, e.g., Zucker, 1996). Nevertheless, these processes\ncannot be completely independent, if only because p cannot exceed unity. How these\nprocesses interact remains unclear even at the neuromuscular junction where they have\nbeen best studied.\nThe equations describing these forms of potentiation can be interpreted in terms of the\nkinetics of some factor governing release probability. It is tempting to identify that factor\nas intracellular calcium concentration in the presynaptic terminal. As we shall see in the\nnext section, the situation is rather more complex.\n13.2.3 Synaptic Release and Presynaptic Calcium\nAs briefly alluded to in Chap. 4, synaptic release is caused by a rapid increase in the\nconcentration of intracellular calcium that follows the invasion of the presynaptic terminal\nby an action potential. This increase in calcium concentration triggers fusion of a vesicle\ninside the presynaptic terminal with the presynaptic membrane and the subsequent release\nof neurotransmitters into the cleft (Fig. 4.2). Sometimes release of a single quantum occurs\nspontaneously, that is, independently of any presynaptic stimulus. The rate of spontaneous\nrelease at any one particular synapse is very low, less than one per minute. The calcium that\nrushes into the presynaptic terminal following the presynaptic spike raises the probability of\nrelease dramatically—by perhaps five orders of magnitude—over the very low spontaneous\nrate, but only for a very brief period (several hundred microseconds).\nThe molecular machinery coupling calcium in the presynaptic terminal with vesicle\nfusion and release is only beginning to be understood (see Zucker, 1996; Bauerfeind, Galli,\nand De Camilli, 1996; Rothman and Wieland, 1996; Sudhof, 1995; Matthews, 1996). The\nfirst indications that calcium was involved in synaptic transmission came from experiments\nin which the concentration of extracellular calcium was manipulated. Evoked release can be\nabolished by eliminating extracellular calcium, and reduced by lowering it. A component\nof spontaneous release persists even in the absence of extracellular calcium. More recently,\ntechnical advances have made it possible to measure presynaptic calcium directly (Delaney\nand Tank, 1994; Regehr, Delaney, and Tank, 1994; Zucker, 1996; Sabatini and Regehr,\n1996; Wu and Saggau, 1995; Helmchen, Borst, and Sakmann, 1997).\nDodge and Rahamimoff (1967) fit the dependence of the amplitude of the observed\npostsynaptic response on the extracellular calcium concentration [Ca2+]0 and magnesium\nconcentration [Mg2+]0 (which antagonizes calcium) with the following equation:\n(13.6)\nwhere Kc and Km are constants, and z is a parameter fit to the data. The best fits were\nobtained for z between 3 and 4. This equation can be derived by assuming that in order\nfor release to occur, calcium must bind to z independent sites, and that Kc and Km are the\nequilibrium constants for calcium and magnesium.\nThe Dodge-Ranamimoff relation relates release probability to the concentration of\nextracellular calcium. The dependence of release probability on the concentration of in-\nternal calcium is a thornier issue. For example, Eq. 13.6 was derived under equilibrium\nassumptions; but the rapidity with which an action potential triggers release (on the order\nof 0.15 msec; Llinas, Steinberg, and Walton, 1981b; Sabatini and Regehr, 1996) indicates\nthat the source of calcium influx must be just tens of nanometers from the synaptic vesicles.\nCalcium concentration cannot equilibrate this quickly. This has led to the notion of Ca2+"}
{"text": "===== Page 10 =====\n13.3 Long-Term Synaptic Enhancement \n• \n317\nmicmdomains (Llinas, Steinberg, and Walton, 1981b; Llinas, Sugimori, and Silver, 1995)—\nneighborhoods of high calcium concentration near the presynaptic calcium channels. Thus,\nit appears that calcium flux, rather than equilibrium bulk calcium in the presynaptic terminal,\ntriggers fast vesicular fusion.\nThe dependence of the release probability on presynaptic calcium concentration suggests\nthat residual calcium might also underlie various components of short-term plasticity (Katz\nand Miledi, 1968). According to this hypothesis, the enhanced release probability following\na train of action potentials results from an increased level of calcium in the presynaptic\nterminal which, by itself, is insufficient to sustain release, but which adds to calcium from\nsubsequent releases and thereby results in a higher release probability. In the original\nform of this hypothesis, the residual calcium simply added to the flux from an action\npotential. More recent evidence indicates that the increase in resting calcium concentration\nassociated with short-term facilitation is too slight (1 fiM; Delaney, Zucker, and Tank,\n1989; Delaney and Tank, 1994), compared with the hundreds of micromolars during the\naction potential (Lando and Zucker, 1994) for any additive effect to be relevant. The target\nof the residual calcium therefore appears to be low affinity targets that modulate release\nprobability.\n13.3 Long-Term Synaptic Enhancement\nAs noted above, the distinction between short and long-term enhancement is somewhat\narbitrary. In what follows we will consider those forms that last up to a few minutes\n(facilitation, augmentation, and posttetanic potentiation) \"short,\" while those that last longer\n\"long.\" There is, however, a more fundamental basis upon which to distinguish them: all\nthe short lasting forms of enhancement appear to depend only on the state of the presynaptic\nterminal for induction, while the longer lasting forms often require some involvement from\nthe postsynaptic side (but see, e.g., Williams and Johnston, 1989; Nicoll and Malenka, 1995).\n13.3.1 Long-Term Potentiation\nLong-term potentiation is a rapid and sustained increase in synaptic efficacy following a\nbrief but potent stimulus. First described in the mammalian hippocampus at the perforant\npath input to the dentate gyrus (Bliss and Lomo, 1973), it has since been observed at\ndiverse synaptic pathways, in the hippocampus, the neocortex, and elsewhere. LTP can last\nfor hours, days, weeks, or longer.\nLTP research is very popular: between 1990 and 1997, over 2000 papers on LTP were\npublished. The excitement stems in large part from the hope that LTP is a model for learning\nand memory, offering the most direct link from the molecular to the computational and\nbehavioral levels of analysis. The field of LTP is also very controversial, so that there is\nonly a surprisingly small number of completely accepted findings. Good reviews can be\nfound in the literature (Madison, Malenka, and Nicoll, 1991; Tsumoto, 1992; Johnston et\nal., 1992; Bliss and Collingridge, 1993; Malenka, 1995).\nAlthough LTP has been found in many neuronal structures, it has been best studied in\nthe hippocampus, at the synapses made from region CAS pyramidal cells via the Schaffer\ncollateral pathways onto region CA1 pyramidal cells (Fig. 13.4), so we will focus on this\nsynapse. Care must be exercised when comparing LTP obtained in different systems, since\ndifferent forms of LTP coexist even in the hippocampus, with some independent of NMD A"}
{"text": "===== Page 11 =====\n318 \n• \nSYNAPTIC PLASTICITY\nreceptor activation (Harris and Cotman, 1986; Johnston et al., 1992; Nicoll and Malenka,\n1995).\nAs emphasized above, it is important to distinguish the rules and mechanisms underlying\nthe induction of LTP (or any other form of synaptic plasticity) from the those governing its\nexpression. Three basic facts about the induction of LTP at the CAS to CA1 synapse are\nclear and essentially undisputed.\n1. Under physiological conditions, induction typically requires (nearly) simultaneous\npresynaptic neurotransmitter release and postsynaptic depolarization. Because of this\ninteresting fact, the mechanism of LTP has been interpreted as Hebbian (Sec. 13.5.1,\nKelso, Ganong, and Brown, 1986; Malinow and Miller, 1986; Wigstrom et al., 1986).\nFigure 13.5 illustrates the dependence of LTP on postsynaptic depolarization. Stim-\nulation at a low constant rate produces a baseline EPSP whose magnitude does not\nchange following a postsynaptic depolarization. A rapid presynaptic stimulus by itself\nduring simultaneous postsynaptic hyperpolarization also fails to induce LTP, although\nFig. 13.5 LONG-TERM POTENTIATION The induction of LTP requires simultaneous pre- and\npostsynaptic activities, as demonstrated in a seminal study (Kelso, Ganong, and Brown, 1986) at the\nSchaffer collateral synapse onto CA1 pyramidal cells in a hippocampal slice (Fig. 13.4). (A) Averaged\nsynaptic responses recorded under current clamp (upper traces) or voltage clamp (middle traces show\nmembrane potential control and lower traces show the inward EPSC). The responses in the control\nperiod are plotted on the left, while the enhanced responses, 20 min after vigorous synaptic input is\npaired with a large depolarizing current to the soma of the cell, forcing it to spike, are indicated on\nthe right. (B) Peak value of this EPSP as a function of time for different experimental manipulations.\nNeither postsynaptic depolarization (achieved by injecting a large depolarizing current into the soma)\nby itself (first trace), nor presynaptic stimulation paired with clamping the soma to —80 mV for two\ndifferent synaptic inputs (second and third traces) is sufficient to induce LTP, as seen by the constant\nresponse amplitudes. Only when presynaptic input is paired with the postsynaptic depolarization is\na long-term enhancement of the synaptic weight observed. Reprinted by permission from Kelso,\nGanong, and Brown (1986)."}
{"text": "===== Page 12 =====\n13.3 Long-Term Synaptic Enhancement \n• \n319\nit does lead to FTP. Only when vigorous synaptic input is paired with postsynaptic\ndepolarization (here via an intracellular electrode) is LTP induced: the synaptic response\nto the same baseline stimulation doubles in size and remains elevated for the duration of\nthe experiment, over an hour. Similar in vivo experiments suggest that the change can\npersist for weeks or longer. The depolarization may arise from the stimulus itself (if it\nactivates a sufficiently large number of fibers), from activation of some other synaptic\ninput, or from somatic depolarization via an intracellular electrode. The depolarization\nis necessary to relieve the Mg2+ block of NMD A receptors (see below).\n2. The second widely accepted fact about the induction of LTP at the CA3 to CA1\nsynapse is that it requires activation of NMDA receptors (Collingridge, Kehl, and\nMcLennan, 1983). NMDA receptors are unique among synaptic receptors in that they\nare directly gated by both voltage and neurotransmitter, so that they pass current only\nwhen the membrane is depolarized sufficiently to relieve a block by magnesium ions.\nThe induction of LTP is inhibited by agents that block the NMDA receptor, such as APS.\nNMDA receptor activation is not required for either normal synaptic transmission in\nthe hippocampus or the maintenance of LTP: once LTP has been induced, blockage of\nNMDA-mediated synaptic transmission by APS does not inhibit the expression LTP. This\nis also true in neocortex: activation of NMDA receptors is required for LTP induction\n(Artola and Singer, 1987; Malenka, 1995).\n3. The third fact about LTP induction is its dependency on a localized increase in the\npostsynaptic concentration of calcium (Lynch et al., 1983; Malenka et al., 1988). When\ncalcium buffers that bind any excess calcium are injected into the postsynaptic cell, the\ninduction of LTP is blocked (Barrionuevo and Brown, 1983).\nA simple model for the induction of LTP that accounts for these observations is illustrated\nin Fig. 12.12. In this model, excitatory NMDA and non-NMDA receptors are colocalized\n(Bekker and Stevens, 1989) at single synapses made by the Schaffer fibers onto spines\nof CA1 pyramidal cells. Release of neurotransmitters always activate the current through\nthe AMPA receptor-gated channel, but activates the current through the NMDA receptor-\ngated channel only when there is sufficient postsynaptic depolarization to remove the\nMg2+ blocking the channel. (Release of a single quantum of neurotransmitter is not\nbelieved to result in sufficient depolarization at the spine head to relieve the Mg2+ block.)\nInflux of calcium through open NMDA channels causes a large, localized and transient\nincrease in the concentration of postsynaptic calcium (see Sec. 12.6.2; Holmes and Levy,\n1990; Zador, Koch, and Brown, 1990; Guthrie, Segal, and Kater, 1991; Yuste and Denk,\n1995; Svoboda, Tank, and Denk, 1996). This accounts for the \"input specificity\" of LTP,\nthat is, the fact that LTP is expressed only at those synapses where the conditions for\ninduction are satisfied, since only here is [Ca2+],- large enough to trigger the biochemical\ncascade that finally leads to the establishment of LTP. More recent research, alluded\nto in Sec. 20.3, shows that under some conditions synapse specificity may break down\n(Bonhoeffer, Staiger, and Aertsen, 1989; Schuman, 1997; Engert and Bonhoeffer, 1997).\nThus the induction of LTP occurs at the postsynaptic site and requires the conjunction of\npre- and postsynaptic activity.\nThe site of expression of LTP remains controversial. Many hypotheses have been\nproposed (Lynch and Baudry, 1987; Kauer, Malenka, and Nicoll, 1988; Muller, Joly, and\nLynch, 1988; Edwards, 1991,1995; Stevens and Wang, 1994; Liao, Hessler, and Malinow,\n1995; Kullmann and Siegelbaum, 1995), which can be summarized as involving a change in"}
{"text": "===== Page 13 =====\n320 . \nSYNAPTIC PLASTICITY\neither the presynaptic element, the postsynaptic element, or both. Conjectures involving a\npresynaptic locus include an increase in (1) the number of release sites, (2) the probability of\nrelease, (3) the amount of neurotransmitter loaded into each vesicle; hypotheses involving\nthe postsynaptic terminal include (4) an increase in receptor affinity or density, and the\n(5) recruitment of previously \"silent\" synapses. As discussed in the previous chapter\n(Sec. 12.3.2), the earlier hypothesis that changes in the geometry of the postsynaptic spine\naffect its synaptic weight does not appear to be correct, at least in the case of passive spines\non hippocampal pyramidal cells.\nIf the induction of LTP occurs postsynaptically but its expression presynaptically, some-\nthing needs to signal this information back across the synaptic terminal. Several possibilities\nexist. The one that has attracted most attention is retrograde messenger molecules, that is,\nsubstances whose production is triggered postsynaptically when LTP is induced and that\ndiffuse backward across the synapse. Proposed retrograde messengers include arachidonic\nacid, and diffusible second messengers nitric oxide and carbon monoxide (Williams et al.,\n1989; Gaily et al., 1990; Schuman and Madison, 1994a; Schuman, 1995; see also Sec. 20.3).\n73.3.2 Short-Term Potentiation\nFor completeness we also mention short-term potentiation (STP), sometimes referred to as\ndecremental LTP. Much less is known about short term potentiation than about its long-\nterm counterpart (Davies et al., 1989; Colino, Huang, and Malenka, 1992; Kullmann et\nal., 1992). First detected in experiments investigating different protocols for inducing LTP,\nit was treated largely as an irritant, a potential source of artifact in experiments on LTP.\nIt is defined operationally in terms of its time constant: it is the potentiation that persists\nlonger than the minute or two of FTP, but not as long as LTP; typically it decays with a\ntime constant of about 15 min. Both its induction and its expression appear to be largely\npostsynaptic.\n13.4 Synaptic Depression\nWe treat short- and long-term synaptic depression together here, not because they are less\ninteresting or important than enhancement, but because they are not as well understood.\nThe most rapid forms of synaptic depression involve a decrease in the probability of\ntransmitter release. Immediately following a release of a vesicle (but not a failure) at a\nsingle site, there is often a 5-10 msec effective \"dead time\" during which release cannot\noccur (Stevens and Wang, 1995; Dobrunz, Huang, and Stevens, 1997). Unlike synaptic\nenhancement, the amount of depression depends not on the number of presynaptic action\npotentials, but rather on the number of vesicles released. A longer lasting form of depression\noccurs (on the order of seconds) following depletion of the available pool of vesicles\n(Stevens and Tsujimoto, 1995; Dobrunz and Stevens, 1997).\nWhile it has long been recognized that what goes up must come down, for years the\ncounterpart to long-term potentiation, termed long-term depression (LTD), could not be\nreliably induced in the CA1 region of the hippocampus (although it had been described\nin the cerebellum and the dentate gyrus; see Levy and Steward, 1983; Ito, 1991). With\nthe advent of a reliable protocol for the induction of long-term depression (Dudek and\nBear, 1992) in both hippocampus and neocortex, LTD can be easily obtained, reducing the\nsynaptic weight to about half of its pre-LTD value (Fig. 13.6). LTD is now receiving the"}
{"text": "===== Page 14 =====\n13.5 Synaptic Algorithms\n321\nFig. 13.6 LONG-TERM DE-\nPRESSION Induction of long-\nterm depression in a pair of\ncultured hippocampal neurons.\nLTD is induced by a moder-\nate amount of synaptic stimu-\nlation via a stimulus electrode\nwhile holding the postsynaptic\nmembrane at —50 mV (dou-\nble arrowhead in B). (A) Sam-\nple trace of an EPSC before\nand after the induction of LTD.\n(B) Time course of synaptic\nstrength (expressed as the ratio\nof peak EPSP after induction\nto peak EPSP before induction)\nas a function of time. Averaged\nover nine such pairs, LTD re-\nduced the normalized synap-\ntic strength to 0.44 ± 0.06.\nReprinted by permission from\nGoda and Stevens (1996).\nsame scrutiny as LTP (for surveys, see Tsumoto, 1992; Dudek and Bear, 1992; Artola and\nSinger, 1993; Malenka, 1994; Linden and Connor, 1995).\nJust as with long-term potentiation, the requisite conditions for induction must be met\nat the postsynaptic terminal; in fact, the conditions required for the induction of LTD are\nremarkably similar to those for LTP. Some forms of LTD require NMDA receptor activation;\nthe essential requirement always appears to be sufficient postsynaptic depolarization in order\nto elevate [Ca2+],- in the postsynaptic terminal above resting levels, but below that required\nfor the induction of LTP (Lisman, 1989; Artola and Singer, 1993; Malenka and Nicoll,\n1993; Linden and Connor, 1995; Cummings et al., 1996; but see Neveu and Zucker, 1996).\nIn other words, there exists a critical threshold of free intracellular calcium concentration\nthat governs the increase or decrease of the synaptic weight in a highly specific manner.\nThe mechanism underlying LTD in the hippocampus is a decrease in the probability of\nsynaptic release (Stevens and Wang, 1994,1995; Bolshakov and Siegelbaum, 1994,1995).\nIf, as suggested above, the mechanism underlying LTP turns out to be an increase in p,\nLTP and LTD would display an elegant symmetry, each exerting differential control on the\nprobability knob.\n13.5 Synaptic Algorithms\nConjectures going back to the turn of the century, Tanzi (1893; reviewed in Brown, Kairiss,"}
{"text": "===== Page 15 =====\n322 • \nSYNAPTIC PLASTICITY\nand Keenan, 1991b) implicate synapses as the locus for physiochemical changes underlying\nlearning. Wood-Jones and Porteus (1928) even speculate about the mechanism underlying\nthese changes. But early work remained silent about the conditions under which the changes\nin efficacy would occur.\n13.5.1 Hebbian Learning\nThe modern approach to synaptic algorithms can be traced to Hebb's very influential\nmonograph (Hebb, 1949), in which he prescribed\nWhen an axon of cell A is near enough to excite cell B or repeatedly or consistently takes\npart in firing it, some growth process or metabolic change takes place in one or both cells\nsuch that A's efficiency, as one of the cells firing B, is increased.\nHebb was therein the first to propose explicitly the conditions under which the change in\nefficacy would occur. It is because his proposal provides an activity-based rule for increasing\nefficacy that it is called an \"algorithm.\" In mathematical terms, it is usually expressed as\n(13.7)\nwhere Au>,-y- is the change in the strength of the synaptic coupling u>,-y between the\npresynaptic neuron / and the postsynaptic cell j; Vj and Vj represent the activities of these\nneurons. Such a pure Hebbian rule has been used extensively in associative, or content-\naddressable, memory networks (Steinbuch, 1961; Hopfield, 1982, 1984).\nIn the simplest mapping of these symbols onto biophysics, wtj = npq (Eq. 13.1), V{\ncorresponds to the presynaptic spiking frequency, and Vj to the postsynaptic membrane\npotential at the spine or in the dendrite just below. Yet it is far from clear what the exact\nrelationship between the algorithmic variables and their biophysical counterpart is. For\ninstance, does Vj really correspond to the instantaneous dendritic membrane potential or is\nit more akin to some low-pass filtered version of Vml\nHebb's original proposal is incomplete, since it offers no prescription for determining\nunder what conditions synaptic efficacy decreases (the weights can only increase) and\nbecause it is unclear what types of memories can and cannot be recovered using this rule.\nSubsequent theoretical work (e.g., Stent, 1973; Sejnowski, 1977a,b; Palm, 1982; Linsker,\n1988; for a survey, see Hertz, Krogh, and Palmer, 1991) elaborated on the conditions under\nwhich the synaptic weight should change.\nOne popular modern variant of Hebb's original rule is known as the covariance rule\n(Sejnowski, 1977a), so named because it is formally identical to a statistical covariance. It\ncan be written simply as\n(13.8)\nwhere (V () corresponds to the average activity of the presynaptic neuron over some suitable\ntime interval (similarly for (Vj)). If the actual presynaptic activity is less than its recent\naverage while the postsynaptic activity is elevated, or vice versa, the synaptic weight will\ndecrease.\nNote that we did not specify over what duration the averages should be taken. If the\naverage is short—on the order of seconds to minutes—this formulation can be used to\ndescribe LTP and LTD, while if it is longer it can be used to describe developmental effects\n(see e.g., Miller, 1994). The covariance rule says the coupling between neurons i and j\nshould increase if their activities are correlated; but that otherwise it should decrease. This"}
{"text": "===== Page 16 =====\n13.5 Synaptic Algorithms \n• \n323\nrule has been used by Hopfield (1984) and many others as the basis for autoassociative\nmemories.\nOne appealing feature of formulating Hebb's rule according to Eq. 13.8 is that it can\nbe expanded to yield terms that have plausible biophysical interpretations. Multiplying the\nright-hand side of Eq. 13.8, and assigning separate positive constants k\\ • • • £4 to the terms,\nwe have\n(13.9)\nThe first term is the usual Hebbian one, corresponding to an increase following simultaneous\npre- and postsynaptic activity. The second and third terms correspond to homo- and\nheterosynaptic LTD.2 The last term corresponds to a tonic increase. While this formulation is\nclearly an oversimplification, the correspondence to interpretable biophysical phenomena—\ncoupled with its widespread use in the field of neural networks—makes it a very useful\nstarting point.\nRecent evidence (in particular Engert and Bonhoeffer, 1997) suggests that synaptic\nspecificity in LTP may break down at short distances. That is, excitatory synapses within\n50-70 [im of the potentiated synapse on the same neuron also have their synaptic weight\nincreased—despite a lack of presynaptic activity. If this is also true in the intact animal,\nthe above learning rules have to be modified to account for less specific synaptic storage\nschemes (as in Eq. 20.6).\n13.5.2 Temporally Asymmetric Hebbian Learning Rules\nMapping V,- onto the presynaptic spiking frequency (an averaged quantity) as above implies\nthat the exact temporal relationship between the arrival times of the presynaptic spike and\nthe postsynaptic depolarization does not matter. Experimental evidence indicates that it\ndoes, with powerful functional consequences.\nEvidence from both hippocampal and neocortical pyramidal cells indicates that in order\nfor the synaptic weight to increase, presynaptic activity has to precede the postsynaptic\none (Levy and Steward, 1983; Gustafsson et al., 1987; Debanne, Gahwiler, and Thompson,\n1994). Particularly compelling evidence comes from a recent experiment by Markram et\nal., (1997) in which they systematically varied the relationship between the presynaptic spike\narriving at the synapse and the timing of the postsynaptic action potential that propagates\nback into the dendritic tree to the postsynaptic site (for more details see Chap. 19). If\nthe presynaptic spike precedes the postsynaptic one, as should occur if the presynaptic\ninput participates in triggering a spike in the postsynaptic cell, long-term potentiation\noccurs, that is, the synaptic weight increases. If the order of temporal arrival times is\nreversed (e.g., from +10 to —10 msec), the synaptic weight decreases. This enables the\nsystem to assign credit to those synapses that were actually responsible for generating the\npostsynaptic spike.\nWhat this teaches us is that the \"static\" Hebb learning rule (e.g., in Eq. 13.8) needs to be\nreplaced by a dynamic version that is asymmetric in time, that is, a positive delay between\nthe arrival times of the presynaptic and postsynaptic spikes does not have the same effect\non the synaptic weight changes as the reverse situation.\nThe use of temporally asymmetric Hebbian learning rules can induce associations over\ntime, and not just between simultaneous events. As a result, networks of neurons endowed\n2. In homosynaptic LTP, the activated input by itself depolarizes the postsynaptic site sufficiently to induce LTP, while in\nheterosynaptic LTP a second simultaneously active synaptic input needs to provide the requisite depolarization."}
{"text": "===== Page 17 =====\n324 \n• \nSYNAPTIC PLASTICITY\nwith such synapses can learn sequences (Minai and Levy, 1993), enabling them to predict the\nfuture state of the postsynaptic neuron based on past experience (Abbott and Blum, 1996).\nAsymmetric Hebbian rules have been applied to a host of biological learning paradigms such\nas bee foraging and reinforcement learning in the basal ganglia (Montague and Sejnowski,\n1994; Montague et al., 1995; Montague, Dayan, and Sejnowski, 1996) and the learning of\nfine temporal discriminations at the single-neuron level (Gerstner et al., 1996).\n73.5.3 Sliding Threshold Rule\nA quite distinct theoretical framework for synaptic plasticity has its origin in a model for\ndevelopmental plasticity in the visual cortex. It is known as the sliding threshold or the\nBCM theory after the initials of the authors who proposed this synaptic rule (Bienenstock,\nCooper, and Munro, 1982; see also Bear, Cooper, and Ebner, 1987).\nAs in a standard learning rule, the synaptic modification is Hebbian, that is, the weight\nchange is proportional to the product of the pre- and postsynaptic activities. The exact form\nof Aw,-; is given by the product of the presynaptic activity VJ and a function <j> of the\npostsynaptic response Vy and a variable threshold Om,\n(13.10)\nFigure 13.7A illustrates (/) as a function of the postsynaptic activity V,-; its key feature is\nthat 4> is zero at zero, becomes negative, and changes sign at a critical threshold 9m. Thus,\nBCM predicts that synaptic input activity that is too weak (that is, that lies below 9m) will\ncause LTD, while strong synaptic input leads to LTP. Some of the evidence discussed in\nSec. 13.3.1 is in agreement with this. Dudek and Bear (1992) provided further support by\nvarying the frequency of presynaptic stimulation (roughly proportional to the V,- term in\nEq. 13.8) over two orders of magnitude and observing LTD at low frequencies, but LTP at\nhigher ones (Fig. 13.7B).\nThe threshold 9 is required to be a supralinear function of the time averaged postsynaptic\nactivity (that is, it must grow more than a linear function; typically, 9m = (V?) is chosen).\nThis has the important consequence that the value of the threshold must be the same at all\nsynapses onto a particular neuron; yet the associated ivy's can still change at different rates\ndepending on the level of presynaptic activity (Eq. 13.8).\nThe sliding threshold results in a stable activity level for the cell. If the activity is\ntoo low, 9m decreases until the appropriate tify 's have increased to bring the postsynaptic\nactivity up and vice versa. Thus, the threshold modification serves to stabilize the synaptic\npopulation, a crucial property of any developmental rule. To what extent the threshold\nseparating LTD from LTP induction changes as a function of the activity of the cell is not\nknown experimentally. A number of possible molecular mechanisms, based on the influx\nof calcium into the spine, exist that can instantiate such a threshold (see Bear, 1995, for a\ndiscussion).\n13.5.4 Short-Term Plasticity\nThe computational implications of rapid forms of plasticity have not received nearly as\nmuch attention as those of long-term forms, perhaps because of the paucity of network\nmodels that make use of real dynamics. Nevertheless, the rapid forms exert large effects on\nthe magnitude of the synaptic response (Dobrunz and Stevens, 1996; Fig. 13.1). Changes\nof this magnitude—as large or larger than those typically induced by LTP—suggest an\nimportant functional role for these rapid forms."}
{"text": "===== Page 18 =====\n13.5 Synaptic Algorithms\n325\nFig. 13.7 \"SLIDING THRESHOLD\" THEORY OF SYNAPTIC LEARNING \nThe Bienenstock, Cooper,\nand Munro (1982) learning rule, proposed within the context of developmental learning, is a Hebbian\nrule with a twist. The synaptic weight change is proportional to the product of the presynaptic activity\nand a function <t> (Eq. 13.10). This function depends on the postsynaptic activity in the manner illustrated\nin (A). A critical feature is the modifiable threshold 9m: postsynaptic activity less than this threshold\ncauses LTD, while higher activity leads to an increased synaptic weight (LTP). The sliding threshold\nchanges as a supralinear function of the time-averaged postsynaptic activity. (B) Direct experimental\nevidence bolstering the arguments for the existence of 0 with a similar shape as used in the BCM\nmodel (Dudek and Bear, 1992). Here, 900 spikes are generated in the Schaffer collaterals to the CA1\npyramidal cells in a hippocampal slice via electrical stimulation, varying at frequencies between 0.5\nand 50 Hz. Presynaptic stimulation frequencies below 10 Hz always lead to a reduction in the slope of\nthe EPSP (relative to baseline), that is, in LTD, which persists without any sign of recovery for at least\none hour. Higher stimulation frequencies lead to LTP. These effects are dependent on NMDA receptor\nactivation. Reprinted by permission from Dudek and Bear (1992).\nA novel way of thinking about short-term changes in synaptic weight has come about\nby work among teams of experimentalists and theoreticians (Markram and Tsodyks, 1996;\nTsodyks and Markram, 1997; Abbott et al, 1997; for a summary see Zador and Dobrunz,\n1997). Short-term depression (see Table 13.1 and Fig. 13.1) affects the postsynaptic response\nto a regular train of spikes at a fixed frequency /. While the response to the first spike is\nlarge, subsequent responses will be diminished until they reach a steady-state (Fig. 13.8A).\nFor firing rates above 10 Hz, the asymptotic relative synaptic amplitude per impulse A(f)\n(with A < 1) is approximately inversely proportional to the stimulus rate,\n(13.11)\nwith C some constant. This depression generally recovers within a second or so. The\npostsynaptic response per unit time is therefore\n(13.12)\nThat is, the steady-state synaptic response is independent of the stimulus rate, rendering\nthe synapse very sensitive to changes in the stimulus rate. The instantaneous response to a\nrapid increase A/ in its presynaptic firing rate is given by\n(13.13)\nAs a consequence, the transient change in the postsynaptic response will be proportional"}
{"text": "===== Page 19 =====\n326 \n• \nSYNAPTIC PLASTICITY\nFig. 13.8 SHORT-TERM DEPRESSION AND ADAPTING SYNAPSES Short-term synaptic depression\ncan implement a Weber-Fechner-like law at the synaptic level, whereby the postsynaptic response\nis proportional to the fractional change in firing rate (Abbott et al., 1997). (A) Field potentials, a\nmeasure of the postsynaptic response, recorded in layer 2/3 of rat visual cortex slices evoked by a\nPoisson train of extracellular stimulation in layer 4. The lines show the data and the dots the fit of a\nmathematical model. Onset and recovery from depression are clearly seen. (B), (C) Responses of a\nsimulated integrate-and-fire neuron with depressing synapses to a sudden step increase in the afferent\nfiring rate at the time indicated by the arrow. In (B) the firing rate of the afferents increased from 25\nto 100 Hz and in (C) from 50 to 200 Hz. Because in both cases A/// = 3, the postsynaptic response\nis nearly equal, only signaling relative changes. The time scale applies to both B and C. Unpublished\ndata from L.F. Abbott, printed with permission.\nto the relative change in firing frequency: increasing the firing rate fourfold, from 25 to\n100 Hz, has as much effect as going from 50 to 200 Hz (Fig. 13.8B and C). As Abbott\nand his colleagues (1997) point out, this behavior is reminiscent of the Weber-Fechner law\nof psychophysics, stating that humans are sensitive to relative and not absolute changes in\nsignal intensity (e.g., in irradiance or sound amplitude).\nThis is rather elegant. The very hardware used to carry out computations (synapses)\ncontinuously adapts to their input, only signaling relative changes, enabling the system to\nrespond in a very sensitive manner in the face of a constantly and widely varying external\nand internal environment. In contrast, digital computers are carefully designed to avoid\nadaptation and other usage-dependent effects from occurring. Interestingly, single-transistor\nlearning synapses—based on the floating-gate concept underlying erasable programmable\nROM digital memory—have now been built in a standard CMOS process (Diorio et\nal., 1996; Koch and Mathur, 1996). Similar to synapses, they can change their effective\nweights in a continuous manner while they carry out computations. Whether they will find\nwidespread applications in electronic circuits remains to be seen.\nA particularly intriguing finding comes from Markram and Tsodyks (1996), who studied\nthe interaction of LTP and short-term plasticity. They found that LTP has no effect on the\nsteady-state response to a train of stimuli, but does affect the transient component. They\nsuggested that the effect of LTP is to \"redistribute\" the synaptic response in time, increasing"}
{"text": "===== Page 20 =====\n13.6 Nonsynaptic Plasticity \n• \n327\nthe response to the first few impulses in a stimulus train at the expense of the next few,\nbut leaving the asymptote unaffected. These experiments suggest that the effects of long-\nterm plasticity might be mediated by modifications of short-term plasticity. Attempts to\nconstruct a general framework for computing with dynamic synapses are underway (Maass\nand Zador, 1998).\n73.5.5 Unreliable Synapses: Bug or Feature?\nWe have seen that single synapses in the mammalian cortex appear to be unreliable: release\nat single sites can occur as infrequently as one out of every 10 times (or even less) that an\naction potential invades the presynaptic terminal (Fig. 4.3). This should be contrasted with\nthe reliability of a transistor in today's highly integrated silicon circuits, which is very, very\nclose to 1. (The probability of failure of a digital CMOS inverter can be estimated to be less\nthan 10~14 per switching event.)\nIt is natural to wonder whether synaptic unreliability is an unfortunate but necessary\nproperty that the brain must accept due to biophysical constraints (in particular, the problem\nof packing on the order of one billion synapses, each firing at least several times each second,\ninto one cubic millimeter of cortical gray matter). It might be possible that synapses this\nsmall simply cannot be made reliable. Alternatively, might there be some computational\nadvantage to this unreliability? Or, formulated as a bon mot, is the lack of synaptic reliability\na \"bug\" or a \"feature\"?\nIf there are indeed constraints on the potential reliability of cortical synapses, they do not\ninvolve the fidelity with which a single presynaptic action potential can be converted into\nvesicular release. We know this because the probability of release at unreliable synapses\ncan sometimes increase nearly to unity, following synaptic enhancement. However, it may\nbe that the limit is not on the fidelity of transduction, but on the total number of vesicles\nthat can be released in some interval. Thus it is possible that the release cannot be sustained\nduring periods of high presynaptic activity if, for example, there is a limit to the uptake\nrate at which released vesicles can be recycled. This issue may be resolved experimentally\nif synapses that are as compact as hippocampal synapses are found for which release is\nreliable even during periods of sustained activity (Smetters and Zador, 1996).\nAn alternative view is that there is some computational advantage to having unreliable\nsynapses. The theme running through this chapter—that change in the probability of release\nis the mechanism underlying many forms of plasticity—suggests one possible advantage.\nIt appears that release probability is a parameter that can be modified conveniently and\ndynamically on a short time scale. In this view the lack of reliability is required to give\na synapse its large dynamic range, since varying either the number of release sites n, or\nthe postsynaptic response q, over an equally large range is much more demanding. Only\nif most synapses have relatively low release probabilities can modulation of p implement\nchanges in efficacy. The tradeoff is between reliability and the bandwidth of modulation of\nthe postsynaptic response.\n13.6 Nonsynaptic Plasticity\nIt should not come as a surprise to us that neuronal plasticity is not restricted to synapses.\nThe most obvious-—and best understood—examples of nonsynaptic plasticity are those\ngoverning short-term changes in neuronal firing. Indeed, the adaptation in firing frequency"}
{"text": "===== Page 21 =====\n328\nSYNAPTIC PLASTICITY\nin response to a constant current input seen in most pyramidal cells can be considered to\nbe a form of nonsynaptic plasticity (Fig. 13.9). Like synaptic adaptation, firing adaptation\noccurs on a spectrum of time scales, from milliseconds to seconds. As discussed in Sec. 9.3,\nadaptation is mediated by changes in a slow and calcium-dependent potassium current.\nChanges in firing patterns over days or longer, corresponding to developmental time scales,\nhave also been observed (Spitzer, 1994; Turrigiano, Abbott, and Marder, 1994).\nThere also is evidence for changes in specific ionic currents during associative condi-\ntioning. Alkon and his colleagues have established a direct link in the living animal—in\nhis case the sea snail Hermissenda—between a classical conditioning task and changes\nin two potassium currents (for a review, see Alkon, 1987). Rotation of this mollusk, as\nwould occur naturally during turbulence in the ocean, elicits a clinging response of its\n\"foot.\" During associative conditioning, a gradient of light is paired with rotation and the\nanimal learns to \"associate\" the light with the rotation. Following training, the light stimulus\nby itself triggers the foot clinging response. A large component of this response can be\ntraced back to an enhanced photoresponse in the type B photoreceptor (in Hermissenda,\nthese photoreceptors generate action potentials and receive direct and indirect synaptic\ninput from hair cells sensitive to the rotation of the animal). Specifically, after training,\na transient and inactivating A-like potassium current and a calcium-dependent potassium\ncurrent in the cell body of the photoreceptor are reduced by 30-40% (Alkon, Lederhendler.\nand Shoukimas, 1982; Alkon et al., 1985). These changes occur postsynaptic to inputs from\nother photoreceptors and from the hair cells, last for days, and are evident after blockage\nof all synaptic input, underlining the fact that learning affects not only synapses but also\nmembrane currents at the soma and elsewhere.\nTheoretical work in this area, exploiting both supervised and unsupervised learning rules\nto modify voltage-dependent membrane conductances in the soma and dendrites to achieve\nFig. 13.9 FIRING FREQUENCY ADAPTATION Is A FORM OF NONSYNAPTIC PLASTICITY \nMany\ncortical neurons show spike frequency adaptation in response to a prolonged current step. Conceptu-\nally, this can be thought of as a form of learning, in this case learning to adapt out the sustained, and\ntherefore predictable, component of the current. This figure portrays the spike train of a region CA1\npyramidal neuron in response to a 2-sec, 10-pA current step. Note that the spike rate begins very high\nand declines steadily. Unpublished data from A. Zador, printed with permission."}
{"text": "===== Page 22 =====\n13.7 Recapitulation \n• \n329\na particular behavior, is still in its infancy (Zador, Claiborne, and Brown, 1992; Bell, 1992;\nKoch etal., 1996).\n13.7 Recapitulation\nBehavioral plasticity or adaptation is critical to an organism's survival. Adaptation occurs\nthroughout the nervous system and on many different time scales, from milliseconds\nto days and even longer. Changes in synaptic strength are widely postulated to be the\nprimary biophysical substrate for many forms of behavioral plasticity, including learning\nand memory, although our understanding of the link remains far from complete. Use-\ndependent forms of synaptic plasticity have been characterized in many preparations.\nSynaptic strength or weight can be characterized by the triplet (n, p, q), where n is\nthe number of release sites, p the probability of synaptic release, and q the amplitude of\nthe postsynaptic response following the docking of a single vesicle. The induction of most\nshort-term forms of plasticity depends only upon the history of activity in the presynaptic\nterminal, while longer term forms require that appropriate conditions be met at both the pre-\nand the postsynaptic sites. It is therefore only these longer term forms of plasticity that can\nimplement Hebbian type of learning. Biophysical modulation of p underlies most short-term\nforms, and appears to account for at least a component of some long-term forms as well.\nBy far the best studied biophysical model of long-term synaptic change is LTP. The\ninduction of LTP requires a conjunction of presynaptic neurotransmitter release, combined\nwith postsynaptic depolarization of the postsynaptic site. There appears to be quite a\nrequirement for the presynaptic input to precede firing activity in the postsynaptic cell.\nThese experimentally observed forms of plasticity can be described at a more abstract level\nin terms of synaptic algorithms, in particular by temporally asymmetric Hebbian learning\nrules. Such formulations are useful because a great deal is known from the literature on\nartificial neural networks about the computational possibilities of Hebbian synapses.\nIt is important to realize the prevalence of usage-dependent forms of synaptic plasticity.\nWhile digital transistors have been designed to be as constant as possible at switching\nspeeds of hundreds of megahertz over the lifetime of the processor, a single synapse will\nvary its weight considerably in response to two or more consecutive spikes. These short-\nterm changes can take many forms, including depression and facilitation. In at least one\ncase, the three different types of synaptic input to layer 4 spiny cells show the entire gamut\nof short-term plasticity: none, short-term depression and short-term enhancement (Stratford\net al., 1996). In summary, synaptic properties show a complex dependency on their previous\nhistory of usage and on the postsynaptic activity. We are only beginning to understand the\ncomputational significance of such dynamic switching elements."}
{"text": "===== Page 3 =====\n332\nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nFig. 14.1 WHAT Is THE FIRING RATE Definition of the firing rate. The starting point is numerous\ntrials in which the same stimulus is repeatedly presented to the animal and the spikes generated by\nsome cell are recorded. These are shown in the raster diagram at the top, taken from a cell in cortical\narea V4 in the awake monkey. The stimulus—a grating—is flashed on at 0 and lasts until 1500 msec.\nTwenty-three of these trials are averaged, smoothed with a Gaussian of 2-msec standard deviation\na and normalized. This averaging window is so small that it effectively defines the instantaneous\nfiring rate f ( t ) . These plots are known aspoststimulus time histograms (PSTHs). The two lower plots\nillustrate an average firing rate (/(?)} obtained from the raster diagrams using Gaussian smoothing\nwith CT set to 20 and 200 msec. In many experiments, only the average number of spikes triggered\nduring each trial, corresponding to a very large value of a (see arrow at 19.5 Hz), is used to relate\nthe cellular response to the behavior of the animal. It is important to realize that a single neuron only\nsees spike trains and not a smoothly varying firing rate. Unpublished data from D. Leopold and N.\nLogothetis, printed with permission.\n(14.1)\nwhere the first sum j is executed over the n identical trials and the second sum over all rij\nspikes at time fy during the jth trial. Notice that the 8 terms have the dimension of spikes"}
{"text": "===== Page 4 =====\n14.1 Rate Codes, Temporal Coding, and All of That \n• \n333\nper St, so that the average {/(?))' has the correct units associated with a rate. Instead of\na rectangular window, a frequent alternative is smoothing the spike train using a Gaussian\nconvolution kernel with standard deviation a,\n(14.2)\n(Fig. 14.1). Because of the success in linking (/(/)) with behavior, it has been assumed by\nsome that only the mean rate, averaged over a significant fraction of a second or longer, is\nthe relevant code in the nervous system and that the detailed time course of spikes is not.\nThe past decade has witnessed a revival in the question to what extent an average rate\ncoding neglects information. (For an expose of these ideas, see the superb textbook by Rieke\net al., 1996.) On the basis of signal-processing and information-theoretical approaches,\nwe know that individual motion-selective cells in the fly (Bialek et al., 1991), single\nafferent axons in the auditory system in the bullfrog (Rieke, Warland, and Bialek, 1993)\nand single neurons in the electrosensory system in weakly electric fish (Wessel, Koch,\nand Gabbiani, 1996) can carry between 1 and 3 bits of sensory information per spike,\namounting to rates of up to 300 bits per second. This information is encoded using\nthe instantaneous rate with a resolution of 5 msec or less. And the elegant experiments\nof Markram and his colleagues (1997), demonstrating the effect a short delay between\na presynaptic and a postsynaptic spike arriving at a synapse can have on its weight\n(Sec. 13.5.4), provide a biophysical rationale for why timing at the 10 msec level is crucial\nfor synaptic plasticity.\nWe summarize this vast body of work (see Rieke et al., 1996) by concluding that in many\ninstances (f(t)}—averaged over a 5-10 msec time frame—appears to be the relevant code\nused to transmit information:\nMore complex neuronal codes do exist and are frequently referred to under the catchall\nterm of temporal coding. (For an exhaustive listing of possible codes, see Perkel and\nBullock, 1968.) However, because of the implication that rate codes do not preserve detailed\ntiming information, we prefer the term coined by Larry Abbott (personal communication),\ncorrelation coding.\nIn an instantaneous firing rate code, the generation of each spike is independent of other\nspikes in the trains (neglecting the refractory period and bursting), only a single number,\nthe rate matters. In a correlation code this assumption is abandoned in favor of coupling\namong pairs, triplets, or higher order groupings of spikes.\nTo give one example of a correlation code, let f ( t ) in response to some stimulus be a\nmaintained response. We assume that this cell is very noisy with distinct spike patterns on\nindividual trials. Averaging over them all leads to a flat response of amplitude fc. In a rate\ncode, fc is the only information available to a postsynaptic neuron. Closer inspection of\nthe microstructure of spiking reveals that the intervals between consecutive spikes are not\nindependent of each other, but that two short spike intervals are always followed by a long\none. Any code that fails to exploit these higher order correlations among four consecutive\nspikes would miss something. For experimental evidence of such codes see the references\n(Segundo et al., 1963; Chung, Raymond, and Lettvin, 1970; Optican and Richmond, 1987;\nEskandar, Richmond, and Optican, 1992; Richmond and Optican, 1992).\nA generic problem with the assumption of correlation codes is the question of decoding.\nIt is unclear what sort of biophysical mechanisms are required to exploit the information\n1. Sometimes also written as ( f ( t } ) j - to express its dependency on the size of the averaging window."}
{"text": "===== Page 5 =====\n334 \n. \nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nhidden in such correlations among spikes. They might be prohibitively complicated to\nimplement at the membrane level.\nSo far we have said little about population coding. Once again, one can distinguish two\nbroad types of codes, correlated ones and noncorrelated ones (Abbott, 1994). The latter are\nstraightforward: here the information from numerous neurons is combined into a population\ncode but without taking account of any correlations among neurons (Knight, 1972a,b). There\nis plenty of good evidence for such codes in a great variety of different sensory and motor\nsystems, ranging from the four cricket cereal interneurons encoding the direction the wind is\nblowing from (Theunissen and Miller, 1991) to the larger ensembles encoding the direction\nof sound in the barn owl (Knudsen, du Lac, and Esterly, 1987; Konishi, 1992) and eye\nmovements in the mammalian superior colliculus (Lee, Rohrer, and Sparks, 1988) to the\nposterior parietal cortex in the monkey encoding our representation of space (Pouget and\nSejnowski, 1997).\nCorrelation population codes exploit the exact temporal relationships among streams of\naction potentials. One way to discover such codes is to record from two or more neurons\nsimultaneously and to measure their cross-correlation function. For instance, it may be that\ntwo presynaptic neurons always generate spikes within 1 or 2 msec of each other. Much\ntechnological advance has occurred in this area in recent years, so that multi-unit recordings\nhave now become routine.\nIn a variety of sensory systems in both invertebrates and vertebrates, physiological\nevidence indicates that cross correlations among groups of cells appear to encode various\nstimulus features (Freeman, 1975; Abeles, 1982a, 1990; Strehler and Lestienne, 1986;\nEckhorn et al., 1988; Gray et al., 1989; Bialek et al., 1991; Eskandar, Richmond, and\nOptican, 1992; Konishi, 1992; Singer and Gray, 1995; Decharms and Merzenich, 1996;\nWehr and Laurent, 1996). The best evidence to date linking neuronal synchronization\ndirectly to behavior comes from the bee's olfactory system (Stopfer et al., 1997). When\npharmacological agents were used to block cellular synchronization—without interrupting\nneuronal activity per se—fine olfactory discrimination was disrupted.\nMuch theoretical work using pulse-coded neural networks has focused on the idea that\nspike coincidence across neurons encodes information (Sejnowski, 1977a; Abeles, 1982a,\n1990; Amit and Tsodyks, 1991; Koch and Schuster, 1992; Griniasty, Tsodyks, and Amit,\n1993; Abbott and van Vreeswijk, 1993; Zipser et al., 1993; Softky, 1995; Hopfield, 1995;\nMaass, 1996; van Vreeswijk and Sompolinsky, 1996). Indeed, it has been proposed that\nthe precise temporal correlation among groups of neurons is a crucial signal for a number\nof perceptual processes, including figure-ground segregation and the binding of different\nattributes of an object into a single coherent percept (Milner, 1974; von der Malsburg,\n1981; von der Malsburg and Schneider, 1986), selective visual attention (Niebur and Koch,\n1994), and even the neuronal basis of awareness (Crick and Koch, 1990; for a review see\nKoch, 1993).\nFrom our point of view as biophysicists, a correlation population code based on coinci-\ndence detection has the significant advantage that it is straightforward to implement at the\nmembrane level (witness Fig. 21.2).\nThe question of rate versus correlation coding remains with us. Yet this stark either-or\ndichotomy is not very useful. Clearly, the timing of spikes, at least at the 10 msec level, is\nimportant. And in some systems, detailed information across groups of cells will also prove\nto be of relevance, although for which properties and at what time scale is an open question.\nIt therefore behooves us to study how accurately and reliably neurons can generate\nindividual action potentials and how robust these are to noise. We here lay the groundwork by"}
{"text": "===== Page 6 =====\n14.2 Integrate-and-Fire Models \n• \n335\nintroducing pulse neurons as well as firing rate models and describing their basic properties.\nWe believe that such single-cell models represent the most reasonable tradeoff between\nsimplicity and faithfulness to key neuronal attributes. The following chapter will deepen our\ndiscussion of stochastic aspects of neuronal firing. We will also discuss firing rate models.\n14.2 Integrate-and-Fire Models\nWe turn to a very simple, but quite powerful model of a spiking cell with a long and distin-\nguished history, first investigated by Lapicque (1907, 1926) before anything specific was\nknown about the mechanisms underlying impulse generation. In its vanilla flavored version,\nit is known as the integrate-and-fire model (Stein, 1967a,b; Knight, 1972a; Jack, Noble, and\nTsien, 1975; Tuckwell, 1988b; frequently also referred to as voltage threshold or Lapicque's\nmodel in the older literature). Its simplicity rivals physics' linear oscillator model, yet it does\nencapture the two key aspects of neuronal excitability: a passive, integrating subthreshold\ndomain and the generation of stereotypical impulses once a threshold has been exceeded.\nIn the world of high speed electronics, integrate-and-fire models have their counter-\npart in the class of one-bit analog-digital converters known as oversampled Delta-Sigma\nmodulators (Wong and Gray, 1990; Aziz, Sorensen, and van der Spiegel, 1996).2\nThe nonleaky or perfect integrate-and-fire unit consists but of a single capacitance for\nintegrating the charge delivered by synaptic input in addition to a fixed and stationary\nvoltage threshold Vth for spike initiation (Fig. 14.2). The leaky or forgetful integrate-and-\nfire model includes a resistance, accounting for leakage currents through the membrane.\nWhile integrate-and-fire models do not incorporate the detailed time course of the action\npotential, the effect of adaptation can be included. Indeed, the current-frequency relationship\nof such an integrate-and-fire cell with a handful of parameters can be very close to that of\na much more complex, conductance-based cell model.\n14,2.1 Perfect or Nonleaky Integrate-and-Fire Unit\nWe will be considering a number of variants of integrate-and-fire \"units.\" All are char-\nacterized by a subthreshold domain of operation and a voltage threshold Vtn for spike\ngeneration. The perfect integrate-and-fire unit deals with subthreshold integration via a\nsingle capacitance C. While unphysiological, it is mathematically tractable, which is why it\nis frequently invoked for pedagogical purposes. For the sake of mathematical convenience\nwe assume the input to be a current I(t), arising either from synaptic input or from an\nintracellular electrode. The generalization to a conductance-based input is straightforward.\nThe voltage trajectory of the perfect integrator is governed by the first-order differential\nequation\n(14.3)\nTogether with an initial condition Eq. 14.3 specifies the subthreshold time course of the\nmembrane potential.\nOnce the potential reaches Vtb, a pulse is triggered and the charge that has accumulated\non the capacitance is shunted to zero (through the open switch in Fig. 14.2A). This would\nnormally be accomplished by the various conductances underlying spiking. Sweeping the\n2. A significant body of mathematics has sprung up around these AS modulators that should be explored for its relevance\nto neuroscience; see, for example Norsworthy, Schreier, and Temes (1996)."}
{"text": "===== Page 7 =====\n336 • \nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nFig. 14.2 INTEGRATE-AND-FIRE MODELS Three basic variants of integrate-and-fire units. Com-\nmon to all are passive integration within a single compartment for the subthreshold domain and\nspike generation accomplished with a voltage threshold Vih- Whenever the membrane potential V(t)\nreaches Vth, a pulse is generated and the unit is short-circuited. For a duration tlef following spike\ngeneration, any input I ( t ) is shunted to ground (corresponding to an absolute refractory period).\n(A) The perfect or nonleaky integrate-and-fire model contains but a capacitance. (B) The leaky or\nforgetful integrate-and-fire unit accounts for the decay of the membrane potential by an additional\ncomponent, a leak resistance R. (C) The adapting integrate-and-fire unit with six free parameters\n(Eqs. 14.12 and 14.13) shows firing rate adaptation via the introduction of gadapt» corresponding\nto a calcium-dependent potassium conductance (in addition to the absolute refractory period).\nEach spike increments the amplitude of the conductance; its value decays exponentially to zero\nbetween spikes.\ncharge to \"ground\" has the effect of instantaneously resetting V(t) to zero. Because the\nmodel has no pretense of mimicking the currents involved in shaping the action potential,\nspike generation itself is not part of the model. Formally, we model the action potential\nby assuming that at the instant t' at which V(t') — Fth (or the first time V(t) exceeds\nVfa for models with instantaneously rising EPSPs) an output pulse, described by the delta\nfunction 8 (t — t'), is generated. The successive times t{ of spike occurrence are determined\nrecursively from the equation\n(14.4)"}
{"text": "===== Page 8 =====\n14.2 Integrate-and-Fire Models\n337\nAs discussed in section 6.4, a canonical way in which experimentalists characterize a\ncell's behavior is by determining its discharge or /-/ \ncurve, the relationship between\nthe amplitude of an injected current step and the average firing frequency (defined over an\ninterval longer than the interspike interval; as in Eq. 14.1, {/) is computed as the inverse\nof the interspike interval).\nIn response to a sustained current, the membrane potential will charge up the capacitance\nuntil Vih is reached and V is reset to zero. The larger the current, the smaller the intervals\nbetween spikes and the higher the firing rate, according to\n(14.5)\nThree features are worthwhile here. (1) The firing rate is linearly related to the input current\n(Fig. 14.3B). (2) Arbitrarily small input currents will eventually lead to a spike, since no\ninput is forgotten. (3) The output spike train is perfectly regular. Of course, real neurons\nrarely, if ever, respond to a sustained current injection with a regular discharge of spikes\nbut instead show substantial variability in the exact timing of the spikes. This is particularly\ntrue of neurons recorded in vivo (Holt et al., 1996). The following chapter will deal with\nthis situation.\nFig. 14.3 SPIKING IN A LEAKY INTEGRATE-AND-\nFIRE MODEL \nAverage firing frequency, deter-\nmined as the inverse of the interspike interval,\nas a function of the amplitude of a maintained\ncurrent input, for a leaky integrate-and-fire unit\n(Fig. 14.2B). (A) Exemplar trace of such a unit\nreceiving a current step input with / = 0.5 nA.\nBefore the membrane potential has time to reach\nequilibrium, the unit spikes. Vth = 16.4 mV, C =\n0.207 nF, R = 38.3 Mfl, and fref = 2.68 msec.\n(B) /-/ or discharge curve for the same leaky unit\nwith refractory period (Eq. 14.11). The slope is\ninfinite at threshold (/th = Vth/R). The firing rate\nsaturates at l/fref. For comparison, the /-/ curve\nof the nonleaky unit without refractory period with\nconstant slope l/(VthC) is superimposed. (C) An\nadapting conductance (with Ginc = 20.4 nS and\n*adapt = 52.3 msec) is added to the leaky integrate-\nand-fire unit (see Fig. 14.2C) and the resulting /-\n/ curve is compared against the discharge curve\nof the biophysical detailed compartmental model\nof the layer 5 pyramidal cell (Fig. 17.10). The\ndegree of matching between simple and very com-\nplex models is quite remarkable and supports our\ncontention that suitably modified integrate-and-fire\nmodels do mimic numerous aspects of the behavior\nof neurons. Adaptation is already evident when\nconsidering the first interspike interval (between\nthe first and second spikes). Adaptation linearizes\nthe very steep /-/ curve around /^ (compare with\nB)."}
{"text": "===== Page 9 =====\n338 • \nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nThe dynamic firing range of nerve cells is limited by the fact that the sodium current\nresponsible for spiking must recover from inactivation. Potassium currents furthermore\nlimit the peak firing range. The effect of the absolute refractory period is mimicked by\npostulating that following spike generation, the membrane potential is set to zero for a fixed\nduration tref; any current arriving within this window is shunted away. This introduces a\nnonlinear saturation into the /-/ curve of the perfect integrator,\n(14.6)\nThe output of such an integrator to an arbitrary input current consists of a series of\nimpulses, £\\ S(t — ti), all of which are spaced at least ?ref apart.\n14.2.2 Forgetful or Leaky Integrate-and-Fire Unit\nThe model considered so far will sum linearly multiple subthreshold inputs irrespective of\ntheir temporal relationship because no account is made of a leak. A more realistic behavior\nis obtained by incorporating a leak resistance into the subthreshold domain (Fig. 14.2B),\n(14.7)\nHaving first met this equation in Chap. 1 (Eq. 1.5), we know that the evolution of the\nsubthreshold voltage is completely characterized by convolving / (t) with the associated\nGreen's function, e~^r (with r = RC). The time course of the membrane potential of\nthe leaky integrate-and-fire unit to a step of constant current /, switched on at t — 0 and\nremaining on, can be obtained by solving Eq. 14.7,\n(14.8)\nThe membrane charges exponentially up to its stationary value V = I R.\nThe integrator model will only follow this equation as long as the voltage remains below\nVth, since upon reaching the threshold a spike is initiated and the voltage is reset to zero\n(Fig. 14.3A). The minimal sustained current necessary to trigger an action potential, that\nis, the threshold current, is\n(14.9)\nFor any current / larger than !&, an output impulse will be generated at time T&, such that\nIR(1 — e~T*lr) — Vth holds. Inverting this relationship yields the time to spike as\n(14.10)\nSolving this equation for the minimal duration needed for a sustained current of a fixed\namplitude to generate a spike generates what is known as the strength-duration curve\n(Noble and Stein, 1966; Jack, Noble, and Tsien, 1975). Since the voltage is reset following\nan impulse and if we assume that the input current persists, the membrane will again charge\nup to the threshold, triggering the next spike 7^, + fref later (Fig. 14.3A).\nIf we take proper account of the refractory period by assuming that for fref following\neach spike all input current is simply lost (due to the shunting effect of the conductances\nunderlying the afterhyperpolarization), the continuous firing rate as a function of the injected\ncurrent will be (Fig. 14.3B)"}
{"text": "===== Page 10 =====\n14.2 Integrate-and-Fire Models \n• \n339\n(14.11)\nFor currents below I& no spike is triggered and at / = /a,, the slope of the /-/ \ncurve is\ninfinite. For large currents, the firing rate saturates at the inverse of the refractory period\n(Fig. 14.3B). In the absence of a refractory period, the slope of the /-/ \ncurve levels off to\na constant value of l/(VthC), identical to the slope of the nonleaky unit.3 Its steepness can\nbe increased by reducing the threshold voltage or by decreasing the membrane capacitance.\nDue to the refractory period, the /-/ curve gently bends over to level off at /max = l/fref\nfor (unphysiologically) high current levels.\n14.2.3 Other Variants\nBesides the generic version of the integrate-and-fire model discussed above, a number of\nvariants are in use.\n1. In order to better account for the 50-100 msec time course of adaptation, Wehmeier\nand colleagues (1989) introduced a purely time-dependent shunting conductance gadapt\n(with a reversal potential equal to the resting potential, here assumed zero). Each spike\nincreases this conductance by a fixed amount G-mc; between spikes, gadapt decreases\nexponentially with a time constant Tadapt • Such an effective calcium-dependent potassium\nconductance imitates both the absolute and the relative refractory period following\nspike initiation. We will refer to such a unit as an adapting integrate-and-fire model\n(Fig. 14.2C). Note that a refractory period tref is still necessary in order to mimic the\nvery short-term aspect of adaptation. In the subthreshold domain, this unit is described by\n(14.12)\n(14.13)\nIf V reaches V^ at time t', a spike is generated at this point in time and gadapt(O\nis incremented by Ginc- This model is completely characterized by six parameters:\nVJh, C, R, tK{, Gjnc and Tadapt.\n2. An alternative to this output-dependent membrane conductance is to increase the voltage\nthreshold following each spike in a deterministic manner, for instance, using the rule\n(14.14)\nwhere t — t' is the time from the last impulse, Vih.o the threshold in the absence of any\nadaptation, and a the maximal normalized voltage threshold (Calvin and Stevens, 1968;\nHolden, 1976).\n3. In order to account for those neurons that do not show any profound afterhyperpolar-\nization following spiking, the membrane potential can be reset to a value closer to Vih\n(e.g., 20% of VJh) instead of zero. This is equivalent to resetting the potential to zero but\nadding a constant current and can have a considerable effect on the jitter in pulse timing\n(Troyer and Miller, 1997).\n3. This can be seen upon developing the In term in Eq. 14.11 into a Taylor series, with £«(!+ x) <*> x — Jc2/2for|jt| <g 1."}
{"text": "===== Page 11 =====\n340 \n. \nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\n4. In a strategy to imitate the seemingly random nature of spike times, some authors resort to\ndrawing the voltage threshold from some probability density distribution (Holden, 1976;\nGestri, Masterbroek, and Zaagman, 1980). Yet in real neurons, the spiking mechanism\nitself appears to be quite reliable (Calvin and Stevens, 1968; Mainen and Sejnowski,\n1995). In the case of the perfect integrator, a random threshold and a constant input can\nbe shown to be equivalent to a random input and a constant threshold. Frequently, the\nformer situation is both mathematically and computationally easier to deal with than the\nlatter (for more details, see Gabbiani and Koch, 1998).\nWhat all of these models share is a mechanism for passive integration of synaptic inputs,\na voltage threshold for spike initiation, and a lack of specific spiking currents.\nIn Chap. 17 we will learn that an action potential in a full-blown model of a pyramidal cell\n(with eight voltage-dependent conductances) is, indeed, generated whenever the somatic\nmembrane potential exceeds —49 mV. This is because the synaptic current flowing into the\nsoma—caused by rapid EPSPs on the time scale of milliseconds—primarily charges up the\ncapacitance. Relative to this rapid charging current, the ionic currents in the subthreshold\nregime change on a much slower time scale.\nHow well does this /-/ curve compare against curves obtained from much more detailed\nand sophisticated models? Presaging Sec. 17.5, we plot the /-/ \ncurve of the layer 5\npyramidal cell, including the effect of firing rate adaptation, in Fig. 14.3C. Notice the\nvery low slope of the /-/ \ncurve around threshold, in contrast to the infinite slope of the\n/-/ \ncurve of the leaky integrate-and-fire unit. If an adapting conductance is incorporated\ninto the leaky integrator (Eqs. 14.12 and 14.13 and Fig. 14.2C), it is surprising how well\nthis single-cell model (with just six degrees of freedom) resembles the much more detailed\ncompartmental model based on membrane conductances.\nDue to the presence of the leak term, integrate-and-fire models have been difficult to fully\ncharacterize analytically but have also been surprisingly successful in describing neuronal\nexcitability. They have been applied to model the firing behavior of numerous cell types:\nneurons in the limulus eye (Knight, 1972b), a motoneurons (Calvin and Stevens, 1968),\nneurons in the visual system of the housefly (Gestri, Masterbroek and Zaagman, 1980),\ncortical cells (Softky and Koch, 1993; Troyer and Miller, 1997), and others.\nWhile singing the praise of integrator models, it must be pointed out that many cells\ndo not behave like integrate-and-fire units. For instance, cerebellar Purkinje cells (Jaeger,\nDeSchutter, and Bower, 1997) or the many types of oscillating neurons that constitute the\ncentral pattern generators found throughout the animal kingdom (Marder and Calabrese,\n1996) have such strong inherent nonlinearities, generated by powerful intrinsic currents, that\nany attempts to directly map their behavior onto this class of models would fail miserably.\nApproximations are possible, though. For instance, bursting (Chap. 16) could be treated by\nletting the rapid Na+ spikes be handled by the integrate-and-fire threshold mechanism. The\nslow dynamics governing at what instant the burst is triggered are generated by incorporating\nvoltage-dependent conductances into the unit.\n14.2.4 Response Time of Integrate-and-Fire Units\nWhen a spiking, nonadapting membrane (such as the squid axon) receives a sustained\nsuprathreshold current input, its membrane potential never reaches an equilibrium but moves\nalong a limit cycle. That is, it undergoes periodic changes in its state variables (Chap. 7).\nWhen the integrate-and-fire neuron spikes, and the state variable is reset, it loses memory of\nthe previous input current and begins to respond to the new current by charging toward the"}
{"text": "===== Page 12 =====\n14.3 Firing Rate Models\n341\nthreshold. It follows from these considerations that if there is a step change in current, the\nintegrate-and-fire neuron must converge to its limit cycle by the end of the first interspike\ninterval after the change, because everything during this first interval is exactly the same as\nduring subsequent intervals.\nFigure 14.4 compares the step response of an integrate-and-fire unit, a compartmental\nmodel of a cortical pyramidal neuron, an experimental record derived from a neuron in cat\nvisual cortex, and a mean-rate neuron. The first interspike interval already reflects the new\nfiring rate—the convergence occurs on as short a time interval as can be defined (that is, the\ninterspike interval). The inclusion of adaptation currents (Fig. 14.3B) does not substantially\naffect this analysis. Cortical cells (Fig. 14.4C) also reach their maximum firing rate by the\nfirst interspike interval. Thereafter, the firing rate decreases slowly due to the temporal\ndynamics of adaptation.\nJust because the subthreshold dynamics are governed by r does not imply that the\nneuron must respond to a suprathreshold input with the same dynamics. Returning to\nexpression 14.10 for the time to spike Tth, we notice that the larger the injected current,\nthe sooner the cell spikes (Fig. 14.5A). Furthermore, Tit, actually decreases as the input\nresistance, and therefore T increases (Fig. 14.5B). This can easily be explained by recalling\nthat 7jh is the time it takes for the membrane potential to reach the fixed threshold Vth-\nIncreasing the input resistance will shorten this time, even if overall it would have taken\nthe membrane longer to reach its ultimate steady-state value RI (which is never reached\nsince a spike is triggered and the membrane potential reset once V hits Vth).\n14.3 Firing Rate Models\nThe potential in a continuous firing rate unit, such as those at the heart of most neural\nnetworks, has the same dynamics as that in the leaky integrate-and-fire unit,\n(14.15)\nA subtle but far-reaching difference is that the instantaneous output of this unit f ( t ) is a\ncontinuous but nonlinear function of V(t):\nFig. 14.4 SPIKING CELLS Sample spike rasters in\nresponse to a step current injection. (A) Leaky integrate-\nand-fire unit with refractory period spiking in response\nto a current step of 1.6 nA (for parameters, see legend to\nFig. 14.3A). The arrows indicate the time at which the\ncurrent injection commenced. (B) Somatic membrane\npotential in the layer 5 pyramidal cell model in response\nto a 1.5-nA current input. (C) Response of a cell in the\nprimary visual cortex of the anesthetized adult cat to a\n0.6-nA current injection (from Ahmed et al., 1993). The\nfiring rate does not increase gradually; the effect of the\nchange in current is fully visible in the first interspike\ninterval. (D) Output of a nonadapting firing rate model\nwith T = 20 msec. In the linear regime of the cell's\n/-/ \ncurve, the firing rate can be considered to be a\nlow-pass-filtered version of the step input."}
{"text": "===== Page 13 =====\n342\nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nFig. 14.5 INTEGRATE-AND-FIRE UNITS CAN RESPOND MUCH FASTER THAN T Membrane time\nconstant r = RC of a leaky integrate-and-fire unit and the time T^ it takes such a unit, starting at\nV = 0, to reach Vt|, and spike (Eq. 14.10). (A) As the amplitude of the injected current increases, the\nunit can spike very rapidly. The true time to spike is < 7*th, since the unit's initial state is usually V > 0.\nThe parameters are as in Fig. 14.3A. (B) Input resistance R is varied over two orders of magnitude.\nAs r diverges, TH, converges to C Va,//, the time it takes for the voltage across a capacitance to reach\nVth- While this may be counterintuitive, it follows from the fact that Tit, is the time it takes to reach\na fixed threshold value, while T dictates the approach toward a steady-state value beyond Vih- The\ninjected current / = 4.3 nA. All else as in the upper panel. For small values of R, the current fails to\nreach /a, and 7\"th diverges.\n(14.16)\nMost commonly g is a sigmoidal function, that is, a monotonically increasing, positive, and\nsaturating function. A popular choice is\n(14.17)\nwhere ft is some positive constant (Fig. 14.6). Other functions have also been used (e.g.,\ntanh V or V2). V is sometimes identified with the so-called generator potential, the\nsomatic membrane potential when the spikes are disabled (e.g., by blocking the fast sodium\nconductance). Under certain conditions, the generator potential correlates well with the\nfiring rate when spikes are blocked (Katz, 1950; Granit, Kernell, and Shortess, 1963; Stein,\n1967a; Dodge, Knight, and Toyoda, 1968). If / is thought of as the firing rate of the unit,"}
{"text": "===== Page 14 =====\n14.3 Firing Rate Models\n343\nthen the function g can be identified with the /-/ \ncurve of the cell under investigation and\ncan be directly fitted against experimental data. Given the continuous and smooth nature of\ng, no true current threshold exists.\nA circuit implementation of a firing rate neuron is shown in Fig. 14.6. Here the ideal\noperational amplifier is assumed to draw no input current (that is, it has an infinite input\nimpedance), converting the difference between V and ground into the potential / = g(V).\nIn this class of models the firing rate changes smoothly in response to a rapid change in\nI. For small steps in input current g(V) is approximately linear. In the linear regime, the\nfiring rate is given by convolving the input current with a first-order low-pass filter with\ntime constant r = RC, and is therefore a smoothed version of the input.\n14.3.1 Comparing the Dynamics of a Spiking Cell with a Firing Rate Cell\nAs we mentioned above, the voltage in an integrate-and-fire unit in response to a sustained\nsuprathreshold input moves along a limit cycle. This is not true for the potential in a firing\nrate model. V reaches its equilibrium value in a time dictated by T ; the firing rate follows\nV without further delay.\nIf T is increased in either model by increasing the neuron input resistance /?, the rate of\nchange of the subthreshold voltage also increases. This allows the spiking neuron to reach\nthreshold more quickly (Fig. 14.7). In contrast, it will take the firing rate unit longer to reach\nits steady-state voltage because the equilibrium voltage is also increased, implying that the\ndynamics in the firing rate model slow down as T increases (Fig. 14.7). In the extreme case,\nwhen T -» oo and the leaky integrate-and-fire model converges to a perfect integrator\nmodel, the firing rate model does not even asymptotically approach an equilibrium, while\nthe spiking unit settles into its steady-state limit cycle faster than it does for a finite T.\nSimilarly, decreasing T will not make a spiking neuron respond faster.\nA spiking mechanism can therefore speed up a neuron's response to a step change in the\ninput. In a population of spiking cells with uniformly distributed initial conditions, there\nwill be cells whose T& will be arbitrarily close to zero. Knight (1972a) made this point,\nproving rigorously that an infinite population of either perfect or leaky integrate-and-fire\nunits uniformly distributed in phase will respond instantaneously to any suprathreshold\nstimulus. This is true for increases or decreases in input. In theory, how fast a neuron that\nreceives input from such a population could detect a change in its inputs is limited only\nFig. 14.6 CONTINUOUS FIRING RATE SINGLE-CELL\nMODEL \nAs in the integrate-and-fire models (Fig.\n14.2), the input current in a firing rate neuron charges up\nan R C element. The instantaneous firing rate is obtained\nby passing the membrane potential V through a smooth,\nstationary nonlinear!ty g. In the circuit idiom used here,\nthis nonlinearity is implemented by an ideal operational\namplifier. The associated adapted /-/ curve, specified\nby Eq. 14.17, g(V) = g(RI), is a continuous, positive,\nand saturating function with no threshold."}
{"text": "===== Page 15 =====\n344\nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nFig. 14.7 RESPONSE TIME IN SPIKING\nAND FIRING RATE MODELS Effect of\nchanging the membrane time constant on\n(A) a nonspiking (or firing rate) neuron and\n(B) an integrate-and-fire neuron. The input\ncurrent to the cell changes from zero to\n0.85 nA at time 0. Subthreshold parameters\nwere the same in both panels (C = 1 nF; R\nwas varied from 20 M£2 to infinity; no re-\nfractory period). The subthreshold voltage\nof the integrate-and-fire model in B is ex-\nactly the same as for the nonspiking model\nin A until the threshold Vth (dashed line in\nA) is crossed. Increasing R increases the\nrate of change of voltage, but also increases\nthe equilibrium voltage. Nonspiking neu-\nrons therefore converge more slowly as the\ntime constant increases. As r -> oo (when\nthe leaky integrate-and-fire unit turns into\na perfect one) the integrate-and-fire model\nresponds earlier.\nby the number of statistically independent inputs (Panzeri et al., 1996). This is unlike the\nresponse of one or even a population of firing-rate neurons. Because of the low-pass filtering\nstage, their firing rate cannot change instantaneously.\nIt has been argued that the temporal dynamics of neurons embedded within a network are\nprimarily determined by the time course of the synaptic currents. This has as the consequence\nthat the subthreshold RC time constant should more properly be replaced by one or more\nsynaptic time constants (e.g., Amit and Tsodyks, 1991; Amit and Brunei, 1993; Burkitt,\n1994; Suarez, Koch, and Douglas, 1995; Brunei, 1996). A very simple, yet physiologically\ncorrect way to express the firing rate is\n(14.18)\nwhere hss is the steady-state /-/ discharge curve of the cell, and / (t) is the total current\nflowing into the soma. This equation dispenses with the subthreshold voltage since it only\nplays a very limited role for suprathreshold currents. The subthreshold domain will come\ninto play for inputs hovering just around threshold (e.g., the lower trace in Fig. 14.7B).\nIn order to turn this into a complete single-cell model, two additional ingredients are\nneeded (Abbott, 1994).\n1. How to obtain the current / (t) 1 The simplest manner is to use the standard neural network\n\"linear sum over all inputs\" formulation (see Eq. 14.20). We can be more accurate and\nincorporate the nonlinear effects occurring in the dendritic tree (synaptic interaction,"}
{"text": "===== Page 16 =====\n14.4 Neural Networks \n• \n345\nsynaptic saturation, and so on). This is discussed at length in Sec. 18.4. Almost any\ndegree of biophysical complexity can be accounted for. What cannot readily be included\nare back-propagating action potentials and the like (Chap. 19) since we assume that the\ncoupling between dendrites and the spike-initiation zone is one-way only.\n2. What is the relationship between the instantaneous firing rate f(t) and the steady-state\nrate? The net effects of synaptic time constants and adaptation currents can all be lumped\ninto a single, low-pass temporal filter, which from a phenomenological point of view,\ncan be described by a first-order differential equation,\n(14.19)\nwhere Teg is the above-mentioned effective time constant (in the 20-30 msec range, in\nparticular if one wants to account for both NMDA and adaptation currents), not related\nto the passive membrane time constant rm. This constitutes an alternative procedure to\nEqs. 14.15 and 14.16 to define a firing rate model (Abbott, 1994). It gives rise to the\nsame phenomenological equation but with a different physiological interpretation.\nWe conclude that because the discharge rate is often quite linear over the relevant range\nof firing rates (Granit, Kernell, and Shortess, 1963; Ahmed et al., 1993), a linear threshold\nunit with no threshold may often be a satisfactory approximation for the firing rate of a real\nneuron.\n14.4 Neural Networks\nNeural networks consist of a large number of neurons connected using a scalar synaptic\nweight Wij. Almost always, the single-cell model used is a mean rate one, from the earliest\npublications of Steinbuch (1961) to Wilson and Cowan (1972), Hopfield (1984), and more\nrecent work (summarized in Arbib, 1995).\n14.4.1 Linear Synaptic Interactions Are\nCommon to Almost All Neural Networks\nThe evolution of the network, often termed neurodynamics, is governed by a coupled system\nof single-cell equations. For any one cell i (out of n such cells) it takes the form\n(14.20)\nwhere _/} is the firing rate of the jth neuron and /, corresponds to the external current\ninjected into the cell. /) is related to V; via Eq. 14.16, that is, via a stationary nonlinearity\n(14.21)\nIn our usual circuit idiom (Fig. 14.8) in which both /) and Vj are voltages, the synaptic\ncoupling between the two neurons, that is, to/,-, has the dimension of a conductance.\nNegative, hyperpolarizing synapses are implemented by inverting the output of the amplifier.\nThe evolution of the circuit in Fig. 14.8 can be expressed by an equation of the form shown\nin Eq. 14.20."}
{"text": "===== Page 17 =====\n346\nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nFig. 14.8 Two INTERACTING FIRING CELLS IN A\nNEURAL NETWORK \nCircuit model of two inter-\nacting mean rate neurons. Such continuous output\nunits constitute the standard working horse of neural\nnetworks. Common to all is that the coupling among\nneurons is characterized by a scalar Wjj that can take\non any real value, depending on whether the synapse\nis inhibitory or excitatory. The interaction among\nsynaptic inputs is strictly linear. Local learning rules\nof the type discussed in Sec. 13.5 are used for deter-\nmining the amplitude of the u),-/s. A qualitatively\nvery similar model of linear synaptic interactions\nhas been used in the neural network community for\nstudying the computational power of networks of\nspiking units.\nDifferent from the biophysics of synaptic conductance inputs (Sec. 4.5), a change Sfj\nin the firing activity of the j'th presynaptic neuron leads to a change uijjSfj \nin the current\ndelivered to the operational amplifier. In effect, the synaptic inputs act as current sources,\nand no nonlinear interaction among synaptic inputs exists.\nFiring rate models incorporating a low-pass filter to capture the passive properties of the\nunderlying membrane have been applied widely in abstract neural network analysis (Wilson\nand Cowan, 1972; Cohen and Grossberg, 1983; Hopfield, 1984; Arbib, 1995; an excellent\ntextbook covering this area in a relatively intuitive manner is the one by Hertz, Krogh, and\nPalmer, 1991) and in the analysis of the dynamics of real neurobiological networks (Knight,\nToyoda, and Dodge, 1970; Abbott, 1991; Traub and Miles, 1991; Amit andTsodyks, 1991;\nCarandini and Heeger, 1994).\nThe study of the evolution of networks of spiking neurons—usually either of the\nintegrate-and-fire or of the Hodgkin-Huxley variety—has only began of late because the\ndiscontinuous nature of their output and the attendant mathematical difficulties (Hansel\nand Mato, 1993; van Vreeswijk and Abbott, 1993; Usher et al., 1994; van Vreeswisk and\nSompolinsky, 1996; Maass, 1996). Given the revival of interest in the role of spike timing in\ncomputation, more progress is likely to be just around the corner. The equations of motion\nof such a network remain governed by linear synaptic interactions that usually take the form\n(14.22)\nwith an auxiliary equation for the voltage to deal with the refractory period (or adaptation)\nfollowing spike generation (Usher et al., 1994). The second sum on the right-hand side\nincludes all times ^ at which the j\\h neuron generated a spike. Each time this happens\nall of the postsynaptic targets of j receive a current pulse of amplitude Wij. (Propagation\ndelays can easily be incorporated into this notation.)\n14.4.2 Multiplicative Interactions and Neural Networks\nThe bottom line of the previous section is that the vast majority of neural networks have\nbeen built on the assumption of linearity of synaptic interactions. The nonlinearity that is\nnecessary for any true information processing to occur resides solely in the firing mechanism\nat the output of the cell (Vth for spiking models and g for rate neurons)."}
{"text": "===== Page 18 =====\n14.4 Neural Networks · 347\nIn view of the many different types of nonlinear interactions that can occur in the\ndendritic tree, including AND-NOT interactions (Sec. 5.1), NMDA synapses (Sec. 5.2), and\nthe voltage-dependent calcium and sodium membrane conductances found in the dendritic\ntree as discussed as length in Chap. 19, it appears wise to cast around for some canonical\nsingle-cell model that captures some of these nonlinearities yet is simple enough to be still\namenable to analysis.\nFrom a computational point of view, the simplest nonlinearity is multiplication, as in\n(14.23)\n(with α φ 0). A substantial body of evidence supports the presence of multiplicative-like\noperations in the nervous system (Koch and Poggio, 1992). Physiological and behavioral\ndata strongly suggest that the optomotor response of insects to moving stimuli is medi-\nated by a correlation-like operation. Psychophysical work in humans from a number of\nindependent groups strongly supports models of the correlation type, albeit with spatio-\ntemporal filters different from those in insects. Simple cells recorded in the primary\nvisual cortex of the cat (Emerson, Bergen, and Adelson, 1992) appear to encode some\nof the stages in the most popular of these algorithms, the spatio-temporal energy model\nof motion perception (Adelson and Bergen, 1985). From a mathematical point of view,\nall these motion algorithms can be implemented by the multiplication of two variables\n(Buchner, 1984; Hildreth and Koch, 1987; Poggio, Yang, and Torre 1989; Koch and\nPoggio, 1992).\nAnother instance of a multiplication-like operation in the nervous system is the mod-\nulation of the receptive field location of neurons in the posterior parietal cortex by the\neye and head positions of the monkey (Andersen, Essick, and Siegel, 1985; Zipser and\nAndersen, 1988; Van Opstal and Hepp, 1995; Brotchie et al., 1995). This operation serves\nto transform the image from a retinal coordinate system into one that takes eye and body\npositions into account. A final example is the output of an identified neuron—the descending\ncontralateral movement detector—in the visual pathway of the locust that signals rapidly\napproaching objects. Its firing rate can be accurately described as the product of the angular\nimage velocity and a term that depends exponentially on the angular size of the approaching\nobject on the animal's retina (Hatsopoulos, Gabbiani, and Laurent, 1995).\nThe generalization of multiplicative algorithms are polynomial ones. The output of a\npolynomial cell consists of the sum of contributions from a set of products,\n(14.24)\nwhere χ = (x\\, x%,...) represents the synaptic input and the scalar T(x) the output of the\nunit. Single-cell models that implement such a function are known as sigma-pi or higher\norder\n4 units (Feldman and Ballard, 1982; Volper and Hampson, 1987; Mel, 1992). If the\nhighest order product in Eq. 14.24 is quadratic, that is, only terms such as x¡Xj and xf are\nrepresented and d\\ and all higher coefficients are all identical to zero, the neuron is known\nas a second-order or multiplicative unit.\nThe evolution of the membrane potential is identical to the one used in the linear threshold\nones. For a second-order multiplicative unit it is\n(14.25)\n4. Order refers to the maximal number of variables multiplied in each term (3 in the case of Eq. 14.24)."}
{"text": "===== Page 19 =====\n348 \n· \nSIMPLIFIED MODELS OF INDIVIDUAL NEURONS\nAssociated with this single-cell model are second-order \"synapses\" w'¡jk. Such a synapse\nonly contributes to the postsynaptic potential if both inputs j and k are simultaneously\nactive. In principle, up to n2 synapses exist per neuron. For real neurons with dendritic\ntrees, the connectivity matrices w¡j and w'¡jk are not independent of each other but are\nfunctions of the cable properties and of the specific synaptic architecture used. In general,\nthey are also functions of time.\nAs an illustrative example, let us reinterpret the interaction occurring between an\nexcitatory and a silent inhibitory synapse in a passive dendritic tree of a direction-selective\ncell—assuming that the conductance inputs ge and g, are small—in terms of a multiplicative\nunit (Sec. 5.1). We assume that the excitatory postsynaptic conductance change ge is\nproportional to the presynaptic firing frequency fe of the neuron e and that the amplitude\nof the silent inhibition g, is proportional to the firing frequency /; of an inhibitory unit\nsynapsing onto the cell. With E¡ = 0, we can reexpress the steady-state potential of the\ndirection-selective cell j in Eq. 5.13 as\n(14.26)\nThe synaptic strengths are specified in terms of the input and transfer resistances and batter-\nies as Wje — EeKes, Wjee = EeKesKee and Wjei — EeKisKie. Note the unconventional\nnature of the dimensions of / and w. Equation 14.26 directly illustrates the connection\nbetween the nonlinear interactions among synapses and multiplicative neural network units.\nMel (1992, 1993, 1994) argues persuasively that NMDA synaptic input in combination\nwith voltage-dependent sodium and calcium conductances distributed throughout the den-\ndritic tree implements something akin to Eq. 14.24 (Sec. 5.2). Synaptic input distributed in\nspatial clusters throughout the dendritic tree approximates a polynomial, in the sense that\nsimultaneous excitation of m neighboring synapses (where m can be large) causes a larger\nsomatic response than activation of a similar number of synapses distributed in a diffuse\nfashion throughout the tree (Figs. 5.7-5.9). Different from Eq. 14.26, this operation is both\nmore robust and less specific, since the absence of any one particular input will have little\neffect on the overall output (as indicated by the broad peak in Fig. 5.8B).\nIt is easy to see the power of polynomial units in the case of binary Boolean functions and\ncircuits. For instance, a single such unit can implement an exclusive-or function, something\nthat a single-layer neural network of linear threshold units is unable to do. (For a rigorous\ninvestigation of this, see Brack, 1990; Brack and Smolensky, 1992; for background material\nsee Koch and Poggio, 1992). Of course, neurons always have the sigmoidal /-/ nonlinearity\nto fall back upon (g in Eq. 14.16 or hss in Eq. 14.18).\n14.5 Recapitulation\nWe started off this chapter by defining the instantaneous firing frequency f ( t ) . It is a fictive\nvariable that can be obtained by averaging the spiking response of a single neuron to multiple\npresentations of the same stimulus. Due to the stochastic nature of the neurona! response\nthe exact microstructures of spike trains are rarely reproducible from trial to trial. This is\nwhy the temporal average (/(i)} of the firing rate is the most common variable measured\nduring neurophysiological experiments. In a handful of experiments {/), evaluated over\nfractions of a second or longer, has been directly related to the behavior of the animal.\nThere is no question that firing rate codes that preserve temporal information at the 5-\n10 msec level are used in the nervous system. To what extent more complex correlation"}
{"text": "===== Page 20 =====\n14.5 Recapitulation · 349\ncodes—exploiting information encoded among η-tuples of spikes from one neuron or spik-\ning information across multiple neurons—exist remains a subject of considerable debate.\nWhat is the simplest model of a single neuron that captures some of its key operations?\nTwo families of models are in common use today: integrate-and-fire and continuous firing\nrate models. While the former retains the timing information of individual action potentials,\nthe latter assumes that it is only the average or mean firing rate of a neuron that matters to\nits postsynaptic targets. Both neglect the dendritic tree and both eliminate the complex time\ncourse of the sodium and potassium membrane conductances underlying spiking.\nThe key insight behind the various guises of the integrate-and-fire model is that from\na phenomenological point of view the neuron possesses two domains of operation, a\nsubthreshold and a suprathreshold one. In the subthreshold domain, synaptic inputs are\nintegrated and decay away; their temporal evolution is governed by the time constant τ.\nOnce the voltage threshold is reached, a pulse is generated and the membrane potential is\nreset. Different versions of integrate-and-fire models, incorporating various mechanisms\nto account for adaptation, can be well fitted to the discharge curves of cortical and other\ncells. It will be argued in Sec. 17.3 that firing in response to fast synaptic input in complex,\nconductance-based single-cell models is, indeed, initiated whenever a voltage threshold is\nexceeded.\nIn response to a suprathreshold stimulus, these units, in accordance with their biological\ncounterparts, can spike in a time T^ <K τ. A network of integrate-and-fire units can respond\nalmost instantaneously to a stimulus. The take home lesson is that the dynamics of the\nsubthreshold domain do not carry over into the suprathreshold domain.\nIn firing rate neurons the continuous output variable is an instantaneous function of the\nvoltage. Since the evolution of the voltage is dictated by a time constant, the firing rate\nwill always be low-pass filtered with respect to the input current, distinct from the response\nof real neurons, and different from integrate-and-fire units. If one would like to retain the\ncontinuous nature of the firing rate model, a more physiologically correct way to achieve\nthis would be to make the steady-state firing rate a function of the total current (synaptic,\ndendritic, or otherwise) at the cell body. The output of such a neuron can be interpreted as\nthe firing rate associated with a population of spiking cells.\nAt the heart of the vast majority of neural networks lies the assumption that synaptic\ninputs interact in a linear manner. The nonlinearity that is necessary for computation is\nrelegated to the firing mechanism at the output. A biophysically more faithful and more\ncomplex model that incorporates multiplicative interactions among synaptic inputs is the\npolynomial or sigma-pi unit.\nMultiplication is a key operation underlying many neuronal operations. Chapter 5 treated\nthe evidence in favor of the view that a dendritic tree endowed with NMDA synapses and\nvoltage-dependent membrane conductances (see Chap. 19 as well) can implement a robust\nversion of such a polynomial unit. The nonlinear operations underlying the polynomial\ninteractions do not depend on the threshold occurring at the cell body but precede it. The\ncomputational power of such neurons is considerably beyond that of their feeble-minded\nlinear threshold counterparts."}
{"text": "===== Page 2 =====\nStochastic Models of Single Cells\n351\ncan the observed randomness be explained on the basis of the cell's biophysics and synaptic\ninput (Calvin and Stevens, 1968; Softky and Koch, 1993; Shadlen and Newsome, 1994;\nSoftky, 1995; Konig, Engel, and Singer, 1996)? The mathematical theory of stochastic point\nprocesses and the field of statistical signal processing offer a number of tools adequate for\nanalyzing the properties of spike trains. We will study these here and will relate them to\nsimple models of biophysics. This will enable us to infer something about the integrative\nmechanisms underlying neuronal firing activity.\nIn the previous chapter we exclusively dealt with deterministic input / (i) to spiking neu-\nrons. Given the highly stochastic nature of neurons evident in Fig. 15.1, it is imperative that\nwe now begin to deal with random variables, that is, observables that take on discrete (such\nas the number of spikes) or continuous (such as the membrane potential) values with certain\nprobabilities. For an introduction into the field, we refer the reader to Papoulis (1984); for\nmonographs on stochastic neural activity, see Holden (1976), Tuckwell (1988c), and Smith\nFig. 15.1 VARIABILITY OF NEURONAL SPIKING \nA high-contrast bar is swept repeatedly over the\nreceptive field of a cortical cell in the awake macaque monkey. Much variability in the microstructure\nof spiking is evident from trial to trial. The poststimulus time histogram in the middle corresponds\nto the averaged firing rate (/(/)> (using 20 msec bins) taken over 40 trials. The lower plot illustrates\nthe associated interspike interval (ISI) histogram. It shows a lack of very short intervals, indicative\nof a refractory period, and an exponentially decreasing likelihood of finding very large gaps between\nspikes. The lack of reproducibility of the detailed spike pattern is the primary reason arguing for the\nidea of a mean rate code (Eq. 14.1). Yet neurons deep within the cortex can faithfully reproduce the\nmicrostructure of spiking over several hours (Fig. 15.11). From W. Newsome, K. Britten, personal\ncommunication."}
{"text": "===== Page 3 =====\n352 · STOCHASTIC MODELS OF SINGLE CELLS\n(1992). Gabbiani and Koch (1998) should be read as a complementary text to this chapter,\nas they discuss in more detail signal processing approaches toward spike train analysis and\ntheir numerical implementation into a suite of freely available MATLAB routines.\n15.1 Random Processes and Neural Activity\nFor starters, let us assume that we are dealing with discrete random variables. We consider\na stochastic or random process, that is, a family of such random variables parameterized\nby time t. In other words, at every instant t the random process has the discrete value Ν(t),\nwhile the entire process is specified by (N(t), t > 0). In the following, we are frequently\nfaced by questions of the form: \"What is the probability that Ν is as large as some specific\nnth?\" In order to answer these, we need to specify a probability distribution from which the\nrandom variable Ν is drawn. Before we do so, let us remind our readers of several important\nconcepts.\nOne is the idea of a point process, which is simply a process that can be mapped onto a\nset of random points on the time axis. Two examples of point processes are the occurrence\nof action potentials and the times at which a synaptic vesicle is relased at some synapse. A\npoint process is described by a series of delta pulses, Σ δ (t — ¿¿)·\nWe can introduce a new random variable, defined as the interval between consecutive\nevents, T¡ — ί,·+ι — t¡, to help us introduce the notion of a renewal process. This is a\npoint process in which the random variables T¡ are independent and identically distributed.\nThe notion of a renewal process comes from industrial practice: the probability that some\nmachine will fail within some interval is identical for all machines and will not vary from one\ninterval to the next. Applied to a spike train, it would imply that the chance of finding some\nparticular interspike interval (Fig. 15.1) is independent of whether a short or a long interspike\ninterval preceded it. This does tend to be true to a first approximation for many cortical cells.\nOn the other hand, it is certainly not true for bursting cells, where the probability for finding\na long interspike interval after two or three very short ones is strongly enhanced (see the\nfollowing chapter). Renewal processes are much simpler to describe from a mathematical\npoint of view than nonrenewal processes. Lastly, a stationary stochastic process (or a random\nprocess) is one whose statistical properties do not depend on the time of observation but\nremain constant in time.\nSince true stationarity is very difficult to verify in practice, it is common to use the less\nstringent concept of a wide-sense or weakly stationary process. This is a random process\nwhose mean is constant and whose autocorrelation function only depends on t — t', that is,\na process whose first- and second-order statistical properties are time invariant.\nLet us now focus on the most common discrete renewal process, the Poisson process.\n15.1.1 Poisson Process\nA Poisson process is the simplest possible random process with no memory and is char-\nacterized by a single parameter, the rate or mean frequency μ. It is of great relevance to\nneurobiology, since a number of discrete biophysical events appear to follow a Poisson\ndistribution closely. The best studied example of a Poisson process is the spontaneous\nrelease of individual packets of the neurotransmitter acetylcholine at the frog neuromuscular\njunction (Fatt and Katz, 1952; for an overview see Stevens, 1993, and Chap. 4.2). At the\npostsynaptic terminal this release gives rise to so-called miniature endplate potentials,"}
{"text": "===== Page 4 =====\n15.1 Random Processes and Neural Activity \n· 353\nwhose arrival times can be accurately modeled by a Poisson process. As we will see below,\nthe distribution of action potentials in cortical cells can be approximated to a certain degree\nby a modified Poisson process.\nA number of different, but equivalent, definitions of a Poisson process exists. We define\n{N(t), t > 0} to be a simple Poisson process with mean rate μ if:\n1. Given any ÍQ < ii < i2 < · · · < tn~\\ < tn, the random variables N(tk) — N(tk^{),\nk — 1, 2, ···,«, are mutually independent.\n2. For any 0 < t\\ < ti the average number of events occurring between t\\ and i2 is\nμ(ί2 - ii).\nThe first condition specifies that the number of events occurring in one interval is\nindependent of the number of events occurring in any other interval, provided they do\nnot overlap. The second property tells us that the expected number of events is proportional\nto the rate times the duration of the interval.\nAs no point in time is singled out in this definition of the Poisson process, it is a stationary\nprocess.\nIt follows from these conditions (Feller, 1966) that the actual number of events, Ν fa) —\nN(ti),isa random variable with the Poisson probability distribution\n(15.1)\nwith k = 0,1,2,···.\nThe parameter μ specifies the average number of events per unit time. With t\\ = t and\nf2 = ί + Δί, we have for the probability that exactly k events occur in the interval Δί,\n(15.2)\nIf μ,Δί <C 1, that is, if much less than one event is expected to occur in the interval Δί,\nthe e~* term in Eq. 15.2 can be expanded into a Taylor series 1 — χ + x\n2/2 — x\n3/3 + · · ·.\nWe then have for the probability that none or a single event occurs in the interval Δί,\n(15.3)\n(15.4)\nThese estimations are accurate within (Δί)\n2, that is, the error in these estimates goes as a\npolynomial of order (Δί)\n2 to zero as Δί -> 0. In other words, as the product of the average\nfiring frequency and the interval is made smaller and smaller the approximation becomes\nbetter and better. Figure 15.2 illustrates how the events generated by a Poisson process are\ndistributed compared to the spike distribution taken from a neuron. Both processes have\nthe same overall rate. On account of the refractory period, we modified the Poisson process\nto prevent two spikes from directly following each other.\nA commonly used procedure for numerically generating Poisson distributed action\npotentials with mean rate μ is based on this approximation. A random number r, uniformly\ndistributed between 0 and 1, is generated for every interval Δί; if r < μΔί, a spike is\npresumed to have occurred between ί and ί + Δί; otherwise, no spike is generated. (This\nassumes that the probability of generating a spike in the interval considered, that is, μΔί\nis much less than one.)"}
{"text": "===== Page 5 =====\n354 \n. \nSTOCHASTIC MODELS OF SINGLE CELLS\nFig. 15.2 SPIKE TRAINS AND THE RANDOM POISSON PROCESS Spike train from an extracellularly\nrecorded cell in the parietal cortex of a behaving monkey (upper trace; data kindly provided by J.\nFuster) and from a synthetic Poisson process with a refractory period of 1 msec (lower trace). The\nmean firing rate of both processes is 30 Hz. Notice the relatively rare occurrences of large gaps\nbetween spikes. In a Poisson process, the probability of occurrence of gaps of duration Τ decreases\n„„ α-Τμ.\n15.1.2 Power Spectrum Analysis of Point Processes\nA number of signal processing techniques rely on evaluating average spike train properties\nin response to random stimulus ensembles. This works best if the system at hand can be\ndescribed as a linear, time-invariant one. Under these conditions, the power spectrum 5(/)\nof the associated spike train reveals details about the filter function that is used by the system\n(e.g., does the neuron act as a low-pass or as a band-pass filter?).\nA basic result of the theory of stochastic processes (Papoulis, 1984) is that the power\nspectrum of a Poisson process is flat at all frequencies except for a delta peak at the origin. To\nbe more precise, the spectrum associated with an infinite train of delta impulses distributed\naccording to a Poisson process of rate μ is\n(15.5)\nThis is in accordance with our intuition that all spectral components should be equally\nrepresented in a completely random spike train; no particular frequency (such as l/μ) is\nsingled out. The autocorrelation function, defined as the inverse Fourier transform of the\npower spectrum, has a similar shape, taking on a constant value of μ\n2 everywhere except\nat the origin, where it is a 3(t) function.\nNeurons, however, do not fire totally without memory, due to the presence of a refractory\nperiod. A mathematically convenient manner in which this can be accounted for (Perkel,\nGerstein, and Moore, 1967; Bair et al., 1994) is by introducing the renewal density function,\nwhich denotes the probability for a spike to occur between t\\+t and t\\ +1 + dt, given that\na spike was just generated at t\\. For a pure Poisson process, the renewal density function is\na constant equal to the rate. In our case, however, the renewal density has a dip around zero,\nindicating a reduced probability of spiking due to the refractory period. This can easily\nbe measured experimentally. Assuming that the dip due to the refractory period can be\naccounted for by a Gaussian of variance σ?ε{, the associated power spectrum is,\n(15.6)"}
{"text": "===== Page 6 =====\n15.2 Stochastic Activity in Integrate-and-Fire Models \n· 355\nTo ensure that the power is always positive, the maximum firing rate must be limited:\nμ < l/(\\/2jfaref). This spectrum, parameterized by the mean rate μ and the width of\nthe refractory period σκ{ is constant for large values of / but dips toward its minimum\nat / = 0 (Fig. 15.3). A longer refractory period (that is, a larger value of aref) causes a\ndeeper trough at low frequencies. Note that this result appears at odds with intuition, since a\nrefractory period seems to demand a dip in the neighborhood of the inverse of the smallest\ninterspike interval, that is, at high temporal frequencies.\nA mathematically more general manner to account for the refractory period is by\npostulating that the interspike intervals are independently and identically distributed random\nvariables, which themselves are the sum of a random refractory period and a statistically\nindependent interval due to a known stationary process (Franklin and Bair, 1995).\nSpike train analysis of cortical cells (Bair et al., 1994) showed that the spectrum of 40%\nof these neurons can be fitted using Eq. 15.6 (left column in Fig. 15.3). The good match\nbetween the analytical model, experimental data, and synthetic spike train supports the\nhypothesis that once the refractory period is accounted for, a Poisson hypothesis constitutes\na first-order description of cortical spike trains.\n15.2 Stochastic Activity in Integrate-and-Fire Models\nLet us now take an integrate-and-fire model, as described in the previous chapter, and\nbombard it with stochastic, Poisson distributed, synaptic inputs. In what is to come,\nwe will neglect the membrane leak, as well as adaptation and the refractory period, to\nget at the principle underlying stochastic activity in these models. Using the nonleaky\nFig. 15.3 POWER SPECTRUM OF CORTICAL CELLS Representative spike train, interspike interval\nhistogram, and power spectrum for a neuron recorded from cortical area MT in a behaving monkey\n(left column) compared against a modeled point process (right column). The synthetic data assume that\nspikes are Poisson distributed with an absolute refractory period drawn from a Gaussian distribution\n(of 5 msec mean and 2 msec standard deviation). Equation 15.6 has been fitted to the neuron's\nspectrum (curve labeled \"Theory\") with μ — 58 Hz, and aref = 3.5 msec). The spectrum of many\nnonbursting cells can be fitted against this simple model, which satisfactorily captures many aspects\nof their random behavior. Reprinted in modified form by permission from Bair et al., (1994)."}
{"text": "===== Page 7 =====\n356 \n. \nSTOCHASTIC MODELS OF SINGLE CELLS\nintegrate-and-fire model allows us to derive closed-form expressions for many variables of\ninterest. Conceptually, R = 0 corresponds to a cell for which the time between spikes is\nmuch shorter than the membrane time constant, so that the decay of the membrane potential\ncan be neglected. We later investigate R φ 0 using numerical simulations.\nWe assume that the integrator cell receives excitatory synaptic input from a Poisson\ndistributed random process Ne(t) with mean rate μ<, and synaptic weight ae. Conceptually,\neach synaptic input can be thought of as dumping ae amount of charge onto the capacitance\nC, raising the membrane potential by ae/C toward threshold. We assume here that C has\nbeen folded into the synaptic weight ae; it will not be included in the equations to come\n(equivalently, C = 1). The random process V(t), that is, the membrane potential, shows\nrandom jumps of amplitude ae. In the absence of a threshold, the membrane potential can\nbe described as\n(15.7)\n15.2.1 Interspike Interval Histogram\nLet us proceed methodically by first starting with trivial cases, working up to more complex\nsituations. In this spirit, we assume that ae > Vth, that is, each synaptic input is, by itself,\nsufficient to cause a spike. (Think of a single retinal ganglion cell making a few hundred\nsynaptic contacts on a single geniculate relay cell.) Trivially, the probability distribution of\nthe output spike will be identical to that of the synaptic input.\nIf the perfect integrator generated a pulse at t = 0 and was reset, what is the waiting\ntime ΓΙ for the next output pulse to occur? In the case of ae > Vth> this is equivalent to\nasking what is the waiting time for the next synaptic input to occur. This function can be\ncomputed if we know the probability for no event to occur between (0, i), which is\n(15.8)\nThe distribution function for the next spike to occur is one minus this result, that is,\n1 — e~^\"\nT; the longer one waits, the more likely an input is to occur. The probability\ndensity function p\\ (i) for ί is the temporal derivative of the distribution function,\n(15.9)\nThe choice of our initial observation point, here t = 0, does not affect this result since the\nPoisson process is ahistoric. This is an important point to note; the density function does\nnot depend on whether or not an event just occurred. (Of course, given the existence of a\nrefractory period, this will not be true for real neurons at very small time scales.) Therefore,\nPi(t) corresponds to the density of time intervals between adjacent pulses.\nThe average duration between events is\n(15.10)\njustifying our interpretation of μ€ as the mean rate.\nWhat happens if we assume that nth inputs are needed to trigger a pulse (with ntb —\n[Yfn/o-e + 1J, where |_*J is the largest integer less than or equal to *)? This is similar to a\nGeiger counter that is rigged to click every time it detects «th radioactive decays. Answering\nthis question requires us to compute the time T& in which we expect a Poisson process with\nrate μ<· to generate exactly «& events. This is given by the probability for nth — 1 events\nto occur between now and i, multiplied by the probability for a single event to occur in the\ninterval [i, t + St),"}
{"text": "===== Page 8 =====\n15.2 Stochastic Activity in Integrate-and-Fire Models \n· 357\n(15.11)\nThe probability density function of T& is the limit of this expression divided by St as\n5i-> 0,\n(15.12)\nThis expression for the waiting time is known as the nthth-order gamma density.\nThe density function pnth (t) is obtained experimentally by binning consecutive interspike\nintervals from spike trains, as in Figs. 15.1 and 15.4. In this guise it is called the interspike\ninterval (ISI) histogram. The mean interspike interval is\n(15.13)\nFig. 15.4 INTERSPIKE INTERVAL DISTRIBUTION\nIn the interspike interval distribution the occurrences\nof the intervals between adjacent spikes is his-\ntogrammed. This is done here analytically for pulses\ngenerated by a perfect integrate-and-fire unit that re-\nceives Poisson distributed input with a constant rate\nof μ = 1000 (per second). The threshold of the unit\nvaries, with (A) «th = 1 (each input generates an out-\nput pulse), (B) nth = 10, and (C) η& = 100. The ISIs\nare gamma distributed (Eq. 15.12). As expected, the\nmean output rate decreases as the number of synaptic\ninputs needed to reach threshold nth increases (the\nmean interspike interval is at 1, 10, and 100 msec\nfor the three panels; see arrows). The normalized\nstandard deviation, called the coefficient of variation\nCy (Eq. 15.15) scales as Μ^/ñ^. In other words, the\nrelative jitter in the timing of output pulses becomes\nsmaller as nth becomes larger. If the cell is integrating\nover a significant number of small inputs, the ISI dis-\ntribution approaches the normal distribution (as in C)."}
{"text": "===== Page 9 =====\n358 \n. \nSTOCHASTIC MODELS OF SINGLE CELLS\nand the variance is\n(15.14)\nFigure 15.4 shows waiting time densities for a nonleaky integrate-and-fire neuron receiving\nexcitatory input at a constant rate //,<· in which the effective voltage threshold is increased\nby a decade, corresponding to «th = 1, 10, and 100. As nth of the gamma distribution\nincreases, the probability density of the interspike intervals rapidly tends toward a Gaussian\ndistribution.\n15.2.2 Coefficient of Variation\nThe most common way to quantify interspike variability is via the normalized standard\ndeviation, that is, the square root of the variance of the ISI histogram divided by its mean,\n(15.15)\nThis measure is known as the coefficient \nof variation. For the perfect integrator model\nconsidered here, we have\n(15.16)\nFor «th = 1, associated with an exponential ISI (Fig. 15.4A), CV = 1. As the number\nof random inputs over which the unit integrates (or averages) becomes larger, the normal-\nized ISI becomes narrower and narrower and CV smaller and smaller (Fig. 15.4C). The\nrelationship between spike train variability and CV is illustrated graphically in Fig. 15.5.\nThis result can be explained qualitatively by invoking the central limit theorem, which\nstates that as the number η of independent random variables x¡ goes to infinity, the random\nvariable defined by the mean of all jt¡'s, (χ) — (1 / η ) 52\"_i*i, has an asymptotically\nGaussian distribution, with a mean identical to the mean of the population and with a\nstandard deviation scaling as l/^/n of the standard deviation of the individual jt,· 's. If we\nwere to compute the average height of all students in one class, there would be a great\ndeal of variability around the mean. Yet the Cy associated with the average height of all\nmen in the United States is minute. In other words, if a neuron can only be brought to fire\naction potentials by summing over dozens or more of independent synaptic inputs, it should\nfire very regularly. And as we noted above, a neuron with little variability in its interspike\ninterval cannot readily exploit temporal coding (since little information can be encoded in\na regular interspike interval).\nIncorporating an absolute refractory period iref into the integrator shifts the interspike\ninterval probability distribution by the same amount to the right. It gives the perfect\nintegrator cell a characteristic time scale, so we cannot expect it to have identical statistics\nat all firing rates. The new CV is\n(15.17)\nThe effect of the refractory period is to regularize the spike train, lowering its variability.\nThis is particularly true for high firing rates, as (Tib) approaches fref.\nIf we were to measure the interspike interval variability generated by a sustained\ncurrent injection into an integrate-and-fire cell (Fig. 15.5E) or into a squid giant axon"}
{"text": "===== Page 10 =====\n15.2 Stochastic Activity in Integrate-and-Fire Models \n· 359\nFig. 15.5 SPIKE TRAIN INTERSPIKE VARIABILITY Sample spike trains and interspike interval\nhistograms for a perfect integrator model with an absolute refractory period tKf of 2 msec for Poisson\ndistributed synaptic input. The mean interspike interval is in all cases identical to 12 msec. (A) Each\nsynaptic input gives rise to a spike. The ISI is a shifted exponential; the deviation of the associated\nCv from unity reflects the regularizing effect of the refractory period. (B)-(D) n^ is increased to\n2, 5, and 10, respectively. To retain the same average firing frequency, the input firing frequency\nwas also increased by the same amount. The ISI can be described by a gamma density (Eq. 15.12).\n(E) Response to a sustained current injection.\n(see Fig. 6.9), it would tend to zero. Biological pacemaking systems like the heart or the\nrhythm underlying breathing have Cv's in the low percent range. In some systems the\nCy can be a miniscule fraction of one percent. The neurons in the weakly electric fish\nEigenmannia that are responsible for triggering the clocklike electric organ discharge in the\n0.1-1 kilohertz frequency range have a spike jitter of less than 1 ^sec (Kawasaki, Rose,\nand Heiligenberg, 1988).\nIn all of these examples, Cv < 1, with the upper bound given by a pure Poisson process.\nYet in many instances interspike interval distributions of real neurons have Cy 's greater than\n1 (Wilbur and Rinzel, 1983; see also Sec. 16.1). This can be achieved in a so-called doubly"}
{"text": "===== Page 11 =====\n360 \n· \nSTOCHASTIC MODELS OF SINGLE CELLS\nstochastic Poisson process (Saleh, 1978), where one stochastic process is used to generate\nthe rate of a second one. Such a process was postulated to characterize—successfully—a\nnumber of properties of retinal ganglion cell spike trains at low light levels (Saleh and\nTeich, 1985). In this limit, the distribution of photons absorbed at the retina is expected to\nbe Poisson. If each photon results in a slowly decaying input current to a ganglion cell that\ngenerates on average two spikes per incoming photon, the interspike interval distribution\nof the model will have a CV greater than 1. The cause of this additional variability lies in\nthe random number of spikes generated for each incoming Poisson pulse (Gabbiani and\nKoch, 1998).\nSpecial care must be taken when analyzing the variability of bursting cells (see the fol-\nlowing chapter). If a bursty cell switches between bursting and nonbursting, the variability\nof its interspike intervals can be larger than that of a Poisson process, that is, CV > 1\n(Wilbur and Rinzel, 1983). Numerical techniques have been developed to account for the\nhighly correlated adjacent interspike intervals that occur during bursting (a measure termed\nCV2; Holt etal., 1996).\n15.2.3 Spike Count and Fano Factor\nSo far, we have restricted ourselves to a consideration of the temporal jitter. However,\nneurons show considerable variation in the number of spikes triggered in response to a\nparticular stimulus. This can be quantified by the ratio of the variance of the number of\nspikes generated within some observational window Γ to the mean number of spikes within\nthe same time period, the so-called index of dispersion or Fano factor (Fano, 1947),\n(15.18)\nCounting Poisson distributed spikes of mean rate μ over an observational interval of\nduration T, leads to a mean number of spikes Τ μ with variance Τ μ. In other words, for an\nideal Poisson process, no matter for what duration it is observed, F(T) — 1.\nExperimentally, F(T) should be estimated by computing the average number of spikes\nand their variance for a fixed counting time Τ using many repetitions of the same stimulus.\nThe counting time should now be systematically varied over the range of interest.\n3 The Fano\nfactor is usally plotted in log-log coordinates, and the slope of the best fit through these\npoints is reported. For a true Poisson process, the slope is 1. Slopes bigger than 1 imply\nthat the variance in the number of events grows faster than the mean, indicating long-term\ncorrelations in the data.\n4\nAs illustrated in Fig. 15.6, the F(T) factor in different neural systems is frequently close\nto unity for observational times on the order of 100 msec, compatible with the Poisson\nhypothesis (modified by a refractory period that lowers F(T) to a value slightly less than\nunity; Teich, 1992). Yet for large windows, say in the second to minute domain, long-term\ntrends in the data are very apparent (revealed by the very large F(T) values). These are\nmost likely due to state changes in the underlying neural networks. Obviously, modeling\ndata over these time spans forces abandonment of the Poisson hypothesis. In some sensory\nsystems F(T) can be considerably less than 1 (van Steveninck et al., 1997; Berry, Warland,\nand Meister, 1997).\n3. A frequently used approximation is to estimate ( ( N ) , Var[W]) for a fixed trial, say of 1 sec duration. This procedure is\nrepeated under circumstances (that is, by varying the stimulus intensity) that give rise to different values of {N).\n4. Indeed, the Fano factor can be related to the autocorrelation function of the underlying point process (Gabbiani and Koch,\n1998)."}
{"text": "===== Page 12 =====\n15.2 Stochastic Activity in Integrate-and-Fire Models \n· 361\nFig. 15.6 VARIABILITY IN SPIKE COUNT Fano Factor (Eq. 15.18), that is, the variance of the\nnumber of spikes normalized by their mean, within an observational counting window of duration Τ\nin different spiking systems. The top panel shows F(T) for the spontaneous activity of four cells taken\nfrom the visual cortex of the anesthetized cat (Teich et al., 1997). For Τ < 0.1 sec, the variability is\nclose to unity, as expected from a Poisson process. However, for larger observational windows, F(T)\nstrongly increases, indicative of long-term correlations in the firing behavior, possibly due to slow\nstate changes in the cat. The qualitatively same behavior for spontaneous activity can be observed in\nthe middle panel, plotting F(T) for four visual interneurons in the cricket (Turcott, Barker, and Teich,\n1995), although F(T) is not as large as in the cortex. The lower plot illustrates F(T) (along with its\nstandard deviation) associated with 10 simulated runs of a Poisson process with a refractory period.\nSince a refractory period imposes some degree of order, F(T) is less than that of a pure Poisson\nprocess but still close to unity. Reprinted by permission from Teich, Turcott, and Siegel (1996).\nIn general, the jitter in the timing of events need not be related to the jitter in the number\nof events. Under conditions when the spiking can be described as a renewal process, that is,\nif the interspike intervals are independently and identically distributed, the distribution of\nspike counts for long observation times will be approximately Gaussian distributed. Since\nthe same is true for the interspike interval distribution (e.g., Fig. 15.4C), these two measures\nare related via\n(15.19)\nin the limit as the observational period Τ -> oo (Cox, 1962). This is a property of spike\ntrains that is not widely appreciated.\nMany studies have measured interspike intervals as well as spike count variability,\nalthough usually not simultaneously, of spontaneous or stimulus-induced activity. One of\nthe aims is to infer something about the dynamics of the spiking threshold or about the\ndifferent states the neuron can be in (Werner and Mountcastle, 1963; Smith and Smith,\n1965; Calvin and Stevens, 1967, 1968; Noda and Adey, 1970; Burns and Webb, 1976;"}
{"text": "===== Page 13 =====\n362 \n· \nSTOCHASTIC MODELS OF SINGLE CELLS\nVogels, Spileers, and Orban, 1989; Snowden, Treue, and Andersen, 1992; Softky and Koch,\n1993; Usher et al., 1994; Turcott, Barker, and Teich, 1995; Teich, Turcott, and Siegel, 1996;\nTeich et al., 1997). It was only recently that attempts have been made to reconcile the\nobserved high variability of neurons with the known biophysical properties of single cells\nand network properties. In order to discuss some of these, let us now throw inhibition into\nthe brew.\n15.2.4 Random Walk Model of Stochastic Activity\nWe do this in the form of adding an inhibitory synaptic process of rate μ,· and weight\na¡ > 0. Now the random process V(t~) undergoes random \"up\" jumps (of amplitude ae)\nand \"down\" jumps (of amplitude a¡; Fig. 15.7). In the absence of a threshold, the potential\ncan be modeled by\n(15.20)\nwith V(0) =0 and V < VO,· The expected value of this process follows from the fact that\nEq. 15.20 is linear, as\n(15.21)\n(15.22)\nwhere μ = ae^e — α, μ, is termed drift. The variance of the voltage over time is\n(15.23)\n(15.24)\nwhere σ\n2 = α\n2. μ<, + α?μ,· is called the variance parameter. The drift corresponds to the\nnet input current into the unit. As long as it is different from zero, the average membrane\npotential as well as the jitter will diverge (in the absence of either a leak or a threshold). Note\nthat the standard deviation around (V) increases as the square root of time, reminiscent of\nthe jitter in the motion of a particle diffusing in one or more dimensions (Sec. 11.1). This\nis not surprising, since both represent instances of random walks. (For an introduction into\nthe theory of the random walk, see Feller, 1966, or Berg, 1983.)\nThe probability distribution of V(t) in the absence of any threshold and membrane leak\ncan be computed analytically. We spare the reader its derivation, referring him or her instead\nto Eq. 9.42 in the monograph by Tuckwell (1988b). Figure 15.7A illustrates one particular\nrealization of the random process V(t), together with (V(t)) and its associated jitter.\nWill the membrane potential ever reach threshold V^, and generate a pulse? And how\nlong is this event expected to take? This question, known as the \"time of first passage to\nthreshold\" problem, has a nontrivial but well characterized probability density function\n/th(f) associated with it (see Eq. 9.54 in Tuckwell, 1988b and Fig. 15.7B). If rth is the\nrandom time taken for V to go from its initial state at V = 0 to V = Vth, one can\nestimate that\n(15.25)\nor\n(15.26)"}
{"text": "===== Page 14 =====\n15.2 Stochastic Activity in Integrate-and-Fire Models\n363\nt (msec)\nFig. 15.7 RANDOM WALK OF THE MEMBRANE POTENTIAL \nIllustration of the random walk model\nof excitatory and inhibitory synaptic input into a nonleaky integrate-and-fire unit as pioneered in this\nform by Stein (1965). The cell receives Poisson distributed excitatory input (with rate μ,, = 1000 Hz),\neach increasing the voltage by 0.5 mV, and Poisson distributed inhibitory input (at a rate of 250 Hz),\neach input decreasing the potential by 0.5 mV. Threshold is reached at 16 mV. (A) One instantiation\nof such a random walk, together with the expected mean potential and its standard deviation. The\nunit generates a spike at around Τ& = 50 msec. (B) Probability density fa,(t) for the first passage to\nthreshold, that is, for the time it takes before the voltage threshold is reached for the first time.\nIf the drift is positive, the unit will eventually spike with probability 1. If the net input is\nnegative, then one must wait for a fluctuation in the input to elevate V to threshold. This\nis less and less likely as the effective inhibition exceeds excitation with a finite chance that\nthreshold is never reached.\nIn the former case, that is, for αΐμί > β,·μ,·, we can easily compute the first and second\nmoments associated with T&. Specifically, the mean time to fire is\nand the variance of this mean is\n(15.27)\n(15.28)"}
{"text": "===== Page 15 =====\nSetting μι = 0 in these expressions gives us back our earlier equations. Adding in inhibition\nleads to the expected result that the mean membrane potential decreases and that (T^)\nincreases. A bit less expected is that the variance of (V) as well as the jitter associated with\n(7ih) both increase. Intuitively, this can be explained by noting that we are adding a new\nsource of unpredictability, even if it is inhibitory. Indeed, by adding enough inhibition one\ncan always achieve a situation in which the drift is close to zero but the jitter becomes very\nlarge. This is termed balanced inhibition or balanced excitation and inhibition and was\nfirst investigated by Gerstein and Mandelbrot, 1964 (for more recent accounts, see Shadlen\nand Newsome, 1994; Tsodyks and Sejnowski, 1995 and, in particular, van Vreeswijk and\nSompolinsky, 1996). Since the amount of inhibition increases in parallel with the amount of\nexcitation, threshold is not reached by integrating a large number of small inputs over time\n(as with μ > 0), but by a large enough \"spontaneous\" positive fluctuation in excitation,\ncomplemented by a large enough downward fluctuation in inhibition. This is of course much\nless likely to occur than if the drift is large (and positive).\nA straightforward generalization of these results involves more than two synaptic\nprocesses. Conceptually, one process that is Poisson distributed with rate μ and amplitude\na can always be replaced by η processes, all independent of each other, firing at rate μ/ η\nand amplitude a, or by η independent processes of rate μ but of amplitude a/n. Indeed,\nall we need to assume is that the synaptic inputs impinging onto the cell derive from\nthe superposition of arbitrary point processes that are independent of each other. If these\nprocesses are stationary (see above), the superposition of such processes will converge to\na stationary Poisson process (Cox and Isham, 1980).\nFinally, we do not want to close this section without briefly mentioning Wiener processes\nor Brownian motion. What makes the mathematical study of the random walk models\ndiscussed so far tortuous is that each sample path is discontinuous, since upon arrival\nof a synaptic input, V (i) changes abruptly by ±a. The associated differential-difference\nequations are less well understood than standard differential equations. Introduced into\nthe neurobiological community by Gerstein and Mandelbrot (1964), Wiener processes are\nthoroughly studied and many of the relevant mathematical problems have been solved.\nA Wiener process can be obtained as a limiting case from our standard random walk\nmodel by letting the amplitude a of each synapse become smaller and smaller, while\nthe rate at which the input arrives becomes faster and faster. In the appropriate limit\none obtains a continuous sample path, albeit with derivatives that can diverge (indeed\nthe \"derivative\" of the path is white noise). The crucial difference as compared to the\ndiscrete Poisson process is that for any two times t\\ and t2, V(¿2) — V(fi) is a contin-\nuous Gaussian random variable with zero mean and variance fe — ii) (whereas for the\nrandom walk models, Vfe) — V(fi) takes on discrete values). Because the basic intuition\nconcerning the behavior of a simple nonleaky integrate-and-fire model can be obtained\nwith the random walk model discussed above, complemented by computer simulations\n364 \n· \nSTOCHASTIC MODELS OF SINGLE CELLS\nBecause we assume that the unit had started out at V — 0 (implying that it had just spiked\nand was reset), 7^, corresponds to the average interval between adjacent spikes. While we\nare unable to predict the exact trajectory of the membrane potential, we can perfectly well\npredict certain time-averaged aspects of spike trains.\nThe normalized \"width\" of the ISI, expressed by the coefficient of variation, is\n(15.29)"}
{"text": "===== Page 16 =====\n15.2 Stochastic Activity ¡η Integrate-and-Fire Models \n· 365\nthat are to follow, we shall not treat Wiener processes here, referring the reader instead to\nTuckwell (1988b).\n15.2.5 Random Walk in the Presence of a Leak\nSo far, we neglected the presence of the membrane leak resistance R, which will induce\nan exponential voltage decay between synaptic inputs, leading to \"forgetting\" inputs\nthat arrived in the past compared to more recent arrivals. It was Stein (1965) who first\nincorporated a decay term into the standard random walk model. Heuristically, we need to\nreplace Eq. 15.19 by\n(15.30)\nBecause the trajectories of both Ne and N¡ are discontinuous, jumping by ±1 each time a\nsynaptic input arrives, dN/dt can be thought of as a series of delta functions, increasing V\nabruptly by ae (or decreasing by a,·). In the absence of any threshold, the mean membrane\npotential is\n(15.31)\nwhere the drift μ = α6μβ — α,μ, has the dimension of a current.\n5 Equation 15.31 was\nderived under the assumption that the initial value of the membrane potential has decayed\naway as e~'/\nr, with τ = RC.\nDifferent from the nonleaky case, where the membrane potential diverges (as {V(t)} =\nμί/C in physical units; seeEq. 15.21), the membrane leak stabilizes the membrane potential\nat a level proportional to the drift, that is, the mean input current. The variance will also\nremain finite\n(15.32)\nIt is straightforward to generalize this to the case of η inputs.\nWhat about the mean time to spike? Unfortunately, even after several decades of effort,\nno general expressions for the probability density of Γ(η and related quantities, such as the\nISI distribution, are available and we have to resort to numerical solutions. Qualitatively,\nthe leak term has little effect on the ISI for high rates, because there is not sufficient time\nfor any significant fraction of the charge to leak away before Vth is reached. In other words,\nwe can neglect the effect of a leak if the interspike interval is much less than τ.\nAt low firing rates, the membrane potential \"forgets\" when the last firing occurred, so\nthat the subsequent firing time is virtually independent of the previous time. In this mode,\nthe leaky integrator operates as a \"coincidence\" detection device for occasional bursts of\nEPSPs. More interspike intervals will occur at large values of t than expected from the\nmodel without a leak and the associated Cv value is larger than for the nonleaky integrator.\nA number of attempts have been made to include stochastic synaptic activity into\ndistributed cable models, including solving the cable equation in the presence of white\nnoise current (Tuckwell and Wan, 1980; see also Chap. 9 in Tuckwell, 1988b). Since\nthe complexity of these models usually precludes the development of useful intuition\nconcerning the computational properties of nerve cells, we will not discuss them any further\nand refer the reader to Tuckwell (1988b,c).\n5. In the previous pages we assumed—implicitely, for the most part—that each synaptic input increases the membrane potential\nby ae/C although we did not explicitly write out the dependency on C."}
{"text": "===== Page 17 =====\n366 \n. \nSTOCHASTIC MODELS OF SINGLE CELLS\n15.3 What Do Cortical Cells Do?\nSo, how random are cortical cells? And is their observed degree of variability compatible\nwith their known biophysics?\nBefore we discuss this, a word of caution. The majority of experimental studies assume\nthat the statistics of spikes do not vary significantly during the period of observation. Since\nthis degree of stationarity is very difficult to verify empirically, a more reasonable concept\nis that of a weakly stationary process, for which only the mean and the covariance need to be\ninvariant under time translation. For a spike train generated in response to a physiological\nstimulus (such as a flashed bar of light) nonstationarity is practically guaranteed since\nboth single-cell and network effects conspire to lead to firing frequency adaptation. This\nis reflected in the poststimulus time histogram, which usually is not flat (e.g., Fig. 15.1).\nSeveral techniques exist to deal with such nonstationary data (for instance, by only using\nthe adapted part of the spike train or by normalizing for the local rate μ(ί); Perkel, Gerstein,\nand Moore, 1967; Softky and Koch, 1993).\nInvestigating the response of cat motoneurons in the spinal cord to intracellular current\ninjections, Calvin and Stevens (1967, 1968) concluded that in two out of five cells they\nconsidered in great detail, the low degree of observed jitter in spike timing (Cy ^ 0.05)\ncould be explained on the basis of noisy synaptic input charging up the somatic potential\nuntil it reaches Vih\n afld triggers a spike (Fig. 15.8A). This is expected if the cell behaved\nlike an ideal integrate-and-fire unit under the random walk assumptions (Eq. 15.21).\n15.3.1 Cortical Cells Fire Randomly\nWhat about cells in the cerebral cortex? How randomly do they fire? And what does their\nsurprisingly high degree of randomness imply about the neural code used for information\ntransmission? Anecdotal evidence (Fig. 15.8B) illustrates that under certain conditions, the\nsomatic membrane potential of cortical neurons does behave as expected from the random\nwalk model discussed above, integrating a large number of small inputs until threshold is\nreached. But is this really compatible with their variability? Although the firing variability\nof thalamic and cortical cells has been studied experimentally for quite some time, this was\ncaried out predominantly during spontaneous firing, when rates are very low (Poggio and\nViernstein, 1964; Noda and Adey, 1970; Burns and Webb, 1976; Teich, Turcott, and Siegel,\n1996). Softky and Koch (1992,1993) measured the firing variability of cells from cortical\nareas VI and MT in the awake monkey responding to moving bars and random dots with\nmaintained firing rates of up to 200 Hz. Applying appropriate normalization procedures and\nexcluding bursting cells, Softky and Koch found that Cy was close to unity—as expected\nfrom a Poisson process (Fig. 15.9).\nThis poses somewhat of a paradox. At high enough firing rates, when the decay of the\nmembrane potential can be neglected, Cy is inversely proportional to the square root of\n«th, the number of excitatory inputs necessary to trigger threshold (Eq. 15.16). If «th is\nlarge, as commonly held (that is, 20, 50, or 100),\n6 the cell should fire in a very regular\nmanner, which cortical cells patently do not do. On the basis of detailed compartmental\nsimulations of a cortical cell with passive dendrites, Softky and Koch (1993) showed that\n6. Based on evidence from spike-triggered averaging, in which spikes recorded in one neuron are correlated with monosynaptic\nEPSPs recorded in another pyramidal cell; the amplitudes usually fall below 0.5 mV (Mason, Nicoll, and Stratford, 1991; for a\nreview see Fetz, Toyama, and Smith, 1991). This implies that on the order of 100 simultaneous excitatory inputs are required\nto bring a cortical cell above threshold."}
{"text": "===== Page 18 =====\n15.3 What Do Cortical Cells Do? · 367\nFig. 15.8 DOES THE MEMBRANE POTENTIAL DRIFT UP TO V^t \nIntracellular membrane potential\nin two different cell types. (A) Somatic potential in a motoneuron in the spinal cord of an anesthetized\ncat in response to a constant intracellular current injection (Calvin and Stevens, 1968). Portions of the\nmembrane potential between adjacent spikes are superimposed, matched against a theoretical model\nin which the depolarization to threshold is expected to be linear in time (Eq. 15.22) and evaluated\nagainst a fixed Vjh (dotted horizontal line). This model explains the jitter in spike timing in two out\nof five motoneurons investigated entirely on the basis of synaptic input noise. Reprinted in modified\nform by permission from Calvin and Stevens (1968). (B) Intracellular recording from a complex cell\nin cat visual cortex (Ahmed et al, 1993), responding to a bar moving in the preferred direction. The\ndepolarizing event around 80 msec was not quite sufficient to generate a spike. This cell does appear\nto integrate over many tens of milliseconds, as witnessed by the slow rise and fall of the membrane\npotential. It remains a matter of intense debate whether spiking in cortical cells can be explained by\ntreating the cell as integrating over a large number of small excitatory and inhibitory inputs or whether\ncortical cells detect coincidences in synaptic input at a millisecond or finer time scale. Unpublished\ndata from B. Ahmed and K. Martin, printed with permission.\nneither (1) distributing synaptic inputs throughout the cell, (2) including Hodgkin-Huxley-\nlike currents at the soma, (3) incorporating a \"reasonable\" amount of synaptic inhibition, nor\n(4) using weakly cross-correlated synaptic inputs changed this basic conclusion appreciably."}
{"text": "===== Page 19 =====\n368 \n· \nSTOCHASTIC MODELS OF SINGLE CELLS\nFig. 15.9 FIRING VARIABILITY OF CELLS IN MONKEY CORTEX \nComparison of the variability\nof spike timing, quantified using the coefficient of variation Cν, as a function of the mean interspike\ninterval for cortical cells and modeled data. The scattered crosses are from VI and MT cells recorded\nin the behaving monkey and are normalized for their mean firing rates. Their Cy is high, close to\nunity. Superimposed are the CV curves computed for a random walk leaky integrate-and-fire model\nusing conventional assumptions (lower trace; wth = 51, τ = 13 msec, and iref = 1 msec) and less\nconventional ones (middle bold trace; nth = 51 and τ = 0.2 msec). The thin, top curve corresponds\nto the limiting case of a pure Poisson spike train (nth = 1) in combination with fref = 1 msec. The\neffect of this absolute refractory period is to render the spike timing very regular at very high firing\nrates. Reprinted in modified form by permission from Softky and Koch (1993)\nCortical cells cannot integrate over a large number of small synaptic inputs and still fire as\nirregularly as they do. Instead, either the time constant of the cell has to be very small, in\nthe 1 msec range (in which case synaptic input that arrived more than 2 or 3 msec ago has\nbeen forgotten), or n^ must be very small, in the neighborhood of 2-5 (Fig. 15.9).\nMeasuring variability in the spike count confirmed this high degree of randomness. Log-\nlog plots of the mean versus the variance in the number of spikes in the cortex of the monkey\nconsistently give rise to slopes of around 5/4, implying large values for F(T) (Tolhurst,\nMovshon, and Dean, 1983; Zohary, Hillman, and Hochstein, 1990; Snowden, Treue, and\nAndersen, 1992; Softky and Koch, 1993; Teich, Turcott, and Siegel, 1996). As before, the\nstandard integrator model of pyramidal cells should lead to much smaller values of the Fano\nfactor F(T).\n15.3.2 Pyramidal Cells: Integrator or Coincidence Detector\nSoftky and Koch (1992, 1993) argue for a coincidence detection model in which distal\ndendrites generate fast sodium action potentials which are triggered when two or more\nexcitatory synaptic inputs arrive within a millisecond of each other. A handful of these active\nsynaptic events, if coincident at the soma, will trigger a spike. (This effectively reduces nth\nto the required small values.) Indeed, Softky (1994, 1995) claims that the experimental\nevidence for active dendritic conductances is compatible with submillisecond coincidence\ndetection occurring in the dendrites of pyramidal cells (see Sec. 19.3.3).\nThis has rekindled a debate over the extent to which pyramidal cells either integrate\nover many small inputs on a time scale of tens of milliseconds or detect coincidences at"}
{"text": "===== Page 20 =====\n15.3 What Do Cortical Cells Do? · 369\nthe millisecond or faster time scale (Abeles, 1982b; Shadlen and Newsome, 1994; Konig,\nEngel, and Singer, 1996). In the former case, the code would be the traditional noisy rate\ncode that is very robust to changes in the underlying hardware. If neurons do use precise\nspike times—placing great demands on the spatio-temporal precision with which neurons\nneed to be wired up—the resultant spike interval code can be one or two orders of magnitude\nmore efficient than the rate code (Stein, 1967a,b; Rieke et al., 1996).\nAs illustrated in Fig. 15.10A, the Geiger counter model leads to very regular spiking,\nsince the cell integrates over a large number of inputs (here «th = 300). High irregularity\nof the spike discharge can be achieved in several ways. If the neuron has a very short time\nconstant, it will only fire if a sufficient number of spikes arrive within a very small time. Such\na coincidence detector model is illustrated in Fig. 15.10B and is simulated by considering\nV to be the sum of all EPSPs arriving within the last millisecond. If at least 35 EPSPs are\ncoincident within this time, an output spike is generated. This model is, of course, sensitive\nto disturbances in the exact timing of the synaptic input. It is precisely this sensitivity that\ncan be used to convey information.\nShadlen and Newsome (1994), Tsodyks and Sejnowski (1995), and van Vreeswijk and\nSompolinsky (1996) seek an explanation of the high variability in a balanced inhibition\nrandom walk, in which the drift is zero (as in Gerstein and Mandelbrot, 1964), that is,\nthe average net current is zero, excitation balancing inhibition. The unit spikes whenever\na fluctuation leads to a momentary drop in synaptic inhibition in combination with a\nsimultaneous excess of excitation. (This is achieved in Fig. 15.IOC by having the 150\ninhibitory cells fire at the same rate as the 300 excitatory cells, but with twice their\npostsynaptic weight.)\nIt can be shown analytically (van Vreeswijk and Sompolinsky, 1996) that large networks\nof simple neurons, randomly and sparsely interconnected with many relatively powerful\nexcitatory and inhibitory synapses whose activity is approximately \"balanced,\" display\nhighly irregular patterns of spiking activity characteristic of deterministic chaos. Such\nnetworks can respond very rapidly to external input, much faster than the time constant of\nthe individual neurons, van Vreeswijk and Sompolinsky (1996) argue that irregular spiking\nis a robust emergent network property not depending on intricate cellular properties. Yet\nthis ability to respond rapidly comes at a price of continual activity, requiring a constant\nexpenditure of metabolic energy. Experimentally, it remains an open question to what extent\nindividual cells receive balaced input, that is to what extent sustained changes in excitatory\ninput are opposed by equally powerful sustained changes in inhibition.\nA related solution to the dilemma of obtaining high variability while retaining the\nintegrating aspect of cortical cells is correlated synaptic inputs. If excitatory synaptic inputs\nhave a tendency to arrive simultaneously, less temporal dispersion will occur and inputs\nwill be more effective. Voltage-dependent sodium conductances in the dendritic tree will\nbe particularly sensitive to such simultaneous inputs, effectively lowering the number of\ninputs needed to reach threshold.\nIncluding more complex spike generation dynamics, such as bursting (Wilbur and Rinzel,\n1983; but see Holt et al., 1996), or incorporating short-term adaptation into the synapses\n(Abbott et al., 1997; Sec. 13.5.4) will increase Cv and F(T). The somewhat arbitrary\nchoice of resetting the membrane potential following spike generation to the unit's resting\npotential offers yet another way to increase variability. Intracellular data from cortical cells\nfrequently reveal the lack of any hyperpolarization following an action potential. Bell et\nal., (1995) and Troyer and Miller (1997) argue that the membrane potential should be reset\nto a much more positive value, thereby lowering the effective n^."}
{"text": "===== Page 21 =====\n370\nSTOCHASTIC MODELS OF SINGLE CELLS\nFig. 15.10 THREE MODELS or SYNAPTIC INTEGRATION \nThe input to the simulated neuron is\nshown in the left panel: 300 excitatory Poissou distributed synaptic inputs, firing at a mean rate of\nμβ = 100 Hz. The output units on the right also fire at this average rate. (A) Ideal integrate-and-\nfire model. Each synaptic input increases V by 0.05 mV, corresponding to »th = 300. As expected,\nthe output pulses have a clocklike regularity. (B) In this cartoon of a coincidence detector model,\nonly inputs within the previous millisecond contribute toward V (corresponding to an effective\nsubmillisecond time constant). If na, = 35 EPSPs arrive \"simultaneously\" (here, within 1 msec),\nthe unit spikes. Notice the elevated mean membrane potential. In this model, the timing of spikes\ncan code information. (C) Standard random walk model with a balance of excitation and inhibition.\nIn addition to the 300 excitatory inputs, 150 inhibitory inputs have been added, firing at the same\nrate; ae = 0.6 mV (corresponding to an effective «th = 25) and a, = 1.2 mV. This model achieves\na high variability but at a cost of substantial inhibition. Since no information is encoded in the time\nof arrival of spikes, it is very robust. Adding a leak term to the first and third models does not affect\nthese conclusions substantially. Reprinted by permission from Shadlen and Newsome (1994)\nWhat is clear is that the simple Geiger counter model of synaptic integration needs to\nbe revised. A plethora of mechanisms—such as synaptic inhibition, short-term synaptic\ndepression, correlated synaptic input, a depolarizing reset, active dendritic processing on a\nfast time scale—will interact synergistically to achieve the observed random behavior of\ncortical cells.\nLest the reader forget the cautionary note posted earlier, the preceding paragraphs apply\nspecifically to cortical cells. Given the great diversity apparent in the nervous system,\nthe question of the variability of neurona! firing and its implication for the code must be\ninvestigated anew in each specific cell type. In cells where synaptic input but modulates the\nstrongly nonlinear dynamics of firing, as in oscillatory or bursting cells (Jaeger, DeSchutter,\nand Bower, 1997; Marder and Calabrese, 1996), the source of variability could be quite\ndistinct from those discussed here."}
{"text": "===== Page 22 =====\n15.3 What Do Cortical Cells Do?\n371\n15.3.3 Temporal Precision of Cortical Cells\nYet under some conditions, cortical spike trains can look remarkably consistent from\ntrial to trial. This is exemplified in a quite dramatic manner by Fig. 15.11, from an\nexperiment carried out in a monkey that had to respond to various patterns of random dot\nmotion (Newsome, Britten, and Movshon, 1989). If one averages—as is frequently done\nin practice—over many such presentations of different clouds of randomly moving dots,\nthe spiking activity of the cell looks uneventful (beyond the initial transient; left column).\nYet if one pulls out all of the responses to exactly the same random dot movie, a highly\nrepeatable pattern of spiking becomes visible. Such reliable responses could be observed\nin the majority of MT cells examined (Bair and Koch, 1996). These stimulus-induced\ntemporal modulations disappear for coherent motion, that is, when the entire cloud of dots\nmoves as one across the receptive field, explaining why they are not visible in Fig. 15.1\n(Bair, 1995).\nFig. 15.11 \nCORTICAL CELLS CAN FIRE IN A VERY PRECISE MANNER \nResponses of an MT cell\nin a behaving monkey to patterns of randomly and independently moving dots. The left column shows\nthe cortical response to different random dot patterns. Averaged over 90 trials, the cell's response is\nwell described by a random point process with μ = 3.4 Hz (excluding the initial \"flash\" response;\nsee the bottom panel). When only trials over the 2-hour-long experiment that were stimulated by\nthe identical random dot movie are considered (right column), strikingly repeating patterns can be\nobserved. (Viewing this figure obliquely best reveals the precision of spiking.) Notice the sharp peak\nfollowing the 1-sec mark or that nearly all spikes in the final 400 msec cluster into six vertical streaks.\nDespite this precision, observed in the majority of MT cells, spike count variability F(T) is still very\nhigh. The poststimulus time histogram, using an adaptive square window, is shown at the bottom.\nReprinted by permission from Bair and Koch (1996)."}
{"text": "===== Page 23 =====\n372 · STOCHASTIC MODELS OF SINGLE CELLS\nWhen contemplating Fig. 15.11, it should be kept in mind that at the beginning of the\nexperiment, the animal is thirsty and eager to perform, while at the end it is satiated and\nnot very motivated. Furthermore, the animal continuously makes small eye movements,\ntermed microsaccades. Yet despite this, a cell removed at least six synapses from the visual\ninput fires in a very repeatable pattern throughout this time.\nBair and Koch (1996) quantify the degree of precision by computing the temporal jitter,\nthat is, the standard deviation in time of the onset of periods of elevated firing (such as\nthe \"event\" indicated by a black bar near 1740 msec in Fig. 15.11; see also Sestokas and\nLehmkuhle, 1986). For this event, the jitter is 3.3 msec. For 80% of all cells, the most precise\nresponse has a jitter of less than 10 msec, while in some cases it is as low as 2-4 msec.\nAlthough somewhat counterintuitive, even these spike trains can be generated by a\nPoisson process, but in this case with a time-varying rate μ,(ί) (called inhomogeneous\nPoisson process). As in a homogeneous Poisson process, the number of spikes in any one\ntime interval is independent of the number of spikes in any other nonoverlapping interval.\nThe time intervals between adjacent spikes also remain independent of each other but are\nnot identically distributed anymore (Tuckwell, 1988b). In the case of the data shown in\nFig. 15.11, the Fano factor F(T) is close to unity when evaluated for the duration of the\nstimulus (Britten et al., 1993).\nWe conclude that individual cortical cells are able to reliably follow the time course of\nevents in the external world with a resolution in the 5-10 msec range (see also Mainen and\nSejnowski, 1995).\nLet us hasten to add that much evidence has accumulated (as cited in Sec. 14.1) indicating\nthat the precision of spike generation in one cortical cell with respect to another cell's firing\ncan be even higher (e.g., Lestienne and Strehler, 1987). Best known is the demonstration\nof repeating patterns among simultaneously recorded cells by Abeles et al., (1993). Pairs\nof cells may fire action potentials at predictable intervals that can be as long as 200 msec,\ntwo orders of magnitude longer than the delay due to a direct synaptic connection, with a\nprecision of about 1 msec. This is the key observation at the heart of Abeles's synfire chain\nmodel of cortical processing (Abeles, 1990).\nAny rash judgment on whether or not the detailed firing pattern is noise or signal should be\ntempered by Barlow's (1972) statement about the firing of individual nerve cells that \"their\napparently erratic behavior was caused by our ignorance, not the neuron's incompetence.\"\n15.4 Recapitulation\nIn this chapter we tried to address and quantify the stochastic, seemingly random nature of\nneuronal firing. The degree of randomness speaks to the nature of the neural code used to\ntransmit information between cells. Very regularly firing cells are obviously not very good\nat encoding information in their timing patterns; yet this will not prevent information from\nbeing encoded in their mean firing rates, albeit at a lower rate (in terms of bits per spike)\nbut in a robust manner.\nThe spiking behavior of cells has traditionally been described as a random point process,\nin particular as a renewal process with independent and identically distributed interspike\nintervals. Cortical cells firing at high rates are at least as variable as expected from a simple\nPoisson process. Variability is usually quantified using two measures: Cy to assess the\ninterspike interval variability and F(T) for spike count variability, with both taking on"}
{"text": "===== Page 24 =====\n15.4 Recapitulation · 373\nunity for a Poisson process. If spike trains are generated by a renewal process, F = Cy in\nthe limit of large observational intervals.\nThe power spectrum of cortical spike trains is flat with a dip around the origin, as expected\nfrom a Poisson process modified by a refractory period. (This refractory period only comes\ninto play at very high firing rates, serving to regularize them.) The rate of the spiking process\nis usually not constant in time, but is up or down regulated at the 5-10 msec level. The\nprincipal deviation from Poisson statistics is the fact that adjacent interspike intervals are\nnot independent of each other (even when neglecting bursting cells), that is, spike trains\ncannot really be described by a renewal process.\nAs always in science, this conclusion gives birth to intertwining considerations. What\ntype of models of synaptic integration give rise to the high degree of randomness apparent\nin neuronal firing and how do these constrain the nature of the neuronal code? The standard\nGeiger counter model predicts that when an integrate-and-fire unit needs to integrate over\na large number of small synaptic inputs, it should fire very regularly. Since this is patently\nnot true in the cortex, it needs to be abandoned. Out of the many alternatives proposed, two\ndivergent views crystallize. One school retains the idea that neurons integrate over large\nnumber of excitatory inputs with little regard for their exact timing by invoking a large\ndegree of inhibitory inputs (following the random walk model advocated by Gerstein and\nMandelbrot, 1964), a depolarizing reset, or correlated synaptic input. The other school sees\ncortical cells as coincidence detectors, firing if small numbers of excitatory events arrive\nsimultaneously at the millisecond (or even submillisecond) scale (via powerful and fast\ndendritic nonlinearities). Under these circumstances, detailed timing information can be\nused to transmit information in a manner much more efficient, yet also more demanding,\nthan in a mean rate code. Only additional experimental evidence can resolve this issue.\nOf course, at small enough time scales or for a handful of spikes, the debate loses its\nsignificance, since it becomes meaningless to define a rate for a 20 msec long segment of a\nspike train with just two action potentials.\nWhat this dispute shows is that integrate-and-fire units serve as gold standard against\nwhich models of variability are evaluated. Given their relative simplicity—-compared\nagainst the much more complex conductance-based models of firing—this is quite re-\nmarkable.\nThe highly variable character of cortical firing allows neurons to potentially pack one or\nmore bits of information per action potential into spike trains (as done in sensory neurons\ncloser to the periphery). Information theory as applied to a band-limited communication\nchannel has taught us that the optimal code—optimal in terms of using the entire bandwidth\navailable—looks completely random, since every redundancy has been removed to increase\nthe efficiency. One could infer from this that neurons make optimal use of the limited\nbandwidth of axons using a sophisticated multiplexed interspike interval code from which\nall redundancies have been removed, and that neurons, properly decoded, maximize the\nexisting channel bandwidth. To what extent they actually do for physiologically relevant\nstimuli remains an open issue."}
{"text": "===== Page 2 =====\n16.1 Intrinsically Bursting Cells\n375\nat a constant frequency of between 5 and 12 Hz (as in Fig. 16.1C), while others re-\nspond by repetitive single spikes. It is not uncommon for an IB cell to flip back and\nforth between these two firing modes. Even more complex behavior, such as the initi-\nation of a long train of bursts in response to a brief hyperpolarizing current stimulus,\nhas been reported (Silva, Amitai, and Connors, 1991). The spikes themselves are con-\nventional sodium carried ones. Their amplitude during the burst frequently decreases—\npresumably because the sustained depolarization partially inactivates /N-,—while their\nduration increases.\nIntrinsic bursting cells show a unique laminar distribution and morphology. In studies\nusing intracellular staining, IB cells correspond to large pyramidal neurons restricted—with\nfew exceptions—to layer 5, with stereotyped dendritic morphologies and axonal projection\npatterns. IB cells have an extended apical bush in layers 1 and 2, while nonbursting cells\nin layer 5 lack this feature (Fig. 16.ID; Larkman and Mason, 1990; Gray and McCormick,\n1996; Yang, Seamans, and Gorelova, 1996).\nFig. 16.1 INTRINSICALLY BURSTING AND REGULAR SPIKING PYRAMIDAL CELLS Structure-\nfunction relationship among pyramidal cells from rodent cortex. Typical firing patterns seen in\nresponse to an intracellularly injected current step in (A) re guiar spiking, and (C) intrinsically bursting\nlayer 5 pyramidal cells from mouse cortex (Agmon and Connors, 1992). Drawings of biocytin-filled\nlayer 5 (B) regular spiking and (D) intrinsically bursting (IB) pyramidal cells from the rat. IB cells\ntend to have an extensive apical dendrite (reaching all the way up to layer 1) while regular spiking\ncells show a smaller degree of dendritic arborization. The output targets of the two cell types are quite\ndistinct, with bursting cells projecting outside the cortex proper. Unpublished data from L. J. Cauller,\nI. Biilthoff, and B. W. Connors, printed with permission."}
{"text": "===== Page 3 =====\n376 \n« \nBURSTING CELLS\nNeurons whose cell bodies are located in layer 5 constitute the primary output of the\nneocortex. Whenever the cortex decides to do something, the message has to be sent\noutside the cortical system via layer 5 cells. This includes the one-million-axon (in humans)\npyramidal tract that leaves the motor cortex and adjacent areas and projects into the spinal\ncord (Porter and Lemon, 1993).\nSome evidence suggests that the cells projecting outside the cortex are of the bursting\ntype. IB cells project to the pons and to the superior colliculus, while nonbursting layer\n5 pyramidal cells project to the contralateral cortex via the corpus callosum (Wang and\nMcCormick, 1993; Kasper et al., 1994). It is known that many of the identified layer 5\ncells in the rat motor cortex that project to the spinal cord have the firing characteristics\nof IB cells (Tseng and Prince, 1993). It is obviously important to know whether all cells\nprojecting to subcortical (and nonthalamic) targets are of the bursting type.\n16.2 Mechanisms for Bursting\nBursting cells have been analyzed using biophysically based conductance models as pio-\nneered by Traub and his colleagues (Traub and Llinás, 1979;Traub, 1979,1982; Traub etal.,\n1991; summarized in the monograph by Traub and Miles, 1991) and from the perspective\nof dynamical systems theory (Chap. 7; Rinzel, 1987; Wang and Rinzel, 1996; Rinzel and\nErmentrout, 1997).\nThere is likely to be aplurality of mechanisms that cause bursts. In Sec. 9.1.4 we discussed\nthe critical role of a somatic low-threshold calcium current responsible for mediating bursts\nin thalamic cells (Fig. 9.4).\nCentral to the genesis of bursting in cortical cells are fast, sodium-driven action potentials\nat the cell body, which not only propagate forward along the axon, but also invade the\ndendritic tree (Rhodes and Gray, 1994). Backpropagation is aided by a low density of\ndendritic sodium channels that boost the signal. (For a more detailed discussion of this, see\nSees. 19.1 and 19.2.) Dendritic calcium conductances build up local depolarization. In the\nmeantime, somatic repolarization establishes a pronounced potential difference between\nthe cell body and the dendritic tree, giving rise to ohmic currents. These will depolarize\nthe soma and the spike initiating zone in the form of an afterdepolarizing potential (ADP).\nIt is this ADP that causes the rapid sequence of fast sodium spikes. The entire chain of\nevents is finished off by a hyperpolarization induced by a combination of voltage- and\ncalcium-dependent potassium currents.\nIn this view of bursting, dendritic calcium currents are at the root of the prolonged\ndepolarization that triggers the fast, somatic sodium spikes. However, it appears that in\nsome systems one or more sodium currents, in combination with the \"right\" dendritic tree\nmorphology, can by themselves initiate bursting (Turner et al., 1994; Franceschetti et al.,\n1995; Azouz, Jensen, and Yaari, 1996).\nMainen and Sejnowski (1996) derive what this right morphology should be. They\nsimulate the dynamics of spiking in a number of reconstructed cortical cells with different\ndendritic anatomies and endowed with the same distribution and density of voltage-\ndependent sodium, potassium, and calcium currents (Fig. 16.2). Changing only the dendritic\ntree geometry, but not the mix of voltage-dependent conductances, gives rise to the entire\nspectrum of intrinsic responses observed in the cortex: from nonadapting and adapting cells\nto bursting ones. The large dendritic tree associated with layer 5 pyramidal cells prolongs\nthe sodium spike propagating back into the dendritic arbor sufficiently for bursts to occur,"}
{"text": "===== Page 4 =====\n16.3 What Is the Significance of Bursting? \n· 377\nFig. 16.2 CELLULAR MORPHOL-\nOGY AND INTRINSIC FIRING PAT-\nTERNS Different \nreconstructed\nneurons are endowed with the same\ndensities of voltage-dependent con-\nductances in soma and dendritic tree\nin a computer model: a fast sodium\ncurrent, three potassium currents,\nand a high-threshold calcium cur-\nrent (Mainen and Sejnowski, 1996).\nThe distinct firing patterns, evoked\nin response to sustained current in-\njection (of 50, 70, 100, and 200 pA\nfor A to D, respectively), run the\ngamut of adapting, nonadapting,\nand bursting behavior seen in the\ncortex. The four cells are (A) layer 3\naspiny stellate cell from rat cortex,\n(B) layer 4 spiny stellate, (C) layer\n3 pyramid, and (D) layer 5 pyramid\nfrom cat visual cortex. Reprinted\nby permission from Mainen and Se-\njnowski (1996).\nwhile no such behavior is observed in the much more compact population of stellate or\npyramidal cells in superficial layers.\nThese results go hand in hand with the observed correlation between cellular morphology\nand intrinsic firing pattern reported above and emphasizes the often subtle and complex\ninterplay between membrane-bound conductances and cable structure.\n16.3 What Is the Significance of Bursting?\nAnytime two spatially overlapping but morphologically distinct cell classes project to\ndifferent locations (as the layer 5 bursting and nonbursting cells), one should assume that\nthey encode and transmit different features (since otherwise the job could be done with the"}
{"text": "===== Page 5 =====\n378 · \nBURSTING CELLS\nhelp of a single cell class whose axons branch to innervate both areas). Viewed in this light,\nthe existence of a cell type that responds to sustained current steps in a bursty manner and\nprojects outside the cortical system demands an explanation in functional terms.\nOne broad hypothesis, first entertained by Crick (1984a), is that bursting constitutes\na specialized signal, a particular code symbol, distinct from isolated spikes, conveying a\nparticular type of information.\nThe relevance of bursts for conveying information reliably has been ascertained for the\nelectrosensory system of the electric fish (Gabbiani et al., 1996). Although of a quite distinct\nevolutionary lineage than the cerebral cortex, it is profitable to review this study briefly.\nGiven the conservative nature of biology, reinventing and reusing the same mechanism\nover and over again, it would not be surprising if its lesson also applied to the cortex\n(Heiligenberg, 1991).\nGabbiani et al., (1996) subject the fish Eigenmannia to random amplitude modulations of\nan electric field in the water whose carrier frequency is equal to the fish's own electrical organ\ndischarge frequency that has been pharmacologically disabled (Fig. 16.3A), while recording\nfrom pyramidal cells in the electrosensory lateral line lobe (ELL). Signal detection theory\ncan be used to determine which particular features of the electric field are represented by the\nfiring of these cells. Assuming that the ELL pyramidal cells implement a linear summation\nand threshold operation, it can be inferred that the cells respond best to up or down strokes\nof the electric field.\nSlightly more than half of the spikes generated by the pyramidal cells occur in short\nbursts (on average three spikes with a mean spike separation of 9 msec; Fig. 16.3A). The\nsignal-to-noise ratio of this signal detection operation—based only on those spikes that\nmake up a burst—is substantially higher than the signal-to-noise ratio of the signal based\non isolated spikes (Fig. 16.3B). In other words, spikes taken from a burst signal the presence\nof a particular stimulus feature, here an \"abrupt\" change in the electric field, more reliably\nthan isolated spikes. Intracellular evidence implicates the apical dendritic tree of the ELL\npyramidal cells as the locus where the burst computation is carried out (Turner et al., 1994).\nIt is possible that bursty neurons communicate their messages to their target cells with the\naid of two output channels multiplexed onto a single axon, one employing isolated spikes\nand the other bursts. Depending on the type and amount of short-term synaptic plasticity\npresent at the output synapses of bursting cells, they could transmit one or the other channel\n(Sec. 13.5.4; Abbott et al., 1997; Lisman, 1997).\nThis would work in the following fashion. Initially the output synapses of bursting cells\nhave a very low probability of synaptic release po. A single spike is unlikely to cause a\nrelease of a vesicle but will, very briefly, enhance the probability of release for the next\nspike. Such transient but powerful facilitation would lead to isolated spikes being effectively\ndisregarded while bursts would cause a synaptic release with probability close to 1. The price\nthe nervous system would pay for the usage of bursts is their limited temporal resolution as\ncompared to individual spikes (in the Gabbiani et al., 1996, study, the interval between the\nfirst and the last spike is 18 ± 9 msec).\nConversely, a synapse with high initial probability of release and a powerful but short act-\ning short-term depression will disregard the additional spikes contained within a burst. This\nhypothesis predicts that synapses made by the axon of a bursting cell have low p0 and show\na very short lasting but powerful facilitation not present at the synapses of nonbursting cells.\nKoch and Crick (1994) speculate that bursting could be one of the biophysical correlates\nof short-term memory (via elevated, presynaptic calcium concentration during such a burst).\nThe stimulus feature represented by a population of bursting cells would correspond to the\nfeature \"stored\" in short-term memory. Because of the close association between short-"}
{"text": "===== Page 6 =====\n16.4 Recapitulation\n379\nFig. 16.3 RELIABLE ENCODING OF SENSORY\nINFORMATION IN BURSTS \n(A) In experi-\nments carried out in the electric fish Eigenman-\nnia, Gabbiani and colleagues (1996) stimulate\nits electrosensory system using zero-mean ran-\ndom amplitude modulations (thick smooth line)\nsuperimposed onto a fixed carrier frequency of\nan electric field while recording from pyrami-\ndal cells in the electrosensory lateral line lobe\n(ELL). A large fraction of all spikes belongs to\nbursts (#) that show up as a prominent peak in\nthe interspike intervals (here bursts are defined\nas spikes separated by < 14 msec; see the inset).\n(B) On the basis of signal detection theory in-\ndividual pyramidal cells can be shown to detect\neither up or down strokes of the electric field\namplitude with up to 85% accuracy. The signal-\nto-noise ratio of this pattern discrimination op-\neration, which assumes that ELL pyramidal\ncells perform a linear threshold computation,\nis enhanced (by more than a factor of 2) when\nconsidering only spikes from bursts compared\nto isolated spikes. In other words, burst spikes\nencode information in a more reliable manner\nthan isolated action potentials. Reprinted by\npermission from Gabbiani et al., (1996).\nterm memory and awareness, it is possible that bursting cells are preferentially involved\nin mediating the neuronal correlate of awareness, in particular considering their strategic\nlocation in the cortical layer projecting to the tectum, pulvinar, and other extracortical targets\n(Crick and Koch, 1995).\nQuite a different explanation for the utility of a particular subclass of spike patterns has\nbeen advanced in Traub et al., (1996). In some cells, spike doublets, consisting of two fast\nspikes separated by a stereotyped 4-5 msec interspike interval, are frequently observed.\nSpike doublets are characteristic of GAB Aergic, axo-axonic cortical interneurons (Buhl et\nal., 1994). Computer modeling of the underlying cortical networks show that such doublets\nmediate the frequently observed temporal synchronization of distant neuronal populations.\nTheir oscillatory activities are synchronized with a near-zero phase lag, even though they\nare separated by propagation delays of 5 msec or more.\n16.4 Recapitulation\nBursts, that is, two to five closely spaced, sodium-dependent fast action potentials riding\non top of a much slower depolarization, are a dominant feature of a number of cell classes,"}
{"text": "===== Page 7 =====\n380 · \nBURSTING CELLS\nnot only in the cortex but also among thalamic relay neurons and elsewhere. In the cortex,\nintrinsically bursting cells have a unique morphology and are confined to layer 5, where\nthey constitute the dominant output cell class.\nThe biophysical mechanisms underlying bursting are diverse. A low-threshold calcium\ncurrent at the soma is the principal agent for bursting in thalamic cells (Sec. 9.1.4). Bursts\nin pyramidal cells can originate when sodium spikes propagate back into the dendritic tree,\ncausing parts of the arbor to be depolarized. Under the right circumstances, for instance,\nwhen amplified by dendritic sodium and/or calcium currents, this signals returns to the soma\nin the form of an afterdepolarization that triggers several fast sodium spikes.\nIt has been argued that bursting represents a special code, quite distinct from the firing\nof an isolated action potential. This hypothesis has some experimental support from the\nelectrosensory system of the electric fish. Spikes taken from bursts signal the presence of\nparticular features in the input in a much more reliable manner than isolated spikes.\nThis implies that spiking cells are not restricted to using an asynchronous binary code\nbut might employ a two-channel multiplexing strategy, with the decoding being carried out\nby rapidly adapting synapses.\nA non-exclusive alternative is that bursts serve to synchronize distant neuronal pop-\nulations."}
{"text": "===== Page 1 =====\nINPUT RESISTANCE,\nTIME CONSTANTS,\nAND SPIKE INITIATION\nThis chapter represents somewhat of a tephnical interlude. Having introduced the reader to\nboth simplified and more complex compartmental single neuron models, we need to revisit\nterrain with which we are already somewhat familiar.\nIn the following pages we reevaluate two important concepts we defined in the first\nfew chapters: the somatic input resistance and the neuronal time constant. For passive\nsystems, both are simple enough variables: Rm is the change in somatic membrane po-\ntential in response to a small sustained current injection divided by the amplitude of the\ncurrent injection, while Tm is the slowest time constant associated with the exponential\ncharging or discharging of the neuronal membrane in response to a current pulse or step.\nHowever, because neurons express nonstationary and nonlinear membrane conductances,\nthe measurement and interpretation of these two variables in active structures is not as\nstraightforward as before. Having obtained a more sophisticated understanding of these\nissues, we will turn toward the question of the existence of a current, voltage, or charge\nthreshold at which a biophysical faithful model of a cell triggers action potentials. We\nconclude with recent work that suggests how concepts from the subthreshold domain, like\nthe input resistance or the average membrane potential, could be extended to the case in\nwhich the cell is discharging a stream of action potentials.\nThis chapter is mainly for the cognoscendi or for those of us that need to make sense\nof experimental data by comparing therp to theoretical models that usually fail to reflect\nreality adequately.\n17.1 Measuring Input Resistanpes\nIn Sec. 3.4, we defined KH (/) for passive cable structures as the voltage change at location\ni in response to a sinusoidal current injection of frequency / at the same location. Its\n381\n17"}
{"text": "===== Page 2 =====\n382\nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\ndc component is also referred to as input resistance or R¡n. Three difficulties render this\ndefinition of input resistance problematic in real cells: (1) most membranes, in particular\nat the soma, show voltage-dependent nonlinearities, (2) the associated ionic membrane\nconductances are time dependent and (3) instrumental aspects, such as the effect of the\nimpedance of the recording electrode on Rm, add uncertainty to the measuring process.\nWhile we will not deal with the last problem (see Smith et al., 1985 for further information),\nwe do have to address the first two issues.\nIn order to gain a better understanding of the input resistance, we need to focus on the\nsteady-state current-voltage relationship illustrated in Fig. 17.1. I^uc(Vm), or sometimes\nsimply /oo(Vm), is obtained by clamping the membrane potential at the soma to Vm, letting\nthe system adapt and measuring the clamp current flowing across the somatic membrane.\nUsing the standard Hodgkin-Huxley formalism, we can express the clamp current as\n(17.1)\nwhere the summation index i includes all conductances, whether or not they are voltage-\ndependent (for more details, see Chap. 6). Equation 17.1 does not include an additive term\ncorresponding to the current flowing into or out of the dendrites. Not surprisingly, and\nFig. 17.1 STATIONARY CURRENT-VOLTAGE RELATIONSHIP Stationary I-V relationship for the\nlayer 5 pyramidal cell model of Fig. 3.7. Operationally, /^atlc (Vm) is measured by clamping the somatic\nmembrane potential to Vm, letting the system adapt and measuring the clamp current flowing across\nthe somatic membrane. The currents to the right of their reversal potentials (indicated by arrows) are\noutward going, that is, positive. Thus, for Vm > EK> all potassium currents are positive while for\nthe entire voltage range shown here, /Na is negative. Typical of excitable cells, the I-V relationship\nis highly nonlinear, in particular for depolarizing membrane potentials. It intersects the zero current\naxis at three points. The resting potential of this system, specified by the leftmost zero crossing is\nVrest = —65 mV. The chord conductance is defined as the ratio of the current at any holding potential\nVm to the voltage difference between Vm and the resting potential Vrest. For points A, B, and C it\nis positive, and for D, negative. The slope conductance is defined as the local tangent to the I-V\ncurve, that is as the slope of the curve. The slope conductance at A and Β is positive and at C and\nD negative. For the linear portion of the /-V curve, the chord conductance is almost identical to the\nslope conductance (e.g., point A). The inverse of the slope conductance is usually what is meant by\n\"input resistance.\""}
{"text": "===== Page 3 =====\n17.1 Measuring Input Resistances \n· 383\ndifferent from a linear ohmic resistance, the total somatic membrane current is a nonlinear\nfunction of the membrane potential, in particular for depolarizing values. The resting state\nof the cell is given by the leftward (Fig. 17.1) stable zero crossing at —65 mV.\nWe can better understand Ioo(Vm) in Fig. 17.1 by recalling that for any current that can\nbe expressed as the product of a driving potential and a conductance, values of Vm to the\nleft of the associated reversal potential correspond to an inward, that is, negative current.\nConversely, holding the potential to a more depolarized value than the reversal potential\ncauses an outward (positive) current to flow.\nFor example, the long slope to the left of Vrest is due to the passive leak conductance\nand the anomalous rectifier current /AR, the hump between —65 and —48 mV is caused by\nactivation of the delayed-rectifier K+ current, and the sharp negative peak around —45 mV\nin Fig. 17.1 by /Na- The large outward current at more positive values of Vm is almost\nexclusively carried by the noninactivating /DR, overshadowing the inactivating 7Na. This\nI-V curve gives us the opportunity to define two quite different conductances.\n17.1.1 Membrane Chord Conductance\nThe steady-state membrane chord conductance is defined as the ratio of the total current\nflowing at any particular potential Vm to the applied membrane potential relative to the\nresting potential,\n(17.2)\nThe chord conductance derives its name from its construction: conceptually, it can be\nobtained by pulling a chord or string between the points, (Vrest. 0) and (Vm, /oo(Vm))·\nPhysically, the chord conductance tells us about the sign and the absolute value of the total\nmembrane current flowing at any one potential. If the I-V relationship has several stable\nresting points, the chord conductance can be defined around each resting point.\n17.1.2 Membrane Slope Conductance\nThe steady-state membrane slope conductance is defined as the local slope of the I-V\nrelationship at the operating point Vm,\n(for a single compartmental model). Analytically, the slope conductance corresponds to the\nlinear term in the Taylor series approximation of /^f\"0 around Vm. The slope conductance of\nthe entire membrane consists of the sum of the individual membrane conductances g¡ (Vm)\nplus a derivative term to account for the membrane nonlinearities.\nWhen using a current pulse to measure G¡¿atic (Fig. 17.2), one must be careful to wait\nlong enough until all the slow membrane currents have reached equilibrium.\nThe Inverse of the Slope Conductance Is the Input Resistance\nBecause the slope conductance characterizes the response of the membrane current to\nsmall changes in the membrane potential relative to Vm, its inverse is usually what is meant\nby input resistance R-m (which is the convention we will follow). Figure 17.2 illustrates this\nprocedure graphically. Injecting a small current into the soma of the pyramidal cell model\ncauses the membrane potential to peak before it settles down to its final value. Dividing\n(17.3)"}
{"text": "===== Page 4 =====\n384 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nFig. 17.2 MEASURING THE INPUT RESISTANCE Somatic membrane potential in response to a\ncurrent step of 1-pA amplitude in the pyramidal cell model. At Vrest = —65 m V, the steady-state slope\nconductance G?¿atlc = 1/0.0165 = 60.6 nS, corresponding to an input resistance Rm of 16.5 ΜΩ.\nThe initial overshoot is due to activation and subsequent inactivation of the transient A current.\nBlocking /A causes this hump to disappear and increases /?„, to 38 ΜΩ. Reprinted by permission\nfrom Bernander( 1993).\nthis steady-state change in potential by the injected current gives the input resistance as\n16.5 ΜΩ. The hump disappears if the transient A current is blocked. Now the potential\nrises smoothly to its final value.\nFor a linear I—V relationship the second term in Eq. 17.3 disappears and the slope\nconductance is equal to the chord conductance and corresponds to 1/ΛΓ;,· (e.g., point A in\nFig. 17.1). In other words, for linear systems these difficulties with the definition of the\ninput resistance do not arise.\nIn principle, G^\natic can be measured under either voltage clamp or current clamp. For\ninstance, to compute the slope conductance at rest, we can inject a small current δ I into the\ncell and measure the steady-state voltage change SV (as in Fig. 17.2) or, conversely, we\ncan clamp the potential from Vrest to a slightly displaced value Vrest ± δ V and measure the\nchange in clamp current 57. For small enough values of 57 and δΥ, both methods converge\nto the same result.\nNegative Slope Conductance\nBecause the magnitude of the second term in Eq. 17.3 can often be much larger than\nthe first term but of opposite sign, the slope conductance can be negative, zero, or positive.\nTo understand the significance of this, let us look at the example in Fig. 17.3. At rest, the\nslope conductance at the soma is 57 nS. In the presence of excitatory NMDA synaptic\ninput to the dendritic tree, the soma depolarizes by about 3 mV. Injecting a hyperpolarizing\ncurrent to bring the membrane potential back to its resting potential now yields a reduced\nslope conductance of 50.3 nS. Quadrupling the amount of NMDA input drops the slope\nconductance to 20 nS.\nThis result appears paradoxical: adding membrane conductance, that is, opening ionic\nchannels, causes a decrease in the local input conductance. It can be explained in a qualitative\nmanner by appealing to Eq. 17.3. (We here gloss over the fact that the NMDA synapses in\nFig. 17.3 are distributed throughout the cell, while Eq. 17.3 applies to a single compartment.)"}
{"text": "===== Page 5 =====\n17.2 Time Constants for Active Systems\n385\nFig. 17.3 NEGATIVE SLOPE CONDUCTANCE Activating NMDA synapses can lead to a paradoxical\ndecrease in the slope conductance d¡oo(Vm)/dVm. At rest, the slope conductance is 57 nS (as\ndetermined by the amplitude of the current step divided by the amplitude of the hyperpolarizing\npotential after it has converged at around 500 msec). At t = 700 msec, synaptic input of the NMDA\ntype that is distributed throughout the dendritic tree is activated, depolarizing the soma by 3 mV\n(dashed line). Because this shift in the potential activates or inactivates some of the voltage-dependent\nsomatic conductances, confounding the measurement of d!co(Vm)/dVm, the potential is brought back\nwith the help of a hyperpolarizing clamp current (solid line). Repeating the current pulse experiment\nreveals a slope conductance of 50.3 nS, even though the total membrane conductance has increased\nwith activation of the NMDA synapses.\nThe first term corresponds to the total amount of NMDA conductance added. This positive\nterm is overwhelmed by the [dg¡(Vm)/dVm](Vm \n— E¡) term. Due to the negative slope\nregion of the NMDA receptor (see Fig. 4.8A) and the reversal potential E¡ = 0, this term\nis negative and dominates the total conductance in this voltage regime. We shall promptly\ndiscuss the relevance of the slope conductance to the spiking threshold.\nIn our discussion of input conductance, we neglected any temporal aspects of the I—\nV relationship. Both chord as well as slope conductances can also be defined for the\ninstantaneous I—V relationship or for any other point in time. (For more details see the\ndiscussion in Jack, Noble, and Tsien, 1975.)\n17.2 Time Constants for Active Systems\nThe membrane time constant tm is a widely used measure of the dynamics of neurons. For\na single RC compartment or an infinite, homogeneous cable, rm is simply RC. But—as\ndiscussed at length in Sec. 3.6.1—complexities arise when one considers extended passive\nstructures. In general, for a passive compartmental model of a cell, a finite number of distinct\nequalizing time constants τ; can be defined (Eq. 3.49).\nThese can be evaluated directly, without solving the cable equation, by efficient matrix\ninversion techniques (see Appendix C; Perkel, Mulloney, and Budelli, 1981). Usually, it is\ndifficult to infer more than the first two or three (at best) of these time constants from the\nvoltage record (e.g., Fig. 3.12). The amplitude and distribution of these time constants are\nrelated in interesting ways to the neuronal geometry and its possible symmetries. (For a"}
{"text": "===== Page 6 =====\n386\nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\ndetailed discussion see Rail and Segev, 1985; Rail et al., 1992; Holmes, Segev, and Rail,\n1992; Major, Evans, and Jack, 1993a,b; Major, 1993.)\nA more problematic issue is the fact that real neurons show a passive response only under\nvery limited conditions. Usually, one or more voltage-dependent membrane conductances\nare activated around Vrest· For instance, a mixed Na+/K+ current /AR becomes activated\nupon hyperpolarization, counter to the action of most other conductances which activate\nupon depolarization. The action of this anomalous or inward rectifier (Chap. 9) is to\nreduce jR¡n and Tm as the membrane is hyperpolarized (e.g., Spruston and Johnston, 1992).\nFigure 17.4 illustrates this in an exemplary way in an intracellular recording from a\nsuperficial pyramidal cell (Major, 1992). Hyperpolarizing the membrane potential from\n—62 to —112 mV reduces the slowest decay time constants by a factor of 3 (from 21.5 to\n6.9 msec) and the input resistance by a factor of 2 (from 88.7 to 37.1 ΜΩ).\nOne possibility for obtaining the \"true\" value of the voltage-independent components\nwould be to block all contaminating ionic currents by the judicious use of pharmacological\nchannel blockers. Yet this hardly corresponds to the natural situation of a neuron receiving\nsynaptic input.\nThe problem is, of course, that one cannot associate a single time constant with an\nactive voltage-dependent membrane. Most researchers neglect this observation and fit an\nexponential to portions of the voltage tail of the cell's response to small current pulses\n(Fig. 17.4; lansek and Redman, 1973; Durand et al., 1983; Rail et al., 1992). This yields\nFig. 17.4 VARIABLE MEMBRANE TIME CONSTANT Experimental record from a layer 2 pyramidal\ncell in the visual cortex of a rat brain slice taken by a sharp intracellular electrode. Shown is the\nmembrane potential in response to a very short current pulse on a semilogarithmic scale at four\ndifferent holding potentials. The solid lines correspond to the voltages reconstructed by \"peeling\" the\nexperimental waveforms (Sec. 3.6.1). As the membrane is depolarized from —62 to —112 mV, the\nslowest decay—corresponding to rm—decreases from 21.5 to 6.9 msec, a reduction by a factor of 3.\nThis fact makes the exact determination of a passive time constant difficult. Reprinted by permission\nfrom Major (1992).\nt (msec)"}
{"text": "===== Page 7 =====\n17.3 Action Potential Generation and the Question of Threshold \n· 387\nestimates of rm of between 14 and 16 msec for the pyramidal cell model at rest, matching the\nobserved time constant of 23 msec for this cell recorded in the anesthetized cat reasonably\nwell. In the following chapter we will study how up or down regulating the synaptic input\ncan change τη by an order of magnitude or more.\n17.3 Action Potential Generation and the Question of Threshold\nThroughout the previous chapters, we always assumed that the criterion for spike initiation\nis that the membrane potential exceeds a particular threshold value. To what extent is this\nsimple assumption warranted? After all, it could well be possible that the threshold needs\nto be breached within a certain time window or that even further conditions, such as a\nminimal current, need to be fulfilled. We will now investigate this question in more detail\nfor a membrane patch model. The more involved question of threshold generation in active\ncable structures is picked up in Chap. 19.\nBut first a preamble dealing with the instantaneous current-voltage relationship and\nrelated matters. Much of this section is based on material from Noble and Stein (1966) and\nKoch, Bernander, and Douglas (1995).\n17.3.1 Current-Voltage Relationship\nIn order to understand under what conditions the cell can be thought of as having a voltage\nor a current threshold, we need to reconsider the steady-state current-voltage relationship\nI^\ntic(Vm) (Fig. 17.1) as well as the momentary or instantaneous I—V curve, denoted\nby /o(Vm) (Jack, Noble, and Tsien, 1975; Fig. 17.5). Numerically, this is obtained by\ninstantaneously moving the somatic membrane potential from its resting value to a new value\nand assuming that all active conductances remain unchanged with the notable exception\nof the fast sodium activation particle m. Given the very fast time constant of sodium\nactivation, m is practically always at equilibrium with respect to the voltage, while all\nother conductances have not had time to change from their values at Vrest.\nIn the subthreshold domain, both the steady-state and the instantaneous curves have very\nsimilar shapes. For the pyramidal cell model of Fig. 3.7, both currents possess the same\nintersection with the zero current axis at VKst, are outward (positive) for more depolarized\npotentials, and become negative around —48 mV. However, the steady-state current quickly\nbecomes positive again, attaining very large values. This large outward current is almost\nexclusively carried by the noninactivating delayed rectifying potassium current, /DR, which\novershadows the inactivating /Na- Conversely, the instantaneous current remains negative\nuntil it reverses close to the sodium reversal potential of 50 mV. This current is dominated\nby /Na, though with increasing depolarization the driving potential of the potassium currents\nand the anomalous rectifier current increase while the driving potential of Ι^Ά decreases.\n17.3.2 Stability of the Membrane Voltage\nIf the transient activation and inactivation of currents throughout the dendritic tree are\nneglected, the firing behavior of the cell can be defined by the nonlinear and stationary I—V\ncurve, / = /^,\natlc(Fm) (Fig. 17.5) in parallel with a membrane capacity. If a current /ciamp\nis injected into this circuit,\n(17.4)"}
{"text": "===== Page 8 =====\n388 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nA)\nFig. 17.5 STEADY-STATE AND INSTANTANEOUS CURRENT-VOLTAGE CURVES \n(A) Somatic mem-\nbrane potential Vm was voltage clamped and the steady-state current /^*\nnc(Vm) (same as in Fig. 17.1)\nat the soma of the pyramidal cell was recorded. The instantaneous I—V curve /o(V m) is obtained by\ninstantaneously displacing the membrane potential from VKSt to Vm and measuring the initial current.\nAll somatic membrane conductances retain the values they had at Vrest, with the sole exception of the\nfast sodium activation process—due to its very fast time constant (50 μ&κ) we assume that it reaches\nits steady-state value at Vm. Note the very large amplitudes of /o (due to /Na activation) and /^\natic\n(due to /DR activation), /o crosses over close to the reversal potential for /Na. (B) Detail of A in the\nvicinity of the resting potential and spike threshold. Both curves reverse at VKst. The slope of /^f\"\n0\ncorresponds to the input (slope) conductance rest. Both curves cross zero at around —48 mV. The\namplitude of /oo(Vm) at the local peak around —54 mV represents the current threshold !& for spike\ninitiation, while the location of the middle zero crossing of IQ corresponds to the voltage threshold\nVth for spike initiation (indicated by the thin stippled area). Reprinted by permission from Koch,\nBernander, and Douglas (1995).\ndetermines the dynamics of the somatic membrane potential. Different from the phase\nspace models treated in Chap. 7, this system has a single independent variable Vm and the\nassociated phase space consists of a line. Its equilibrium points are defined by /ciamp =\nI^\ntlc(Vm). In the absence of an externally injected current, these are given by the zero\ncrossings of Fig. 17.1. However, not all of these points are stable. In a manner analogous\nto Sec. 7.1.2, we analyze their stability by considering small variations of the membrane\npotential 8V around these singular points Vm. This leads to\n(17.5)"}
{"text": "===== Page 9 =====\n17.3 Action Potential Generation and the Question of Threshold \n· 389\nIn other words, in regions where the slope conductance dI^ÍK/dV is positive, the dynamics\nfollow an equation of the type 8V — — aáV (with a > 0) and the system will settle\ndown to the equilibrium point (that is, 8V —> 0). Under steady-state conditions, /^,atlc\nshows three zero crossings, one at Vrestj one around —48.5 mV, and one at —43 mV.\nThe slope at the leftmost zero crossing is positive, implying that Vm at Vrest is stable to\nsmall perturbations. Displacing the system by δ V > 0 away from Vrest causes a positive,\noutward ionic current to flow, a current that will drive the membrane potential back to Vrest.\nConversely, a hyperpolarizing deflection will lead to an inward current that will tend to\nrepolarize the soma.\nBy the same logic, the second equilibrium point at —48.5 mV is not stable. If the\nmembrane is displaced by &Vm < 0 away from this point, a positive, outward current\nwill flow, causing the membrane potential to decrease and to move further from this zero\ncrossing until the system comes to rest at Vrest. If δ Vm > 0, an inward current will flow,\nwhich further depolarizes the system. Since the smallest voltage perturbation will carry Vm\naway from this zero crossing, this point is unstable.\nThe rightmost zero crossing at —43 mV is never attained under physiological conditions,\nsince for these values of Vm, /^\natic lies in a realm of phase space not accessible to the system\nunder normal conditions.\n17.3.3 Voltage Threshold\nTo understand the origin of the voltage threshold, we make use of very simple considerations\ndiscussed more fully in Noble and Stein (1966) and Jack, Noble, and Tsien (1975). We\nassume that the somatic membrane receives a rapid and powerful current input caused, for\ninstance, by numerous highly synchronized excitatory synaptic inputs (as in Fig. 17.6A)\nand we neglect the extended cable structure of the neuron.\nThis has the effect of charging up the membrane capacitance very rapidly, thereby\ndisplacing the voltage to a new value without, at first, affecting any other parameters of the\nsystem. In the limit of an infinitely fast input, the system moves along the instantaneous\nI—V curve /Q. If the input is small enough and displaces the potential to between Vrest and\n—48 mV, /o will be outward, driving the membrane potential toward less depolarized values\nand ultimately back to Vrest- This voltage trajectory corresponds to a subthreshold EPSP.\nIf, however, the input pulse takes Vm instantaneously beyond the zero crossing at\n—48 mV, /o becomes negative, driving the membrane to more depolarized values. In\na positive feedback loop, this will further increase the inward current. The net effect\nis that the membrane depolarizes very rapidly: the membrane generates an action po-\ntential (Fig. 17.6B). Because during this time the other membrane currents will have\nstarted to change, /o can no longer be used to determine the subsequent fate of the\npotential. Nonetheless, the zero crossing of the instantaneous I—V curve determines the\nvoltage value at which the membrane is able to generate sufficient current to drive the\naction potential without further applied current. We therefore identify this zero crossing\nof the instantaneous I—V curve with the voltage threshold for spike initiation Vth (about\n-48 mV).\nOur argument predicts that any somatic current—whether delivered via an electrode\nor via synaptic input—that is sufficiently fast and powerful to depolarize the somatic\nmembrane beyond V^ will initiate an action potential. We confirmed this by plotting in\nFig. 17.6A the somatic potential Vm(t) in response to rapid, synchronized synaptic input\nto the dendritic tree that is sufficient to fire the cell at a brisk rate of 21 Hz. Figure 17.6B\nshows a single spike in response to a brief but strong current pulse injected directly into"}
{"text": "===== Page 10 =====\n390 · INPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nFig. 17.6 DIFFERENT PATHS TO SPIKE INITIATION Generation of a somatic spike in the pyramidal\ncell model can occur in different ways, depending on the dynamics of the input. (A) Somatic voltage\ntrace in response to random synaptic input to the dendritic tree. 100 fast (ip^ = 0.5 msec) AMPA\nsynapses fired synchronously according to a Poisson process at a rate of 75 Hz. The current /soma\ndelivered by this synchronized synaptic input to the cell body is plotted at the bottom. This current\nwas measured by clamping the soma to VKSt and measuring the clamp current. Even though /&\nis frequently transiently exceeded by a large amount, the somatic membrane potential is usually\nnot sufficiently depolarized to exceed Vy, and initiate a spike. The variability in the amplitude\nof the EPSCs derives from the fact that following each synchronized synaptic event, a new set\nof 100 excitatory synapses is randomly chosen. A single action potential in response to (B) a\n0.2-msec 10-nA current pulse (indicated in the lower trace) and (C) a sustained current step. For\n^ciamp = 0.29 nA the potential levels out at —56.5 mV, while for 0.31 nA a spike occurs with a delay\nof 1 sec. The first two plots demonstrate conditions under which action potentials are generated if a\nvoltage threshold Vth is exceeded, while the last plot corresponds to a paradigm where spike initiation\noccurs if a current threshold 7t|, is exceeded. Reprinted by permission from Koch, Bernander, and\nDouglas (1995)."}
{"text": "===== Page 11 =====\n17.3 Action Potential Generation and the Question of Threshold\n391\nthe soma. Careful inspection of both voltage trajectories indicates that each and every time\nVm (t) reaches Vth = ~~48 mV, an action potential is initiated (see dotted lines and the arrow\nin Fig. 17.6A, B).\nFigure 17.6A reveals that many somatic EPSPs fail to trigger spikes. This fact can\nbe exploited to operationalize the voltage threshold by histogramming the peak values of\nsubthreshold somatic EPSPs. If spike initiation truly occurs at Vth, such a histogram should\nshow peak EPSP amplitudes up to Vth, followed by an abrupt absence of any local maxima\nin Vm until, at high values, the peak voltages of the action potential itself appear. This is\nexactly the distribution seen in Fig. 17.7, with a complete lacuna of any local peaks in the\nsomatic membrane potential between —48.5 and +20 mV.\nThis histogram was obtained using rapid synaptic input. Slow1 synaptic input will not\nlead to such a crisp threshold. Experimentally, using rapid and powerful synaptic input that\nhovers around threshold should optimize the conditions for detecting a voltage threshold\nusing this technique.\n17.3.4 Current Threshold\nFigure 17.6C illustrates a different path along which spikes can be initiated. If a small,\nsustained current /damp of 0.29 nA current is injected into the soma, the somatic membrane\npotential responds with a slight overshoot, due to the activation and subsequent inactivation\nof the transient A current, before settling to a steady-state somatic potential (at —56.5 mV).\nIncreasing /damp by 20 pA is sufficient to initiate an action potential 1 sec after the onset of\nthe current input. The extended delay between input and spike is primarily caused by /A,\nas shown by Connor and Stevens (1971a,b,c). As a consequence, the cell can spike at very\nlow frequencies (Fig. 9.7).\nFig. 17.7 OPERATIONALIZED DEFINITION OF A VOLTAGE THRESHOLD \nThe pyramidal cell model\nwas bombarded for 20 sec with fast excitatory synaptic input (of the type shown in Fig. 17.6A, with\nthe cell firing at 21 Hz) and the amplitude of the local maxima of the somatic membrane potential\nhistogrammed. The three peaks correspond to the noisy synaptic background around Vrest = —65 mV\n(leftmost peak), synchronized EPSPs that failed to elicit action potentials (central peak), and action\npotentials (rightmost peak). The maximum value of the central hump is —48.6 mV, providing a\nconvenient way in which the voltage threshold Vth can be measured. Reprinted by permission from\nKoch, Bernander, and Douglas (1995).\n1. Fast and slow is always relative to the dynamics of the large sodium current determining spike initiation."}
{"text": "===== Page 12 =====\n392 \n. \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nFrom an operational point of view, if the current delivered to the somatic membrane varies\nvery slowly, the membrane can be thought of as having a current threshold /a, = 0.295 nA.\nTo explain this, we turn to the steady-state I—V plot in Fig. 17.5. Injecting a sustained and\ndepolarizing current step of amplitude /ciamp into the soma corresponds to shifting the I^tic\ncurve downward by an equal amount. If /damp is small, the principal effect of this is to\nmove the location of the zero crossings of I^tK. In particular, the leftmost zero crossing—\nand therefore the new somatic resting potential— shifts to the right and the middle zero\ncrossing shifts to the left. At a critical value of /damp* labeled /m, the two zero crossings\nmerge. At this point the system still has an equilibrium point. If the current is increased\nany further, this portion of the /¿£atlc curve drops below zero and loses its equilibrium. The\ninward current generated by the system forces the membrane to depolarize, driving Vm\ntoward E^a and initiating an action potential. In fact, for sustained current injections the\nmembrane has no stable equilibrium point, displaying instead a stable limit cycle: as long as\nthe suprathreshold current persists, the membrane generates a series of spikes. From these\nconsiderations we predict that the current threshold corresponds to the local maximum of\nthe /oo curve, here equal to 0.295 nA, very close to the value obtained by integrating the\nappropriate equations (Fig. 17.6C).\nNote that the steady-state somatic membrane potential at which threshold current is\nreached (about —54.2 mV) is different from Vth because of the nonlinear and nonstationary\nsomatic membrane. In a linear and stationary system, such as in integrate-and-fire models\n(Eq. 14.9) a linear relationship holds between the two,\n(17.6)\n17.3.5 Charge Threshold\nHodgkin and Rushton (1946; see also Noble and Stein, 1966) proposed that the threshold\ncondition for the excitation of a cable is that a constant amount of charge gth be applied.\nIs this true for our cell?\nWhen a significant amount of current—either from synaptic input or from a micro-\nelectrode—is very rapidly delivered to the cell body, the voltage change will primarily\nbe determined by the capacitive current, since the membrane currents take time to change.\nUnder these conditions, the cell should also show a charge threshold. In order to understand\nthis, let us follow a train of thought laid out by Noble and Stein (1966). Because we\nare considering rapid events, the steady-state I-V in Eq. 17.4 must be replaced by the\ninstantaneous I-V curve:\n(17.7)\n(as before, we here neglect the dendritic tree). By rearranging and integrating with respect\nto voltage, we obtain an expression for how long the current step has to be applied (from\nVrest = 0) until the voltage threshold has been reached,\n(17.8)\nThe integration can be carried out for any I-V relationship that does not depend on time,\nsuch as /o or /<*, (Fig. 17.5). If for short times, the injected current /damp is much bigger\nthan the subthreshold ionic currents /o (on the order of 1 nA or larger; see Figs. 17.5A and\n17.6), /o can be neglected. The total charge delivered by a step current of amplitude /damp\nand duration T^ is"}
{"text": "===== Page 13 =====\n17.3 Action Potential Generation and the Question of Threshold\n393\n(17-9)\nPut differently, for short, intense stimuli, a constant charge 2th must be supplied to the\ncapacitance in order to reach threshold (Noble and Stein, 1966).\nWe numerically evaluate this for a reduced one-compartment model of the pyramidal\ncell (Koch, Bernander, and Douglas, 1995) as well as for the full model by injecting 7clamp\ncurrent into the soma and recording the duration of the pulse rpuise necessary to generate an\naction potential. This threshold charge is plotted in Fig. 17.8 as a function of the amplitude\nof the injected current step. For the one-compartment model Q starts out high but decreases\nrapidly and saturates at Q^ = 7.65 pC.\nIf a 7.65 pC charge is placed onto a 0.508 nF capacitance (its value in the one-\ncompartment model), the potential changes by 15.1 mV, very close to W We conclude\nthat in a single-compartment active model, a charge and a voltage threshold both imply the\nsame thing: for rapid input the cell spikes whenever the input current dumps gth charge\nonto the capacitance, bringing the membrane potential to Vti,. The existence of a charge\nthreshold immediately implies an inverse relationship between the amplitude of the applied\ncurrent and TU,. It is known that both the Hodgkin-Huxley equations for a clamped axon and\nthe actual space-clamped giant squid axon membrane give rise to such a strength-duration\ncurve (Hagiwara and Oomura, 1958; Cooley and Dodge, 1966; Noble and Stein, 1966).\nThis simple story becomes more complicated when we repeat the same experiment in\nthe 400 compartment model (Fig. 17.8). The amount of charge delivered by the electrode\nthat is necessary for the cell to spike decreases monotonically with increasing current. It\nonly flattens out for unphysiologically large amplitudes. Under these conditions, no single\nFig. 17.8 Is THERE A CHARGE THRESHOLD FOR SPIKING? Relationship between the charge gth\ninjected by a current pulse of constant amplitude /c|amp and duration rpuise necessary for spiking to\noccur. For a single-compartment model of the pyramidal cell (where the effect of the entire passive\ndendritic tree is mimicked by adjusting the capacitance and the leak resistance at the soma), gth rapidly\nreaches an asymptote (solid line). That is, for moderately strong current inputs a spike is initiated\nwhenever the limiting charge gth is delivered to the capacitance; this charge obeys gth = CVth- In\nother words, for a single compartment the existence of the voltage threshold for spiking is equivalent\nto a charge threshold. For weaker inputs more charge is required. No such constant threshold condition\nis observed for the full pyramidal cell model (dashed line). Due to the presence of the distributed\ncapacitances in the dendritic tree, action potential initiation does not obey a charge threshold condition."}
{"text": "===== Page 14 =====\n394 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nequation governing the evolution of the somatic membrane potential can be written because\nof the presence of the many equalization time constants between the soma and the dendritic\ncompartments. Only if the current injection is very large, with the associated very short\ndurations <0.05 msec, can one neglect the dendritic capacitance and observe a fixed Qt\\,.\nWe conclude that in any cell with a substantial dendritic tree, the occurrence of an action\npotential is not caused by the charge at the spike initiating zone exceeding some fixed\nthreshold value.\n17.3.6 Voltage versus Current Threshold\nAn advocatus diaboli might argue that the cell always possesses a voltage threshold, with\nthe value of this threshold increasing from —48 mV for very fast inputs to —54.2 mV for\nslow ones. Yet in our eyes a single, but variable threshold is conceptually less elegant than\ntwo fixed but different thresholds (V¡h and7Λ). Furthermore, given the very broad, local\npeak of /^,\natic in Fig. 17.5B, Vih is ill defined, in particular in the face of ever existing noise.\nAs we will see in the following chapter, the number of simultaneously activated somatic\nEPSPs needed to trigger a spike is within 10% of the number predicted based on Vth· As\nlong as the synaptic current delivered to the soma 7soma has a fast rise time compared to\nthe activation and inactivation time constants of all the somatic currents with the exception\nof sodium activation (that is, on the order of 1 msec) and is powerful enough to displace\nVm past Vth, the cell will spike. To what extent synaptic input approximates such behavior\nor can be better described by a slowly varying current input under in vivo physiological\nconditions depends on many factors beyond the scope of this chapter.\nWe are not arguing that every neuron must have I-V curves that are qualitatively similar\nto the ones in our model of a pyramidal cell. For instance, the steady-state I—V curve for\nthe Hodgkin-Huxley membrane patch model is a monotonically increasing function of Vm\nwith no local maxima (Fig. 6.9A). In this case I^\ntlc(Vm~) cannot be used to predict /a,. The\ninstantaneous I-V curve /o of a patch of squid axon around rest (Fig. 6.6) does, on the\nother hand, look qualitatively similar to the /o(Vm) curve of the pyramidal cell and so can\nbe used to estimate in a qualitative manner Vth (see Sec. 6.3.1).\n17.4 Action Potential\nWe would be remiss if, after this extensive discussion of the conditions under which\nspikes can be initiated, we would not describe the action potential itself. Its form is highly\nstereotypical, with little variation from one spike to the next (Fig. 17.6). The spike amplitude\nfrom threshold at —49 mV to the peak at 30 mV is about 80 mV, and the full width at half\nheight is 0.9 msec. These numbers compare favorably with experimentally recorded spikes\nfrom slice and in vivo (Bindman and Prince, 1983; Spain Schwindt, and drill, 1990, 1991;\nHirsch and Gilbert, 1991; Pockberger, 1991). The rapid upstroke during the initial part of\nthe action potential, on the order of several hundred millivolts per millisecond, is caused by\nthe sodium-mediated inward current charging up the membrane capacitance. The smaller\nthe capacitance (as, for instance, in small cells) or the larger the density of fast, sodium\nchannels, the faster the upstroke and, ultimately, the steeper the slope of the discharge\ncurve. Ultimately, it is the amplitude of the upstroke that limits the speed with which action\npotentials can be generated.\nTwo ionic currents are the primary culprits in generating and shaping the action potential.\nThe largest is the current supporting the action potential itself 7Na peaking at 70 nA. Due to"}
{"text": "===== Page 15 =====\n17.5 Repetitive Spiking\n395\nadaptation, peak /Na for consecutive spikes is reduced to 50 nA. The second largest current\nis the delayed-rectifier potassium current, /DR coming in at 23 nA. The gargantuan size of\nthese two ionic currents, compared to anything the synapses in the dendritic tree can deliver\nto the soma, explains the almost complete independence of the shape of the action potential\nto the way in which the spike was triggered. The total calcium current flowing is miniscule\ncompared to /Na- Its primary function is not to drive the action potential itself but to activate\ncalcium-dependent potassium currents controlling adaptation.\n17.5 Repetitive Spiking\nUntil now, we have only considered the events leading to a single action potential. Yet,\ncells usually spike repeatedly. The response of the full pyramidal cell model to a sustained\ncurrent injection is shown in Fig. 17.9. Similar to the modified Morris-Lecar equations\n(Eqs. 7.16 and 7.17; see also Figs. 7.10 and 7.11) and different from the squid giant axon,\nthe pyramidal cell model can fire at very low frequencies: injecting 0.31 nA of sustained\ncurrent causes the cell to spike about once every second. The current responsible for these\nvery long interspike intervals is the inactivating potassium current /A (Connor and Stevens,\n1971a,b,c; Getting, 1989; for a detailed discussion see Fig. 9.7 and related text). Due to its\nhyperpolarizing reversal potential, this potassium current reduces the evoked depolarization\n(see the bump in Fig. 17.2). Because 7A inactivates with increasing depolarization, different\nfrom the large, delayed rectifier potassium current, it gradually decreases in size, enabling\nVm to slowly creep upward until threshold is reached.\n17.5.1 Discharge Curve\nIf the input is powerful enough to trigger two or more spikes, the firing frequency adapts\nwithin 50 msec to between one-half and one-third of its initial value. Adaptation can be\nJ<ig. 17.9 REPETITIVE 1 IRING Generation of several action potentials in the pyramidal cell model\nin response to sustained current injections at the cell body. The minimum current step that evokes a\nspike, rheobase, is 7th = 0.295 nA. Injecting 0.4 nA evokes repetitive spikes with very long interspike\nintervals, caused by the presence of the transient 7A current. At higher spiking frequencies, the firing\nadapts due to the calcium-dependent potassium currents. The voltage curves are shifted for viewing\npurposes. No noise has been simulated, approximating the situation one would encounter in cultured\ncells. Reprinted in modified form by permission from Bernander (1993)."}
{"text": "===== Page 16 =====\n396 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nvisualized as reducing the gain of the /-/ curve (Fig. 17.10A; see also Fig. 14.2C). As\ndiscussed in Sec. 9.2.2, it is caused by one or more calcium-dependent potassium currents,\nacting as a sustained negative input that offsets the positive current delivered via the\nelectrode. At a very low spiking frequency, no adaptation occurs since [Ca\n2+],· decays\nto its resting levels prior to the next occurrence of a spike.\nThe curve for the first ISI shows a biphasic shape. Initially, the slope is low from /a, to\naround 0.6 nA (primary slope). For larger input currents the slope is much larger (secondary\nslope). This portion of the /-/ curve can be fitted rather well by a logarithmic relationship\nbetween / and / (Figs. 6.10 and 17.10A), as first observed for the uniformly polarized\nHodgkin-Huxley equations (Agin, 1964). The firing frequency associated with the second\nISI displays a very similar pattern, except that the primary slope spans a larger current range.\nThough the shape of the adapted /-/ curve in Fig. 17.10A appears deceptively linear,\nthe slope varies by more than a factor of 2. Figure 17.10A shows the linear fit:\n/ = 49.5 ·(/-0.22) \n(17.10)\nwith the constant of proportionality corresponding to about 50 spikes per second per\nnanoampere of injected current, in good agreement with experimental in vitro (Mason\nand Larkman, 1990) and in vivo (Jagadeesh, Gray, and Ferster, 1992) observations in cat\ncortex. The figure also shows a logarithmic fit,\n/ = 75-(In 7 + 0.59). \n(17.11)\nThis fit is somewhat more accurate, except at very low input currents. While the difference\nbetween these two fits may seem negligible, the logarithmic fit will be more useful in the\nfollowing.\nNote that the discharge curve does not show any strong evidence of saturation in the\nfiring frequency, a saturation that is a key property of almost all nonlinear neural network\nmodels. It is in any case unclear from where, under physiological conditions, synaptic\ncurrents greater than 2 nA would originate.\n17.5.2 Membrane Potential during Spiking Activity\nHow can the somatic membrane potential be characterized while the system is spiking\nrepeatedly, that is, moving along its stable attractor? A very useful heuristic measure is the\ntime-averaged somatic membrane potential\n(17.12)\nwhere Τ includes several interspike intervals in response to a sustained current 7. In order to\navoid the confounding effects of adaptation, (V m) should be evaluated only after adaptation\nis complete. Plotting (V m) as a function of 7 (solid curve in Fig. 17.10B) results in a curve\nwith a linear relationship close to the resting potential, whose inverse slope is identical to\nthe cell's input conductance.\nConceptually—as well as by visual inspection—this curve naturally falls into two\nsections. Below the point at which the cell ceases to spike and the curve shows a cusp, that is,\nfor 7 < 7th, it becomes identical to the inverse of the steady-state 7-V curve (Fig. 17.5B).\nThe portion of the (Vm) versus 7 curve that lies beyond 7th shows two remarkable features.\nFirstly, the dynamic range of (Vm) over the physiological range of input currents is small."}
{"text": "===== Page 17 =====\n17.5 Repetitive Spiking · 397\nFig. 17.10 DISCHARGE CURVE AND AVERAGED MEMBRANE POTENTIAL \nCharacterization of\nthe firing behavior of the pyramidal cell model. (A) Discharge curve computed as the inverse of\nthe first, second, and fully adapted interspike intervals (solid) as a function of the amplitude of the\ncurrent step (see also Fig. 14.2C). Linear (dotted) and logarithmic (dashed) fits are shown for the\nsteady-state curve (Eqs. 17.10 and 17.11). (B) Introducing the averaged somatic membrane potential\n(Vm) = (l/T)f0 \nVm (i)dt enables the definition of a dynamic l-V relationship between a sustained\ninput current and (V m) (solid line). For / < 7th, this curve coincides with the conventionally defined\ninverse of the /^j\" lc( Vm) curve (Fig. 17.5). The dashed curve is a logarithmic fit, indicating a diodelike\nrelationship between the average membrane voltage and the current. (C) Relationship between (Vm)\nand the output spike frequency. Remarkably, the total dynamic range of (Vm) is only about 12 mV for\nphysiological firing frequencies. The dashed line indicates the best linear fit (Eq. J7.15). Reprinted\nby permission from Koch, Bernander, and Douglas (1995)."}
{"text": "===== Page 18 =====\n398 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\nAs the adapted output spike frequency / ranges from 5 to 150 Hz, (Vm) is confined to a\nfairly limited range of about 12 mV. Secondly, the upper portion of the curve can be fitted\nvery well by a logarithmic relationship, except for the \"hook\" around 7th (dashed curve in\nFig. 17.10B).\nThis relationship can also be viewed with {Vm) as the independent variable. From this\nperspective, the current / can be thought of as a cumulative spike current, and the I-\n(Vm) curve as a dynamic I-V relationship /cJnanuc({Vm}) (dynamic because the somatic\nmembrane potential travels on a stable limit cycle in response to the sustained input) that\nreverses at Vrest· /c»namic({ Vm)) is quite distinct from /^,atlc(Vm). In the latter, the membrane\npotential is clamped to Vm and the current flowing at this voltage is evaluated, while in the\ndynamic case, a sustained current is injected while the membrane is left free to spike. The\ndynamic I-V curve is relatively straightforward to measure experimentally.\nThe logarithmic relationship implies a diodelike behavior, with the current depending\nexponentially on the average membrane potential,\n(17.13)\n(where Vm is expressed in units of millivolts; dashed line in Fig. 17.10B). In analogy\nto the steady-state slope conductance G?^'10 (Eq. 17.3), we can define the dynamic input\nconductance as\n(17.14)\n(Fig. 17.1 IB). The cumulative effect of all currents active during spiking resembles a\nvoltage-gated hyperpolarizing conductance that shunts large input currents, thereby stabiliz-\ning the membrane potential. In the absence of this \"spike\" current the membrane would de-\npolarize to much higher values. For instance, with an input conductance of G^K = 60.6 nS\nat Vrest, a 2 nA injected current step should depolarize the membrane to —34 mV in the\nabsence of any active currents, while the actual potential (Vm) is approximately —48 mV.\nWe conclude that the dynamic conductance (7.^naimc acts similarly to an imperfect voltage\nclamp, providing a stable sink for currents originating in the dendrites, a rather unusual way\nto look at the spike generation mechanism (Koch, Bernander, and Douglas, 1995).\nFinally, we can combine the logarithmic relationship between injected current and firing\nfrequency (Eq. 17.11) with the exponential dependence of /spike on (Vm) (Eq. 17.13) to\narrive at a linear relationship between the averaged membrane potential and the firing\nfrequency,\n(17.15)\nThis relationship is useful for analog electronic circuit implementation of neurons (Ma-\nhowald and Douglas, 1991; Douglas, Mahowald, and Mead, 1995), since it allows one to\ninterpret the membrane potential across some transistor as an effective firing frequency\nwithout adding additional circuitry to implement some output nonlinearity. As can be seen\nin Fig. 17.IOC, the true spike frequency curve (as ascertained from the simulations) lies\nabove Eq. 17.15 for small firing frequencies. Before concluding, several remarks pertinent\nto the definitions of these dynamics variables are in order.\n1. The low-pass nature of the dendritic tree, caused by the distributed capacitance, will\nfilter out the high-frequency components associated with the rapid up and down swing"}
{"text": "===== Page 19 =====\n17.5 Repetitive Spiking · 399\nFig. 17.11 DYNAMIC DISCHARGE CURVE \nTime-averaged membrane potential (Vm> permits the\nintroduction of a dynamic I-V curve as the relationship between the average somatic membrane\npotential in the full pyramidal cell model and the total current flowing across the somatic membrane.\n(A) The static I-V curve is computed under voltage clamp by measuring the clamp current and\nits dynamic counterpart under current clamp (it is the inverse of the curve in Fig. 17.10B). In the\nsubthreshold regime I < 7,h the two curves coincide (labeled /oo). (B) The sustained and dynamic\nsomatic input conductances as a function of Vm or (V m) defined as the slopes of the I-V curves in A.\nFor V < Va,, Gfn\nynamic = G?¿atic. The dynamic input conductance allows the introduction of a time\nconstant that characterizes how rapidly the adapted firing rate responds to new input. Reprinted by\npermission from Koch, Bemander, and Douglas (1995).\nof the action potentials as they propagate from the soma into the apical tree. Thus, {Vm)\nis qualitatively closer to the membrane potential seen by dendritic sites than Vm (t). Also,\nslowly activating or inactivating conductances do not care about rapid fluctuations in\nVm, but are responsive to a low-pass filtered or average potential.\n2. /cJnami°((Vm}), in combination with the cell's discharge curve, leads to a linear rela-\ntionship between the average membrane potential (Vm) and the firing rate in the fully\nadapted case (Eq. 17.15)."}
{"text": "===== Page 20 =====\n400 \n· \nINPUT RESISTANCE, TIME CONSTANTS, AND SPIKE INITIATION\n3. The time-averaging technique discussed here represents one way in which the dynamics\nof a complex neuron can be reduced to the mean-field dynamics of the standard neural\nnetwork unit (Sec. 14.4). It is not advisable to use (V m) when considering fast events\nthat depend on the exact timing (e.g., fast dendritic spikes discussed in Sec. 19.3).\n4. The ratio of the membrane capacitance and (j.^\nnamic\n can be thought of as a dynamic\ntime constant tdynamic. It is a measure of how rapidly the averaged membrane potential,\nand therefore the firing rate, can vary in response to a change in /S0ma· As can be seen\nin Fig. 17.1 IB, if sufficient current is injected for the cell to fire in a sustained manner\nat 65 Hz, (Vm) — -50 mV, increasing G^\nnaimc by about a factor of 4 relative to rest.\nThis implies that the effective time constant of the cell has decreased to about 4 msec.\nDue to the presence of the many membrane conductances generating and modulating\naction potentials, the firing rate can respond much faster to a rapid increase in /S0ma than\nsuggested by the conventional definition of rm. For higher firing rates, Tdynamic can be\nclose to 2 msec, implying that neurons can respond at the millisecond or faster scale\n(Softky, 1994). Whether cells make use of this very high bandwidth is another matter.\nOur enthusiasm for such a new measure must be tempered by two considerations. First,\nthe /o^\nnanuc({Vm}) curve is defined for the adapted firing frequency, and adaptation takes\nbetween 20 and 100 msec to occur. A second difficulty is that raynamic is defined for a fictive,\ncontinuous firing rate and for a constant current input. This is a meaningless quantity on\na time scale less than the interspike interval. This sampling problem renders the exact\ndefinition of a time course problematic at very low firing frequencies.\n17.6 Recapitulation\nThis chapter treated the characterization of a nonlinear and nonstationary neuron from the\npoint of view of the soma on the basis of two measures: the relationship between the current\nflowing across the somatic membrane and the voltage and the local slope of such an I—\nV curve. The inverse of this last variable is the slope or input resistance. The traditional\n/—V curve I^\ntic(Vm) is defined under voltage clamp conditions: the voltage is fixed and\nthe current flowing at this voltage is recorded. The local peak around Vrest defines the\nminimal current 7th necessary to initiate spiking. Under the assumption that the activation\nand inactivation variables of the voltage-dependent currents change much more slowly than\nthe activation m(t) of the sodium current, it is possible to define an instantaneous current-\nvoltage curve /o(Vm). (Conceptually, this can be measured by yanking the membrane\npotential from Vrest to Vm and instantaneously recording the resulting membrane current.)\nStability considerations of the sort explored in Chap. 7 dictate that the zero crossing of /o,\nif the slope is negative, corresponds to the voltage Vth that needs to be exceeded for a spike\nto occur.\nWe conclude that fast and powerful versus sustained but peri-threshold stimuli define\ntwo different threshold conditions: in the first case Vm must exceed Vth while in the latter\ncase 7soma must exceed /th. The larger the current delivered to the soma by a synaptic input,\nthe faster Vth can be reached and the sooner the cell spikes. This process is not limited by τ\nbut by the amplitude of the upstroke of the action potential (that is, by the amout of sodium\ncurrent and the somatic capacitance). For cells with no or little distributed capacitances,\nthat is, with small dendritic trees, a voltage threshold is equivalent to a charge threshold, in"}
{"text": "===== Page 21 =====\n17.6 Recapitulation · 401\nwhich a fixed amount of charge needs to be placed onto the somatic capacitance in order to\nreach Vth-\nIntroducing {Vm) as the temporal average of the somatic membrane potential (including\nspikes) allows us to extend most of the above concepts into the spiking regime. Biophysi-\ncally, {V m) approximates the average membrane potential experienced by a distal dendritic\nsite, where the high-frequency components of the back-propagating somatic action potential\nhave been filtered out by the capacitances distributed throughout the cable.\nThe dynamic current-voltage curve /oonan\"C({Vm)) is defined as the inverse of the\nrelationship between the sustained current injected via a microelectrode and the average\nmembrane potential (Koch, Bernander, and Douglas, 1995). It can be thought of as a single,\neffective spike current that serves to stabilize the membrane potential in the neighborhood\nof Vth and that reverses at Vrest. This current can be described rather well by an exponential\nrelationship: increasing (Vm) by 3.7 mV roughly doubles /. The fact that this current\nbehaves similarly to a diode in the forward direction makes such a relationship particularly\neasy to implement using electronic circuits. A similar exponential relationship also exists\nfor a patch of squid axon described by the Hodgkin-Huxley equations (not shown).\nOne caveat. These dynamical measures are based on time-averaged quantities; they\ncannot be used under conditions where the detailed time course of spiking is thought to be\nrelevant for postsynaptic processing."}
{"text": "Actual and expected voltage attenuation between the two synaptic sites used in Fig. 18.1 and the soma as computed from the\nstationary input (Kn in ΜΩ) and transfer resistance (K¡s in ΜΩ). e\nL's is the attenuation expected from the electrotonic length\nmeasurement (Eq. 3.33); L\"is is defined via Eq. 3.38 as the logarithm of the voltage attenuation. In an infinite cable L = V. Due\nto the pronounced asymmetries in the cable structure, these two measures here differ markedly. The natural logarithm of the actual\nvoltage attenuation L\"'*, as defined by the peak EPSP values in Fig. 18.1, is larger than L\"¡s due to the distributed capacitances\nthat preferentially remove high temporal frequencies (in particular for the distal, apical input)."}
{"text": "===== Page 6 =====\n18.2 Massive Synaptic Input \n· 407\n18.2 Massive Synaptic Input\nNow that we have seen the effect of a single synapse, we are ready to deal with the much more\nphysiological situation in which a hundred or more synapses are simultaneously activated.\nHow will this affect the somatic potential and current and, ultimately, the firing frequency of\nthe cell? Because of the nonlinear relationship between synaptically induced conductance\nchanges and the membrane potential, this question is not easy to answer in general. Let\nus start by studying the situation when the neuron is bombarded with numerous individual\nexcitatory and inhibitory inputs.\n18.2.1 Relationship between Synaptic Input and Spike Output Jitter\nWhen a cortical neuron is presented with the appropriate stimulus, it rapidly and reliably\nincreases its probability of firing. Individual cortical neurons respond to a dynamic random\ndot motion stimulus with a highly reproducible temporal modulation of their firing rate,\nprecise to a few milliseconds (Fig. 15.11). The time that it takes for such a cell to significantly\nmodulate its firing rate—as determined by averaging over many presentations of an identical\nstimulus—is almost always less than 10 msec and occurs in neurons that are at least six\nsynapses removed from the periphery. This precision is surprising because the propagation\nof an input through a multilayer network with continuous mean rates causes rise times to\nbecome increasingly shallow (due to the low-pass filtering effect treated in Sec. 14.2.4).\nYet, as the sensory triggered \"wave\" of activity propagates through many layers of the\ncortex, the signal does not appear to become appreciably rounded off, but is only delayed\nbetween consecutive stages by about 10 msec. This has been assessed directly by comparing\nthe latency and rise time of neurons in different cortical areas to the same stimulus. In the case\nof an awake monkey, neurons in the primary visual cortex responded to a visual stimulus\nwith an average 10-90% rise time of 8 msec while neurons in a subsequent processing\nstage, cortical area V4, show a virtually identical rise time of 7 msec but with an additional\n26 msec latency (Marsalek, Koch, and Maunsell, 1997).\nLet us address this important problem in the following manner. Suppose an instantaneous\nsensory event in the world triggers a volley of activity in η excitatory synaptic inputs. We\nwill assume that the arrival time of the input is centered around t — 0 and that its standard\ndeviation in time, here called input jitter, is o-m. If we further suppose that these η synapses\nare sufficient to trigger one (or more) spikes (that is, η > n^) we can compute the standard\ndeviation in time, termed the output jitter σοιη, of the spike triggered in response to this\ninput. This will help us understand to what extent temporal jitter in the input is preserved,\namplified, or reduced by multiple processing stages.\nWe first deal with a simplified problem by assuming that the probability density of the\narriving synaptic inputs p-m(t) is the uniform density on the interval [0,1]. (p-m(t) = I\nfor ί € [0, 1], and 0 otherwise.) Different from before, we assume that each input only\ntriggers a single synaptic event of postsynaptic strength ae. These inputs are integrated onto\nthe capacitance of a nonleaky integrate-and-fire unit (of voltage threshold Vth = «««*)·\nTogether with the uniform density, this assures us that at t = 1 the voltage will be exactly\nnae (in the absence of a threshold).\nThe probability that the voltage at time t has attained the value η^αβ is given by the beta\ndensity (Papoulis, 1984)\n(18.1)"}
{"text": "===== Page 7 =====\n408 \n· \nSYNAPTIC INPUT TO A PASSIVE TREE\nThe standard deviation of this distribution is\nOr, the output jitter is inversely proportional to the number of excitatory synaptic inputs.\nThis makes sense, since the time it takes to reach threshold will be proportional to the drift\nthat is dictated by n. The jitter in the time to threshold is inversely related to the drift.\nOf course, our derivation only holds under conditions when the membrane leak can be\nneglected (that is, when most of the synaptic input arrives within a fraction of τ).\nDetailed simulations using a leaky integrate-and-fire model as well as our customary\nlayer 5 pyramidal cell compartmental model bear this out (Fig. 18.3A; for the details,\nconsult Marsalek, Koch, and Maunsell, 1997). In the presence of massive excitatory input\n(n = 250, with nth = 66; see previous section), whose arrival time has a Gaussian\ndistribution in time, the relationship between a-m and aout is a linear one, with a slope of\naround 0.116, as expected from the approximation in Eq. 18.4. Adding inhibitory synaptic\ninput increases this slope, but not substantially (Fig. 18.3B). Any more inhibition and the\ncell fails to reach threshold in a growing fraction of all trials.\nIn the case of massive synaptic input, the temporal output jitter will be smaller than the\ninput jitter. It follows that for a cascade of spiking cells, the output jitter converges to zero\n(Marsalek, Koch, and Maunsell, 1997; see also Abeles et al., 1994; Hermann, Hertz, and\nPriigel-Bennett, 1995). In real networks the jitter is unlikely to become vanishingly small\nsince we neglect several additional sources of timing variability. The two most dominant\nsources are likely to be:\n1. The inhomogeneous spike propagation times between consecutive layers of neurons due\nto variations in the diameter and length of the associated axons and axonal terminations\n(wiring jitter). We considered these axonal delays in Fig. 6.17 and found that they are\nsmall (on the order of ±0.5 msec).\n2. Jitter in the delay between presynaptic spikes and the opening of the postsynaptic synaptic\nchannels (synaptic jitter).\nWe conclude that the output jitter in a network with many layers of spiking neurons (such\nas in Abeles's (1990) synfire-chain model) converges to a fixed but small number, bounded\nby the \"jitter\" in the anatomical connections and in the synaptic transduction process.\n(18.2)\nIn the case of the uniform probability density, a¡n = 1/2 V3, allowing us to rescaleEq. 18.2\nin terms of the output jitter,\n(18.3)\nWhat happens if the number of synaptic inputs n greatly exceeds the number of inputs\nrequired to bring the cell to fire «th? Making the approximations that n + 3 \"» n + 2^ n,\nthat n + 1 — «th ^ n> and nth + 1 ^ n^, we arrive at\n(18.4)"}
{"text": "===== Page 8 =====\n18.2 Massive Synaptic Input \n· 409\nFig. 18.3 RELATIONSHIP BETWEEN INPUT AND OUTPUT JITTER \nWhat is the relationship between\nthe standard deviation in time of synaptic input a-m and the standard deviation in time of the resultant\noutput spike σοΜΊ Marsalek, Koch, and Maunsell (1997) treated this question using a leaky integrate-\nand-fire model and the more complex layer 5 pyramidal cell compartmental model. (A) 250 excitatory\nfast voltage-independent synaptic inputs are triggered once. The probability density function p-m(t)\nis Gaussian with standard deviation a-m. For the integrate-and-fire model, synapses are assumed to be\nlinear current sources, with ae = 0.23 mV; τ = 10 msec. For the compartmental model, synapses are\nof the fast AMPA type, distributed throughout the dendritic tree. In both cases, the effective voltage\nthreshold Vth = 16 mV and «u, = 66. Equation 18.4 predicts a linear relationship between σ,η and aout\nwith slope 0.116 (line labeled \"approximation\"). Error bars correspond to the standard deviation from\nfive runs with 50 spikes each. (B) A qualitatively similar result is obtained if 62 inhibitory synapses are\nadded (hyperpolarizing current synapses with a, = 0.23 mV for the integrator and GABAA synapses\nfor the more detailed model). Any more inhibition, and the cell frequently fails to trigger a spike.\nThus, a spiking cell with a passive dendritic tree can reduce input jitter provided that η is much bigger\nthan no,. Reprinted by permission from Marsalek, Koch, and Maunsell (1997)."}
{"text": "===== Page 9 =====\n410 \n· \nSYNAPTIC INPUT TO A PASSIVE TREE\n18.2.2 Cable Theory for Massive Synaptic Input\nStandard one-dimensional passive cable theory (Jack, Noble, and Tsien, 1975; Rail, 1989)\nassumes that the electrotonic structure of the cell does not change in response to synaptic\ninput. As long as synaptic input is treated as a de- or hyperpolarizing current this is certainly\ntrue. The independence of the electrical structure from the input allows us to characterize\nthe spatio-temporal integrative properties of the cable or the cell in terms of the transfer\nand input impedances as well as space and time constants.\nYet as we emphasized from the beginning of this book, even the activity of a single\nsynapse has an impact on the input impedance Ki{ (t) (e.g., Eq. 4.24). While the actual\nsize of this effect is negligible for one or a few isolated synaptic inputs, it can dominate\nthe behavior of the system for large inputs. This is particularly true given that patch-clamp\nrecordings of pyramidal and Purkinje cells find very high values for the passive membrane\nresistance (on the order of 100,000 Ω-cm\n2; see Appendix A).\nLet us exemplify our argument with some numbers. Assume a small, spherical cell with a\nradius equal to r = 15 μπι. Its input conductance Gjn = 4nr\n2/Rm amounts to only 0.28 nS\nand its time constant to 100 msec for Rm = 100, 000 Ω-cm\n2. Activation of a single synapse\nwith gpeak = 0.5 nS will immediately, albeit briefly, triple the effective input conductance\nand reduce the time constant to a third of its original value. A second synapse will not\nfind the same electrotonic structure present as the first one did. It is immaterial for this\nargument whether the synapse considered is excitatory or inhibitory, as long as it increases\nthe local membrane conductance. The fact that synaptic inputs can significantly affect the\nelectrotonic structure of the cell depends on the fact that Rm is high relative to the inverse of\nthe synaptic-induced conductance change. For low values of Rm, say in the neighborhood of\n2,000 Ω -cm\n2, many more synaptic inputs would have to be activated in order for the synaptic\nconductance to dominate the conductance contributed by the passive membrane of the cell.\nAs expressed in a perfunctory manner in Sec. 4.9.1, Eq. 4.24 describes the dependency of\nKn(t) on the exact timing of the synaptic input gsyn(t). Clearly, for two or more synapses,\nthe dependency can be considerably more complex, since the input impedance and other\ncable properties also depend on the relative timing between the inputs and whether or not\nthey are correlated.\nIn order to study the effect of massive synaptic input on the cell, we will disregard such\ntemporal effects by assuming that the inputs arriving at different synapses are independent\nof each other and replace the series of presynaptic spikes by an instantaneous rate f¡. We\nalso disregard any detailed dynamics of the spiking by a \"suitable\" temporal averaging\nprocedure that reduces the total synaptic input from η independent synapses to any one\nparticular compartment to a single number,\n(18.5)\nwhere g[ represents the integrated conductance for a single input,\n(18.6)\nWhen g¡ (t) has the form of an α function (Eq. 4.5), g[ = egpeakfpeak- If the synapse is of the\nNMDA type, this average conductance will be voltage dependent, rendering the following\nequations slightly more complex. Obviously, this averaging method does not incorporate\nany short-term synaptic changes observed in brain slices (Sec. 13.5.4), such as short-term\ndepression and facilitation."}
{"text": "===== Page 10 =====\n18.3 Effect of Synaptic Background Activity \n· 411\nWith this averaging procedure in place, we can replace the different types of synapses in\neach compartment (AMPA, NMD A, GAB A A , and so on) as well as the leak conductance\nby a single \"effective\" conductance,\n(18.7)\nin series with a single effective battery,\n(18.8)\nwhere A is the membrane area of the compartment needed to convert the leak conductance\nper unit area, Gm, into an absolute conductance. Equation 18.7 tells us that as the presynaptic\nfiring frequency (f¡) increases, so does the postsynaptic membrane conductance, regardless\nof whether the synapses are excitatory or inhibitory. Only the conglomerate reversal poten-\ntial £eff is influenced by the signs and amplitudes £,· of the synaptic batteries: if inhibitory\ncells fire more strongly than excitatory ones, the effective synaptic reversal potential will be\npulled toward more hyperpolarizing values and vice versa. We conclude that the effective\nmembrane resistance should not be thought of as a fixed parameter, but as a dynamic variable\nthat can change within a fraction of a second, in contrast to the intracellular resistivity and\nthe membrane capacity, which appear much more difficult to modulate rapidly.\nFor a single infinite cylinder with the appropriate boundary condition, we can write down\na slightly modified cable equation,\n(18.9)\nwhich contains a single term accounting for all synaptic conductances. Defining the length\nconstant of the cable (Eq. 2.13) as\n(18.10)\nand the time constant as\n(18.11)\nwe arrive back at the standard normalized cable equation (Eq. 2.7),\n(18.12)\nIn the remainder of this chapter, we will deal with three distinct scenarios of time-averaged\nmassive synaptic input. We start off by analyzing the effect of diffuse synaptic activity\nthat occurs throughout the cell before we turn toward an analysis of the effect of massive\nexcitatory and shunting inhibitory synaptic activity on the cell.\n18.3 Effect of Synaptic Background Activity\nNeurons, just like people, do not exist in isolation but are embedded within a tightly\ninterwoven network of other nerve cells. A typical neocortical pyramidal cell is the recipient\nof anywhere between 5000 and 20,000 synapses from other neurons, while Purkinje cells"}
{"text": "===== Page 11 =====\n412 · SYNAPTIC INPUT TO A PASSIVE TREE\nin the cerebellum may receive up to 200,000 synapses. In the lightly anesthetized animal as\nwell as in the awake behaving animal, these cells are \"spontaneously\" active, which means\nin practice that they generate action potentials that cannot readily be accounted for by the\npresence of any simple sensory stimulus (such as a bright bar or a loud tone). The origin of\nthis spontaneous activity is presently unknown but could be due to several sources: (1) the\nremnants of spontaneous activity at the sensory periphery that has percolated to the more\ncentral stages (e.g., photon shot noise in the photoreceptors), (2) the spontaneous spiking\ndue to channel fluctuations (Sec. 8.3.2), or (3) the spontaneous release of synaptic vesicles\nin the absence of significant presynaptic activity.\nExperimentally recorded values of the spontaneous firing in cortex range, from low\nvalues of 0.25-2.5 spikes per second in the anesthetized cat visual cortex (Gilbert, 1977;\nLeventhal and Hirsch, 1978) to larger values of 5-10 Hz in extrastriate areas, such as the cat\nmotor cortex (Woody, Gruen, and McCarley, 1984), the rat sensory-motor cortex (Bindman\nand Prince, 1983), the auditory cortex in rhesus monkeys and baboon (Abeles, 1990), and the\ninferior temporal cortex of the behaving monkey (R. Desimone, personal communication).\nThese nonzero rates of background activity are in sharp contrast to the situation prevailing\nfor brain slices or for cultured neurons. The removal of afferent fibers (by cutting to\nextract the slice from the brain), combined with the absence of the right combination\nof neuromodulators and growth factors, makes neurons rather reticent about firing in the\nabsence of a direct electrical or pharmacological stimulus.\nWhat consequence does this background activity, expressed via the time-averaged rate\n(fb), have on any one neuron? Barrett (1975) first brought up the possibility that synaptic\nbackground activity might affect the electrical properties of a motoneurons (see also Rail,\n1974). The subject remained dormant until a number of groups took up the issue via\ndetailed compartmental simulations (Holmes and Woody, 1989; Bernander et al., 1991;\nRapp, Yarom, and Segev, 1992) and analytical investigations (Abbott, 1991).\nWe will summarize the results of these studies by focusing on the changes in R-m, Tm, L,\nand Vrest wrought by varying the background firing rate (fb)·\n18.3.1 Input Resistance\nThe input resistance of the finite cable (looking toward the sealed end boundary at the other\nterminal) is specified by Eq. 2.21 as\n(18.13)\nwhere L = Ι/λ is the electrotonic length of the cable. In the limit of an infinite high\nbackground activity, that is, for (fb) ->· oo, Rm decreases as (fb)~\nl f 2· Basic intuition tells\nus that additional synaptic input, independent of whether excitatory or inhibitory, increases\nthe membrane conductance and thereby drives down the effective membrane resistance /?eff\n(Fig. 18.4A).\nThe same plot also shows Rm for the compartmental model of the pyramidal cell with\nthe standard complement of voltage-dependent somatic membrane conductances (\"active\"\nneuron) and for the same model but in the absence of these active conductances (\"passive\"\nmodel). (The geometry of the single cylinder was adjusted to coincide with the passive\npyramidal cell model at (fb) = 0.) All three models share the same features: as (fb) is\nturned up from 0 to 2 Hz, the input resistance drops by a factor of 10, from 193 to 20 ΜΩ,\nin the passive case, and by a factor of 4, from 50 to 12 ΜΩ, in the active case. The deviation"}
{"text": "===== Page 12 =====\n18.3 Effect of Synaptic Background Activity \n· 413\nFig. 18.4 EFFECT OF SYNAPTIC BACKGROUND ACTIVITY ON R-m,rm,L, AND Vrest \nIllustration of\nthe effect of varying the synaptic background activity {//,) on four variables characterizing spatio-\ntemporal integration in three systems; a finite, passive cylinder (3060 μηι long and 5.8 μτη thick; with\nsealed-end boundary condition; bold line labeled \"single cable\"), the complete model of the layer 5\npyramidal cell with active conductances (\"active\" neuron) and the same model but with all active\nsomatic conductances removed (\"passive\" neuron). All cases show the same effect: as (fb) increases,\nthe membrane becomes more and more leaky, and /?¡n as well as rm decrease and the electrotonic\nlength of the passive cable or the electrotonic distance between a point in the layer 1 portion of the\ndistal apical tree and the soma in the pyramidal cell becomes ever larger. (f¡,) is very close to zero in\nbrain slices, while (fb) > 1 Hz corresponds to the situation in the living brain. The behavior of the\ncable is given by the equations in the text. #¡n, rm, and Vrest are all measured at the soma. Inhibitory\nand excitatory synapses are assumed to have identical rates (fb). Note the logarithmic scale on the\nupper two panels. Reprinted by permission from Bernander (1993).\nof the compartmental model from the cylinder at high values of (fb) is largely explained\nby the inhomogeneous synaptic distribution along the pyramidal cell (inhibitory synapses\ncluster in a neighborhood of the soma while excitation is more distal). Further differences\nresult from the voltage-dependent components.\nThe dependency of /?¡n and other electrical parameters on {fb) becomes more complex in\nthe presence of voltage-dependent NMDA input. As discussed in Sec. 17.1.2 and Fig. 17.3,\nincorporating NMDA conductances into the membrane can have the paradoxical effect of\nreducing the slope conductance. For the situation discussed here, this implies that under"}
{"text": "===== Page 13 =====\n414\nSYNAPTIC INPUT TO A PASSIVE TREE\ncertain conditions the input resistance can actually increase as the background firing rate is\nincreased (Bernander, 1993).\n18.3.2 Time Constant\nEquation 18.11 characterizes the dependency of the passive time constant on (fb). We\nexpect that as (ft,) —> oo, rm goes to zero as I/ (//,). The fractional decrease in the somatic\ntime constant is similarly large in the compartmental model: from 100 to 12.8 msec for the\npassive case and from 33.7 to 5.9 msec in the active case (Fig. 18.4B). This decrease in\nτη as (fb) increases implies that the pyramidal cell becomes much more sensitive to the\ndegree of temporal dispersion in any synaptic input. To demonstrate this, Bernander and\ncolleagues (1991) computed nth> the average number of excitatory non-NMDA synapses,\ndistributed throughout the dendritic tree, necessary to trigger a spike. These synapses were\neither all activated simultaneously or spread out over a 25 msec extended time window\n(with each synapse activated only once).\nIt is apparent from Fig. 18.5 that for the desynchronized case «th increases significantly\nas (fb) increases. Under slice conditions, 115 synchronized synapses are necessary to bring\nthe cell above threshold, whereas 145 are needed if the input is desynchronized. This small\ndifference between synchronized inputs and inputs arriving smeared out in time is due to\nthe long integration period of the cell (rm = 100 msec). For a 1 Hz background activity,\n113 synchronized or 202 desynchronized inputs are needed to fire the cell. At (fb) = 7 Hz,\nwhen 35,000 synaptic events are bombarding the cell every second, 3.5 times as many\nFig. 18.5 TEMPORAL INTEGRATION \nThe ability of the standard layer 5 pyramidal cell to distinguish\ncoincident arriving synaptic input from a desynchronized one increases as the spontaneous background\nactivity (fb) increases and τη decreases (Fig. 18.4B). The minimal number of excitatory voltage-\nindependent synaptic inputs η u, (withgpeak = 0.5 nS) needed to trigger a spike at the soma as a function\nof(fb). The synapses are spread over the dendritic tree in accordance with the known distributions and\nare either all activated simultaneously or activated once over a 25-msec-wide window, independently\nof each other. For (fb) *» 0 (a situation prevalent for most in vitro studies), the difference between\nsynchronized and desynchronized inputs is minor but becomes substantial when the overall network\nactivity is high. The average background activity can therefore be seen as the signal controlling\nthe temporal tuning properties of the cell. Each simulation is run numerous times using a different\ndistribution of synapses. Reprinted by permission from Bernander et ah, (1991)."}
{"text": "===== Page 14 =====\n18.3 Effect of Synaptic Background Activity\n415\ndesynchronized inputs are needed to trigger the cell than if the inputs arrive together. As\n(fb) increases, the cell becomes more and more selective to differences in the arrival times\nof synaptic inputs.\n18.3.3 Electroanatomy\nAccording to Eq. 18.10, the electrotonic length of a finite cable should increase as (fb)l/2. As\nthe membrane becomes more and more leaky, distant points are, electrically speaking, less\nand less coupled to the soma. Because the dendritic tree contains no active conductances,\nthe electrotonic distance from the soma to a point in the apical bush (about 1 mm away\nfrom the soma) is the same in the active and the passive model. Given the square root\ndependency of L on (fb), this distance, computed as the sum of all branch segments on the\npath between the location and the soma increases by a factor of 2.5 as {/&) increases from\n0 to 2 Hz.\nA graphic demonstration of the effect of varying (fb) on the electroanatomy of the cell\nis given in Fig. 18.6 using the graphical morphoelectronic transform (MET) (see Sec. 3.5.4)\nof Zador (1993) and Zador, Agmon-Snir, and Segev (1995). Each dendritic compartment\nis \"stretched\" or \"shrunk\" such that its length in the graph is proportional to its electrotonic\nFig. 18.6 VARIABLE ELECTROANATOMY OF PYRAMIDAL CELL Graphic illustration of the change\nof the electrotonic dimensions of the pyramidal cell with synaptic background activity with the aid of a\nmorphoelectronic transform. The extent of the individual dendrites is proportional to their electrotonic\nlength; the distance of each compartment from the soma is proportional to the electrotonic distance of\nthat point from the soma. In the absence of any background activity—approximating the conditions\nprevalent in cultured or in slice neurons—the pyramidal cell is very compact. As (fb) is increased\nto 2 Hz, distal sites on long and thin dendrites are more and more decoupled from the soma. The\nscale bar corresponds to the distance over which the sustained voltage decays by e°·5 in an infinite\ncable. The arrows point to the location of the apical and basal synapses used in Fig. 18.1. Reprinted\nby permission from Bernander et al., (1991)."}
{"text": "===== Page 15 =====\n416\nSYNAPTIC INPUT TO A PASSIVE TREE\nlength L j . L increases from 1.2 to 2.6 for the most distal compartment in layer 1 and from\n0.2 to 0.7 for a synapse at the tip of a basal dendrite. That basal dendrites stretch more than\napical ones is due to their higher synaptic innervation and the corresponding lower effective\nvalues of Rm.\nYet another way to visualize the change in electrotonic properties is to plot the somatic\nEPSPs evoked by a fast non-NMDA excitatory synapse located either in the distal apical\ntree, along a basal dendrite, or at the soma (Fig. 18.7). While the amplitude of the somatic\nand basal EPSPs decrease by a factor of 2 or 3 as (/¿} changes from 0 to 5 Hz, the effect\non the distal apical synaptic input is much more dramatic. For (fh) = 5 Hz, almost no\ndeviation from the somatic resting potential is seen.\n18.3.4 Resting Potential\nOf the four variables considered here, only £eff depends on the balance of excitatory and\ninhibitory input (Eq. 18.8). For large values of {/¿}, Vsoma depolarizes by close to 10 mV\n(Fig. 18.4D). This is a consequence of the fact that 80% of the synapses are excitatory and\nthe assumption that the synaptic background activity is the same for excitatory as well as\nfor inhibitory afferents. The somatic potential does not increase indefinitely with {/¿}, but\nsaturates due to the interaction between the net excitatory and inhibitory synaptic currents\nand the voltage-dependent potassium currents /A and /M, driving the potential to —95 mV\n(for more details, see Bernander, 1993).\nIt is known that the resting potential differs significantly between neurons recorded in\nvivo and in vitro. Numerous slice studies of pyramidal cells report resting potentials in the\nrange of —84 to —67 mV (with a mean of —74 mV; Thomson, Girdlestone, and West,\nFig. 18.7 SYNAPTIC INPUT IN THE PRESENCE OF BACKGROUND ACTIVITY Somatic EPSP in\nresponse to activation of a single non-NMDA synapse at one of three locations in the pyramidal\ncell (from top to bottom: soma, basal, and apical tree; see Fig. 18.6) at three different levels of\nsynaptic background activity. The largest EPSPs are obtained under slice conditions ((fh) = 0). For\n25,000 synaptic events per second ((fh) = 5 Hz), the peak somatic, basal, and apical EPSPs are\nreduced by factors of 1.7, 2.8, and 14.9 relative to their peak values at (ft,} = 0. The middle set of\nEPSPs corresponds to the panel of Fig. 18. IB. Notice the much shorter rise and decay times at higher\nbackground frequencies. Reprinted in modified form by permission from Bernander (1993)."}
{"text": "===== Page 16 =====\n18.3 Effect of Synaptic Background Activity \n· 417\n1988; Mason and Larkman, 1990; Spain, Schwindt, and Crill, 1990; Hirsch and Gilbert,\n1991; Mason, Nicoll, and Stratford, 1991), while in vivo Vrest is usually in the range of\n—81 to —57 mV (with a mean of —64.5 mV; Bindman and Prince, 1983; Bindman, Meyer,\nand Prince, 1988; Holmes and Woody, 1989; Pockberger, 1991). Whether this difference is\ncaused primarily by the difference in synaptic background activity remains open.\n18.3.5 Functional Implications\nThe principal effects of modulating the synaptic background activity on the spatio-temporal\nproperties of individual neurons are clear-cut: as {ft,} increases from no activity to physio-\nlogical values in the 5-10 Hz range, input resistance, and space and time constants decrease.\nFunctionally, the cell becomes more and more extended and sensitive to temporal synchrony.\nThis conclusion is not dependent on the numerical details and occurs in simple, analytical\ncable models (Abbott, 1991; Bernander, 1993) as well as in detailed compartment models\nof Purkinje (Rapp, Yarom, and Segev, 1992) and pyramidal cells (Bernander et al., 1991).\nThis phenomenon is contingent upon two key assumptions: (1) that the effective membrane\nresistance in the absence of synaptic background activity is high and (2) that on the order\nof several thousand (or more) spontaneous synaptic events occur every second. If these\nconditions are not met, background activity will have either no or only a negligible effect\non the spatio-temporal properties of the cell. Note well that these effects would not be\nobserved if synaptic inputs were treated as current inputs.\nWhile it is undoubtedly important to understand the effect of (fb) at the level of an\nindividual cell, a more functionally relevant question is to what extent the spontaneous\nfiring activity is under the control of other brain systems. In particular, to what extent and\nhow fast can neuromodulators up or down regulate (fb) for entire groups of neurons? If\nthey can, the phenomena discussed here could have a number of interesting implications\nfor neuronal information processing strategies.\nFor instance, in the dark few cells in the visual cortex will be active and the membrane\ntime constant fm will be long, enabling the cell to integrate the input over a long time.\nConversely, for bright, high contrast visual stimuli, the overall network activity may be\nmuch higher, leading to a small value of rm and short integration times. This adaptive\ngain-control mechanism is somewhat akin to that used by the network of coupled rods in\nthe retina (Detwiler, Hodgkin, and McNaughton, 1980). As exemplified in Fig. 18.6, larger\nvalues of (fb) decouple distal sites from the soma. Because the apical tufts in layers 1 and 2\nconstitute the dominant target zones for cortico-cortical feedback connections (Friedman,\n1983; Rockland, 1994; Salin and Bullier, 1995), a large degree of afferent activity would\ndecouple this feedback by increasing its distance from the soma, thereby making the cell\nmore responsive to direct sensory input arriving on the proximal parts of the apical trunk.\nIt is obvious that the properties of individual neurons influence the behavior of the\nembedding neuronal network, such as whether or not it converges to a fixpoint and its\nconvergence time. We have here an instance of the converse, where a collective property of\nthe network—its average activity—modulates the properties of individual members of the\nnetwork. Particularly intriguing is the fact that the temporal resolution of neurons increases\nas the mean network activity increases. This illustrates the strong two-way interdependency\nof the many structural levels of neurobiological hardware, in stark contrast to computer\nhardware. An intriguing but open question is the issue of stability: under what conditions\ncan a network that modulates the electrical properties of its elements arrive at a stable\nequilibrium point, that is, how can homeostasis be achieved?"}
{"text": "===== Page 17 =====\n418 \n· \nSYNAPTIC INPUT TO A PASSIVE TREE\n18.4 Relating Synaptic Input to Output Spiking\nSo far, we have always evaluated the effect of synaptic input to the somatic membrane\npotential, since it is here—or in the axonal hillock close to the cell body—that the output\nof the cell—spikes—is generated. But, ultimately, we wish to understand the relationship\nbetween synaptic input and the firing activity of the cell.\nThis is a difficult problem due to the nonstationary nonlinearities at the soma. If EPSPs\narrive while the somatic potential hovers just below the spiking threshold, it might be\nenough to push the potential above threshold, while the arrival of EPSP following an action\npotential might have a negligible effect on the output frequency. Furthermore, it also makes\na difference whether the synapse is activated only once or repeatedly. We will sidestep\nthese problems by adopting the stance we did in the previous sections, that is, we assume\nthat the synaptic inputs are independent of each other and that each individual rate can\nbe replaced by a suitable average (Eq. 18.5) which we assume for simplicity's sake to be\ndirectly proportional to the presynaptic firing frequency.\nAs opposed to conventional cable theory, we will not compute the somatic potential\nevoked by synaptic input but the current /syn,j, flowing longitudinally down the dendrite\ninto the soma. It is this current that will be seen by the spiking mechanism at the cell body and\naxon hillock and that will ultimately cause the cell to generate axon potentials. Knowledge\nof /syn.s.\nm conjunction with the /-/ curve, allows us to go directly from synaptic input to\nfiring output. We here follow the exposition of Bernander, Douglas, and Koch (1994).\n18.4.1 Somatic Current from Distal Synaptic Input\nWe follow Abbott (1991) in computing the current flowing out of one terminal of a single\ncable when one or more synapses are activated at the other terminal at X = L. (The cable\nhas the electrotonic length L — Ι/λ and diameter d.) We can think of the X — 0 terminal\nas being connected to an RC compartment representing the soma. Activating the synapses\ncauses ionic current to flow into the cable and along the transversal intracellular resistivity\ntoward the low-impedance cell body.\nBecause we are averaging over a fraction of a second or longer we can neglect the\ncapacitive term in our modified cable equation (Eq. 18.12), and—following Eq. 2.2—\nexpress the current /8γη,5 flowing into the soma by the derivative of the voltage along the\ncable divided by the longitudinal resistance,\n(18.14)\nThe excitatory synapses increase the membrane conductance at X = LbygJ (fe) (with{/e)\nthe presynaptic firing rate of the excitatory synapses and gj the time-averaged conductance\nper area; Eq. 18.6) in series with the synaptic reversal potential Ee. In order to compute the\ncurrent, we need to know the voltage. As in Eq. 2.18, the voltage in this finite cable can be\nexpressed as\n(18.15)\nSolving this requires two boundary conditions. The voltage at the synaptic terminal is given\nby the usual synaptic equation (Eq. 4.12) as\n(18.16)"}
{"text": "===== Page 18 =====\n18.4 Relating Synaptic Input to Output Spiking\n419\nAs argued in the preceding chapter, the spike mechanism at the soma acts as a sort of voltage\nclamp, preventing the membrane potential from making long-term excursions beyond Fth-\nFor the sake of convenience, Abbott (1991) clamps the potential at the X = 0 terminal\nto 0. This also specifies the input resistance R-m at the synaptic terminal as the resistance\nassociated with a killed-end terminal (Eq. 2.24),\n(18.17)\nRemembering that the derivative of sinh(L — X) is cosh(L - Χ)/λ and that sinh\n2(x:) -\ncosh (x) = — 1, we arrive at the following expression for the current flowing into the\nvoltage-clamped terminal at X = 0,\n(18.18)\nIf the input is small relative to the local input resistance, the current is linearly related to\nthe input frequency. As (fe) increases, the local postsynaptic potential starts to saturate,\nlimiting the amount of current that enters through the synapse. Finally, the driving potential\nEe — V(L) is close to zero and no amount of presynaptic firing activity will push any\nadditional current through the synapse and toward the other terminal (Fig. 18.8). The\nlimiting current (in the sense that it cannot be exceeded without amplification) at the X = 0\nterminal becomes\n(18.19)\nTwo important remarks. The saturation of the net current delivered by the synaptic input to\nthe soma has very little to do with current losses along the cable. If no current leaks across\nthe membrane by postulating Geff —> 0,1^ s only increases by a factor of sinh(L)/L.\nFig. 18.8 LONGITUDINAL CURRENT IN A SINGLE CABLE \nExcitatory synaptic input is applied to\none end of the same finite cable (of electrotonic length L = 1.138 used in Fig. 18.4), and the other\nterminal, conceptually corresponding to the soma, is clamped to the resting potential. The longitudinal\ncurrent /syn,s flowing from the synapses into the X = 0 terminal clamped to 0 (Eq. 18.18), is computed\nas a function of the averaged presynaptic firing rate of the excitatory input {fe}. The saturating\ncharacteristic has little to do with current leaking out across the membrane. Indeed, preventing any\ncurrent from leaking across the membrane only increases the maximal current 7S\nS^ s by 23%. The\nculprit is local saturation of the membrane potential at the synaptic terminal. No amount of synaptic\ninput can generate more current /syni¡ (unless the membrane contains amplifying currents, as discussed\nin the following chapter)."}
{"text": "===== Page 19 =====\n420 \n· \nSYNAPTIC INPUT TO A PASSIVE TREE\nAlso, /syn,i does not correspond to the current that is computed with the aid of the transfer\nresistances (see the closing comments to Sec. 3.5.3).\n18.4.2 Relating /out to /ta\nThe key idea is to assume that the spike generation process at the cell body and axonal hillock\ncan be treated as a stationary nonlinearity whose exact form is given by the discharge curve,\nthat is, by the plot of the output frequency /out against the amplitude of the injected current\nstep (see Figs. 17.10A and 18.10A). It does not matter to this spike triggering mechanism\nwhether the current it sees is injected from a microelectrode or is delivered from a synapse\nvia the dendritic tree. All that is relevant is that a particular current 7 can be associated\nto a particular output spiking frequency along the /-/ \ncurve. We incorporate the spike\nfrequency adaptation into our consideration by directly making use of the fully adapted\n/-/ curve.\nKnowing the relationships between the steady-state input frequency (/¡n) and the somatic\ncurrent 7syn>J (as in Eq. 18.18) on the one hand, and the adapted /-/ \ncurve on the other\nallows us to derive an unique function relating (/¡n) to /out,\n(18.20)\n(see Fig. 18.9).\nBernander Douglas, and Koch (1994) use this procedure to derive the input-output\nrelationship for the layer 5 pyramidal cell. /synjS is estimated on the basis of two different\nFig. 18.9 COMPLETE MEASURE OF SYNAPTIC EFFICIENCY Schematic illustrating the procedure\nlinking the time-averaged presynaptic input frequency (/¡n) to the adapted output frequency /out\nof the cell via Eq. 18.20. It requires knowledge of the relationship between the presynaptic firing\nfrequency (/¡n) and the current flowing from the synapse into the soma /syilií (lower right) and the\ndischarge curve /-/ (lower left; here a linear function, although this is not necessary). The former\nfunction accounts for properties of the synapses and the dendritic tree, the latter for the somatic\nspike-generating properties. The resulting function /Out((/m)) accounts for the synaptic conductance\ninput, synaptic saturation, and active, voltage-dependent membrane conductances. The intermediate\nvariable is current, and not—as in the standard cable equation—voltage. Reprinted by permission\nfrom Bernander (1993)."}
{"text": "===== Page 20 =====\n18.4 Relating Synaptic Input to Output Spiking \n· 421\nmethods. Knowledge of /syn,s requires that the somatic membrane potential Vm (t) be known.\nIn the derivation of Eq. 18.18 it was assumed that V(0) was clamped to 0. When the cell\nspikes, this is patently wrong. Since the considerations of this section are predicated on\nusing a time-averaged input frequency, why not use the time-averaged membrane potential\n(Vm) of Eq. 17.12? As we discussed in Sec. 17.5.2, this quantity changes only little for\nlarge swings in spiking activity. Indeed, the cumulative action of the voltage-dependent\nsomatic currents is to clamp, albeit imperfectly, (Vm) to around —50 mV, corresponding\nto moderate values of /out in the 50-Hz range. Bernander and colleagues (1994) compute\nthe clamp current required to hold the somatic potential to —50 mV for a fixed synaptic\ninput and subtract the clamp current needed to hold VSoma to —50 mV in the absence of\nany synaptic input. This method was also used experimentally to measure /syn,s in response\nto a constant synaptic input in a motoneurons (Powers, lobinsón, and Konodi, 1992). In\nthe second method, /syn,i is computed directly (without voltage clamping the soma) as the\ncurrent flowing between the first segment of the apical tree and the soma. Both methods\ngive identical results.\nAs can be seen in Fig. 18.10B and C, when 500 excitatory non-NMDA synapses are\neither placed directly onto the cell body or spread throughout the basal dendrites (almost\nexclusively restricted to layer 5), /syn-i is linear in (/¡n) over the range of interest. Saturation\nonly becomes evident as the same 500 synapses are moved to layer 4 or to the more distal\nparts of the apical tree in superficial layers. When all inputs are confined to the superficial\nlayers 1, 2, and 3, 7syn>i is at most 0.65 nA. The saturation effect is even more extreme\nwhen synaptic input is restricted to the top two layers 1 and 2. Even if all 500 synapses are\nactivated at an unphysiological 500 Hz, at most 0.25 nA of current reaches the soma (and\nthis across an apical dendrite that has an unusually thick trunk of about 4.4 μ,ηι). This strong\nsaturation is due to the fact that the distal input is associated with high input impedances,\ndriving the local potential quickly to the synaptic reversal potential. No amount of further\nsynaptic input can increase the synaptic current. Saturation would not occur if synaptic\ninput were treated as a pure current. As in the case of the single cable, the minuscule current\ncontributed from distal sites is not due to leakage through the membrane as the current\nspreads to the soma. Indeed, eliminating leakage by setting Rm —>· σο causes the maximal\ncurrent to increase by only 2% above base level.\nIn the last panel in Fig. 18.10, these somatic currents are translated on the basis of the\nadapted discharge curve (Fig. 18.10A) into an output firing rate. Because of the current\nthreshold, synaptic input that delivers less than /th — 0.29 nA current to the soma will be\ncut off. For instance, by itself, input to layers 1 and 2 will not lead to any maintained spike\ndischarge, no matter how big the synapses and how vigorous their presynaptic activity,\nwhile the addition of synapses to layer 3 leads to a very weak discharge. And yet, these\ndendrites make up 26% of the surface area of the tree. Above threshold, the adapted /-/\ncurve is reasonably linear (Eq. 17.10), with a slope of 50 spikes per second per nanoampere\nof current, leading to linear input-output relationships for synaptic input to the soma or the\nbasal dendrites (contributing 62% of the total membrane area of the neuron). As a sanity\ncheck, Bernander and colleagues explicitly computed the adapted firing rate as a function\nof the input and superimposed this onto Fig. 18.10D, with little difference.\n18.4.3 Functional Considerations\nThe standard measures of the synaptic efficiency of cable theory are voltage or charge\nattenuation (see Chap. 3). The advantage of using the total current /syn,s flowing from one\nor more synapses into the soma (Powers, Tobinson, and Konodi, 1992; Bernander, Douglas,"}
{"text": "===== Page 21 =====\n422 \n. \nSYNAPTIC INPUT TO A PASSIVE TREE\nFig. 18.10 RELATING SYNAPTIC INPUT TO THE FIRING FREQUENCY Characterizing the input-\noutput behavior of the layer 5 pyramidal cell using the procedure illustrated in Fig. 18.9. (A) For\nease of comparison, its adapted /-/ \ndischarge curve. (B), and (C) The input, 500 excitatory non-\nNMDA synapses, is placed onto the soma or the portion of the dendrites running through a specific\ncortical layer. The postsynaptic conductance increase is related linearly to the presynaptic firing rate\n(/in). These synapses cause the current /Syn,s to flow into the soma. The different curves correspond\nto synaptic placement (from top to bottom) onto the soma, layer 5, layer 4, layers 1 to 3, layers 1\nand 2, and layer 1 only. Similar to Eq. 18.18, dendritic saturation causes the distal input to saturate.\n(D) /Syn,i((/in» is passed through the cell's discharge curve to predict the relationship between (/¡n>\nand /out (dashed curves). Computing this function by direct simulation (solid curves) leads to very sim-\nilar results. Because 7SyniJ for synaptic input from layers 1 and 2 is less than /a, (about 0.295 nA), these\nsynapses—assuming a passive dendritic membrane—cannot contribute by themselves to the main-\ntained discharge. Experimentally, it is known that synaptic input to the apical tuft can trigger spikes in\nthe cell body far away (Cauller and Connors, 1992, 1994). This argues for the existence of dendritic\nvoltage-dependent currents that counteract saturation and amplify /syn,s (see the following chapter).\nand Koch, 1994) is that this measure includes the effects of synaptic nonlinearities and\ninteractions among different inputs. Furthermore, as we will show in the following chapter,\n4yn,s can also be derived in the presence of voltage-dependent dendritic conductances,\nwhen all other measures break down. If two synaptic sites are well separated such that the\nvoltage in one does not readily influence the voltage at the other location (for instance, input"}
{"text": "===== Page 22 =====\n18.5 Shunting Inhibition Acts Linearly \n· 423\ninto the basal dendrites coupled with input to the apical tuft), the total current at the soma\nwill be the sum of the individual /syn,i components.\nComputing the current flowing into the soma is relatively straightforward as long as Vsoma\nis fixed (see the derivation of Eq. 18.18). However, this is not the case if /synjS is above /th,\nleading to spike generation. Under these conditions, Vsoma(i) moves on a stable limit cycle,\nmaking it difficult to characterize synaptic input using a single voltage measure, such as the\npeak somatic EPSP. Yet the method advocated here works despite these complexities (as\nwitnessed by the close match in Fig. 18.10D between the predicted input-output curves and\nthe actual ones) by making use of the time-averaged somatic potential {Vm} (Eq. 17.12). Its\nkey assumption is that the system is in equilibrium, that is, the input can be characterized\nby an averaged input rate and the postsynaptic firing rate has adapted (typically within\n100 msec). In a more sophisticated model, adaptation could be incorporated by making the\n/-/ curve time dependent. Likewise, short-term synaptic facilitation and depression could\nalso be dealt with.\nAs applied to the pyramidal cell, the main conclusion is that in the absence of voltage-\ndependent dendritic conductances distal synaptic input by itself can only deliver a very\nlimited amount of current to the soma. In conjunction with other synaptic input, such as\nto the basal dendrites, distal input could increase the maintained rate by about 12 spikes\nper second.\nIn the neocortex, much of the synaptic input into layer 1 originates in higher cortical\nprocessing stages (Friedman, 1983; Rockland, 1994; Salin and Bullier, 1995). In the\nabsence of amplifying dendritic conductances, cortico-cortical feedback could only weakly\nmodulate the firing of the pyramidal cell. Yet Cauller and Connors (1992, 1994) have\nprovided experimental evidence that synaptic input onto the layer 1 portion of layer 5\npyramidal cells in the somatosensory cortex can evoke action potentials. This, together with\nthe evidence reviewed in the following chapter, argues that voltage-dependent conductances\nin the dendritic tree serve to amplify distal inputs.\n18.5 Shunting Inhibition Acts Linearly\nInhibitory synaptic input with a reversal potential close or equal to the local resting\npotential, shunting inhibition, first mentioned toward the end of Chap. 1 and treated in\nconsiderable detail in Sec. 5.1.4, acts nonlinearly, akin to a division (Blomfield, 1974). This\ntype of nonlinear operation has been proposed to be the crucial biophysical mechanism\nunderlying retinal direction selectivity (Torre and Poggio, 1978; Koch, Poggio, and Torre,\n1982). More recently, both Carandini and Heeger (1994) and Nelson (1994) have made\nshunting inhibition, activated by a recurrent feedback loop, the centerpiece of a gain\nnormalization circuit in the visual cortex (Fig. 1.11) and in electric fish. However, none\nof these studies considered the effect that the spiking mechanism has on the divisive\naction of shunting inhibition. When this is done, a surprising conclusion emerges (Holt\nand Koch, 1997).\nFor the sake of clarity, let us first consider shunting inhibition to the leaky integrate-\nand-fire model of Eq. 14.7. Using our standard synaptic averaging technique, shunting\ninhibition can be mimicked by decreasing the leak resistance appropriately, with the new\nvalue given by\n(18.21)"}
{"text": "===== Page 23 =====\n424\nSYNAPTIC INPUT TO A PASSIVE TREE\nwhere R is the value of the resistance in the absence of any input. The result can be inspected\nin Fig. 18.11A. Increasing g¡ shifts the curve toward the right, with the slopes remaining\nconstant, contrary to what one would expect based on models that only treat the subthreshold\ndomain (as in Fig. 1.10 and throughout Chap. 5). Adding a refractory period and adaptation\ndoes not change this conclusion. The same is also true if one considers the much more\nrealistic scenario of GABA^ inhibitory receptors, with a reversal potential of —70 mV,\nwhich are distributed at and around the soma. As in the previous section, the total amount\nof inhibition is postulated to be proportional to the presynaptic firing frequency (Eq. 18.5).\nHolt and Koch (1997) compute the average /out-/in curve by varying the input frequency of\nthe excitatory voltage-independent synaptic input distributed throughout the tree. Changing\nthe amount of inhibition leads to a clear shift in the curves with little effect on the slopes\n(Fig. 18.11B).\nFig. 18.11 SHUNTING INHIBITION AND SPIKING \nShunting inhibition has a subtractive rather than a\ndivisive effect on firing rates. This is demonstrated in two different single-cell models. (A) Discharge\ncurves for a leaky integrate-and-fire unit with different values for the leak conductance gieak — gt+1/R\n(for R = 62.5ΜΩ). g¡ is the amplitude of the inhibitory conductance change whose reversal potential\nis equal to the unit's resting potential (here zero). Varying gieak in steps of 10 nS from 10 to 70 nS\n(from left to right) shifts the curve, rather than changing the slope of the discharge curve. (B) The\nsame observation is made in the pyramidal cell model, with GABAA inhibition around the soma and\nexcitatory voltage-independent input distributed throughout the cell. The fully adapted postsynaptic\nfiring rate is plotted as a function of the average input frequency to the excitatory synapses for four\ndifferent settings of presynaptic firing rates to the GAB A/t synapses (0.5,2,4, and 6 Hz, left to right).\nReprinted by permission from Holt and Koch (1997)."}
{"text": "===== Page 24 =====\n18.5 Shunting Inhibition Acts Linearly\n425\nThis effect is most easily understood in the integrate-and-fire model. In the absence of\nany spiking threshold, the membrane potential in response to a synaptic input current of the\nshunting type\nrises until\n(18.22)\n(18.23)\n(Fig. 18.12). Under these conditions, the steady-state leak current is proportional to the input\ncurrent and the divisive effect is observed (Eq. 1.35). However, in the integrator model, V\nnever rises above the spiking threshold Vth. No matter how large the input current, the leak\ncurrent can never be larger than Vthgieak· The effect of the leak conductance can be replaced\nby a current whose amplitude is equal to the time-average value of the current through the\nleak conductance: {/ieak} = gleak (V) · This trick reduces the leaky integrate-and-fire unit to\na perfect integrator. The cell will still fire at exactly the same rate because the same charge\n/(Λχη — I\\eak)dt is deposited on the capacitor in the same time interval, although for the\nleaky integrator the deposition rate is not constant.\nFig. 18.12 WHY SHUNTING INHIBITION HAS A SUBTRACTIVE EFFECT \n(A) Time-dependent\ncurrent across the leak conductance /ieak (in nA and equal to V(f)gieak) in response to a constant\n0.5-nA current injected into a leaky integrate-and-fire unit with (solid line) and without (dashed line)\na voltage threshold V^,. The sharp drop in /i^ occurs when the cell fires, since the voltage is reset.\n(B) Same for 1-nA current. Note that /ieak in the presence of a voltage threshold has a maximum\nvalue well below /ieak in the absence of a voltage threshold. (C) Time-averaged leak current (/ieak) (in\nnanoamperes) as a function of input current, computed from Eqs. 18.24 and 18.25. Below threshold,\nthe spikeless model and the integrate-and-fire model have the same (/ieak), but above threshold (/ieak)\nis reduced considerably. For /syn just greater than threshold, the cell spends most of its time with\nV K Vth, so (/leak) is high. For high 7syn, the voltage increases approximately linearly with time and\nV has a sawtooth waveform, as shown in B. This means that (/ieak) = (max /ieak)/2 = Vth£ieak/2.\nReprinted by permission from Holt and Koch (1997)."}
{"text": "===== Page 25 =====\n426 · SYNAPTIC INPUT TO A PASSIVE TREE\nFor a constant input that is just above threshold, {V} will be close to Vth and (/ieak) will\nbe large. For larger synaptic input currents, the time-averaged membrane potential becomes\nsmaller and smaller (since V has to charge up from the reset point) and, therefore, the time-\naveraged leak current decreases for increasing inputs (compare Fig. 18.12A and B). It can\nbe shown that\n(18.24)\nif 7syn < Vthgieak- Otherwise\n(18.25)\nFor even quite moderate levels of 7syn just above V^gieak» the lower expression is approxi-\nmately equal to gieak Vth/2, independent of 7syn (Fig. 18.12C), and the leak conductance can\nbe replaced—to a good first approximation—by a constant offset current. The discharge\ncurve for the resulting perfect integrate-and-fire neuron is\n(18.26)\nThe same mechanism explains the result for the compartmental models. Here, the voltage\ndoes rise above the firing threshold but because the spike repolarization currents are large,\nany current from synapses flowing into the soma is ignored during the brief spike. The\ntime-averaged somatic membrane potential (Fig. 17.10B) always remains close to Vih-\nTwo caveats. These results hold because inhibition is at or very close to the soma. For\nmore distal inhibition, as for instance in retinal ganglion cells, the local potential at the\ninhibitory synapse it not limited by Vth and shunting inhibition will work in a more divisive\nmode. Yet for such distal sites, its effect on the spike triggering zone far away will be\nsmall. The results discussed here assume that the synaptic input changes slowly. It is at\npresent unclear what the effect of very high, but transient input frequencies to the inhibitory\nsynapses will be.\n18.6 Recapitulation\nThe past few pages treated synaptic input arriving at the passive dendritic tree of a spiking\ncell. Only the cell body is assumed to contain voltage-dependent membrane conductances.\nWe first discussed unitary EPSPs and EPSCs, concluding that the former can be used to\npredict quite well the voltage threshold Vth of the cell. We then investigated the exact\nrelationship between the temporal jitter in a normally distributed set of synapses σ·1Ώ and\nthe jitter in the output spike aout. Surprisingly, we discovered that aout α σίη, with the\nconstant of proportionality being much less than unity. Once again (e.g., Chap. 14) for\nstrong synaptic input (that is, in the above threshold domain), the passive membrane time\nconstant plays only a minor role in determining the temporal behavior of the cell. This has\nimportant implications, in particular for the faithful conservation of timing relationships\nacross many layers of neurons (Abeles, 1990; Marsalek, Koch, and Maunsell, 1997).\nFor the remainder of the chapter we assume that synaptic input is not correlated at a fine\ntime scale and that, indeed, we can define an average input frequency {/}. This allows us\nto treat massive synaptic input in three separate cases."}
{"text": "===== Page 26 =====\n18.6 Recapitulation · 427\nNeurons receive between 103 and 105 synapses onto tneir dendritic tree from other\nneurons that fire spontaneously between 1 to 10 or more spikes per second in the behaving\nanimal. This massive synaptic background activity has several effects on the spatio-temporal\nbehavior of the cell. As this background firing rate goes up, the electrotonic dimensions\nof the cell increase (that is, distal synaptic input becomes more decoupled) and the input\nresistance and time constant decrease by one or more orders of magnitude. This implies,\namong other things, that the cell becomes more sensitive to temporal coincidences as the\noverall network activity increases. Given the rapid changes in firing activity in large parts\nof the brain, as assessed by EEG and other macroscopic techniques recording large-scale\nbrain activity, this implies that the collective behavior of the network can directly influence\nthe spatio-temporal integrative properties of individual neurons.\nWe also described a method that characterizes the efficiency of massive synaptic input,\ninvolving the current 7synjJ that is delivered by synaptic input to the soma. Deriving 7syn)i\nfor synaptic input to a particular spatial region in the tree firing at a frequency {/;„) has the\nmajor advantage that the resultant function can be combined with the /-/ \ncurve measured\nat the soma to yield a complete relationship between the sustained presynaptic input firing\nfrequency {/¡η) and the adapted output firing rate /out.\nWhen this method is applied to study synaptic input into the distal part of the apical tree of\na pyramidal cell, it is seen that the current from these sites (1) saturates already at very small\npresynaptic firing rates and (2) influences the maintained firing rate only marginally. This\nsaturation is only to a very minor extent caused by current leaking across the membrane\nbetween the synaptic site and the soma. As we will see in the next chapter, introducing\nvoltage-dependent currents into the dendrite can both prevent saturation and amplify these\nsmall currents, effectively turning the apical tree into a better \"wire.\"\nFinally, we reexamined the effect of shunting inhibition. While it acts in a divisive manner\nin subthreshold cable models, it behaves in a linear, subtractive manner when spiking is\naccounted for. This is due to the fact that the average somatic membrane potential does\nnot exceed the voltage threshold, limiting the maximal inhibitory current. Thus, at least for\nmodels where inhibition acts in a maintained manner at or close to the cell body, shunting\ninhibition does not implement a divisive normalization operation as postulated by many.\nHowever, at the next level of organization, the small recurrently connected network,\nlinear inhibition can act again in a divisive manner (Douglas et al., 1995). We conclude\nwith the observation that a biophysical mechanism that implements one type of operation\nin the subthreshold domain might implement a quite different one in the suprathreshold\ndomain. The morale is that it is dangerous in neurobiology to study any one mechanism\nat only a single, isolated level of complexity. Phenomena at multiple levels, such as ionic\nchannel, synapse, dendrite, neuron, small network, and so on, interact in highly nonlinear\nand nonintuitive ways. This is, of course, a characteristic of any evolved systems and makes\nthem so interesting."}
{"text": "===== Page 27 =====\nVOLTAGE-DEPENDENT EVENTS\nIN THE DENDRITIC TREE\nSo far, we worked under the convenient fiction that active, voltage-dependent membrane\nconductances are confined to the spike initiation zone at or close to the cell body and that the\ndendritic tree is essentially passive. Under the influence of one-dimensional passive cable\ntheory, as refined by Rail and his school (Chaps. 2 and 3), the passive model of dendritic\nintegration of synaptic inputs has become dominant and is taught in all the textbooks.\nParadoxically, from the earliest days of intracellular recordings from the fat dendrites of\nspinal cord motoneurons with the aid of glass microelectrodes, active dendritic responses\nhad been witnessed (Brock, Coombs, and Eccles, 1952; Eccles, Libet, and Young, 1958).\nToday, there exists overwhelming evidence for a host of voltage-dependent sodium and\ncalcium-conductances in the dendritic tree. In the following section we summarize the\nexperimental evidence and discuss current biophysical modeling efforts focusing on the\nquestion of the existence and genesis of fast all-or-none electrical events in the dendrites.\nWe then turn toward possible functional roles of active dendritic processing.\nOne word of advice. It has been argued that linear cable theory as applied to dendrites\nand taught in the first chapters of this book is irrelevant in the face of all this evidence for\nactive processing and can be relegated to the dustbin. However, this would be a mistake.\nUnder many physiological conditions these nonlinearities will not be relevant. Even if they\nare, the resistive and capacitive cable properties of the dendrites profoundly influence the\ninitiation and propagation of dendritic action potentials and other active phenomena. Thus,\nfor a complete understanding of the events in active dendritic trees we need to be thoroughly\nversed in cable theory.\n19.1 Experimental Evidence for Voltage-Dependent\nDendritic Membrane Conductances\nThe issue of dendritic all-or-none electrical events must be seen as separate from the\nbroader question of the existence and nature of active, that is, voltage-dependent, membrane\nconductances in the dendritic tree.\n428\n19"}
{"text": "===== Page 14 =====\n442 \n. \nVOLTAGE-DEPENDENT EVENTS IN THE DENDRITIC TREE\nFig. 19.7 SUBMILLISECOND COINCIDENCE DETECTION \nThe model of Softky (1995) responds\nsensitively to jitter in the arrival times of synaptic input. This mechanism could be exploited to encode\ninformation in the relative spiking times of a group of neurons. (A) If all basal dendrites are endowed\nwith weak Hodgkin-Huxley conductances and with the three synaptic elements illustrated in Fig. 19.6\nthat are activated independently of each other at 25 Hz, the somatic EPSPs due to the dendritic spikes\nsummate, causing the cell to fire vigorously at 55 Hz. Note that one \"synaptic event\" here implies two\nsimultaneously activated excitatory inputs followed 1 msec later by inhibition. (B) Adding jitter to the\nactivation times of all synapses (the jitter is drawn from a Gaussian distribution centered at zero and with\na standard deviation of σ) eliminates dendritic spikes, causing the cell to fire only weakly (illustrated\nhere for σ = 0.5 msec). (C) Output firing rate as a function of σ; an average jitter of 0.27 msec in\nall three synaptic activation times causes the output to drop twofold. Reprinted by permission from\nSoftky (1995).\n19.3.4 Nonlinear Spatial Synaptic Interactions Using Active Currents\nWhile the previous section dealt with temporal interaction among synaptic inputs in an\nactive dendritic tree, let us now turn toward spatial interactions.\nChapter 5 treated nonlinear interaction among synaptic inputs. Due to the dependency\nof the NMDA membrane conductance on the postsynaptic membrane potential (Eq. 4.6),\nthe EPSP in response to the simultaneous (or near-simultaneous) activation of two or more\nNMDA synapses can be much larger than the sum of the EPSPs due to the activation\nof each input by itself. The closer these synapses are spaced to each other, the larger\nthe EPSP (Fig. 19.8B). Cooperativity among NMDA synapses needs to be contrasted to"}
{"text": "===== Page 15 =====\n19.3 Synaptic Input into Active Dendrites: Functional Considerations\n443\nthe suppressive interactions among neighboring voltage-independent synapses in a passive\ndendritic tree. Due to the reduction in the driving potential and the absence of any mitigating\nincreases in the synaptic-induced conductance change, the resultant EPSP will always be\nless than the sum of the individual synapses (Fig. 19.8A). Mel (1992, 1993) shows how\nthis sensitivity to synaptic placement can be exploited to implement a robust multiplicative\noperation in the dendritic tree.\nHow does the addition of voltage-dependent dendritic calcium and sodium conductances\nimpact the neuron's sensitivity to spatial clustering of synapses? Mel (1993) investigates\nthis problem by randomly distributing a total of 100 synapses in clusters of k synapses each\nover the dendritic tree, where k ranges from 1 to 15 (see also Sec. 5.2.1). Each synapse is\nactivated independently by a 100 Hz Poisson point process using one of several different\npostsynaptic scenarios. In one, all synaptic input is of the AMPA type, which is voltage\nindependent, but the dendritic tree of the layer 5 pyramidal cell (Fig. 5.6) is endowed with\nFig. 19.8 SPATIAL CLUSTERING OF SYNAPSES IN AN ACTIVE DENDRITIC ARBOR \nEffect of spatial\nproximity of synaptic input on the cellular response in the presence of voltage-dependent dendritic\nmembrane conductances. Mel (1993) distributes 100 synapses in 100/fc clusters of k synapses each\nthroughout the layer 5 pyramidal cell (Fig. 5.6) and activates each of them independently with a\n100-Hz presynaptic train of spikes. The cellular response, expressed as the number of somatic action\npotentials within 100 msec, is averaged over 100 different random synaptic distributions. (A) For\na passive dendritic tree and voltage-independent synaptic input, synaptic saturation dictates that\nspreading the synapses as far away from each other will maximize the cell's response. (B) Shifting\n90% of the synaptic conductance to NMDA receptors favors activating spatially adjacent synapses.\n(C) In the presence of gca and #AHP in the distal portion of the tree, a similar cluster sensitivity results\n(here for AMPA input only). (D) This synaptic input frequently triggers a slow all-or-none dendritic\nevent as portrayed here. (E) Replacing the slow dendritic gca with fast Hodgkin-Huxley-like sodium\nspikes makes little difference as does the scenario summarized in (F), a combination of calcium\nconductances in the distal dendritic tree with sodium spikes at dendritic branch points in the presence\nof strong synaptic input of the NMDA type. Reprinted in modified form by permission from Mel\n(1993)."}
{"text": "===== Page 16 =====\n444 \n· \nVOLTAGE-DEPENDENT EVENTS IN THE DENDRITIC TREE\na L-type high-threshold, noninactivating calcium conductance gca m conjunction with a\ncalcium- and voltage-dependent potassium conductance #AHP (as well as radial diffusion\nand calcium buffering). These conductances are evenly distributed throughout the tree with\nthe exception of the most proximal 60 μιη stretch of each basal dendrite and the first 450 μπι\nlong section of the apical shaft, which are devoid of any such conductances. Under these\ncircumstances, a sufficiently strong synaptic input can trigger a slow all-or-none dendritic\nevent (Fig. 19.8D).\nThis gives rise to a very pronounced cluster sensitivity (Fig. 19.8C). Arranging the\nsynaptic input into 20 clusters of five synapses each quadruples the cell's spiking response\nat the soma compared to clustering the same synapses in 33 groups of three synapses each.\nReplacing the dendritic gca by a rapidly activating sodium conductance in conjunction\nwith a delayed-rectifier potassium conductance yields fast dendritic spikes that lead to an\ninverted U-form cluster dependency (Fig. 19.8E). Finally, a last block of trials consists\nof the slow and distal calcium-conductance scenario, coupled with fast Hodgkin-Huxley\nmembrane conductances at all dendritic branch points (distal 40 μιη segment of each distal\napical branch and the distal 40 μιη of each apical and basal dendritic tip) and NMDA\nsynaptic input (Fig. 19.8F). These and many more permutations of parameters (different\nconductances with different voltage dependencies, different patterns of synaptic input, and\nso on) always yield the same robust sensitivity of the cell's response to the spatial patterning\nof synaptic input.\nMel, Ruderman, and Archie (1998) demonstrate in a convincing manner how this\nnonlinear synaptic interaction can implement a crucial operation found throughout sensory\nsystems. The best characterized neurons in the mammalian visual cortex are simple cells,\nwhose distinguishing feature is that they respond optimally to visual stimuli (such as a\nbar) having a particular orientation (e.g., horizontal) at some position within their narrow\nreceptive field (Hubel and Wiesel, 1962). A complex cell differs in a number of ways from\na simple cell, chiefly in that it shows orientation tuning at many locations throughout its\nreceptive field, which is much larger than the optimal bar stimulus. The transition from\nsimple to complex cells is common to many systems and occurs anytime some selectivity,\nbe it for disparity, face detection, and so on, must be generalized across space.\nThe original Hubel and Wiesel (1962) model as well as subsequent models (Movshon,\nThompson, and Tolhurst, 1978; Heeger, 1992) posit that this tuning across space is accom-\nplished by pooling the output of a set of simple cells with different receptive field positions.\nHowever, much evidence has accumulated challenging the notion of a purely hierarchical\nprocessing. In particular, complex cells receive direct, monosynaptic input from the lateral\ngeniculate nucleus (LeVay and Gilbert, 1976; Ferster and Lindstrom, 1983; see also Malpeli\net al., 1981; Ghose, Freeman, and Ohzawa, 1994). Mel and colleagues (1998) show how\nthe center-surround receptive field of geniculate cells can be transformed in a single step\ninto oriented complex cells using intradendritic computations.\nThe principal idea is similar to Mel's (1993) study of pattern discrimination using\nsynaptic clustering (Sec. 5.2.2 and Fig. 5.9). As before, the layer 5 pyramidal cell receives\nmixed AMPA/NMDA excitatory synaptic input from a number of geniculate cells (either\n100 or 1000). The dendritic membrane of the cell incorporates the standard g^a and £κ\nHodgkin-Huxley conductances, whose density is one quarter of the somatic density, in\nqualitative agreement with Stuart and Sakmann's (1994) findings discussed above. Mel,\nRuderman, and Archie (1998) mimic a developmental learning rule that preferentially\nencourages the growth of geniculo-cortical synapses into the neighborhood of those patches\nof dendritic membrane that already show strong depolarization (Shatz, 1990; Cline, 1991)."}
{"text": "===== Page 17 =====\n19.3 Synaptic Input into Active Dendritas: Functional Considerations\n445\nFig. 19.9 ORIENTATION TUNING IN A COMPLEX CELL IN VISUAL CORTEX \nThe sensitivity of\nthe firing rate to the exact spatial arrangement of synapses in the dendritic tree is exploited by\nMel, Ruderman, and Archie (1998) to implement nonlinear superposition. This is exemplified by\nreplicating orientation selectivity at multiple locations in the receptive field of a complex cell in the\nprimary visual cortex directly excited by neurons in the lateral geniculate nucleus. The dendritic tree\nof the pyramidal cell includes sodium- and potassium-dependent Hodgkin-Huxley conductances.\n(A) Input is provided by 1024 randomly sampled geniculate cells that activate mixed—NMDA and\nAMPA—synapses (white squares; the black squares indicate inhibited cells). (B) A developmental\nrule for mapping these inputs onto the cortical cell assures that geniculate input activated by a vertical\nbar anywhere in the receptive field is organized in spatial clusters along the dendrites, while the\nsynapses activated by an obliquely oriented input activate an equal number of synapses that are\nspread homogeneously throughout the dendritic tree (the 150 most active synapses are shown here).\nThis local activity could be caused, for instance, by simultaneous active synapses on a\nneighboring patch of dendrite. An outcome of learning rules of this type is that strongly\ncorrelated inputs are more likely to form synapses at nearby sites in the dendritic tree.\nIn the case of Fig. 19.9, geniculate input is correlated with respect to an ensemble of\nvertically oriented bars form neighboring synapses (within less than 100 μηι of each other\nalong the dendrite) while geniculate inputs triggered by diagonally oriented bars are spread\nthroughout the dendritic tree. This results in the formation of clusters corresponding to a\nset of geniculate afferents aligned in such a way as to be maximally activated by a vertical\nbar and least by a horizontal one.\nWith many such clusters, the cell shows orientation selectivity throughout its receptive\nfield (Fig. 19.9D). The bar stimulus will always activate roughly the same number of\nsynapses; but for nonoptimal orientations these are spread diffusely throughout the cell\nand their postsynaptic response is reduced. That orientation tuning is mediated by cluster"}
{"text": "===== Page 18 =====\n446 \n· \nVOLTAGE-DEPENDENT EVENTS IN THE DENDRITIC TREE\nFig. 19.9 CONTINUED (C) The spatial correlation function for synaptic input as a function of\ndistance along the dendrite bears this out: for a vertical bar, synaptic input to the tree is correlated\nin space (up to approximately 100 μηι). This spatial clustering leads to an optimal activation of\nNMDA and voltage-dependent inward currents. Very little such correlation is evident for the oblique\ninput. (D) Averaging the spiking response of the cell for bars of different orientations presented at six\ndifferent spatial positions yields a tuning curve very similar to those recorded in the cat visual cortex\n(Orban, 1984). Spatially scrambling the synapses or blocking the NMDA input and the dendritic\ngNa (in this latter passive scenario, the AMPA conductances were amplified thirtyfold to compensate\nfor the loss of current) eliminates orientation tuning. Reprinted in modified form by permission from\nMel, Ruderman, and Archie (1998).\nsensitivity caused by voltage-dependent conductances can be demonstrated in two ways:\nscrambling the spatial location of all synapses (that is, the mapping from the geniculate to\nthe cortical cell is randomized) or blocking both the NMDA and dendritic gNa conductances,\nwhich eliminates orientation selectivity (Fig. 19.9D). Mel, Ruderman, and Archie (1998)\nconclude that the position independence of orientation tuning of complex cells in the primary\nvisual cortex could originate within the dendritic tree of a single cell, rather than by the\nconvergence of the output of many cortical simple cells as usually assumed. Functionally,\nsuch a neuron corresponds to a polynomial threshold unit, computationally more powerful\nthan the standard linear threshold unit (Sec. 14.4.2).\nNote that this sort of nonlinear superposition might well be commonplace. Take as an\nexample neurons in the most anterior portion of the inferotemporal cortex that strongly fire to\na unique view of a three-dimensional object, such as a paperclip or a face, almost anywhere\nwithin the monkey's visual field (Logothetis and Pauls, 1995). They could achieve this\nfeat by exploiting the same biophysical mechanism: nonlinear interaction among adjacent\nsynapses in an active dendritic tree.\n19.3.5 Graded Amplification of Distal Synaptic Input\nIn the previous chapter, we derived an analytical relationship between synaptic input to\none end of a single finite cable and the current necessary to clamp the membrane potential\nat the other terminal (Eq. 18.18). As illustrated in Fig. 18.8, this current saturates once\nthe synaptic input is large enough to depolarize the local membrane potential close to\nthe synaptic reversal potential. This insight was extended to synaptic input into the distal\napical tuft of the layer 5 pyramidal cell (Fig. 18. IOC). The total somatic current that can be"}
{"text": "===== Page 19 =====\n19.3 Synaptic Input into Active Dendrites: Functional Considerations \n· 447\ndelivered by 500 AMPA synapses distributed throughout layers 1,2, and 3 (all the dendrites\npast the first major branch point along the apical tree; see Fig. 18.6) saturates at 0.65 nA,\nwhich is not enough to generate more than a handful of spikes per second at the soma. The\nculprit is the high axial resistance of the apical trunk, causing Vm in the distal dendrite to\nquickly approach the synaptic reversal potential. And this in a cell whose apical trunk is\nquite thick (4.4 μιη diameter in the middle of layer 5; see Fig. 3.7) and has relatively low\ninput impedances (see Table 3.1). The thinner dendrites in a rat pyramidal cell (Larkman\nand Mason, 1990) would give rise to even smaller values of /syn,s·\nIt seems odd that a large fraction of the dendrite would be dedicated to synaptic input (in\nthis case, originating to a large extent in higher cortical areas) that only has a minor effect on\nthe output of the cell. Following earlier suggestions by Spencer and Kandel (1961) and more\nrecent authors, Bernander, Douglas, and Koch (1994) investigated how voltage-dependent\nmembrane conductances in the dendrites could be used to amplify distal synaptic input in\na continuous manner.\nLet us first consider the principle of the idea with the aid of a highly simplified model\nof a neuron (Fig. 19.10A). The three compartments crudely mimic the distal synaptic\ninput (at right), the intervening apical tree (middle compartment), and the cell body, whose\nmembrane potential is clamped to the average potential (Vm) during spiking (Fig. 17.10B).\nIn the absence of any voltage-dependent membrane conductance, 7syniS will saturate (as in\nEq. 18.18).\nBecause such saturation reduces the dynamic bandwidth of the input, Bernander, Dou-\nglas, and Koch (1994) argue that a voltage-dependent potassium conductance #κ needs to\nbe introduced within the synaptic compartment in order to counteract saturation. The current\ndoes this by providing a hyperpolarizing current proportional to the degree of membrane\ndepolarization. We can derive the voltage dependency of this current by demanding that\n(19.1)\nIn the absence of #κ, ¿syn.s equals /syn, the current flowing through the synapse (Fig. 19.10B).\nKirchhoff 's current conservation law demands that the current supplied by g& must equal\nthe difference between these two currents, that is, /κ = /Syn,s — Ayn-\nFrom Fig. 19.10B this difference current has a parabolic shape: in the absence of synaptic\ninput ¿>K = 0; likewise around gsyn — 180 nS, when the curves overlap again. At\nintermediate values of gsyn, activation of gK makes up for the discrepancy between the two\ncurves. Combining these equations, we find that the closed-form solution for the voltage\ndependency of gn is a simple fractional polynomial (Fig. 19.IOC),\n(19.2)\nAs the membrane potential depolarizes, gn activates and opposes this increase. At large\nvalues of V, §κ inactivates, similar to the transient inactivating A type potassium current\n(Sec. 9.2.1), leading to an overall linear relationship between input and output, with κ\\ =\n5 mV (Fig. 19.10B). Amplification of this \"linearized\" current, that is,\n(19.3)\ncan be achieved by an inward current between the synaptic site and the soma. Depending\non the local membrane potential, it will generate additional current across the membrane.\nThe derivation of the voltage dependency of the associated noninactivating conductance\ngca is tedious but straightforward (done in Fig. 19. IOC for ΚΊ = 2). Note that different"}
{"text": "===== Page 20 =====\n448 \n· \nVOLTAGE-DEPENDENT EVENTS IN THE DENDRITIC TREE\nFig. 19.10 PRINCIPLE OF DEN-\nDRITIC AMPLIFICATION Lin-\nearization and amplification in\nthe three-compartment \nmodel\nshown in (A) with Vsoma \n=\n—50 mV. All capacitances have\nbeen neglected. (B) \"Somatic\"\ncurrent /soma flowing across the\nconductance g in response to\na sustained synaptic input gsyn.\nDue to synaptic saturation in the\nrightmost compartment, the cur-\nrent is sublinear (dashed \"Pas-\nsive\" curve). The introduction of\nan inactivating potassium con-\nductance gK with the voltage de-\npendency shown in (C) linearizes\nAyn.s as a function of gsyn (dotted\n\"Linearized\" curve). Positioning\nan inward current (here gca) be-\ntween the synaptic input site and\nthe soma amplifies the synap-\ntic response (thin \"Linearized\nand amplified\" curve). Reprinted\nby permission from Bernander,\nDouglas, and Koch (1994).\ncombinations of κ\\ and KI in Eq. 19.3 give rise to different voltage dependencies for the\npotassium and calcium currents, but to the same current as long as the product κ\\ ΚΊ remains\nconstant.\nBy analogy with this simple model, the saturating behavior of synaptic input into the\napical tuft for the pyramidal cell of Fig. 3.7 can be eliminated by evenly spreading a\nnoninactivating potassium conductance throughout layers 1,2, and 3. The determination of\nits exact voltage dependency is carried out using an iterative learning scheme that modifies\ngvi(Vm), recomputes /syn,s, and changes its activation curve proportional to the difference\nbetween the actual somatic current and the desired linear one (Bernander, 1993). For reasons\nof stability, potassium inactivation was not considered here (unlike the simple model of\nFig. 19. IOC). Subsequently, an inward current is inserted into the membrane around the\nfirst branch point of the apical dendrite (close to the layer 3 to layer 4 transition) and"}
{"text": "===== Page 21 =====\n19.3 Synaptic Input into Active Dendrites: Functional Considerations\n449\nits activation curve is derived (Fig. 19.1 IB); it resembles a noninactivating high-threshold\ncalcium conductance. Imaging experiments have visualized such a localized band of calcium\nactivity 500 μηι away from the cell body in neocortical pyramidal cells (Yuste, Delaney,\nand Tank, 1994).\nIn the presence of these two dendritic currents, distal synaptic input into the model cell\ndelivers up to 1 nA of current to the soma (Fig. 19.11 A), triggering 40 spikes per second in\nthe absence of any other input (Fig. 19.11C). This sizable contribution is specific to apical\ntuft input (Bernander, Douglas, and Koch, 1994). Basal input will not be amplified since\nthe calcium conductance is too far removed from the soma. The effect of the linearizing\nand amplifying currents remains hidden from the cell body and does not show up in the\ncell's /-/ curve.\nFig. 19.11 DENDRITIC AMPLIFICATION IN A\nPYRAMIDAL CELL \nThe dynamic output range\nof the synaptic input into the apical tuft of\nthe layer 5 pyramidal cell can be expanded via\nnoninactivating voltage-dependent calcium and\npotassium conductances (Bernander, Douglas,\nand Koch, 1994). (A) The total somatic current\ndelivered by 500 voltage-independent synaptic\ninputs into layers 1, 2, and 3 saturates quickly\n(see also Fig. 18.10). Spreading a potassium con-\nductance gn into all compartments in the api-\ncal tuft (beyond the major branching point of\nthe apical dendrite in Figs. 3.7 and 18.6) and\nadding a hot spot of calcium conductances gca at\nthis branch point (with the voltage dependency\nshown in B) linearizes and amplifies the somatic\ncurrent /soma (dashed line in A). The final input-\noutput relationship of the cell in (C) (computed\nby either clamping the soma to the mean so-\nmatic membrane potential or without restraining\nVsoma in any way) reveals that the distal input\ncan modulate the cell's output firing rate in a sig-\nnificant manner. The function of these dendritic\noutward and inward currents is to linearize and\namplify distal synaptic input. In this view, all-or-\nnone calcium spikes are a mere epiphenomenon.\nReprinted by permission from Bernander, Dou-\nglas, and Koch (1994)."}
{"text": "===== Page 22 =====\n450 \n· \nVOLTAGE-DEPENDENT EVENTS IN THE DENDRITIC TREE\nThe model treated here is quite simple and neglects many important aspects, such\nas calcium current inactivation, the detailed dynamics of the conductances, the effect of\nmultiple calcium and potassium or other conductances, and so on. Furthermore, the exact\ndistribution and voltage dependency of the two currents are only derived in a heuristic\nad hoc manner. Yet the conclusion is similar to that of a biophysically more sophisticated\nsimulation of active currents in cerebellar Purkinje cells (DeSchutter and Bower, 1994a,b):\nvoltage-dependent inward dendritic currents can counteract the attenuation of distal synaptic\ninputs. In the Purkinje cell, this renders the somatic EPSP independent of the position of\nthe synaptic input.\nIn this view, any all-or-none voltage events mediated by the amplifying inward current\nin the dendrites reflect a mere exuberance, an epiphenomenon. What is relevant is that the\nlimited range of the current delivered to the soma is expanded by linearization and amplified\nin a graded manner.\nA much more elegant and principled way of introducing a multitude of voltage-dependent\ndendritic currents is based on local, unsupervised learning rules that adjust the dynamics\nand the voltage dependency of the conductances. Such algorithms are related to the synaptic\nlearning rules treated in Chap. 13 and achieve homeostasis by keeping the average calcium\nconcentration at some setpoint (LeMasson, Marder, and Abbott, 1993) or by maximizing a\nquantity such as the mutual information between synaptic input and the output of the cell\n(Bell, 1992). Such learning schemes can operate continuously throughout the lifetime of\nthe cell.\n19.4 Recapitulation\nIt was held for a long time that voltage-dependent membrane conductances were restricted\nto the cell body and axon. Given the overwhelming evidence for the presence of a variety\nof sodium, calcium, and potassium conductances in the dendritic tree it is imperative that\nsuch active membranes be included into biophysically realistic compartmental models. We\nmust also understand the functional role of such conductances.\nBecause of the great diversity of voltage-dependent currents found in so many different\ncell types throughout the animal kingdom, it is difficult to summarize the experimental\ndata. The dendritic membrane of many neurons, in particular pyramidal cells, contains a\nrelatively low but homogeneous distribution of ionic channels. This is enough to support\nthe propagation of spikes from the axon and soma back into the dendritic arbor. That the\nspike is initiated in the axon and not locally in the dendrite can be explained by the fact that\nthe difference in voltage threshold for spike initiation in the dendrite and Vih in the axon\n(due to much higher densities of active channels and/or intrinsic activation curves that are\nshifted toward more hyperpolarized values) is less than the voltage attenuation from the\ndendrite to the site of spike initiation\nUnder certain conditions, both rapid (1 msec) all-or-none sodium spikes and slower (10-\n50 msec) calcium-based action potentials can be observed in the dendritic tree. These may\nor may not propagate to the soma. The extent to which faster or slower spikes are initiated\nunder physiological conditions in the dendrites and propagate to the soma remains unclear.\nNumerous specific functions have been ascribed to active dendritic conductances, some\ndependent on dendritic spikes while others work in the absence of action potentials. Among\nthe former resides the hypothesis that the timing of the back-propagated spike relative to the\ntiming of any near-simultaneous presynaptic input is a crucial variable controlling whether"}
{"text": "===== Page 23 =====\n19.4 Recapitulation · 451\nthe weight of the associated synapse decreases or increases. A number of researchers\nsimulated the ability of locally generated spikes in dendrites and spines to instantiate various\nlogical operations as well as temporal coincidence detection.\nTwo quite plausible proposals for the functional role of active inward currents in the den-\ndrites see them as providing the biophysical substrate either to implement a multiplicative-\nlike operation in a distributed fashion among spatially clustered groups of synapses (the\nneuron as a sigma-pi element) or to linearize and amplify distal synaptic input selectively\nso that this input can modulate the cell's discharge more effectively (dendritic amplifier).\nAny such proposal raises many more questions than it answers, one of the most pertinent\none being the question of the necessary developmental and learning rules that can assemble\nall of this hardware with the required temporal and spatial specificity (that is, inserting a\ncertain number of channels of a specific type into a specific location at some subneuronal\nsite). While a passive dendritic tree would require much less specificity to \"wire up,\" its\ncomputational abilities are far less than those of an active dendritic arbor."}
{"text": "===== Page 1 =====\nTHE MEMBRANE EQUATION\nAny physical or biophysical mechanism instantiating an information processing system that\nneeds to survive in the real world must obey several constraints: (1) it must operate at high\nspeeds, (2) it must have a rich repertoire of computational primitives, with the ability to\nimplement a variety of linear and nonlinear, high-gain, operations, and (3) it must interface\nwith the physical world—in the sense of being able to represent sensory input patterns\naccurately and translate the result of the computations into action, that is motor output\n(Keyes, 1985).\nThe membrane potential is the one physical variable within the nervous system that\nfulfills these three requirements: it can vary rapidly over large distances (e.g., an action\npotential changes the potential by 100 mV within 1 msec, propagating up to 1 cm or\nmore down an axon within that time), and the membrane potential controls a vast number\nof nonlinear gates—ionic channels—that provide a very rich substrate for implementing\nnonlinear operations. These channels transduce visual, tactile, auditory, and olfactory stimuli\ninto thanges of the membrane potential, and such voltage changes back into the release of\nneurotransmitters or the contraction of muscles.\nThis is not to deny that ionic fluxes, or chemical interactions of various substances with\neach other, are not crucial to the working of the brain. They are, and we will study some\nof these mechanisms in Chap. 11. Yet the membrane potential is the incisive variable that\nserves as primary vehicle for the neuronal operations underlying rapid computations—at\nthe fraction of a second time scale—in the brain.\nWe will introduce the reader in a very gentle manner to the electrical properties of nerve\ncells by starting off with the very simplest of all neuronal models, consisting of nothing more\nthan a resistance and a capacitance (a so-called RC circuit). Yet endowed with synaptic input,\nthis model can already implement a critical nonlinear operation, divisive normalization and\ngain control.\n1.1 Structure of the Passive Neuronal Membrane\nAs a starting point, we choose a so-called point representation of a neuron. Here, the\nspatial dependency of the neuron is reduced to a single point or compartment. Such an\n5\n1"}
{"text": "===== Page 2 =====\n6 \n• \nTHE MEMBRANE EQUATION\napproximation would be valid, for instance, if we were investigating a small, spherical cell\nwithout a significant dendritic tree.\n1.1.1 Resting Potential\nThe first thing we notice once we managed to penetrate into this cell with a wire from\nwhich we can record (termed an intracellular microelectrode) is the existence of an electrical\npotential across this membrane. Such experiments, carried out in the late 1930s by Cole\nand Curtis (1936) in Woods Hole, Massachusetts, and by Hodgkin and Huxley (1939) on\nthe other side of the Atlantic, demonstrated that almost always, the membrane potential,\ndenned as the difference between the intracellular and the extracellular potentials, or\n(1-1)\nis negative. Here t stands for time. In particular, at rest, all cells, whether neurons, glia or\nmuscle cells, have a negative resting potential, symbolized throughout the book as Vrest.\nDepending on the circumstances, it can be as high as —30 mV or as low as —90 mV. Note\nthat when we say the cell is at \"rest,\" it is actually in a state of dynamic equilibrium. Ionic\ncurrents are flowing across the membrane, but they balance each other, in such a manner\nthat the net current flowing across the membrane is zero. Maintaining this equilibrium is a\nmajor power expenditure for the nervous system. Half of the metabolic energy consumed\nby a mammalian brain has been estimated to be due to the membrane-bound pumps that\nare responsible for the upkeep of the underlying ionic gradients (Ames, 1997).\nThe origin of Vrest lies in the differential distribution of ions across the membrane, which\nwe do not further describe here (see Sec. 4.4 and Hille, 1992). Vrest need not necessarily\nbe fixed. Indeed, we will discuss in Sec. 18.3 conditions under which a network of cortical\ncells can dynamically adjust their resting potentials.\n1.1.2 Membrane Capacity\nWhat is the nature of the membrane separating the intracellular cytoplasm from the extra-\ncellular milieu (Fig. 1.1)? The two basic constitutive elements of biological membranes,\nwhether from the nervous system or from nonneuronal tissues such as muscle or red blood\ncells, whether prokaryotic or eukaryotic, are proteins and lipids (Gennis, 1989).\nThe backbone of the membrane is made of two layers of phospholipid molecules, with\ntheir polar heads facing the intracellular cytoplasm and the extracellular space, thereby\nseparating the internal and external conducting solutions by a 30-50-A-thin insulating\nlayer. We know that whenever a thin insulator is keeping charges apart, it will act like a\ncapacitance. The capacitance C is a measure of how much charge Q needs to be distributed\nacross the membrane in order for a certain potential Vm to build up. Or, conversely, the\nmembrane potential Vm allows the capacitance to build up a charge Q on both sides of the\nmembrane, with\n(1.2)\nIn membrane biophysics, the capacitance is usually specified in terms of the specific\nmembrane capacitance Cm, in units of microfarads per square centimeter of membrane area\n(/iF/cm2). The actual value of C can be obtained by multiplying Cm by the total membrane\narea. The thickness and the dielectric constant of the bilipid layer determine the numerical\nvalue of Cm. For the simplest type of capacitance formed by two parallel plates, Cm scales"}
{"text": "===== Page 3 =====\n1.1 Structure of the Passive Neuronal Membrane \n• \n7\nFig. 1.1 NATURE OF THE PASSIVE NEURONAL MEMBRANE \n(A) Schematic representation of a\nsmall patch of membrane of the types enclosing all cells. The 30-50 A thin bilayer of lipids isolates the\nextracellular side from the intracellular one. From an electrical point of view, the resultant separation\nof charge across the membrane acts akin to a capacitance. Proteins inserted into the membrane, here\nionic channels, provide a conduit through the membrane. Reprinted by permission from Hille (1992).\n(B) Associated lumped electrical circuit for this patch, consisting of a capacitance and a resistance\nin series with a battery. The resistance mimics the behavior of voltage-independent ionic channels\ninserted throughout the membrane and the battery accounts for the cell's resting potential Vrest.\ninversely with the thickness separating the charges (the thinner the distance between the\ntwo plates, the stronger the mutual attraction of the charges across the insulating material).\nAs discussed in Appendix A, the specific capacitance per unit area of biological membranes\nis between 0.7 and 1 /uF/cm2. For the sake of convenience, we adopt the latter, simple to\nremember, value. This implies that a spherical cell of 5-/zm radius with a resting potential\nof —70 mV stores about —0.22x 10~12 coulomb of charge just below the membrane and\nan equal but opposite amount of charge outside.\nWhen the voltage across the capacitance changes, a current will flow. This capacitive\ncurrent, which moves on or off the capacitance, is obtained by differentiating Eq. 1.2 with\nrespect to time (remember that current is the amount of charge flowing per time),\n(1.3)\nFor a fixed current, the existence of the membrane capacitance imposes a constraint on how\nrapidly Vm can change in response to this current; the larger the capacitance, the slower the\nresultant voltage change.\nIt is important to realize that there is never any actual movement of charge across the\ninsulating membrane. When the voltage changes with time, the charge changes and a current\nwill flow, in accordance with Eq. 1.3, but never directly across the capacitance. The charge\nmerely redistributes itself across the two sides by way of the rest of the circuit.\nCan any current flow directly across the bilipid layers? As detailed in Appendix A, the\nextremely high resistivity of the lipids prevents passages of any significant amount of charge\nacross the membrane. Indeed, the specific resistivity of the membrane is approximately one\nbillion times higher than that of the intracellular cytoplasm. Thus, from an electrical point\nof view, the properties of the membrane can be satisfactorily described by a sole element:\na capacitance."}
{"text": "===== Page 4 =====\n8 \n• \nTHE MEMBRANE EQUATION\n1.1.3 Membrane Resistance\nWith no other components around, life would indeed be dull. What endows a large collection\nof squishy cells with the ability to move and to think are the all-important proteins embedded\nwithin the membrane. Indeed, they frequently penetrate the membrane, allowing ions to pass\nfrom one side to the other (Fig. 1.1). Protein molecules, making up anywhere from 20 to 80%\n(dry weight) of the membrane, subserve an enormous range of specific cellular functions,\nincluding ionic channels, enzymes, pumps, and receptors. They act as doors or gates in the\nlipid barrier through which particular information or substances can be transferred from\none side to the other. As we shall see later on, a great variety of such \"gates\" exists, with\ndifferent keys to open them. For now, we are interested in those membrane proteins that act\nas ionic channels or pores, enabling ions to travel from one side of the membrane to the\nother. We will discuss the molecular nature of these channels in more detail in Chap. 8.\nFor now, we will summarily describe the current flow through these channels by a simple\nlinear resistance R. Since we also have to account for the resting potential of the cell, the\nsimplest electrical description of a small piece of membrane includes three elements, C, R,\nand Vrest (Fig. 1.1). Such a circuit describes a passive membrane in contrast to quasi-active\nand active membranes, which contain, respectively, linear, inductance-like, and nonlinear\nvoltage-dependent membrane components. For obvious reasons, it is also sometimes known\nas an RC circuit. Fortuitously, the membranes of quite a few cells can be mimicked by such\nRC circuits, at least under some limited conditions.\nThe membrane resistance is usually specified in terms of the specific membrane resis-\ntance Rm, expressed in terms of resistance times unit area (in units of £2 • cm2). R is\nobtained by dividing Rm by the area of the membrane being considered. The inverse of Rm\nis known as the passive conductance per unit area of dendritic membrane or, for short, as\nthe specific leak conductance Gm = !//?„, and is measured in units of Siemens per square\ncentimeter (S/cm2).\n1.2 A Simple RC Circuit\nLet us now carry out a virtual electrophysiological experiment. Assume that we have\nidentified a small spherical neuron of diameter d and have managed to insert a small\nelectrode into the cell without breaking it up. Under the conditions of our experiments,\nwe have reasons to believe that its membrane acts passively. We would like to know what\nhappens if we inject current /;nj(0 through the microelectrode directly into the cell. This\nelectrode can be thought of as an ideal current source (in contrast to an ideal voltage source,\nsuch as a battery).\nHow can we describe the dynamics of the membrane potential Vm(t) in response to this\ncurrent? The cell membrane can be conceptualized as being made up from many small\nRC circuits (Fig. 1.2A). Because the dimensions of the cell are so small, the electrical\npotential across the membrane is everywhere the same and we can neglect any spatial\ndependencies; physiologists will say the cell is isopotential. This implies that the electrical\nbehavior of the cell can be adequately described by a single RC compartment with a current\nsource (Fig. 1.2B). The net resistance R is determined by the specific membrane resistance\nRm divided by the total membrane area nd2 (since the current can flow out through\nany one part of the membrane) while the total capacitance C is given by Cm times the\nmembrane area."}
{"text": "===== Page 5 =====\n1.2 A Simple RC Circuit \n• \n9\nFig. 1.2 ELECTRICAL STRUCTURE OF A SMALL PASSIVE NEURON \n(A) Equivalent electrical\nmodel of a spherical cell with passive membrane. An intracellular electrode delivers current to the\ncell. By convention, an outward current is positive; thus, the arrow. We assume that the dimensions\nof the cell are small enough so that spatial variations in the membrane potential can be neglected.\n(B) Under these conditions, the cell can be reduced to a single RC compartment in series with an ideal\ncurrent source 7jnj.\nIt is straightforward to describe the dynamics of this circuit by applying Kirchhoff's\ncurrent law, which states that the sum of all currents flowing into or out of any electrical\nnode must be zero (the current cannot disappear, it has to go somewhere). The current across\nthe capacitance is given by expression 1.3. The current through the resistance is given by\nOhm's law,\nNote that the potential across the resistance is not equal to Vm, but to the difference between\nthe membrane potential and the fictive battery Vrest> which accounts for the resting potential.\nDue to conservation of current, the capacitive and resistive currents must be equal to the\nexternal one, or\nA minor, but important detail is the sign of the external current (after all, we could have\nreplaced +/;nj by — 7jnj in Eq. 1.6). By convention, an outward current, that is positive\ncharge flowing from inside the neuron to the outside, is represented as a positive current.\nAn outward going current that is delivered through an intracellular electrode will make the\ninside of the cell more positive; the physiologist says that the cell is depolarized. Conversely,\n(1.4)\n(1.5)\nWith T = RC, with units of £2-F = sec, we can rewrite this as\n(1.6)"}
{"text": "===== Page 6 =====\n10 \n• \nTHE MEMBRANE EQUATION\nan inward directed current supplied by the same electrode, plotted by convention in the\nnegative direction, will make the inside more negative, that is, it will hyperpolarize the\ncell. If the current is not applied from an external source but is generated by a membrane\nconductance, the situation is different (see Chap. 5).\nDue to the existence of the battery Vrest, the electrical diagram in Fig. 1.2B does not,\nformally speaking, constitute a passive circuit, since its current-voltage (/—V) relationship\nis not restricted to the first and third quadrants of the I-V plane. This implies that\npower is needed to maintain this I-V relationship, ultimately supplied by the differential\ndistributions of ions across the membrane. Because the I-V relationship has a nonzero,\npositive derivative for every value of Vm, it is known as an incrementally passive device.\nThis point is not without interest, since it relates to the stability of circuits built using such\ncomponents (Wyatt, 1992). We here do not take a purist point of view, and we will continue\nto refer to a membrane whose equivalent circuit diagram is similar to that of Figs. 1. IB and\n1.2B as passive.\nEquation 1.6 is known as the membrane equation and constitutes a first-order, ordinary\ndifferential equation. With the proper initial conditions, it specifies an unique voltage\ntrajectory. Let us assume that the membrane potential starts off at Vm(t = 0) = Vrest-\nWe can replace this into Eq. 1.6 and see that in the absence of any input (7;nj = 0)\nthis assumption yields dVm/dt = 0, that is, once at Vrest> the system will remain at\nVrest in the absence of any input. This makes perfect sense. So now let us switch on, at\nt = 0, a step of current of constant amplitude /Q. We should remember from the theory of\nordinary differential equations that the most general form of the solution of Eq. 1.6 can be\nexpressed as\n(1.7)\nwhere DO and v\\ depend on the initial conditions. Replacing this into Eq. 1.6 and canceling\nidentical variables on both sides leaves us with\n(1.8)\nWe obtain the value of VQ by imposing the initial condition Vm(t — 0) = t>o + i>i = Vrest.\nDefining the steady-state potential in response to the current as VOQ = RI0, we have solved\nfor the dynamics of Vm for this cell,\n(1.9)\nThis equation tells us that the time course of the deviation of the membrane potential from\nits resting state, that is, Vm (t) — Vrest, is an exponential function in time, with a time constant\nequal to r. Even though the current changed instantaneously from zero to /0, the membrane\npotential cannot follow but plays catch up. This is demonstrated graphically in Fig. 1.3.\nHow slowly Vm changes is determined by the product of the membrane resistance and the\ncapacitance; the larger the capacitance, the larger the current that goes toward charging up\nC. Note that r is independent of the size of the cell,\n(1.10)\nAs we will discuss in considerable detail in later chapters, passive time constants range\nfrom 1 to 2 msec in neurons that are specialized in processing high-fidelity temporal\ninformation to 100 msec or longer for cortical neurons recorded under slice conditions. A"}
{"text": "===== Page 7 =====\n1.2 A Simple PC Circuit\n11\nFig. 1.3 BEHAVIOR OF AN KC\nCIRCUIT (A) Evolution of\nthe membrane potential Vm(t)\nin the single RC compartment\nof Fig. 1.2B when a current\nstep of different amplitudes /o\n(see Eq. 1.9) is switched on\nat / = 0 and turned off at\n100 msec. Initially, the mem-\nbrane potential is at Vrest =\n—70 mV. We here assume\nR = 100 MS2, C = 100 pF,\nT = 10 msec, and four differ-\nent current amplitudes, /o =\n-0.1, 0.1, 0.2, and 0.3 nA. (B)\nNormalized impulse response\nor Green's function (Eq. 1.17)\nassociated with the RC circuit\nof Fig. 1.2B. The voltage Vm(t)\nin response to any current input\n7jnj(0 can be obtained by con-\nvolving this function with the\ninput.\ntypical range for r recorded from cortical pyramidal cells in the living animal1 is between\n10 and 20 msec.\nRemember the origin of the membrane capacitance in the molecular dimensions of the\nbilipid membrane. A thicker membrane would lead to a smaller value for Cm and faster\ntemporal responses.2\nThe final voltage level in response to the current step is R!Q + Vrest (from Ohm's law).\nIf /o > 0, the cell will depolarize (that is, V^ > Vrest), whereas for /0 < 0, the converse\noccurs. The resistance R is also termed the input resistance of the cell; the larger R, the\nlarger the voltage change in response to a fixed current. The input resistance at the cell bodies\nof neurons, obtained by dividing the steady-state voltage change by the current causing it,\nranges from a few megaohms for the very large motoneurons in the spinal cord to hundreds\nof megaohms for cortical spiny stellate cells or cerebellar granule cells.\n1. This is called in vivo. Such experiments need to be distinguished from the cases in which a very thin slice is taken from an\nanimal's brain, placed in a dish, and perfused with a nutrient solution. This would be termed an in vitro experiment.\n2. As an aside to the neuromorphic engineers among us designing analog integrated electronic circuits, Cm — 1 jLiF/cm is\nabout 20 times higher than the specific capacitance obtained by sandwiching a thin layer of silicon dioxide between two layers of\npoly silicon using a standard 2.0 or 1.2 /im CMOS process (Mead, 1989)."}
{"text": "===== Page 8 =====\n12 . \nTHE MEMBRANE EQUATION\nWhat happens if, after the membrane potential reaches its steady-state value VQQ, the\ncurrent is switched off at time f0ff ? An analysis similar to the above shows that the membrane\npotential returns to Vrest with an exponential time course; that is,\n(1.11)\nfor t > toff. (This can be confirmed by placing this solution into Eq. 1.6; see also Fig. 1.3 A.)\nNow that we know the evolution of the membrane potential for a current step, we would\nlike to know the solution in the general case of some time-dependent current input /jnj(0-\nAre we condemned to solve Eq. 1.6 explicitly for every new function /mj(0 that we use?\nFortunately not; because the RC circuit we have been treating here is a shift-invariant, linear\nsystem, we can do much better.\n1.3 RC Circuits as Linear Systems\nLinearity is an important property of certain systems that allows us—in combination\nwith shift invariance—to completely characterize their behavior to any input in terms of\nthe system's impulse response or Green's function (named after a British mathematician\nliving at the beginning of the nineteenth century). Since the issue of linear and nonlinear\nsystems runs like a thread through this monograph, we urge the reader who has forgotten\nthese concepts to quickly skim through Appendix B, which summarizes the most rele-\nvant points.\n1.3.1 Filtering by RC Circuits\nLet us compute the voltage response of the RC circuit of Fig. 1.2B in response to a\ncurrent impulse 8(t). We will simplify matters by only considering the deviation of the\nmembrane potential from its resting state Vrest. Here and throughout the book we use\nV(t) = Vm(t) — Vrest when we are dealing with the potential relative to rest and reserve\nVm(0 for the absolute potential. This transforms Eq. 1.6 into\n(1.12)\nWe can transform this equation into Fourier space, where V(f) corresponds to the Fourier\ntransform of the membrane potential (for a definition, see Appendix B). Remembering that\nthe dV(t)/dt term metamorphoses into i2n V ( f ) V ( P ) , where i2 = — 1, we have\n(1.13)\nA simple way to conceptualize this is to think of the input as a sinusoidal current of\nfrequency /; /jnj(0 = sin(2nft). Since the system is linear, it responds by a sinusoidal\nchange of potential at the same frequency /, but of different amplitude and shifted in time:\nV(t) = A(f) sin(2nft + $(/)). The amplitude of the voltage response at this frequency,\ntermed A(/), is given by\n(1.14)\nand its phase by"}
{"text": "===== Page 9 =====\n1.3 RC Circuits as Linear Systems \n• \n13\n(1.15)\nIn the general case of an arbitrary input current, one can define the complex function A(/)\nas the ratio of the Fourier transform of the voltage transform to the Fourier transform of the\ninjected current,\n(1.16)\nA(f) is usually referred to as the input impedance of the system. Its value for a sustained\nor dc current input, A(f = 0) = R, is known as the input resistance and is a real number.\nIt is standard engineering practice to refer to the inverse of the input impedance as the input\nadmittance and to the inverse of the input resistance as the input conductance in units of\nSiemens (S).\nDoes this definition of A make sense? Let us look at two extreme cases. If we subject\nthe system to a sustained current injection, the change in voltage in response to a sustained\ninput current / is proportional to R, Ohm's law. Conversely, what happens if we use a\nsinusoidal that has a very high frequency /? The amplitude of the voltage change becomes\nless and less since at high frequencies the capacitance essentially acts like a short circuit.\nIn the limit of / -» oo, the impedance goes to zero.\nFor intermediate values of /, the amplitude smoothly interpolates between R and 0.\nIn other words, our circuit acts like a low-pass filter, preferentially responding to slower\nchanging inputs and severely attenuating faster ones: |A(/)| is a strictly monotonically\ndecreasing function of the frequency /.\nExperimentally, the impedance can be obtained by injecting a sinusoidal current of\nfrequency / and measuring the induced voltage at the same frequency. The ratio of the\nvoltage to the current corresponds to | A (/) |. The use of impedances to describe the electrical\nbehavior of neurons and, in particular, of muscle cells has a long tradition going back to the\n1930s (Cole and Curtis, 1936; Falk and Fatt, 1964; Cole, 1972).\nThe result of such a procedure, carried out in a regular firing cell in a slice taken from\nthe visual cortex of the guinea pig, is shown in Fig. 1.4. Carandini and his colleagues\n(1996) injected either sinusoidal currents or a noise stimulus into these cells and recorded\nthe resultant somatic membrane potential (in the presence of spikes). Given their very fast\ntime scale, somatic action potentials do not contribute appreciably to the total power of the\nvoltage signal. Indeed, when stimulating with a sine wave at frequency /, the power of\nthe voltage response at all higher frequencies was only 3.8% (median) of the power of the\nfundamental /. This implies that when judged by the membrane potential and not by the\nfiring rate, and only considering input and output at the soma, at least some cortical cells\ncan be quite well approximated by a linear filter (Carandini et al., 1996).\nThis is surprising, given the presence of numerous voltage-dependent conductances at\nthe soma and in the dendritic tree. It is, however, not uncommon in neurobiology to find\nthat despite of—or, possibly because of—a host of concatenated nonlinearities, the overall\nsystem behaves quite linearly (see Sec. 21.1.3). Sometimes one has the distinct impression\nthat evolution wanted to come up with some overall linear mechanism, despite all the\nexisting nonlinearities.\nWe will study later how adding a simple, absolute voltage threshold to the RC compart-\nment that gives rise to an output spike accounts surprisingly well for the spiking behavior"}
{"text": "===== Page 10 =====\n14 \n• \nTHE MEMBRANE EQUATION\nFig. 1.4 CORTICAL CELLS BEHAVE LIKE AN\nKC CIRCUIT When either noise or sinusoidal\ncurrents are injected into the cell body of regu-\nlarly firing cells in guinea pig visual cortex, the\nmembrane potential can be adequately mod-\neled as resulting from convolving the current\ninput by a low-pass filter of the sort described\nin Eqs. 1.14 and 1.15 (dashed lines; here with\nR = 58.3 Mft and r = 9.3 msec; Vres, =\n-70.7 mV; Carandini et al., 1996). (A) The\namplitude of the filter and (B) its phase. The\nnoise current curve reveals a shallow peak at\naround 8 Hz. We conclude that from the point\nof view of somatic input-output, these cells can\nbe reasonably well described by a single RC\ncompartment. The responses were obtained by\ncomputing the first harmonic of the membrane\npotential response and dividing by the current.\nThe power of the first harmonic was between 9\nand 141 times the power of the higher harmon-\nics. Reprinted by permission from Carandini et\nal., (1996).\nof such cells. This simplest model of a spiking neuron, known as a leaky integrate-and-fire\nunit, is so important that it deserves its own detailed treatment in Chap. 14.\nWe can recover the Green's function h(t)of the RC compartment by applying the inverse\nFourier transform to Eq. 1.13, which results in\n(1-17)\nfor t > 0 and 0 for negative times (the units of the Green's function are ohms per second\n(£2/sec)). Conceptually, the extent of this filter, that is, the temporal duration over which this\nfilter is significantly different from zero, indicates to what extent the distant past influences\nthe present behavior of the system. For a decaying exponential as in an RC circuit, an event\nthat happened three time constants ago (at t = —3r) will have roughly 1/20 the effect of\nsomething that just occurred (Fig. 1.3B). This is expected in a circuit that implements a\nlow-pass operation. Input is integrated in time, with long ago events having exponentially\nless impact than more recent ones.\n1.4 Synaptic Input\nSo far, we have not considered how the output of one neuron provides input to the next\none. Fast communication among two neurons occurs at specialized contact zones, termed\nsynapses. Synapses are the elementary structural and functional units for the construction of"}
{"text": "===== Page 11 =====\n1.4 Synaptic Input \n• \n15\nneuronal circuits. Conventional point-to-point synaptic interactions come in two different\nflavors: electrical synapses—also referred to as gap junctions—and the much more common\nchemical synapses. At about 1 billion chemical synapses per cubic millimeter of cortical\ngrey matter, there are lots of synapses in the nervous system (on the order of 10'5 for a human\nbrain). In order to give the reader an appreciation of this, Fig. 1.5 is a photomicrograph of a\nsmall patch of the monkey retina at the electron-microscopic level, with a large number of\nsynapses visible. Synapses are very complex pieces of machinery that can keep track of their\nhistory of usage over considerable time scales. In this chapter, we introduce fast, voltage\nindependent chemical synapses from the point of view of the postsynaptic cell, deferring a\nmore detailed account of synaptic biophysics, as well as voltage dependent synapses and\nelectrical synapses, to Chap. 4, and an account of their adapting and plastic properties to\nChap. 13.\nUpon activation of a fast, chemical synapse, one can observe a rapid and transient change\nin the postsynaptic potential. Here, postsynaptic simply means that we are observing this\nsignal on the \"far\" or \"output\" side of the synapse; the \"input\" part of a synapse is referred to\nas the presynaptic terminal. When the synapse is an excitatory one, the membrane potential\nrapidly depolarizes, returning more slowly to its resting state: an excitatory postsynaptic\npotential (EPSP) has occurred. Conversely, at an inhibitory synapse, the membrane will\ntypically be transiently hyperpolarized, resulting in an inhibitory postsynaptic potential\n(IPSP). These EPSPs and IPSPs are caused by so-called excitatory and inhibitory postsy-\nnaptic currents (EPSCs and IPSCs), triggered by the spiking activity in the presynaptic cell.\nFigure 1.6 illustrates some of the properties of a population of depolarizing synapses\nbetween the axons of granule cells, also called mossy fibers, and a CA3 hippocampal\nFig. 1.5 SYNAPSES AMONG RETINAL NEU-\nRONS Electron microscopic photograph of a\nfew square micrometers of tissue in the central\nportion of the retina in the monkey. Here a\nmidget bipolar cell (MB) makes two ribbon\nsynapses onto a midget ganglion cell (MG). It\nis surrounded by nine processes belonging to\namacrine cells (Aj to Ag). Some of these feed\nback onto the bipolar cell (e.g., AS), some feed\nforward onto the ganglion cell (e.g., AI ), some\ndo both, and some also contact each other (e.g.,\n\\2 ->• AS). Since neither the bipolar cell nor\nthe amacrine cell processes have been shown\nto generate action potentials, these synapses\nare all of the analog variety, in distinction to\nsynapses in the more central part of the ner-\nvous system that typically transform an action\npotential into a graded, postsynaptic signal.\nReprinted by permission from Calkins and\nSterling (1996)."}
{"text": "===== Page 12 =====\n16\nTHE MEMBRANE EQUATION\nFig. 1.6 A FAST EXCITATORY SYNAPTIC INPUT Excitatory postsynaptic current (EPSC) caused\nby the simultaneous activation of synapses (arrow) made by the mossy fibers onto CAS pyramidal\ncells in the rodent hippocampus (Brown and Johnston, 1983). This classical experiment showed how\na central synapse can be successfully voltage clamped. (A) The voltage-clamp setup stabilizes—via\nelectronic feedback control—the membrane potential at a fixed value. Here four experiments are\nshown, carried out at the holding potentials indicated at the left. The current that is drawn to keep\nthe membrane potential constant, termed the clamp current, corresponds to the negative EPSC. It\nis maximal at negative potentials and reverses sign around zero. The synaptic current rises within\n1 msec to its peak value, decaying to baseline over 20-30 msec. The experiments were carried out in\nthe presence of pharmacological agents that blocked synaptic inhibition. (B) When the peak EPSC\nis plotted against the holding potential, an approximately linear relationship emerges; the regression\nline yields an .x-axis intercept of —1.9 mV and a slope of 20.6 nS. Thus, once the synaptic reversal\npotential is accounted for, Ohm's law appears to be reasonably well obeyed. We conclude that synaptic\ninput is caused by a transient increase in the conductance of the membrane to certain ions. Reprinted\nby permission from Brown and Johnston (1983).\npyramidal cell.3 The figure is taken from an experiment by Brown and Johnston (1983),\nwhich demonstrated for the first time how a synapse within the central nervous system could\nbe voltage clamped. The voltage-clamp technique was previously used on the very large\nsynapse made between the axonal terminals of motoneurons and the muscle, the so-called\nneuromuscular junction (Katz, 1966; Johnston and Wu, 1995). It allows the experimentalist\nto stabilize the membrane potential (via a feedback loop) at some fixed value, irrespective\nof the currents that are flowing across the membrane in response to some stimulus. This\nallows the measurement of EPSCs at various fixed potentials (as in Fig. 1.6). The EPSC\nhas its largest value at a holding potential of —65 mV, becoming progressively smaller and\nvanishing around 0 mV. If the membrane potential is clamped to values more positive than\nzero, the EPSC reverses sign (Fig. 1.6A). When the relationship between the peak current\nand the holding potential is plotted (Fig. 1.6B), the data tend to fall on a straight line that\ngoes through zero around — 1.9 mV and that has a slope of 20.6 nS.\nWhat we can infer from such a plot is that the postsynaptic event is caused by a\ntemporary increase in the membrane conductance, here by a maximal increase of about 20 nS\n3. It should be pointed out that we are here looking at a population of synapses, made very close to the soma of the pyramidal\ncell, thereby minimizing space-clamp problems."}
{"text": "===== Page 13 =====\n1.4 Synaptic Input\n17\n(due to simultaneous activation of numerous synapses) in series with a so-called synoptic\nreversal battery or potential, Esyn = —1.9 mV (since the conductance change is specific\nfor a particular class of ions). Spiking activity in the presynaptic cell triggers, through a\ncomplicated cascade of biophysical events (further discussed in Chap. 4), a conductance\nchange in the membrane of the postsynaptic cell. Typically, the conductance gsyn(0\ntransiently increases within less than 1 msec, before this increase subsides within 5 msec.\nThe equivalent electrical circuit diagram of a synapse embedded into a patch of neuronal\nmembrane is shown in Fig. 1.7A. It is important to understand that from a biophysical,\npostsynaptic point of view, a synapse does not correspond to a fixed current source—in\nthat case the slope of the I—V curve in Fig. 1.6 should have been zero—but to a genuine\nincrease in the membrane conductance. As we will reemphasize throughout the book, this\nbasic feature of the neuronal hardware has a number of important functional consequences.\nBecause of the existence of the synaptic battery, the driving potential across the synapse\nis the difference between Esyn and the membrane potential. The postsynaptic current due\nto a single such synapse is given by Ohm's law\n(1.18)\nInserting this synapse into a patch of membrane (Fig. 1.7A) gives rise to the following\nordinary differential equation (on the basis of Kirchhoff's current law):\n(1-19)\nor, with T = CR, the passive membrane time constant in the absence of any synaptic input,\nwe can transform this into\nFig. 1.7 EQUIVALENT ELECTRICAL CIRCUIT or A FAST CHEMICAL SYNAPSE \n(A) Electrical\nmodel of a fast voltage-independent chemical synapse. This circuit was put forth to explain events\noccurring at the neuromuscular junction by Katz (1969). Remarkably, all fast chemical synapses in\nthe central nervous system, with the exception of the voltage-dependent NMDA receptor-synaptic\ncomplex, operate on the same principle. Activation of the synapse leads to the transient opening of\nionic channels, selective to certain ions. This corresponds to a transient increase in the membrane\nconductance gsyn(t) in series with the synaptic reversal potential Esyn, shown here in parallel with a\npassive membrane patch. (B) If the evoked potential change is small relative to the synaptic reversal\npotential, the synapse can be approximated by a current source of amplitude gSyn(0£syn- In general,\nhowever, this will not be the case and synaptic input must be treated as a conductance change, a fact\nthat has important functional consequences."}
{"text": "===== Page 14 =====\n18 \n• \nTHE MEMBRANE EQUATION\n(1.20)\nFrequently the time course of synaptic input is approximated by a so-called a function.\nIt describes the transient behavior of synaptic input for a number of preparations, such as\nnicotinic input to vertebrate sympathetic ganglion cells or the synaptic input mediated by\nthe mossy fibers (Brown and Johnston, 1983; Williams and Johnston, 1991; Yamada, Koch,\nand Adams, 1998), reasonably well:4\n(1.21)\nWe can now integrate Eq. 1.20 using the same values for fpeak and gpeak> but three different\nvalues for the synaptic reversal potential (Fig. 1.8).\nIf £Syn > Vrest, the synaptic current is inward and—by convention—negative and will\nact to depolarize the membrane. This is the hallmark of an EPSP, as observed at the most\nFig. 1.8 ACTION OF A SINGLE SYNAPSE\nINSERTED INTO A MEMBRANE \nThree\ndifferent types of synaptic inputs and their\ndifferential effect on the membrane po-\ntential. (A) Time course of the synaptic-\ninduced conductance increase, here with\ntPeak=0.5 msec and gpeak \n= \n1 nS\n(Eq. 1.21). The synapse is inserted into a\npatch of membrane (Fig. 1.7A) with R =\n100 MQ, C = 100 pF, and r = 10 msec.\n(B) Postsynaptic current in response to the\nconductance increase if the synaptic rever-\nsal potential is positive (Esyn = 80 mV rel-\native to rest; solid line), negative (£syn =\n—20 mV; dotted line), and zero (so-called\nshunting inhibition; dashed line). By con-\nvention, an inward current that depolarizes\nthe cell is plotted as a negative current.\n(C) Associated EPSP (solid line) and IPSP\n(lower dashed line), relative to Vrest, solved\nby numerical integration of Eq. 1.20. No-\ntice that the time course of the postsynap-\ntic potential is much longer than the time\ncourse of the corresponding postsynaptic\ncurrent due to the low-pass nature of the\nmembrane. Shunting inhibition by itself\ndoes not give rise to any change in potential\n(center dashed line).\n4. gpeak, its peak value, attained at t = f^, defines const = gpeak e'/'peak-"}
{"text": "===== Page 15 =====\n1.5 Synaptic Input Is Nonlinear \n• \n19\ncommon fast, excitatory synapse in the brain, the so-called non-NMDA synapse (named\nafter its insensitivity to Af-methyl-D-aspartic acid) that uses the neurotransmitter glutamate\nwith a reversal potential about 80-100 mV above the resting potential of the cell (for more\ndetails, see Sec. 4.6).\nIf the converse occurs, that is, £syn < Vrest, the current is outward and the membrane\nis hyperpolarized, away from the threshold for spike generation. In the central nervous\nsystem, this is typically caused by a slower form of inhibition due to y-aminobutyric acid\nB receptors (GABAg) at synapses that release the neurotransmitter GAB A and that let\npotassium ions out of the cell and have a reversal potential 10-30 mV below the resting\npotential (Fig. 1.8).\nWhat happens if a synapse is activated whose battery potential is close to the membrane\npotential, that is, Esyn « Kest? If the membrane is at rest, no driving potential exists across\nthe synaptic conductance, since Vm — Esyn & 0, and the membrane potential remains\nunperturbed. But the total conductance of the membrane increases by gsyn(t).\nIf this system is now depolarized by excitatory input, activation of this silent or shunting5\ninhibition causes a reduction in the EPSP amplitude.\nActivation of a GABA/i synapse, one of the most common forms of fast inhibition in\ncortex and associated structures, increases the membrane conductance for chloride ions\nand has a reversal potential in the neighborhood of many cells' resting potential, thereby\nimplementing a form of shunting inhibition.\nHow do we deal with multiple synaptic inputs? Since currents add, we can extend Eq. 1.19\nin a straightforward manner by placing the synapses in parallel with the RC circuit,\n(1.22)\nwhere the sum is taken over all synapses (each of which can have its own reversal potential).\nOf course, this is very reminiscent of the linear summation of inputs in the \"units\" of standard\nneural network theory (see Sec. 14.4).\n1.5 Synaptic Input Is Nonlinear\nWhat is not immediately apparent from Eq. 1.22 is the fact that synaptic input as conductance\nchange is of necessity nonlinear; that is, the change in membrane potential is a nonlinear\nfunction of the synaptic input. Yet this turns out to be crucial. From the point of view\nof information processing, a linear noiseless system cannot create or destroy information.\nWhatever information is fed into the system is available at the output. Of course, any system\nexisting in the real world has to deal with noise, which places restrictions on the amplitude\nof signals that can be discriminated. Therefore in a noisy linear system, information can\nbe destroyed. But what is needed in a system that processes information are nonlinearities\nthat can perform discriminations and decisions. Similarly, in order for a digital system to\nbe Turing universal, a nonlinearity such as negation and logical ANDing is required. As we\nwill see later on, one ever-present nonlinearity is the voltage threshold for spike initiation.\nAs we will see now, another nonlinearity that comes for free with synaptic hardware is\nsaturation.\n5. Because both excitatory and inhibitory fast synapses act to increase a postsynaptic membrane conductance, all of them\ncan properly be said to be shunting. However, in this book we follow widespread usage and only refer to shunting inhibition as a\nconductance increase with a reversal potential in the neighborhood of the cell's resting potential."}
{"text": "===== Page 16 =====\n20 \n• \nTHE MEMBRANE EQUATION\n1.5.1 Synaptic Input, Saturation, and the Membrane Time Constant\nThe nature of this effect can be perfectly well understood for a single synaptic input. If we\nconsider the change in membrane potential relative to rest in response to a slowly varying\nsynaptic input (that is, we can reduce gsyn(0 to g&yn), we can express the dynamics of V as\n(1.23)\nwhere the new value of the input conductance is\n(1.24)\nand the new value of the time constant in the presence of synaptic input is\n(1.25)\nIn other words, each synaptic input, whether excitatory, shunting, or inhibitory, increases\nthe synaptic input conductance, thereby decreasing the membrane time constant (Fig. 1.9).\nThis is, of course, equally true when one considers the effect of numerous simultaneous\nsynaptic inputs. As we shall see further along in this book, under physiological conditions\nneurons can be bombarded with massive synaptic input, which will lower the membrane\ntime constant significantly, as compared to the value of r measured under slice conditions\nin the absence of normal synaptic input.\nHow does the membrane potential behave as a function of gsyn? Solving for Eq. 1.23\nyields for the steady-state potential\n(1.26)\nIf the synaptic input is small, that is, if Rgsya <SC 1 or gsyn <^C 1/R, the denominator is\nroughly equal to 1, and the EPSP is\n(1.27)\nHere, the input can be approximated to a fair degree by a constant current source of amplitude\ngsyn-Esyn- Doubling the input under these conditions leads to a doubling of the voltage\nchange.\nAs the EPSP becomes larger and larger, the driving potential across the synaptic\nconductance Vm—EsyD becomes smaller and smaller, disappearing eventually at Vm = £syn.\nNo matter how large the conductance increase is made, there is no more potential to drive\nions across the membrane. Here, Rgsyn 3> 1 (or equivalently, gsyn ^> \\/R; that is, the\nsynaptic input is considerably larger than the input conductance), and we have\n(1.28)\nThe membrane potential has saturated at the synaptic reversal potential (Fig. 1.9).\n1.5.2 Synaptic Interactions among Excitation and Shunting Inhibition\nAs we are devoting the entire Chap. 5 to the topic of synaptic interaction, we focus here on\nthe specific interaction between excitatory input and shunting inhibition occurring within a\nsingle RC compartment (Fig. 1.10A). For the sake of simplicity, we assume that at t = 0,\nboth excitation (of constant amplitude ge andbattery Ee) and shunting inhibition (of constant"}
{"text": "===== Page 17 =====\n1.5 Synaptic Input Is Nonlinear\n21\nFig. 1.9 SYNAPTIC INPUT, SATURATION, AND\nTHE TIME CONSTANT \nThe effect of varying\nthe synaptic input conductance on (A) the in-\nput conductance G-m (Eq. 1.24), (B) the mem-\nbrane time constant r' (Eq. 1.25), and (C)\nthe steady-state change in membrane potential\nKCO (Eq. 1.26) in a single-compartment model\n(Fig. 1.7A) as a function of gsyn. Conceptu-\nally, if we assume an excitatory synapse (with\nEsyn = 80 mV) and a fixed peak amplitude\nof ge = 1 nS, the x axis is logarithmic in\nthe number of synapses involved in the overall\nsynaptic event. Note that r' as well as Gjn will\nincrease irrespective of whether the synapses\nare depolarizing, shunting, or hyperpolarizing.\nThe fact that the input to neurons comes in the\nform of a change in the membrane conductance\nimplies that the very structure of the neuronal\nhardware changes with the input, since the dy-\nnamics of the cell speeds up in the presence of\nstrong synaptic input.\namplitude g, and battery Ef = VKst) are turned on and remain on. Using Kirchhoff's current\nlaw, we can express the change in membrane potential relative to Vrest in this circuit as\n(1.29)\nAs in Eq. 1.23, we can transform this into\n(1.30)\nwhere the input conductance in the presence of the two synaptic inputs is\n(1-31)\nand the time constant is\n(1.32)\nThe solution to this is a low-pass filter function multiplied with some constant, or"}
{"text": "===== Page 18 =====\n22 \n. \nTHE MEMBRANE EQUATION\nFig. 1.10 NONLINEAR INTERACTION\nBETWEEN EXCITATION AND SHUNT-\nING INHIBITION Inhibitory synaptic\ninput of the shunting type, that is,\nwhose reversal potential is close to\nthe cell's resting potential, can imple-\nment a form of division. (A) This is\ndemonstrated for an RC circuit (/? =\nlOOMfi, C = 100 pF) in the presence\nof both excitation (with battery Ee =\n80 mV) and shunting inhibition (with\nEi = 0). We are here only consider-\ning the change in membrane potential\nrelative to VKst. (B) Time course of\nthe membrane depolarization in re-\nsponse to a step onset of both exci-\ntation (of amplitude ge = I nS) and\nshunting inhibition (for three values\nof gi =0,1, and 10 nS). One effect\nof increasing g; is an almost propor-\ntional reduction in EPSP amplitude.\nA further consequence of increasing\nthe amount of shunting inhibition is\nto decrease the time constant T', from\nits original 10 msec in the absence of\nany synaptic input to 9 msec in the\npresence of only excitation to 4.8 msec\nin the presence of excitation and the 10\ntimes larger shunting inhibition.\n(1.33)\n(This can be checked by replacement into Eq. 1.30.) The steady-state potential for t -> oo\nconverges to\n(1.34)\nWhat is important to realize is that the numerator does not include any contribution from the\nshunting inhibition (since the synaptic reversal potential is equal to the resting potential);\ngi only appears in the denominator. Increasing gf therefore reduces the EPSP from its peak\nvalue in a division-like manner. This is the reason shunting inhibition is frequently also\nreferred to as divisive inhibition (Bloomfield, 1974; Torre and Poggio, 1978). Of course,\ndue to the offset in the denominator, g{ only implements a true division in the limit of\ngt ^ ge + 1/R- Under these conditions,\n(1.35)\nIncreasing g, also affects the speed with which the cell converges to its steady-state, since\nthe time constant decreases with increasing shunting inhibition, as illustrated in Fig. 1.10B.\nFinally, let us consider the voltage gain, that is, the sensitivity of the output to variations\nin the excitatory input: by how much does the amplitude of the EPSP vary if ge varies? This\namounts to computing"}
{"text": "===== Page 19 =====\n1.6 Recapitulation \n• \n23\n(1.36)\nWe see that the gain is maximal in the absence of any shunting inhibition, becoming\nprogressively smaller as gj increases. In the limit of g,- -> oo, the gain becomes zero;\nin the presence of massive amounts of inhibition, the excitatory input becomes swamped\nand is completely dominated by inhibition.\n1.5.3 Gain Normalization in Visual Cortex and Synaptic Input\nOne example demonstrating the use of synaptic conductance changes to implement a\nnonlinear operation crucial to the behavior of neurons in visual cortex has been proposed\nby Carandini and Heeger (1994).\nThe standard model of a simple cell in primary visual cortex (VI) is that its firing rate is\na linear function of the visual input, using a Gabor filter for its spatial receptive field and\nsome low-pass or band-pass filter to account for its temporal behavior (Wandell, 1995).\nWhile much evidence has accumulated in favor of this position, many VI neurons do show\na number of nonlinearities: (1) the response saturates with increasing visual contrast, (2)\nat higher contrast level, the response occurs earlier, and (3) superposition does not hold;\nthat is, the response of the cell to a bar at its optimal orientation superimposed onto a bar\northogonal to its optimal orientation is not equal to the sum of the response to the two bars\nwhen presented by themselves. Carandini and Heeger (see also Nelson, 1994) account for\nthis behavior by using a simple RC model for a VI cell, as in Fig. 1.1 OA, augmented by\nan input from a hyperpolarizing synapse. Their intriguing idea is to have the amplitude\nof the shunting inhibition depend on the response rate of the entire population of cells at\nthis particular location in space, summed over all orientations and direction of motions\n(Fig. 1.11). At high contrasts when ge is large, the network is very active and g, is also very\nlarge; this leads to the divisive normalization witnessed in Fig. 1.10B as well as to a reduction\nin the time constant, explaining the advance of the response at high contrast levels. Heeger,\nSimoncelli, and Movshon (1996) have argued that the very same normalization mechanism\nis also operating in other cortical areas. Note that a more physiological implementation\nof this idea needs to take the additional conductance due to massive, excitatory recurrent\nfeedback among cortical cells into account (Ahmed et al., 1994; Douglas et al., 1995).\nWhile the Carandini and Heeger (1994) model is elegant, leads to a simple mathematical\nmodel of a cortical cell, and can account for much of the data (Fig. 1.11), it does have one\nserious flaw that we can only allude to here. As will become apparent in Sec. 18.5 (Kernell,\n1969, 1971; Holt and Koch, 1997), shunting inhibition acts in a linear, subtractive manner\nwhen treated within the context of a spiking neuron, rather than in the saturating manner we\nare accustomed to from a passive membrane. This is a natural consequence of the biophysics\nof spike generation and throws doubt onto the hypothesis that contrast normalization is\nimplemented using the natural properties of conductance-increasing synapses.\n1.6 Recapitulation\nIn this introductory chapter, we meet some of the key actors underlying neuronal information\nprocessing. Basic to all cells is the capacitance inherent in the bilipid layer, limiting how\nquickly the membrane potential can respond to a fixed input current. The simplest of all\nneuronal models is that of a single compartment that includes a resistance, in series with a"}
{"text": "===== Page 20 =====\n24\nTHE MEMBRANE EQUATION\nFig. 1.11 GAIN NORMALIZATION IN NEURONS IN VISUAL CORTEX Response properties of a\nsimple cell in primary visual cortex of the monkey in response to drifting sinusoidal gratings (Carandini\nand Heeger, 1994). (A) through (D) One cycle of the response to gratings of contrast 0.125, 0.25,\n0.5, and 1.0. The cell saturates with contrast (doubling the contrast doubles the neuronal response\nwhen going from A to B, but not when going from C to D) and advances its response (a shift of\nabout 50 msec occurs between A and D). (E) Amplitude and (F) Phase of the fundamental Fourier\ncomponent to sinusoidal gratings drifting at 6 Hz. Shown are the responses of the cell at its preferred\norientation (open symbols) and 20° away from the preferred orientation (solid symbols). Error bars\nrepresent ±1 standard deviation and the solid lines correspond to the best fit of the model equation\nthat uses shunting inhibition, activated via massive feedback, to carry out this gain normalization.\nReprinted by permission from Carandini and Heeger (1994).\nbattery, and the capacitance. It can be completely described by a linear, low-pass filter. As\nwe will see in a later chapter, such an RC circuit, augmented by a simple voltage threshold,\nconstitutes one of the simplest yet also most powerful models of a spiking neuron: the leaky\nintegrate-and-fire unit.\nWe introduced fast chemical synapses, the stuff out of which computations arise.\nChemical synapses convert the presynaptic voltage signal—via a chemical process—into a\npostsynaptic electrical signal, via a change in the membrane conductance specific to certain\nions. Such a synapse can be described by a time-dependent synaptic conductance gsyn (t) and\na synaptic battery Esyn. In general, synapses cannot be treated as constant current sources.\nChemical synapses, similar to an operational amplifier wired up as a follower, isolate the\nelectrical properties of the postsynaptic site from the presynaptic one. This allows synapses\nto link neurons with very different electrical impedances. Furthermore, the amplitude,\nduration, and sign of the postsynaptic signal can be quite different from those of the\npresynaptic one. Electrical synapses, discussed in Sec. 4.10, share none of these properties.\nThe fact that synapses act by changing, usually increasing, the postsynaptic membrane\nconductance has a number of important consequences. It allows for the natural expression\nof several nonlinear operations, in particular saturation and gain normalization. As an\nexample, we saw how shunting inhibition, mediated by a type of synapse whose synaptic\nreversal potential is close to the cell's resting potential, acts similar to division. We also\nstudied how synaptic input that increases the postsynaptic membrane conductance for some\ncombinations of ions, no matter what its reversal potential, acts to decrease the cell's input\nresistance and thus its membrane time constant. As postulated by Carandini and Heeger\n(1994), the effect of massive feedback synaptic input might, in a very physiological manner,\nimplement gain normalization in cortical areas."}
{"text": "===== Page 1 =====\nUNCONVENTIONAL COMPUTING\nAs discussed in the introduction to this book, any (bio)physical mechanism that transforms\nsome physical variable, such as the electrical potential across the membrane, in such a way\nthat it can be mapped onto a meaningful formal mathematical operation, such as delay-\nand-correlate or convolution, can be treated as a computation. Traditionally only Vm, spike\ntrains, and the firing rate /(f) have been thought to play this role in the computations\nperformed by the nervous system.\nDue to the recent and widespread usage of high-resolution calcium-dependent fluorescent\ndyes, the concentration of free intracellular calcium [Ca\n2+], in presynaptic terminals,\ndendrites, and cell bodies has been promoted into the exalted rank of a variable that can\nact as a short-term memory and that can be manipulated using buffers, calcium-dependent\nenzymes, and diffusion in ways that can be said to instantiate specific computations.\nBut why stop here? Why not consider the vast number of signaling molecules that are\nlocalized to specific intra- or extracellular compartments to instantiate specific computations\nthat can act over particular spatial and temporal time scales? And what about the peptides and\nhormones that are released into large areas of the brain or that circulate in the bloodstream? In\nthis penultimate chapter, we will acquaint the reader with several examples of computations\nthat use such unconventional means.\n20.1 A Molecular Flip-Flop\nThe computation in question constitutes a molecular switch that stores a few bits of\ninformation at each of the thousands of synapses on a typical cortical cell. In order to\ndescribe its principle of operation, it will be necessary to introduce the reader to some basic\nconcepts in biochemistry. The ability of individual synapses to potentially store analog\nvariables is important enough that this modest intellectual investment will pay off. (For an\nintroduction to biochemistry, consult Stryer, 1995).\n20.1.1 Autophosphorylating Kinases\nA group of proteins called kinases phosphorylate particular target proteins, that is they add\na ΡΟ;|~ group. The negative charge of the phosphate group changes the shape of the protein,\n452\n20"}
{"text": "===== Page 2 =====\n20.1 A Molecular Flip-Flop \n· 453\naltering its function in very specific ways. Phosphorylation is a very common mechanism to\nmodulate ionic currents and synapses (Kennedy and Marder, 1992; Hille, 1992; Kennedy,\n1994). Indeed, for the most part metabotropic synaptic receptors act by causing the final\ntarget channel to be phosphorylated. Many of these kinases are activated by elevated levels\nof intracellular calcium.\nIn principle such a kinase could be used as a trigger to a 1-bit storage device. Calcium\nrashes into the cell, directly or indirectly activating the kinase, which in turn switches\nprotein molecules into their phosphorylated \"on\" state. This \"on\" state is then assumed to\nbe read out by another protein or process.\nThe problem with this is twofold. Firstly, another class of enzymes, the phosphatases,\nundoes the work of the kinase by snipping off the PO^\" group. This returns the protein to its\n\"off\" state. The time constant of this degrading action is on the order of minutes. Secondly,\nall the molecules making up biological systems (with the exception of DNA) degrade over\na period of days, weeks, or, at the most, within a few months. This relentless molecular\nturnover means that each and every protein, phosphorylated or not, is eventually replaced\nby newly synthesized, and thereby unphosphorylated, proteins.\nHow can such unstable molecules be used to construct memories that can last for a\nlifetime? One solution is to store information in the very stable DNA molecules in the\nnucleus at the cell body. While it appears plausible that such storage occurs for properties\nthat affect the entire neuron, such as the level of expression of its receptors or the overall\nstate of its firing rate adaptation (Kandel and Schwartz, 1982), it is unclear how the DNA\nin the nucleus could control the synaptic strength of each and every one of the thousands\nof synapses in the dendritic tree of a typical neuron. This has lead to the proposal (Crick\n1984b; Lisman, 1985; Saitoh and Schwartz, 1985; Miller and Kennedy, 1986) that a bistable\nkinase stores the information locally at each synapse. (For older ideas on the regulation of\ncellular growth and differentiation with the help of enzymatic networks see Monod and\nJacob, 1961.)\nThe principle uses something called autophosphorylation, in which a particular kinase,\nhere generically termed kinase-1, can phosphorylate itself in an autocatalytic reaction.\nThe basic switch, illustrated in Fig. 20.1, is built from two proteins, kinase-1 and a\nphosphatase. Some neuronal stimulus, such as a brief but strong calcium transient at a\ndendritic spine, triggers a local kinase, kinase-2. As long as it is present, it phosphorylates\nkinase-1, transforming it from its inactive K\\ to its active K\\ form. The presence of a\nphosphatase turns off this form, by returning it to its native K\\ state. This system can only\nstore information as long as the original kinase-2 is present, for in its absence, all of the\nkinase-1 eventually ends up in its inactive K\\ form.\nThings become interesting if intermolecular autophosphorylation occurs. Here the\nphosphorylated kinase-1 molecules can phosphorylate other, not yet phosphorylated kinase-\n1 molecules. The more K* is present, the larger the rate with which K\\ is transformed into\nits active counterpart. Conversely, the larger the amount of A'*, the smaller the pool of\nremaining K\\ that can be phosphorylated. The kinetic equation describing this first-order\nreaction can be formulated as (Lisman, 1985)\n(20.1)\nwhere K<JI and K^ are the Michaelis constants for the kinase-1 and the phosphatase\nreactions, respectively, and [P] is the concentration of the phosphatase. The Michaelis\nconstant of an enzymatic reaction is defined as the concentration of the substrate (here"}
{"text": "===== Page 3 =====\n454 · \nUNCONVENTIONAL COMPUTING\nFig. 20.1 MOLECULAR FLIP-FLOP SWITCH \nBasic building block of a bistable molecular switch\nthat can retain its memory indefinitely in the face of constant protein turnover. The switch is constructed\nfrom an inactive unphosphorylated and an active phosphorylated form of a protein kinase, labeled K\\\nand K* respectively. The transition from the inactive to the active form of the protein can be initiated\nfrom the outside by another kinase KI (which, in turn, is triggered by some neuronal stimulus such\nas an elevation in the local calcium concentration). The critical component of the switch is the ability\nof the active form of kinase-1 to facilitate the phosphorylation of its own inactive form: the higher\nthe concentration of K^, the larger the reaction rate. The amount of K^, in turn, controls some other\nprocess. Some phosphatase molecule is assumed to return the phosphorylated kinase-1 to its native\nunphosphorylated state. Reprinted in modified form by permission from Lisman (1985).\neither K\\ or K*) at which the reaction rate is half of its maximal value. c\\ and ci are the\nturnover numbers of the kinase-1 and phosphatase.\nThe first term on the right-hand side can be thought of as the product of [K\\] and a\nforward rate constant. Due to autophosphorylation, the rate constant increases with [K*],\nbut saturates at high concentrations of [K\\]. The backward rate constant saturates as well\nand is proportional to the fixed concentration [/*]. Equation 20.1 is supplemented by the\nrequirement that the total amount of kinase-1 in its active and inactive forms be conserved,\n(20.2)\nEquation 20.1 can be expressed in normalized units (Κ = [Κ*]/Τχ \nwith 0 < K <\n1; K'dl = Kdl/7V and K*', = K*r/TK) as\n(20.3)\nWith our choice of parameters (see legend to Fig. 20.2), the first reaction essentially does not\nsaturate and has the shape of a parabola, while the second reaction is a strongly saturating\nfunction of K (Fig. 20.2A). The difference between the two terms is shown in Fig. 20.2B\nand displays, in general, three zero crossings.\nThe stability of Eq. 20.3 can be investigated using the methods introduced in Chap. 7,\nexcept that here everything is simpler. The system is stable if its first temporal derivative\nis zero and its second derivative negative. It is clear that the origin, K = 0, is a stable\npoint, since any small perturbation K = € > 0 will lead to a negative value of dK/dt,\nbringing the system back to the origin. However, if the calcium stimulus that initiates the\nentire reaction by activating kinase-2 is present for a long enough time, it can phosphorylate\nenough kinase-1 into its active form to move K past the second zero crossing in Fig. 20.2B.\nThe system will converge to the third zero crossing and will remain there, even once the\ninitial kinase-2 stimulus has subsided. The basis of attraction of the second stable point is\nlarge enough for it to remain stable even in the face of the ubiquitous turnover of all proteins"}
{"text": "===== Page 4 =====\n20.1 A Molecular Flip-Flop \n· 455\nFig. 20.2 BISTABILITY or THE PHOSPHORYLATED FORM OF KINASE-I \n(A) Turnover, that is, the\nrate of the two reactions, as a function of the relative fraction of kinase-1 in its phosphorylated form,\nΚ = [K*]/TK (Eq. 20.3 and Fig. 20.1). \"Kinase\" refers to the first term in Eq. 20.3, expressing\nthe rate at which kinase-1 is moved from its inactive to its active phosphorylated form, while the\ninverse reaction, here assumed to saturate, is labeled \"Phosphatase.\" (B) Net rate of change in the\nphosphorylated form of kinase-1, that is, the difference between the two curves in the upper panel. The\nstable points of the systems are at Κ = 0 and 0.74. That is, either no active form is present or about\nthree-quarters of kinase-1 is in its phosphorylated form. Parameters are from Lisman (1985) with\nci = 30/sec, c2 = 3/sec, KJI = 1 μΜ, K^ = 2.5 nM, [P] = 5 nM and TK = 50 nM. Reprinted in\nmodified form by permission from Lisman (1985).\n(unless this turnover is faster than the rate of autophosphorylation). In other words, 1 bit of\ninformation can be stored in this system, making it a molecular flip-βορ, even though all\nthe molecules that make up the switch can themselves slowly turn over.\nNothing is free in nature. In this case the cost of maintaining the switch in its \"on\" position\nis represented by the ATP molecules supplying the energy needed to rephosphorylate those\nkinase molecules that are being dephosphorylated (similar to the power requirements for\nmaintaining dynamic RAM).\nThis switch can be turned off by increasing the amount of phosphatase such that the\nloss of the phosphorylated form of kinase-1 cannot be compensated any longer by the\nautophosphorylation.\nThe important concept that has emerged from this proposal (Crick 1984b; Lisman, 1985;\nMiller and Kennedy, 1986) is the feasibility of long-term information storage by a molecular\nswitch that attains its stability using a biochemical reaction with positive feedback."}
{"text": "===== Page 5 =====\n456 · UNCONVENTIONAL COMPUTING\n20.1.2 CaM Kinase II and Synaptic Information Storage\nWhat is the experimental status of the molecular switch idea? Work on the biochemistry and\nbiophysics of long-term potentiation (LTP) and long-term depression (LTD; see Chap. 13)\nhas shown that a brain protein, called Ca2+/calmodulin-dependent protein kinase II (CaM\nkinase If), has many of the properties of the hypothetical kinase-1 discussed above. It has also\na number of differences that make the proposal even more attractive from a computational\npoint of view (Lisman and Goldring, 1988; Lisman, 1989; Patton, Molloy, and Kennedy,\n1993; Lisman, 1994; Hanson et al., 1994).\nFirstly, CaM kinase II exists in high concentrations at \\h& postsynaptic density at synapses\nin the central nervous system (Fig. 4.1; Kennedy, 1993). Secondly, the kinase, immobilized\nwithin the postsynaptic density, is activated by the calcium-calmodulin complex (Hanson et\nal., 1994; Fig. 11.7). Finally, once a particular concentration of calcium has been exceeded,\nthe molecule switches into an \"on\" state that retains its activity even after the removal of the\ninitial Ca2+ stimulus (Miller and Kennedy, 1986). And this calcium-independent form of\nthe phosphorylated CaM kinase II has been found at least up to an hour after the induction\nof LTP (Fukunaga et al., 1993).\nOne important difference between the hypothetical kinase-1 and CaM kinase II is that the\nlatter works by intramolecular autophosphorylation. Figure 20.3A illustrates a schematic\nversion of the structure of this enzyme, which consists of 12 subunits. Each subunit, in\nturn, contains three to four phosphorylation sites. Miller and Kennedy (1986) determined\nexperimentally that calcium is only necessary for the addition of the first two to four\nphosphate groups onto these sites (out of a possible 30 or so sites) on a single CaM kinase\nII molecule. Subsequently, even if Ca2+ is removed, the other sites phosphorylate on their\nown in an autocatalytic reaction. If subunits become dephosphorylated by the action of\nphosphatase or if a new—unphosphorylated—subunit is inserted into the molecule as part\nof the general protein turnover, the remaining phosphorylated sites are more than sufficient\nto rephosphorylate all sites.\nThe fact that the autocatalytic reaction only occurs within a single molecule of CaM\nkinase II (that is, one molecule cannot phosphorylate a subunit in another molecule of\nCaM kinase II) opens the door to the possibility that analog information, rather than 1 bit of\ninformation, can be stored at an individual synapse (Lisman and Goldring, 1988; Fig. 20.3B).\nThis assumes that the calcium transient, initiated by ongoing synaptic activity, does not\nsaturate the postsynaptic density for so long that all CaM kinase II molecules will be in\ntheir phosphorylated states. Each calcium event will cause some molecules to switch. The\nmore calcium, the more molecules will be switched. The amount of information that can be\nencoded in this manner depends on the uncertainty in the fraction of kinase molecules that\nis switched into their \"on\" state. Given the stochastic nature of molecular binding, Lisman\nand Goldring (1988) estimate that about 80 of these molecules could effectively store 3 to\n4 bits of information at each synapse.\nIt is very tempting to estimate the storage capacity of the brain by simply multiplying\nthis number by the total synaptic density of the cortex. But the implicit assumption that all\nsynapses are independent of each other is most certainly incorrect. Furthermore, we know\nalmost nothing about the spatial and temporal specificity of the read-out mechanism of the\nautophosphorylating molecular switch. Even the best RAM memory in the world serves\nlittle purpose if its states cannot be independently accessed.\nThe specific hypothesis we discussed, although exciting and with some experimental\nsupport, needs to be worked out in all of its messy biochemical details. What it does show,\nat least in principle, is that individual molecules can be used to instantiate computations\nusing positive and negative feedback loops."}
{"text": "===== Page 6 =====\n20.1 A Molecular Flip-Flop \n· 457\nFig. 20.3 SYNAPTIC INFORMATION STORAGE VIA CAM KINASE II \nExperimental observations\nimplicate the brain type II Ca2+/calmodulin-dependent protein kinase (CaM kinase II) as the crucial\nprotein implementing the autocatalytic switching behavior proposed by Crick (1984b) and Lisman\n(1985). (A) One such molecule consists of a number of subunits with a total of about 30 phosphoryla-\ntion sites, of which 12 are illustrated here. A rise in Ca2+ leads to formation of a calcium-calmodulin\ncomplex (Fig. 11.7) that induces phosphorylation of some of the sites. If a critical number of these\nsites (probably around three) has been phosphorylated, the molecule can itself phosphorylate the\nremaining sites in the absence of any further Ca2+. This autocatalytic reaction also assures that the\nmolecule remains completely phosphorylated in the face of the degrading action of phosphatase\nand the perpetually occurring protein turnover of individual subunits. (B) Because a phosphorylated\nmolecule of this kinase cannot phosphorylate another molecule, an ensemble of molecules can encode\ngraded information, as long as the initial calcium stimulus does not lead to phosphorylation occurring\non all sites of all molecules. Due to the random nature of this process, the calcium transient causes\na few sites on the four molecules to be phosphorylated (left and center). Here, it is assumed that\nif at least three sites per molecule are phosphorylated, the remaining sites on that molecule will be\nautophosphorylated, resulting in two completely activated and two inactivated molecules, even in the\nabsence of elevated calcium (right). A longer calcium transient would have turned all four molecules\non, allowing for the analog storage of information at each synapse (up to 4 bits). Reprinted in modified\nform by permission from Lisman and Goldring (1988).\nIn closing, let us recall the large number (on the order of 102-103) of regulatory proteins\nthat can interact with calcium and other second messengers, giving rise to a complex web of\ndense interactions. We need to understand in what sense these molecules not only serve as\nmetabolic intermediaries but also represent, store, and manipulate information. In principle,\nthe computations involving Vm and [Ca2+], could be supplemented by molecular or protein\ncomputations. Their advantages are the minimal spatial requirements, usually operating at"}
{"text": "===== Page 7 =====\n458 · \nUNCONVENTIONAL COMPUTING\nthe submicrometer scale, and the speed of the reaction, limited only by the associated\nchemical rate constants (Koshland, 1987; Bray, 1995; Barkai and Leibler, 1997).\n20.2 Extracellular Resources and Presynaptic Inhibition\nThe designer of analog very large scale integrated (VLSI) electronic circuits needs to be\ncareful when considering the spatial placement of various circuit components onto the\nsilicon chip. For instance, the wire carrying the digital clock cannot be placed too closely to\nthe wire carrying analog information since capacitive coupling between the two can induce\nsufficient noise so that the possibly very small analog variable can become corrupted. If\ndone cleverly, the parasitic capacitance of a standard metal-oxide-semiconductor transistor\ncan be exploited for a time-derivative computation rather than being an undesirable feature\nof this particular type of hardware.\nWe would argue that it is one of the defining characteristics of any efficient information\nprocessing system that the algorithms implemented are carefully matched to the physics of\nthe machine. If we are willing to spend enough resources, of which there are fundamentally\nthree—space, time, and power—we can violate such design principles, but at a price. In\ntoday's digital computers we take enormous amounts of time and power to implement\nfunctions that a house fly, with a brain volume of less than 1 mm\n3, can carry out in real time.\nIt is reasonable to assume that evolutionary pressures will have acted on the nervous\nsystem in such a way as to optimize the placement of all circuit elements using constraints\nthat we are only now beginning to be dimly aware of.\nA case in point is the resource limitation imposed by the extracellular space (Montague,\n1996). The amount of space accessible to ions outside neurons and glia cells is very small.\nCorrecting for shrinkage during histological preparation of biological material leads to a\nvolume fraction a, which is the fraction of the volume that is extracellular space. It is around\n20% for most tissues (that is, α = 0.2), with peak values of 30% for the parallel fiber system\nin the cerebellum (Nicholson, 1995; Syková, 1997; Barbour and Hausser, 1997). Of course,\na = 1 for an unencumbered volume (e.g., a beaker of water).\nThe smallness of this space could have important functional consequences, as proposed\nby Montague (1996). As discussed already in Sec. 4.2, the arrival of an action potential at\nthe presynaptic terminal causes voltage-dependent calcium channels to open. The resulting\ninflux of extracellular calcium is the crucial signal that triggers exocytosis, when the vesicle\ncontaining neurotransmitter molecules fuses with the membrane and dumps its content\ninto the synaptic cleft (Bennett, 1997). When not enough calcium is present outside the\npresynaptic terminal, the probability of release, and therefore the average postsynaptic\nresponse, drops. Reducing the extracellular concentration [Ca2+]0 by a factor of 2, from 2\nto 1 mM, reduces the postsynaptic response to 30% of its original value (Mintz, Sabatini,\nand Regehr, 1995). Reducing the extracellular Ca2+ concentration fourfold attenuates the\npostsynaptic response more than tenfold.\nIt has been estimated that on the order of 13,000 Ca2+ ions enter the presynaptic terminal\nto trigger release of a vesicle (Borst and Sakmann, 1996). Given the tight extracellular space,\nfilled with membranes and other organelles that impede the rapid diffusion of ions, this influx\nof Ca2+ into the presynaptic terminal will deplete the calcium concentration just outside\nthe terminal. This reduction in [Ca \n]0 is compounded if the postsynaptic terminal also\ndemands calcium, for instance, if it contains significant numbers of NMDA receptors or\nvoltage-dependent calcium channels."}
{"text": "===== Page 8 =====\n20.3 Computing with Puffs of Gas · 459\nWhen considering the diffusion of calcium, potassium, or other ions outside the cell one\nmust account for the fact that the diffusion coefficient in the tight tortuous space between\nthe glia cells and neurons is not the same as the diffusion coefficient in aqueous milieu.\nThe free diffusion of ions is hindered by these membrane obstructions, by macromolecules,\nby charged molecules and so on. The reduction of D is accounted for by the so-called\ntortuosity factor λ (not to be confused with the electrotonic space constant). The effective\ndiffusion coefficient Deflf as measured in the tissue is related to the coefficient in aqueous\nsolution, D, via\n(20.4)\nIn an idealized aqueous solution λ = 1; experimentally measured values for brain tissue\nfall in the 1.5-1.9 range (Nicholson and Phillips, 1981; Nicholson, 1995). In other words,\nthe effective diffusion coefficient is reduced by a factor 0.3 to 0.4 (see Eq. 11.26).\nTaking both the volume fraction and tortuosity into account modifies the one-dimensional\ndiffusion equation (Eq. 11.18) as follows:\n(20.5)\nwhere the sources and sinks include membrane currents, pumps, buffers, and so on.\nEgelman and Montague (1997) have carried out exploratory simulations of the diffusion\nof Ca\n2+ ions in the extracellular space around the synaptic terminals on the basis of Eq. 20.5\nand estimate that [Ca\n2+]0 can drop by as much as a quarter in response to a single action\npotential invading the presynaptic terminal. A high-frequency burst can further deplete\nextracellular calcium. Given packing densities on the order of one billion synapses per\ncubic millimeter of neuronal tissue (Sec. 4.1), a neighboring synaptic terminal might now\nhave difficulties to release a synaptic vesicle successfully, since the needed Ca\n2+ ions have\nbeen \"stolen\" by the first synapse.\nDepending on how long it will take to replenish extracellular [Ca\n +]0 by pumps and\ncalcium-release mechanisms, the first synapse can inhibit synaptic transmission at all closely\nadjacent synapses. And this irrespective of whether its postsynaptic action is excitatory\nor inhibitory, since this form of inhibition relies on calcium stealing by the presynaptic\nterminal. This is an instance of presynaptic inhibition in the absence of any postsynaptic\nconductance change.\nIt is well possible, of course, that the temporary reduction in [Ca\n +]0 is too brief or\ntoo minute to significantly affect any but directly apposed synapses, or that the effect\nexists but that the brain does not exploit it for computational purposes. Or the reduction of\nf Ca\n2+]0 might only be significant during calcium spikes in the dendrites. Another interesting\npossibility is that synaptic microcircuits (Sec. 5.3), such as the spine-triad arrangement so\nprevalent in the thalamus (Sec. 12.3.4), implement a special form of presynaptic inhibition\nthat relies on the exact spatial placement of the synaptic terminals relative to each other.\n20.3 Computing with Puffs of Gas\nThroughout this book, we have lived with the convenient assumption that the three-\ndimensional arrangements of synapses, dendrites, axons, and cell bodies do not matter and\nthat all neurons can be reduced to sets of one-dimensional cylinders. This simplification is\na powerful one since it allows us to study the spatio-temporal distribution of the membrane"}
{"text": "===== Page 9 =====\n460 . \nUNCONVENTIONAL COMPUTING\npotential and calcium with ease on the basis of one-dimensional cable and diffusion\nequations.\nHowever, one of the most obvious features of almost any piece of nervous tissue is its\nhigh degree of structure: columns, layers, laminae, and other spatial organizational forms\nabound. It therefore behooves us to at least briefly consider the range of possible effects of\nthree-dimensional geometry on neuronal computation.\nAs alluded to in Chap. 13, one or more retrograde messenger molecules have been\npostulated to mediate between the postsynaptic induction of LTP and its, at least partly,\npresynaptic expression. We also pointed out that the specificity of synapses during LTP\nand LTD might not be quite as high as frequently asserted. In the best explored model\nsystem for LTP, the synapses between the output fibers of CA3 pyramidal cells in the\nhippocampus and CAÍ neurons, spillover exists. That is, synaptic plasticity is not only\nconfined to the handful of synapses at the intersection of the single presynaptic axon and\nthe postsynaptic cell recorded from, but is observed at \"neighboring\" synapses as well.\nLTP can be expressed at synapses from the same axon but made onto neighboring neurons\nthat have not undergone the induction process (Bonhoeffer, Staiger, and Aertsen, 1989;\nSchuman and Madison 1994a). In a second form of spillover, excitatory synapses within\n50-70 μιη of the potentiated synapse onto the same neuron (but from nonstimulated axons)\ncan be potentiated as well (Engert and Bonhoeffer, 1997; for a summary, see Murthy, 1997).\nIt has been hypothesized that these effects are mediated by a rather unusual class of\nneuronal messengers, of which the best known is the free radical gas nitric oxide (NO)\n(for reviews see Schuman and Madison, 1994b; Montague and Sejnowski, 1994; Schuman,\n1995; Brenman and Bredt, 1997). Another possible candidate is the gas carbon monoxide\n(Zhuo et al., 1993).\nWhile conventional neurotransmitters like glutamate, GABA, acetylcholine, or nora-\ndrenaline are packaged in synaptic vesicles and released from the presynaptic terminal in\nresponse to an invading action potential, nitric oxide is not released in vesicles but directly\ndiffuses away from its site of production. Because it is gaseous and extremely membrane\npermeant, it readily moves across cell membranes irrespective of dendrites, axons, or other\ncellular processes. The second feature distinguishing it from a conventional neurotransmitter\nis that nitric oxide is \"produced on demand\" by a calcium-calmodulin-dependent enzyme.\nNitric oxide is limited in its spread by the fact that it is rapidly oxidized, with a half-life\nof 4 sec and possibly much less. Given its large diffusion coefficient (3.8 ^m\n2/msec; see\nTable 11.1) it can diffuse 160 μηι in all directions in this time (Eq. 11.26), a sphere that\nencompasses around 20,000 synapses.\nNitric oxide has been implicated in the control of both LTP and LTD. In particular,\ninhibition of the enzyme responsible for producing NO, nitric oxide synthase, blocks the\nestablishment of the NMDA-dependent form of LTP in the hippocampus. Furthermore,\nthe correlation of presynaptic electrical activity and elevated levels of NO is sufficient to\npotentiate transmission at recently activated synaptic terminals (Zhuo et al., 1993).\nA plausible scenario is based on conjoint pre- and postsynaptic electrical activity that\ncauses the [Ca\n2+],· in the spine to rise, triggering the production of NO by nitric oxide\nsynthase (Montague et al., 1994). It immediately diffuses away from this site and into the\nlocal volume of tissue. Its sphere of influence includes its own presynaptic terminal as well\nas nearby synapses, those on the same postsynaptic neuron as well as those on other neurons\n(Fig. 20.4). At synapses that were active within some time window, for instance, where a\nvesicle was recently released, the NO leads to either LTP or LTD through a—as of yet—\nill-characterized cascade of biochemical events (Kennedy, 1994)."}
{"text": "===== Page 10 =====\n20.3 Computing with Puffs of Gas · 461\nFig. 20.4 RETROGRADE MESSENGERS AND SYNAPTIC SPECIFICITY \nSchematic drawing illus-\ntrating the effect that the gas nitric oxide (NO) can have on synaptic weights (Gaily et al., 1985).\nA sufficiently large calcium influx, reflecting an appropriate conjunction of pre- and postsynaptic\nactivity, at spine A causes production of NO. Like a puff of gas, NO will freely diffuse from its site of\nproduction into the adjacent tissue volume. Presynaptic terminals that have recently been activated\nand that see an increase in the local concentration of NO (Eq. 20.6) upregulate their synaptic weight,\nleading to LTP. This volume learning (Gaily et al., 1990) is less specific than classical associative\nHebbian learning, since potentially thousands of synapses in the neighborhood of the primed one\ncould be affected. Reprinted by permission from Gaily et al., (1990).\nAs NO diffuses outward, its concentration drops, due to dilution, chemical degradation\nthrough oxidation to nitrates, and destruction by hemoglobin and other molecules. This\nimposes a limit on the volume throughout which the arrival of NO could trigger the\nbiochemical cascade of events that eventually leads to the presynaptic expression of LTP.\nWithin this volume a large number of synapses could potentially change their synaptic\nweights, even those that lacked either pre- or postsynaptic activity to satisfy Hebb's rule\n(Eq. 13.7 or its variant Eq. 13.8). Experimental evidence suggests that synapses within\n50 μηι of a potentiated one do (Engert and Bonhoeffer, 1997).\nThe appropriate volume learning rule that needs to replace the covariance rule of Eq. 13.8\n(Montague and Sejnowski, 1994) is\n(20.6)\nwhere V& is the presynaptic activity (and (V¿) its time average), [NO]^ the instantaneous\nconcentration of NO (or any other retrograde messenger molecule) at the presynaptic\nterminal, and {[NO]/t> its running average. The index / ranges over some neighborhood\nof the initially potentiated synapse (i, j) from which the NO diffuses. Of note is that\nEq. 20.6 is independent of postsynaptic activity.\nConjoint increases in presynaptic activity and the concentration of NO (relative to their\nmean) cause LTP while a presynaptic increase in conjunction with a relative decrease in the\nconcentration of the retrograde messenger (and vice versa) leads to LTD."}
{"text": "===== Page 11 =====\n462 . \nUNCONVENTIONAL COMPUTING\nIn other words, the unit of synaptic plasticity might not be individual synapses, as\nassumed by neural network learning algorithms, but groups of adjacent synapses, making\nfor a more robust, albeit less specific learning rule. How specific depends, among other\nthings, on the exact temporal relationship between the release of a presynaptic vesicle and\nthe local change in [NO],. In any case, it is obvious that the detailed placement of axons and\nsynapses in three dimensions will greatly affect their ability to locally store information.\nIt is important that plasticity rules, as in Eq. 20.6, be combined with realistic models of\nthe three-dimensional configurations of axons and synapses in order to better understand\ndevelopmental as well as ongoing learning processes in the brain (for an example, see\nMontague, Gaily, and Edelman, 1991).\nThe picture that we are left with is one in which afferent patterns of activity are translated\ninto local hot spots of calcium in spines and other postsynaptic terminals. These generate\nlocal puffs of gas that freely diffuse in three-dimensional space to up or down regulate\nsynaptic weights at neighboring synapses.\n20.4 Programming with Peptides\nWe mentioned in Chap. 4 the principle of synaptic colocalization of fast, classical neu-\nrotransmitters with much slower acting neuromodulators. In many synaptic terminals,\nneurotransmitters—small molecules such as ACh, GABA, or glutamate—are stored in\nsmall and clear vesicles while neuropeptides, short (typically 2-10) amino acid chains,\nare stored in dense core vesicles within the same terminal. The different vesicles can be\nreleased differentially, for instance, one preferentially at low stimulation frequencies and\nthe other during high-frequency bursts. The number of neuroactive peptides in the brain,\nwith very idiosyncratic names relating to the biological function they have historically first\nbeen identified with, totals 50 and keeps on rising. They are found throughout the animal\nkingdom and throughout all nervous structures (for reviews, see Kupfermann, 1991; Marder,\nChristie, and Kilman, 1995).\nThe study of their neuronal effects has progressed farthest in small nervous systems\nthat are readily accessible to neurochemical methods. The model system of choice is the\nstomatogastric ganglion (STG) of the crab Cancer borealis (Selverston and Moulins, 1987).\nThis ganglion consists of approximately 30 neurons and is responsible for controlling\nmovement in the esophagus and stomach. The neurons are tightly coupled with both\nchemical and electrical synapses.\nAbout 50 individually identifiable input fibers project from more forward located ganglia\ninto the very dense central core of the STG. This neuroplex, consisting of nothing but axons,\nsynapses, and dendrites, is about a quarter of a millimeter in diameter and is surrounded\nby the associated cell bodies (Fig. 20.5). Many of the input axons branch widely and\nthroughout the ganglion while some have more restricted branching patterns. Some of the\nassociated terminals resemble neurosecretory organs that are thought to release peptides\ninto the hemolymph to act at distant sites while some resemble conventional point-to-point\ncontacts. Colocalization of neuropeptides also fails to follow any simple rule, with any\ngiven peptide being associated with a different complement of cotransmitters in different\nneurons (for instance, all terminals staining immunocytochemically for peptide A may also\nstain for peptide B, but terminals positive for peptide Β might be negative for A but positive\nfor peptide C; Marder et al., 1997)."}
{"text": "===== Page 12 =====\n20.4 Programming with Peptides\n463\nFig. 20.5 NEUROMODULATORS IN A SMALL NERVOUS SYSTEM \nSummary of neuroactive sub-\nstances present in the inputs to the stomatogastric ganglion (STG) of the crab. Hormones are released\ninto the circulating blood or hemolymph and can thus act globally, while neuromodulators—usually\npeptides—are released by conventional synaptic terminals and can modulate neuronal properties over\nlong time scales. Sensory transmitters act locally and rapidly. Some substances, such as acetylcholine\n(ACh), bind to fast nicotinic as well as to slow muscarinic receptors. The input fibers project into\nthe dense, central core of the ganglion, indicated schematically, with the 30 or so cell bodies (open\ncircles) located in the periphery. The neuromodulators are present in various and complex subsets\nof the input fibers and are colocalized with conventional neurotransmitters. The area of influence\nof these peptides ranges from local synapses to the entire ganglion. As illustrated in Fig. 20.6, their\neffects vary widely in scope, time scale, and sign. Unpublished data from E. Marder, A. E. Christie,\nand M. P. Nusbaum, printed with permission.\nWhile the distribution of neuropeptides and their postsynaptic receptors ranges from the\nvery local to the global and defies any simple classification, their effects on their targets are\nequally varied.\nWhen food moves from the mouth into the esophagus of the crab, the neurons in the STG\ngenerate a variety of different rhythmic motor patterns. Accordingly, most of the STG cells\ncan be identified on the basis of a characteristic oscillatory pattern involving EPSPs, IPSPs,\nbursts, plateau potentials, and the like. Some neurons display these regular sequences in\nisolation, that is, when all synaptic input has been removed, while others rely on network"}
{"text": "===== Page 13 =====\n464 · \nUNCONVENTIONAL COMPUTING\ninteractions. Such central pattern generators (CPG) are a common feature of vertebrate\nand invertebrate motor systems (Marder and Calabrese, 1996).\nThe action of peptides on these oscillatory discharges as well as on the 40 odd stomach\nmuscles enervated by the STG neurons is complex and still ill-understood. We illustrate\nthe possibilities in Fig. 20.6, involving the application of the peptide proctolin, released by\nfibers projecting into the neuropil. Applying proctolin to the entire ganglion (by adding it to\nthe bath solution) changes the properties of the hardware at multiple organizational levels.\n1. It affects specific motor patterns, here the so-called \"pyloric rhythm\" (Fig. 20.6A). Acting\nvia a second-messenger cascade, proctolin modulates a voltage-dependent conductance\nin two identifiable neurons. The net effect is to increase the frequency and modulation\ndepth of the oscillations (Hooper and Marder, 1987).\n2. Even at very low concentrations, proctolin enhances the motor neuron evoked contrac-\ntions of certain stomach muscles (Fig. 20.6B). The muscle may now be sensitive to firing\nrates it did not previously respond to (Marder et al., 1997).\n3. Peptides can also affect the evoked amplitude of individual synaptic connections (Fig.\n20.6C). Dual intracellular impalement in the 1C and GM neurons in the absence of any\nspiking activity (by adding a sodium-channel blocking agent to the bath) fails to reveal\nany direct synaptic connection. Yet in the presence of proctolin a robust and profound\ninhibition can be observed.\nA different substance, crustacean cardio-active peptide (CCAP), which is not present in\nany input fibers to the STG but is released into the hemolymph via neurosecretory structures,\ncan initiate a switch from one pattern of neural activity into a qualitative different one\n(Weimann et al., 1997). The complex action of CCAP on various slow intrinsic membrane\nconductances changes the normal 1:1 alternation between two specific neurons to a 2:1,\n3:1, or 4:1 alternation.\nPeptides can be thought of as reprogramming the nervous system by changing its motor\npattern, its synaptic gains, and its output. In an uncanny way this resembles loading a\nnew program into an application specific integrated circuit (ASIC), with each peptide, or\ncombination of different peptides, loading a slightly different motor program.\nBecause peptides rely on passive diffusion to influence an entire neural network, acting\nakin to a \"global variable,\" the time scale of action is seconds, minutes, or longer (Jan\nand Jan, 1983; Kuffler and Sejnowski, 1983). An extreme form of this can be found in\nthe mammalian suprachiasmatic nucleus, the central \"clock\" that transmits the circadian\n24-hour rhythm to the rest of the brain and body. Grafting experiments have proven that\ncircadian activity rhythms can be sustained via direct action of an as yet unidentified\ndiffusible signal (Silver et al., 1996). The action of these modulatory signals must be viewed\nin constrast to the computational function of fast, synaptic input, which is quite local in\ntime and space.\n20.5 Routing Information Using Neuromodulators\nWe argued in Sec. 9.3 that the action of noradrenaline is contingent on excitatory synaptic\ninput. By itself, its application causes only a small and long-lasting EPSP, which can be\nthought of as an epiphenomenon. Its real action is to close a potassium conductance and"}
{"text": "===== Page 14 =====\n20.5 Routing Information Using Neuromodulators \n· 465\nFig. 20.6 REPROGRAMMING A NEURAL NETWORK VIA A PEPTIDE \nMultifaceted action of proc-\ntolin, one of the many peptides that are released from afferent fibers into the stomatogastric ganglion\nof the lobster (PROC in Fig. 20.5). (A) Intracellular recordings from three identifiable STG neurons\nreveal a complex oscillatory behavior in their membrane potential, characteristic of central pattern\ngenerators. The effect of 1 μ,Μ of proctolin on this \"pyloric\" rhythm is complex; among others, it\nincreases the frequency of oscillation in the AB neuron. Reprinted by permission from Hooper and\nMarder (1987). (B) A hundredfold lower concentration of proctolin enhances contractions of two\nstomach muscles controlled by STG neurons. The motor nerve was stimulated repetitively (upper\nrow); at low stimulation frequencies (left column) no muscle contraction is apparent while a weak\none can be observed at high frequencies (right column). In the presence of proctolin, these contractions\nare much enhanced. Unpublished data from J. C. Jorge-Rivera, printed with permission. (C) Proctolin\nalso affects synaptic gain. In this experiment, all spikes are blocked by the application of ΤΓΧ. A\ndepolarizing pulse in one identifiable neuron fails to cause any change in the membrane potential\nof another. Adding proctolin to the bath reveals a profound and long-lasting inhibitory synaptic\nconnection from one to the other. Unpublished data from J. M. Weimann, printed with permission."}
{"text": "===== Page 15 =====\n466 . \nUNCONVENTIONAL COMPUTING\nthereby increase the gain of the cell's discharge curve (as in Figs. 9.13 and 21.3) without\naffecting the cell's excitability. This mechanism might enable the nervous system to send\ninformation selectively this or that way in a dynamic manner.\nEfforts to build massively parallel computers have lead to the realization that a major\nchallenge facing the computer architect is the problem of routing information efficiently\namong the individual processors, that is, using the least amount of time and/or space (here,\ntransistors). The cortex and similar structures, with upward of 1010 to 1011 processors\noperating in parallel, most likely face a similar conundrum. How is information routed\namong different neurons without quickly exceeding space by connecting every neuron\nwith every other neuron? One possible mechanism to deal with this could be based on\nneuromodulators (Koch and Poggio, 1987).\nLet us assume that a particular input neuron projects to a large number of neurons, that\nis, establishes conventional synapses with them. In the absence of any modulatory input,\nspikes in an afferent fiber evoke synaptic activity in all of its targets. Assume that in the\npresence of the neuromodulator A for which only a subset of neurons MA, has receptors,\nthe excitability of neurons in MA is enhanced (and possibly suppressed in other neurons).\nSubstance A could be released either from a local interneuron—as in Fig. 20.7—or from\nthe axon terminal of a neuron far away (e.g., in the locus coeruleus). By itself A does not\ninduce a signal. Yet in the presence of conventional synaptic input, a neuron that is part of\nMA signals more vigorously than before while neurons that are not part of MA respond in\na much weaker manner. Conceptually, this can be thought of as routing synaptic input to\nthe subset MA of neurons. If another population MB has receptors for neuromodulator B,\nthe information can be routed to a different target.\nIn order for such an addressing scheme to work efficiently, a large number of neuromod-\nulators is required to target specific subsets of neurons. Addressing works by selectively\nand temporarily (on the order of seconds) increasing the output gain of the class of neurons\nthat are meant to be targeted without directly exciting them. This solution to the addressing\nproblem is similar to the traditional telephone exchange system, in which connections are\nmade and broken as required for exchanging information (in contrast to a dedicated-line\nsolution).\n20.6 Recapitulation\nWe here dealt with a number of mechanisms that are not conventionally thought of as\nsubserving specific neuronal computations. None of them involve the membrane potential,\nthe firing rate, or the intracellular calcium concentration.\nThe entire realm of biochemical computations has been neglected. Yet at present there are\nno solid arguments ruling out why specific molecular reactions might not subserve specific\ncomputations. As one instance we introduced a molecular flip-flop switch that relies on\npositive feedback—via autophosphorylation—to implement a long-term memory device\nwith the storage capacity of a few bits that could reside at individual synapses. This is but\none example of a realm of computation about which we know almost nothing. Given the\nextremely large and complex regulatory cascades and networks of proteins and enzymes,\nthe possibilities for nested multilevel computations are staggering. The crucial question is\nwhether the brain avails itself of these possibilities or whether such computations cannot\nbe implemented for reasons having to do with lack of bandwidth, signal-to-noise ratio or\nspecificity."}
{"text": "===== Page 16 =====\n20.6 Recapitulation\n467\nFig. 20.7 ROUTING INFORMATION AMONG NEURONS USING NEUROMODULATORS Speculative\naddressing scheme based on neuromodulators (Koch and Poggio, 1987). The input axon (heavy\nline) is presynaptic to a variety of cells that project both within and outside the system. Neurons\nhave receptors for many neuromodulators (here only three types are indicated schematically, A, B,\nand C). Neuromodulators can be released by an interneuron (as shown here) or by some external\nneuron, diffuse throughout the ganglion, and bind to their receptor sites on a specific subset of\nneurons (MA and so on). The postsynaptic effect of a neuromodulatory substance is to change the\ngain of the firing response of the neuron, that is, the same neuron responds more or less vigorously\nto the same presynaptic input as before (e.g. Fig. 20.6C). Depending on which neuromodulatory\nsubstance has been released, action potentials coming in on the axon will only activate a subset of\nneurons. Functionally, this enables information to be selectively routed to neurons within the ganglion.\nReprinted by permission from Koch and Poggio (1987).\nTwo other candidate mechanisms rely on the precise three-dimensional arrangements\nof neuronal components, either at the subcellular or at the cellular level. Whether or not\nthe short-term depletion of extracellular Ca2+ ions implements a universal presynaptic\ninhibition that works without any conductance changes is pure speculation at the moment,\nbut is too important to neglect from an experimental point of view.\nThat puffs of nitric oxide (and possibly carbon monoxide) are released in nervous tissue\nfollowing local hot spots of synaptic-induced calcium activity opens up new avenues of\nspreading information in a retrograde manner, back across the synapse. Given the inexorable\nsquare-root law of diffusion and the aggressive chemical nature of nitric oxide, NO is\nunlikely to be effective beyond a small fraction of a millimeter from the site of its synthesis.\nThis sphere of influence does include a potentially very large number of synapses. One\nof the \"unfortunate\" consequences of such a diffusing substance is that the specificity of\nHebb's synaptic plasticity rule would be significantly reduced. The unit of learning would\nnot be individual synapses, but groups of adjacent synapses."}
{"text": "===== Page 17 =====\n468 . \nUNCONVENTIONAL COMPUTING\nThe last two mechanisms exploit the very large laundry list of neuroactive substances\n(biogenic amines, neuropeptides, hormones) known to be present in any nervous system to\nimplement global variables that act over a fraction of a millimeter and longer distances and\non a seconds to minutes and longer time scale. We speculated on the role of neuromodulators\nin routing information, for reprogramming a particular neural network to change its mode of\noperation, for adapting the retina or other sensory surfaces to different operating conditions,\nand the like."}
{"text": "===== Page 12 =====\n480 \n· \nCOMPUTING WITH NEURONS: A SUMMARY\nFinding answers to any one of these questions, possibly in the form of a Ph.D. thesis,\nwill ultimately help us better understand the most complex object in the known universe,\nthe human brain. The author as well as the reader of these pages are fortunate enough to\nlive in exciting times where much progress in the neurosciences occurs within a relatively\nshort span of history. Let's get to it: per áspera ad astral"}
{"text": "===== Page 1 =====\nAppendix A\nPassive Membrane\nParameters\nA vexing and confusing question concerns the use of the all-important membrane parameters\nR¡, Cm, and Rm and their associated units.\nA.1 Intracellular Resistivity R¡\nIn physics, the amount of resistance a piece of bulk material offers to electrical current\nflowing across the material is characterized by its specific resistivity p\\ its units are ohms-\ncentimeter (Ω-cm). If the piece of material has a constant cross section A and length £, its\ntotal resistance will be\n(A.I)\nIn biophysics, the specific resistance of the intracellular medium, also known as the\nintracellular resistivity, is denoted by /?,·, which can be thought of as the total resistance\nacross a 1 cm cube of intracellular cytoplasm.\nGiven a cylindrical axon or dendrite of diameter d and length €, its total resistance will\nbe 4/?,·£/ττί/\n2 and its resistance per unit length € of cable,\n(A.2)\nConversely, given a neural process with axial resistance per unit length ra, the total axial\nresistance will be tra. The units of ra are ohms per centimeter (Ω/cm).\nThe resistivity of seawater, from where we all came half a billion years ago, is 20 Ω-cm,\nthat of mammalian saline about 60, and that of physiological Ringer solution 80 Ω-cm. This\nis not surprising, since the resistivity decreases as salts are added to a solution. Computing\nthe intracellular resistivity on the basis of the Nernst-Planck electrodiffusion equation yields\n33.4 Ω-cm for K\n+ ions and 267 Ω-cm for Na\n+ ions (Eq. 11.33). Under the assumption that\nthese are the dominant charge carriers, this leads to a final value of /?,· = 29.7 Ω-cm (see\nSec. 11.3.1).\nBecause the intracellular environment contains many structures in addition to electrolyte,\nsuch as endoplasmic reticulum, the cytoskeleton, the Golgi apparatus, and so on, that restrict\ncharge redistribution within neurons, the above cited values set a lower bound on the true\n481"}
{"text": "===== Page 2 =====\n482 · PASSIVE MEMBRANE PARAMETERS\nvalue of /?,· in neurons. The value of /?,· conventionally used is in the 70-100 Ω-cm range,\nbased on measurements in neurons and axons of marine invertebrates (Foster, Bidinger,\nand Carpenter, 1976; Gilbert, 1975) and on data taken from cat motoneurons (Barrett and\nCrill, 1974)\nHowever, much higher values have been reported. Neher (1970) directly measured a\nvalue of 180 Ω -cm for the resistivity of the cell body of snail neurons, while Shelton (1985)\nand Segev et al., (1992) require values of 225-250 Ω-cm for cerebellar Purkinje cells in\norder to fit various experimentally observed records, such as pulse attenuation and input\nresistance, against detailed cable models.\nIn a careful study of this problem using in vitro cortical pyramidal cells recorded and\nstained with sharp intracellular electrodes and whole-cell patch pipettes, Major (1992; see\nalso Major, Evans, and Jack, 1993a) concludes that a good match between his physiological\nrecords and a detailed cable model requires a R¡ in the 200-300 Ω-cm range (see also\nSpruston and Johnston, 1992, and Spruston, Jaffe, and Johnston, 1994). Tissue shrinkage\nduring the histological recovery of the cell's anatomy does, however, affect these numbers.\nAfter decreasing the diameters of all dendrites by 20% while simultaneously increasing all\nlengths by 20% to invert the effect of shrinkage, Major and his colleagues infer a reduced\nvalue of R¡ = 187 Ω-cm.\nA.2 Membrane Resistance Rm\nThe passive membrane resistivity, that is, the resistance associated with a unit area of\nmembrane, of the electrical component of the neuronal membrane that does not depend on\nsynaptic input or on the membrane potential is denoted by Rm, measured in units of ohms-\nsquare centimeter (Ω-cm\n2). Its inverse is known as the passive conductance per unit area\nof dendritic membrane or, for short, as the leak conductance Gm = l/Rm and is measured\nin units of Siemens per square centimeter (S/cm\n2).\nThe molecular correlate of this leak conductance is not precisely known. A pure phos-\npholipid in saline solution has an extremely high resistance of up to ΙΟ\n15 Ω - cm\n2 (Hille,\n1992). Since measured membrane resistances are considerably lower, some mechanism has\nto permit ions to pass across the membrane.\nThe evidence for voltage-independent \"leak\" channels is not strong. Patch-clamp studies\nof frog sympathetic neurons reveal a nearly ohmic region between —70 and —110 mV\n(Jones, 1989). The underlying conductance is only weakly voltage dependent and is\ninsensitive to blockers that block other known conductances in the cell. After adjusting for\na 10 mV contribution of the sodium-potassium pump, the remainder of the \"leak\" current\nis carried by potassium ions and reverses around —65 mV. Evidence from hippocampal\npyramidal cells suggests a very weakly voltage-dependent potassium current that is active\nat rest and that can be blocked by cholinergic input (Madison, Lancaster, and Nicoll, 1987).\nIn a cylindrical fiber of length I and cross section A — πάί, the membrane resistance\nper unit length of fiber is defined as\n(A.3)\nin units of Ω -cm. The total membrane resistance in a fiber of length € is identical to rm ft. Rm\nvaries widely from preparation to preparation, with the quality of the intracellular recording,\nwith the amount of synaptic input to the cell, and other parameters. As intracellular recording\ntechniques become more mature and sophisticated, the estimates of Rm increase."}
{"text": "===== Page 3 =====\nA.3 Membrane Capacitance Cm \n· 483\nNote that the \"effective\" specific resistivity associated with the membrane, on the order\nof 10\n5 Q-cm\n2/4Q A= 2.5 χ ΙΟ\n11 Ω-cm, is approximately one billion times larger than\nthat of the intracellular fluid, explaining why by far the largest fraction of the cytoplasmic\ncurrent flows within the dendrite or axon rather than across the membrane.\nA.3 Membrane Capacitance c,,,\nThe capacitance of the neuronal membrane is characterized by the specific capacitance per\nunit area Cm, measured in units of farads per square centimeter (F/cm\n2). The generally\nagreed upon value for Cm is 1 μΡ/cm\n2. This amounts to 10~\n7 coulomb (C) charge being\ndistributed on both sides of a 1 cm\n2 piece of neuronal membrane in the presence of a 100 mV\nvoltage difference across the membrane. Assuming a parallel plate capacitor configuration\nand a dielectric constant of 2.1 for the hydrocarbon chains making up the bilipid layers, this\nimplies a separation of 23 A (Hille, 1992). Hence, the reason for the slow time constants\nin the millisecond time range is the molecular dimension of the neuronal membrane.\nIt is intriguing to compare the biological value of Cm against the capacitance in the analog\nCMOS circuit technology used to design neuromorphic electronic circuits (Mead, 1989;\nMahowald and Douglas, 1991; Douglas, Mahowald, and Mead, 1995). Here, a capacitance\nis created by separating two layers of polysilicon with a 40-nm-thick layer of silicon oxide.\nThe specific capacitance is about 0.5 μΡ/cm\n2, about 20 times lower than its biological\ncounterpart (compatible with the larger separation between the two layers). The higher value\nof Cm in biology is partly compensated for by the fact that the voltages across membranes\n(on the order of 0.1 V) are much smaller than the typical gate voltages of around 5 V in\nelectronic circuits. Ultimately, this is due to the multiple gating charges of the voltage-\ndependent channels (see Chap. 8) allowing them to work on a much steeper exponential\nthan transistors.\nThe 1 μΡ/cm\n2 value of Cm appears to be somewhat of an overestimate. The membrane\ncapacitance of a pure bilayer lipid membrane without proteins is between 0.6 and 0.8 /¿F/cm2\n(Cole, 1972; Fettiplace, Andrews, and Haydon, 1971; Benz et al., 1975), placing a lower\nbound on Cm. Mammalian red blood cells have a measured Cm of between 0.8 and\n0.9 μΡ/cm\n2. The \"traditional\" value of unity for Cm is based on the membrane of the\nsquid giant axon and includes the nonlinear capacitances associated with the gating currents\n(for a discussion see Adrian, 1975). In a detailed investigation on dissociated hippocampal\npyramidal cells, Sah, Gibb, and Gage (1988) report Cm = 1.0 ± 0.2 μΡ/cm\n2, while a\nmore recent investigation of neocortical pyramidal cells finds values between 0.65 and\n0.8 μΡ/cm\n2 (Major, 1992).\nA further complication are membrane invaginations and foldings which can multiply\nthe effective membrane area—and therefore Cm—manyfold (as for instance in bullfrog\nsympathetic ganglion cells, where Cm = 3 μΡ/cm\n2 has been inferred from the measured\ntotal cell capacitance and the area of these spherical cells (Yamada, Koch, and Adams,\n1998; see also Segev et al., 1992).\nFrequently, one defines\n(A.4)\nas capacitance per unit fiber of diameter d (in units of F/cm), in analogy to rm. The total\ncapacitance of all of the membrane in a process of length t is given by cml."}
{"text": "===== Page 4 =====\nAppendix B\nA Miniprimer on Linear\nSystems Analysis\nFor those readers who forgot linear systems analysis, crucial to this book, we here provide\nthe briefest of reviews.\nA system is always linear or nonlinear with respect to some particular input and output\nvariable. Indeed, the same physical system can be linear when using one sort of input-output\npairing and nonlinear when considering a different one. If we restrict ourselves to the case\nwhen the input and output variables are single-valued functions of time, termed x(t) and\ny(t), respectively, then for a system L, defined as\ny(f) = L[je(f)] \n(B.I)\nto be linear, it must obey two constraints. Firstly, it must be homogeneous, that is,\n(B.2)\nFor instance, doubling the input should double the output. Secondly, the system must also\nbe additive,\nL[*i(0 + Jt2(f)] = L[jd(f)] + L[*2(0] · \n(B.3)\nThe response of the system to the sum of two inputs is given by the sum of the responses to the\nindividual inputs. These two properties are sometimes also summarized in the superposition\nprinciple, expressed as\n(B.4)\nA further property that some (not all) linear systems possess is shift or time invariance;\nin other words, if the input is delayed by some Δί, the output will be delayed by the same\ninterval. A linear system is shift invariant if and only if\ny(t)=L[x(t)] \nimplies y(t - f,) = L[x(f - fj)]. \n(B.5)\nIf a system possesses these three properties, then its entire behavior can be summarized\nby its response to an impulse or delta function <5(f)·' The impulse response or Green's\nfunction of the system is exactly what its name imples, namely, the response of the system\nto an impulse,\n1. The delta, unit, or Dirac \"function\" is defined by being zero for all values of Í except at the origin, where it diverges. It\nhas the immensely useful property that f_™ f(t)S(t)dt \n= f(0).\n484"}
{"text": "===== Page 5 =====\nA Miniprimer on Linear Systems Analysis \n· 485\n(B.6)\nAny input signal can be treated as an infinite sum of appropriately shifted and scaled\nimpulses, or\n(B-7)\nThe properties of homogeneity, additivity, and shift invariance ensure that the response\nto any arbitrary input x(t) can be obtained by summing over appropriately shifted and\nscaled responses to an impulse function (or, equivalently, the output is a weighted sum of\nits inputs). In short,\n(B.8)\nThe shorthand form of this integral operation, known as a convolution, is *, as in\n(B-9)\nWe conclude that once we know h(t), the response of a linear system to an impulse, the\nresponse to an arbitrary input waveform can be obtained by the linear convolution operation.\nBefore we end this short digression, we briefly want to remind the reader of another way\nto analyze time-invariant linear systems, namely, by using sinusoidal inputs. If the input\nto a linear system is a sinusoidal wave of a particular frequency / (in hertz), the output is\nanother sinusoidal of the same frequency but shifted in time and scaled,\n(B.10)\nThe function A ( f ) is known as the amplitude response and determines how much the output\nis scaled for an input at frequency /, while the phase φ (/) determines by how much the\nsinusoidal wave at the output is shifted in time with respect to the input.\nAny input can always be represented as a sum of shifted and scaled sinusoidals. For\na linear system, the impulse response function and the amplitude and phase functions are\nclosely related by way of the Fourier transform. The Fourier transform of the impulse\nresponse function h(t) is\n(B.ll)\nThe amplitude of this filter h ( f ) corresponds to the amplitude of the Fourier transform of\nthe impulse response function. Note that h ( f ) (throughout the book, the h symbol denotes\nthe Fourier transform of some function h) is a complex function. We then have\n(B.12)\nThe reason we frequently talk in this book about the input being \"filtered\" by the filter\nfunction h ( f ) is that formally the output can be obtained by convolving (another name for\nfiltering) the input by the filter function. In the frequency space representation, convolution\nis turned into a straight multiplication, and Eq. B.8 can be rewritten as\n(B.13)\nAs emphasized before, when discussing linearity it is crucial to discuss with respect to what\nvariable. There are a number of instances in which neurobiological systems can be treated,"}
{"text": "===== Page 6 =====\n486 \n· \nA MINIPRIMER ON LINEAR SYSTEMS ANALYSIS\nto some degree of approximation, by linear systems analysis. Prominent examples are linear\ncable theory, when the input is current and the output voltage (but not when the input is a\nconductance change; see Chap. 1), or receptive field analysis of retinal or cortical neurons\nin the visual system (Palmer, Jones, and Stepnoski, 1991). Here, the input is usually the\nstimulus contrast and the output the mean firing rate.\nCertain nonlinear systems can frequently be treated as a linear system with the addition\nof a simple type of static nonlinearities, such as a threshold (Palm, 1978; French and\nKorenberg, 1989).\nA perhaps surprising linear system is the one that relates a continuous input, call it\nμ(0, to a continuous firing rate f ( t ) via a spiking process. The input, suitably scaled,\ncan be thought of as input into an integrate-and-fire unit (Chap. 14) with no leak and no\nrefractory period. The threshold for generating spikes Vth is not fixed but is some probability\ndistribution pa¡(V) (Gestri, 1971; Gabbiani and Koch, 1997): every time a spike has been\ngenerated the threshold is set to a new value drawn from p^V).2\nFor any input μ(ί) this unit generates a particular random spike train sequence, ab-\nbreviated here as J3; 5(f — í¡)· Obviously, the relationship between the continuous input\nμ(ί) and the discrete spike train is highly nonlinear. However, let us assume a population\nof independent but otherwise identically integrate-and-fire units with identical distributed\nvoltage thresholds. If all receive the same input μ(ί) one can average, as discussed in\nSec. 14.1, over this ensemble and define an instantaneous output rate /(().\nIt can be proven (Gestri, 1971) that varying the input by αμ(?) changes the instantaneous\nfiring rate of this fictive population of cells by af(t). In other words, the relationship\nbetween the input and the instantaneous output rate is a linear one.\n2. If pth(V) is exponentially distributed, the spikes generated by this process have the convenient property that they are\nPoisson distributed (Chap. 15)."}
{"text": "===== Page 7 =====\nAppendix C\nSparse Matrix Methods for\nModeling Single Neurons\nBarak A. Pearlmutter and Anthony Zador\nIn this appendix we describe numerical methods used in the efficient solution of the linear\nand nonlinear cable equations that describe single neuron dynamics. Our exposition is\nlimited to the scale of a whole neuron; we will ignore both the simulation of circuits of\nneurons (see, for example, the monographs by Bower and Beeman, 1998 and by Koch and\nSegev, 1998), as well as the simulation of the stochastic equations governing single ion\nchannel kinetics (Skaugen and Walloe, 1979; Chow and White, 1996).\nThe appendix is divided into two parts. The first deals with the solution of the linear\ncomponent of the cable equation. Since the cable as well as the diffusion equation are linear\nsecond-order parabolic partial differential equations (PDE), this part draws on techniques\nand principles developed in the many other fields that deal with similar equations, though\nnaturally the discussion will focus on those problems of particular interest in neurobiology.\nThe theme common to this part is that for the purposes of numerical solution, the cable\nequation is best discretized into a system of ordinary differential equations coupled by sparse\nmatrices. The main difficulty is that the resulting equations are stiff, that is, they display\ntime scales of very different magnitude; even so it is possible to apply widely available\nand efficient techniques for sparse matrices. The second part deals with the nonlinear\ncomponents of neurodynamics, particularly equations of the Hodgkin-Huxley type and\nthose arriving from calcium dynamics. These nonlinearities are surprisingly benign, and\ncan readily be handled with a few simple techniques, provided that the linear component is\ntreated properly.\nMany of the techniques described are implemented in widely used and freely available\nneural simulators (in particular GENESIS and NEURON; see DeSchutter, 1992; Hiñes, 1998;\nBower and Beeman, 1998) in a manner that is relatively transparent to the user. Nevertheless,\nthere are at least three good reasons for understanding the foundations of these numerical\nmethods. Firstly, when such simulators produce surprising—and possibly spurious—results,\nan understanding of their internal workings can help determine whether the numerical\nmethod is to blame. Secondly, when conducting original research it is inevitable that some\nproblem will arise for which the software must be customized. Finally, and most important,\nunderstanding these techniques provides insight into the underlying neurodynamics itself,\nand hence into the behavior of neurons.\n487"}
{"text": "===== Page 8 =====\n488 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\nC.1 Linear Cable Equation\nC. 1.1 Unbranched Cables and Tridiagonal Matrices\nDiscretization in Space\nThe passive cable equation (Eq. 2.7) describes the spread of potential in one dimension\nalong an unbranched homogeneous cable,\n(C.I)\nwhere Gm = l/Rm and J(x, t) is the injected current. (In this appendix, we use J rather\nthan / to distinguish it from the identity matrix I below.) For the sake of convenience,\nwe set the reversal potential in Eq. C.I to zero; it can be thought of as being absorbed\ninto the offset current J(x, t). The membrane parameters Cm, R¡, Gm, and d are assumed\nto be independent of position along the cable. Note that the subscript on the intracellular\nresistivity R¡ is not used here as a numerical index. We now discretize this partial differential\nequation in space by replacing the second spatial derivative with its simplest, second-order\ndiscrete approximation,\n(C.2)\nresulting in the system of coupled, ordinary differential equations\n(C.3)\nHere V¿ = V(x¡,t) and J¡ = J(x¡,t) denote, respectively, the membrane potential\nand injected current, at some point x¡, where i is the discretization index, and the time\nvariable t has been suppressed. Physically, these indices correspond to the locations along\nthe cable—the nodes or compartments—at which the voltage is specified (see Fig. 2.2B).\nThe resulting system of coupled ordinary differential equations can be written compactly\nin matrix form as\n(C.4)\nwhere the matrix Β is given by\n(C-5)\nwith ψ· — £//(47?¡Ajt2). Here Β' is a tridiagonal second difference matrix with elements\n—2 along the diagonal and +1 off the diagonal\n(C.6)\nand C and G are diagonal matrices with Cm and Gm, respectively, along their diagonals\n(that is, scalar multiples of the identity matrix). Since the matrices C and G are diagonal,"}
{"text": "===== Page 9 =====\nC.1 Linear Cable Equation · 489\nthey do not contribute to the interaction between equations; coupling is exclusively through\nB'. This is consistent with our physical expectations, since points along the cable interact\nonly through the second spatial derivative (scaled by the effective axial R¡) captured in B'.\nAs discussed in detail below, the corner elements of this matrix determine the boundary\nconditions, which here correspond to V(x) = 0 at the boundaries—the killed-end or\nDirichlet condition, which we have choosen here for the sake of convenience (Sec. C.I.3).\nFor the sealed-end or von Neumann boundary condition, we have dV/dx = 0 at the\nboundary, and B' takes slightly different values in the upper and lower rows. Although the\nvon Neumann condition arises more often, here we consider only the Dirichlet condition\nbecause it gives rise to a simpler form.\nThis set of coupled differential equations is (almost) the same set that arises from a\nnetwork of discrete passive electrical components, for instance, the one shown in Figs. 2.3\nand 3.4B. This is no coincidence. Another way to arrive at this set of equations is to first\napproximate the spatially continuous electrical structure specified by Eq. C. 1 with a discrete\nelectrical network that approximates its properties. As we shall see, however, the analogy\nwith a discrete electrical circuit breaks down at the endpoints of the cable, so care must be\ntaken to ensure the exact boundary conditions.\nEigenvalue Analysis\nIt is helpful to analyze the behavior of Eq. C.4 in terms of the eigenvalues of B. For\nnow we neglect the injected current term. We will first consider the eigenvalues of B'—the\nsecond difference matrix from which B is derived—and then consider the eigenvalues of B\nitself. Recall that the eigenvalues mz of any matrix B are those numbers that satisfy\n(C.7)\nwhere V(z) is the eigenvector corresponding to the z-th eigenvalue. For an η χ η matrix Β'\nthere are η eigenvalues and η associated eigenvectors, and the index ζ ranges from 1 to n.\nWe show below that the eigenvalues of the second difference matrix B' are given by\n(C.8)\nand the corresponding eigenvectors are\n(C.9)\nThe two sides are proportional rather than equal, since we are free to choose the magnitude\nof the right-hand side: any multiple of an eigenvector is itself and eigenvector. If the\neigenvectors are scaled so that | |V\n(z)\n 11 = 1, then the solutions to Eq. C.4 can be expressed\nas a sum of exponentials,\n(C.10)\nwhere V¡ \nis the ι'-th component of the z-th eigenvector, and rz' the reciprocal of the"}
{"text": "===== Page 10 =====\n490 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\ncorresponding eigenvalue.1 The constants c\\,..., cn are determined by the projection of\nthe initial conditions V¡ (0) onto the eigenvectors,\n(C.ll)\nor c = MrVo. This last expression holds since the eigenvectors of the symmetric matrix\nΒ are orthogonal. If we denote by Μ the matrix that has the eigenvectors in Eq. C.9 as its\ncolumns, then the vector solution in Eq. C.10 can be rewritten compactly in matrix form as\n(C.12)\nwhere VQ is the vector of initial conditions and m' is the vector of eigenvalues.\nThese equations tell us several things. (1) The eigenvalues appear as the time constants\nin a sum of exponentials. This sum represents the decay of voltage with time. It is precisely\nthese time constants that are \"peeled\" in the classical Rail analysis of electrotonic structure\n(see Sec. 2.3.2). Second, the eigenvalues m'z range from m'j = — 2 + 2cos[jr/(n + l)] < 0\ntom'n — —2+ 2cos[njr/(n + 1)] > —4. (2) The smallest eigenvalue m\\ of Bis just less\nthan 0, so the solution always decays to 0, even in the absence of a membrane leak term;\nthis is due to the killed-end boundary condition we are considering. For the sealed-end case,\nthe matrix is conservative for Gm = 0, charge only redistributes spatially without net loss\nor gain, so 0 is an eigenvalue (that is, the voltage decays to a constant). (3) Finally, note\nthat the end elements V\\ and Vn do not equal zero, contrary to what we might expect from\nthe boundary conditions. Only when we consider the \"phantom\" elements at i = 0 and\nι = η + 1 do we see that the boundary conditions, VQ = 0 and Vn+\\ = 0, are satisfied.\nHow do the eigenvalues of Β—which is the operator that governs the physical behavior\nof the cable equation—relate to those of B'? In general, if the eigenvalues of a matrix Β are\nmz, then the eigenvalues of (B — b\\)/c, where I is the identity matrix, are (mz — b)/c; the\neigenvectors remain unchanged. The dependence on b is called the shifting property? The\neigenvalues mz can be obtained from those of the second difference matrix (Eq. C.8),\n(C.13)\nwhere as before ζ ranges from 1 to n. For B, the smallest eigenvalue (associated with\nz — 1) is just larger than Gm/Cm (since cos[;rz/(n + !)]-> 1 for large n), which\nis just the inverse of the membrane time constant. The largest eigenvalue is just smaller\nthan 4i/f/Cm + Gm/Cm (for large n). However, in a cable of fixed length as the number of\ncompartments becomes larger and larger, the spatial discretization step Δ* becomes smaller\nand smaller and this eigenvalue goes to infinity (and the corresponding time constant to 0),\nas expected from the corresponding eigenvalues of the continuous cable equation.\nHow do these expressions for the eigenvalues and eigenvectors arise? One way to\nunderstand their origin is by analogy to the continuous diffusion equation. The eigen-\nfunctions of the continuous second derivative operator with a Dirichlet boundary condition\n(corresponding to a killed-end condition) on the interval [0, 1] are the sinusoidals\n1. Following neuroscience convention, we express the solution in terms of τ'ζ = —\\/m'z rather than m'v\n2. This shifting property can be used in the change of variables W = Ve''\nT, with Γ — Cm/Gm, for reducing the cable\nequation (Eq. C.I) in V to the diffusion equation CmdW/dt — .. n\nm. 9 W/dx \nin W. This shifting property is a special case\nof the property that for any polynomial (or more generally, any analytic function) p(·), the eigenvectors of /?(B) are p(mz),\nand the eigenvectors are unchanged."}
{"text": "===== Page 11 =====\nC.1 Linear Cable Equation \n· 491\n(C.14)\nwhere — (πζ)\n2 is the e-th eigenvalue. We need to find the corresponding expression for the\ndiscrete second difference operator Ü2:\n(C.15)\nWe posit that the eigenvectors of ΌΪ correspond to the eigenfunctions of the continuous\noperator, and we write the expression\n(C.16)\nand solve for m'z. Application of the appropriate trigonometric identities confirms our\nexpression Eq. C.8 for m'z.\nAnother interpretation is in terms of the discrete Fourier transform (DFT) or rather its\ncousin, the discrete sine transform (DST). Although the DST is usually efficiently computed\nusing the FFT algorithm, the DST is a linear transform, and can therefore be represented as\na matrix S. For an η χ η operator S—required to find the transform of a 1 χ η vector—the\nelements of S are given by\n(C.17)\nThe columns of this matrix are precisely the eigenvectors of B. Therefore, S diagonalizes B,\n(C.18)\nThis is entirely in analogy with the Fourier transform in the continuous case.\nExplicit Discretization in Time\nWhile the sum of exponentials suggests a possible solution to the equation, it does\nnot generalize well to nonlinear problems and for many problems it is computationally\ninefficient (because η exponentials must be evaluated at each time point). In order to solve\nthis system numerically, we transform the system of ordinary differential equations into a\nsystem of algebraic difference equations to be advanced by discrete time steps Δί. Just as\nbefore we replaced the spatial derivative by its finite difference approximation, we replace\nthe temporal derivative by its first-order difference\n(C.19)\nwhere the superscript refers to the index of discretized time. Now we have a choice for how\nto combine this with the right-hand side of Eq. C.4: do we use V or V'\n+1 ? The temptation\nis to use V\nr, since then V\ni+1 is an explicit function of V',\n(C.20)\nThis choice for d V/dt is the basis of the forward Euler scheme. We can advance one time\nstep at a very reasonable cost of three multiplications per node—one for the self-connection\nV¡ and one each for the adjacent nodes V¡±\\.\nWe can rewrite a single iteration of the forward Euler scheme as"}
{"text": "===== Page 12 =====\n492 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\n(C.21)\nwhere By is a tridiagonal matrix and V is a vector. The matrix By is obtained by solving for\nV'\n+1 in the previous equation and is equal to Δί Β with an additional 1 along the diagonal,\n(C.22)\nwhere I is the identity matrix. If we set J = 0, then after ρ iterations, the solution can be\nwritten as a power of By,\nwhere in the second line the solution is expressed in terms of the eigenvalues fz of By and\nthe c's are once again determined by the initial conditions. Note that while in the solution\nof the continuous differential equation (Eq. C.10) the eigenvalues appear in the exponent,\nhere in the discrete difference equation they are raised to a power. This imposes a stability\nconstraint on the eigenvalues. Stability refers to the behavior of the solution for t -» oo,\nwhich for the discrete case translates into ρ —> oo. For the discrete case the solution is\nstable if\n(C.23)\nfor all z, since the eigenvalues are raised to a power. If any eigenvalue does not satisfy\nthis condition, then for sufficiently large ρ it eventually diverges to infinity. By contrast,\nin the continuous case the eigenvalues appeared in the exponent, so the condition was that\nmz < 0 for all z.\nThe forward Euler method is unstable if Δί is chosen too large. Consider the eigenvalues\nof the difference matrix By, which we compute from Eq. C.13 using the shift property,\n(C.24)\nThese eigenvalues are almost identical to those of the original differential system, but with\nthe addition of the 1, which imposes a condition on the discretization parameters. Recall that\nwe know only that mz < 0, so there will exist discretization parameters such that | fz | > 1.\nFor η —> oo and Gm = 0, that is, no membrane leak, the stability condition simplifies to\n|4^rAi/Cm —1| < l,orAí < R¡Cm&.x2/d. Thus if the spatial discretization ΔΛ is made\ntwice as small, the temporal discretization Δί must be made four times smaller to preserve\nstability. Since doubling the spatial discretization step also doubles the size of the matrix,\ncomputation time scales as the third power of n. This very rapidly becomes the limiting\nfactor for large numbers of compartments.\nImplicit Discretization in Time\nOne alternative is to use an implicit or backward Euler scheme, where the spatial\nderivative is evaluated at r + 1,\n(C.25)\nSetting 7/\n+1 = 0, we can rewrite (C.25) in matrix form as\n(C.26)"}
{"text": "===== Page 13 =====\nC.1 Linear Cable Equation \n· 493\nwhere\n(C.27)\nUsing the result that the eigenvalues of the inverse of a matrix are just the inverse of the\neigenvalues (and the eigenvectors unchanged), we write the eigenvalues bz of B¿,\n(C.28)\nThe denominator is identical to Eq. C.24, except for the sign of the — 1. This makes all the\ndifference, since now \\bt \\ < 1 for all z, and the system is stable for all discretization steps.\nThis does not guarantee, of course, that the solution to the discretized equation is close to\nthe solution to the underlying continuous equations. This still requires small values of Ax\nand Δί.\nThe stability of the implicit scheme comes at a very high apparent cost: the inversion of an\nη χ η matrix B(, at each time step. Now in general, the inversion ofannxn matrix requires\nΟ (η\n3) time; this is the same cost as the explicit scheme. Note that Bj is very sparse—in fact,\nit is tridiagonal. This sparseness is critical, since the solution to Eq. C.26 can be obtained\nby Gaussian elimination in O(n) steps. (We defer the description of Gaussian elimination\nfor the tridiagonal matrix until Sec. C.I.2, where it appears as a special case.) Thus, the\nimplicit scheme is stable for all time steps and can be implemented in an efficient manner\nthat scales linearly with the grain of spatial discretization.\nSemi-Implicit Discretization in Time\nThe choice defining the implicit scheme, evaluating the spatial derivative at V\ni+1, is not\nthe only one that leads to a stable scheme. We could also choose to evaluate the spatial\nderivative at the the midpoint between V'\n+1 and V' (that is, by using the discretization\nscheme V\nt+1/2 = (V'\n+1 + V')/2), to \nobtain\n(C.29)\nThis choice is called the semi-implicit or Cmnk-Nicolson algorithm. Setting /,· = 0 as\nbefore, the resulting matrix equations can be expressed in terms of the implicit and explicit\nmatrices,\n(C.30)\nStability is therefore determined by the eigenvalues ez of B^B/. Since B¿ and B/ share\neigenvectors, the eigenvalues of B¿\"'B/ are simply the product of the eigenvalues of B¿'\nand By,\n(C.31)\nObserve that since mz < 0 the numerator is always less than the denominator, so that the\nabsolute value of this expression is less than unity. Hence the Crank-Nicolson method is\nstable for all choices of discretization parameters. This method is almost always preferable\nto the implicit method, because it is more accurate (as well as being stable)."}
{"text": "===== Page 14 =====\n494 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\nThe accuracy of any numerical method is measured by performing a Taylor expansion\nabout At = 0. If there is a nonzero coefficient associated with At then the method is\nfirst-order, while if the first nonzero coefficient is associated with At2 then the method is\nsecond-order. Both the explicit and the implicit discretizations are only first-order, while\nthe semi-implicit discretization is correct up to second-order.\nC. 1.2 Branched Cables and Hiñes Matrices\nSo far we have developed techniques only for unbranched cables. Since many neurons\nhave very extended and complex dendritic trees (see Fig. 3.1), these techniques must be\nextended if they are to be useful. The primary complication introduced by branching is that\nwhile the connectivity of an unbranched cable can be represented by a tridiagonal matrix,\nconnectivity of a branched neuron can only be represented by a special matrix, a Hiñes\nmatrix, of which the tridiagonal is a special case. This matrix has the same number of\nelements as a tridiagonal matrix, 3« — 2 in the η χ η case, but they can be scattered, rather\nthan concentrated along the diagonals. Hiñes (1984) first introduced this transformation in\nthe context of dendritic modeling and showed how to invert this matrix in O(ri) steps—\nthe same as any tridiagonal matrix. This observation was key: until then implicit matrix\ntechniques were believed inefficient for branched cables because the matrix inversion was\nthought to require O(n3) steps. We will see below that there are other cases where the\nsparseness of this matrix can be used to construct algorithms as efficient as those for the\ntridiagonal case.\nThe Hiñes matrix is defined by the structure of its sparseness. A matrix Β is a Hiñes\nmatrix if three conditions are satisfied: (1) The diagonal elements B¡¡ are nonzero; (2) BÍJ\nis nonzero if and only if Bj¡ is nonzero; and (3) for any nonzero B¡j with i < j, there is no\nh such that h > j and B¡h is nonzero. The second condition requires Β to be structurally\nalthough not numerically symmetric, and the third requires that Β have no more than one\nnonzero element to the right of the diagonal in any row, and by structural symmetry, no\nmore than one nonzero element below the diagonal in any column. Notice that while there\nare only two tridiagonal matrices corresponding to an unbranched cable (since numbering\ncan start from either end), there are many Hiñes matrices corresponding to a single tree\nstructure.\nIn order to convert a graph into a matrix, the nodes must be numbered sequentially.\nThere is a simple algorithm for appropriately labeling the nodes of a tree to construct\na Hiñes matrix. The algorithm given here is a generalization of that proposed in Hiñes\n(1984).\nSequentially number nodes that have at most one unnumbered neighbor until there are no\nunnumbered nodes left.\nAt each step in the algorithm there may be many choices for which node to number next;\nalthough all choices are equally suitable for the algorithms discussed, the particular choice\nof numbering adjacent nodes successively wherever possible leads to the matrix elements\nmaximally concentrated near the diagonal.\nThe advantage of this numbering scheme is that Gaussian elimination can be applied\ndirectly. At the abstract level, it amounts to solving for Vi+1 in the matrix equation\nBV+i _ y' by"}
{"text": "===== Page 15 =====\nC.1 Linear Cable Equation \n· 495\nConsider the matrix [B|V'J.\nUsing row operations, put it in the form [I|A], where I is the identity matrix.\nNow h = V\nI+1 =B\n- 1V\nf.\nThe standard scheme for solving tridiagonal systems is a special case of this algorithm.\nC.1.3 Boundary Conditions\nThe partial differential equation for a single unbranched cable (Eq. C.I) has a unique solution\nonly if boundary conditions (BC) are specified at the endpoints. The BCs determine the\nbehavior of the membrane potential or its derivative at the boundary points. The membrane\npotential along a branched cable is governed by a system of coupled partial differential\nequations, one for each branch, and BCs must be specified at the branch points. In matrix\nnotation the BCs at the origin of the cable correspond to the first row of the matrix B, and\nat the end of the cable to the last row of the matrix B. Similarly the BCs at a branch point\ncorrespond to the matrix elements at that point. We shall see that the implementation of\nBCs at a branch point is just a natural extension of the elements at any point along the cable\nand so poses few problems. The BCs at endpoints is rather more subtle and requires careful\nconsideration.\nIn general there are many BCs corresponding to different physical situations. For\nexample, in the killed-end or Dirichlet case (see Sec. 2.2.2) the voltage is clamped to\nzero and the axial current \"leaks\" out to ground. The matrix B' we have been considering\nabove in the eigenvalue analysis implements this condition. To see this, assume that the\nfictive point at χ = 0, that is, VQ is assumed to be zero. Following Eq. C.3, the difference\nequation for the point at χ = AJC is proportional to V2 — 2 Vj + VQ = V-i — 1V\\. Thus, the\ntop two entries in B' are —2 and 1, as are the bottom two entries. The killed-end solution\nis a special case of the voltage-clamp condition, where the voltage at the endpoint is held\nat some arbitrary value. This condition is simply V0 = Kiamp·\nHere we limit our attention to the most important class of boundary conditions for nerve\nequations, the so-called sealed-end or von Neumann condition (Sec. 2.2.2). The sealed-end\ncondition requires that no current flow out of the end. Mathematically, this is expressed as\ndV/dx = 0 at the boundary (Eq. 2.19).\nThere are several ways this boundary condition associated with the continuous equation\ncan be implemented in a discretized system. Perhaps the most intuitive is the one that arises\nfrom consideration of an equivalent electrical circuit model, as in Fig. 3.4B. Following\nOhm's law, the current flow in the axial direction is proportional to Vi — V\\. Thus, the\ndiagonal element is — 1 or half the size of the elements along the rest of the diagonal.\nBecause the endpoint is connected only to a single neighbor, while all other nodes are\nconnected to two neighbors, it seems reasonable that the loss to neighbors should be half of\nthat at all other nodes. With these choices the resulting matrix B' turns out to be symmetric,\nwhich is in line with our intuition. For a single unbranched cable with a sealed end at both\nends, we have\n(C.32)"}
{"text": "===== Page 16 =====\n496 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\nThis implementation has been widely used in neural simulators (but not by Mascagni, 1989,\nor Hiñes, 1989).\nOur intuition can be misleading, that is, the associated electrical circuit (as in Fig. 3.4B)\ndoes not approximate well a finite cable with a sealed-end boundary. While such a matrix\ndoes implement a form of the von Neumann condition, it is correct only to first order,\nthat is, the error is Ο(Δχ). This implementation introduces systematic errors or \"phantom\ncurrents\" (Niebur and Niebur, 1991).\nA more accurate scheme can be derived by considering the Taylor expansion of the\npotential around χ = 0 for a \"ficticious\" element just beyond the end of the cable at\nχ = — AJC,\n(C.33)\nWe can now solve for V-\\ in terms of VQ and V\\ by setting the first derivative to 0 (the BC\ncorresponding to no axial current across the membrane at χ = 0) and setting the second\nderivative to the second difference approximation, d\n2V/dx\n2\\x=o = (V^\\+V\\— 2Vo)/&x\n2,\nto obtain\nV.i = V,. \n(C.34)\nThis gives us the matrix with the second-order correct von Neumann boundary conditions:\n(C.35)\nC. 1.4 Eigensystems and Model Fitting\nWe have seen how the morphology of a neuron—its branching geometry and connectivity—\ntogether with the passive membrane parameters—Rm, Cm, and /?,·—collectively determine\nthe sparse matrix Β which governs the dynamic behavior of the neuron. Equation C. 10 shows\nhow the eigensystem acts as the link between dynamics and morphology: the time course\nof membrane potentials can be expressed as the sum of exponentials whose decay constants\nare the eigenvalues of B. Thus, starting from the physical description of the neuron, we can\nuse the eigensystem to compute its response to stimuli, although in practice it is usually\nmore efficient to use an implicit matrix scheme.\nSuppose we wish to determine the eigensystem of a model neuron. For very simple\nmorphologies, for instance, a single terminated cylinder, and homogeneous membrane\nparameters, Rail (1977) has used the underlying partial differential equation to derive some\nrelatively simple expressions for the eigenvalues as a function of the membrane parameters\n(as in Eq. 2.38). These expressions provide some insight into the behavior of passive cables.\nFor somewhat more complex morphologies (for instance, several cylinders attached to a\nsingle lumped soma) there are correspondingly more complex expressions involving the\nroots of transcendental equations. But as the morphologies become more complex the\nexpressions become more difficult to evaluate and provide less insight. Analytic solutions\nare not generally useful for arbitrary dendritic trees."}
{"text": "===== Page 17 =====\nC.1 Linear Cable Equation \n· 497\nFor arbitrary trees there are at least two options for determining the eigensystem. First, we\ncould compute a solution V¡ (?) from initial conditions, using for example an implicit matrix\nscheme as outlined above, and then attempt to extract the time constants τζ and associated\ncoefficients c'iz = V¡ cz by fitting them to the solution. One method of fitting is the so-\ncalled \"exponential peeling\" or just \"peeling\" method (Rail, 1977). In fact, exponential\nfitting is intrinsically very difficult. The reason is that exponentials are a poor set of basis\nfunctions, because they are so far from orthogonal. The inner product of two exponentials\nover an infinite interval is given by\n(C.36)\nOrthogonality would require that this expression be zero when τ\\ Φ τι, which is not the case.\nIn fact, the general exponential fitting problem is equivalent (in the limit of a large number\nof time constants) to numerically inverting a Laplace transform, which is well known to\nbe ill-conditioned (Bellman, Kalaba, and Locket, 1966). Since many combinations of time\nconstants and coefficients fit almost equally well, no technique can reliably extract the time\nconstants in the presence of noise. Typically only the first two or three eigenvalues can be\nextracted, as suggested by the characteristic decrease in the eigenvalues in Eq. C.8.\nIf we are working with a model of a neuron, a second and much more reasonable\nalternative is to extract the eigenvalues directly from the matrix B. (This option is not\navailable to us if we are working with a real neuron, in which case we must fall back on some\nnonlinear fitting technique.) Typically we are interested only in the first few eigenvalues,\nso we exploit the sparseness of Β by observing that multiplication of Β with a vector is\ncheap—O(n) for an η χ η Hiñes matrix. We use the power method, which depends on the\nfact that the principal eigenpair dominates. Defining a new matrix Ρ = αϊ — Β (where I\nis the identity matrix) with eigenvalues pz — a — mz, the smallest eigenvalue of Β (that\nis, the largest time constant) can be determined by the following algorithm, which finds the\nprincipal eigenpair of Ρ for any initial V:\nrepeat\n(C.37)\nuntil V is stable.\nStandard deflation can be used to extend this procedure directly to the computation of the\nnext several smallest eigenvalues of Β (Stoer and Bulirsch, 1980).\nC.1.5 Green's Functions and Matrix Inverses\nPrior to the ascent of sparse matrix techniques, methods based on Green's functions (also\ncalled transfer impedances) and Laplace transforms were widely used (Butz and Cowan,\n1984; Koch and Poggio, 1985a; Holmes, 1986; see Sec. 3.4). The Green's function gives\nthe response at any point ζ to a delta pulse of current applied at some other point _/;\ntheir properties were discussed in Sec. 2.3.1 (e.g., Eq. 2.31). These methods have been\nlargely abondoned because they are typically less efficient and more difficult to generalize\nto synaptic and nonlinear conductances. Nevertheless, for theoretical work they are often\nvery convenient, and can sometimes offer a different perspective (see also Abbott, 1992).\nOne interesting application of the Green's function is the morphoelectrotonic transform"}
{"text": "===== Page 18 =====\n498 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\n(Zador, Agmon-Snir, and Segev, 1995; see Sec. 3.5.4). Another is in analyzing the effects\nof Hebbian learning (Pearlmutter, 1995). Here we reconsider the Green's functions in terms\nof sparse matrix methods.\nOne way to compute the Green's function is in terms of the eigensystem of the matrix\nB. First we expand the solution V¡ (t) at a point Xj in terms of the initial conditions V¡ (0)\nat ( = 0 and the Green's function K'ij(t),\n(C.38)\nNotice that K'ij is identical to K¡j of Eq. 3.16 and in the following, except for a constant of\nproportionality with the dimensions of an impedance. Because the eigenvectors V¡ \nform\na complete basis, we can use them to represent the initial conditions,\n(C.39)\nwith c7 constants. From Eq. C.10 we can write Vj(t) as\n(C.40)\nBy orthogonality and completeness we can compute the constants as the projection of the\ninitial condition vector on the eigenvectors,\n(C.41)\nHence\n(C.42)\nthat is,\n(C.43)\nUnderstanding the eigenvalue expansion of the Green's function can be useful in theoretical\nwork and for small test problems. In practice, computing the Green's function from the\neigensystem is often not the most efficient way. Rather, it is often better to exploit the fact\nthat the Green's function can be considered an inverse operator. This reduces the problem\nto computing the inverse of a matrix. Once again, we can exploit the sparseness of Β to\ncompute the Green's function.\nExtension to Two and Three Spatial Dimensions\nThe techniques we have described for solving the cable and diffusion equations in one\ndimension are readily generalized to two or even three spatial dimensions. Although the\nsolution of electrical potential in several dimensions is not common (see, however, Chap. 2),\nthe diffusion of second messengers such as Ca\n2+ often requires a consideration of two- or\nthree-dimensional effects.\nThe separability of the three-dimensional diffusion operator suggests an easy and efficient\nmethod for solving multidimensional diffusion. Consider the three-dimensional Laplacian\noperator in Cartesian coordinates,"}
{"text": "===== Page 19 =====\nC.2 Nonlinear Cable Equations \n· 499\n(C.44)\nSimilarly we can write the discrete approximation using matrix operators,\n(C.45)\nHere L^, L^, and Lz are very sparse matrices that approximate the second derivative in the\nx, y, and ζ directions, respectively. These operators can be applied sequentially, using for\nexample a stable implicit method, to advance the solution from t to t + 1.\nThe main difficulty associated with solving three-dimensional diffusion is bookkeeping,\nalthough the bookkeeping with this approach is simpler than if L,^ is used directly. The\nmatrices that comprise L3(< are sparse, but not tridiagonal or Hiñes. The locations of the\nnonzero entries depend on the precise spatial discretization.\nC.2 Nonlinear Cable Equations\nWe have focused on the numerical solution of the linear cable equation for two reasons.\nFirst, it is impossible to solve the nonlinear equations efficiently without first understanding\nthe techniques for linear solution. Second, the methods of numerical solution provide insight\ninto the equations themselves. Our approach was to compare the eigensystems of the discrete\nand continuous systems, and the correspondences were strong.\nHere we consider two important classes of nonlinear cable equations that arise frequently\nin the study of single neurons. The first class is a generalization of the Hodgkin-Huxley equa-\ntions, which describe the propagation of the action potential. The second class arises from\nthe effect of nonlinear saturable ionic buffering in diffusion. Unfortunately, we know of no\napproach to understanding the numerical solutions of these nonlinear equations that provides\nthe same kind of insight. This may be because most numerical techniques begin by lineariza-\ntion (eliminating the interesting properties of these equations), while the behavior of the non-\nlinear equations is best understood using phase space analysis (see Chap. 7) or by numerical\nsimulation. In the first section we provide an overview of the most widely used and efficient\nmethod for solving generalizations of the Hodgkin-Huxley equations. It is a direct method\nthat is so easy to implement that since its introduction a decade ago (Hiñes, 1984) it seems\nlargely to have supplanted the predictor-corrector method of Cooley and Dodge (1966) that\nreigned before. It is now standard on many widely distributed neural simulators, including\nGENESIS and NEURON (see www.klab.caltech/MNM for more information on these). In the\nnext section we describe a simple scheme for handling nonlinear saturable buffers.\nC.2.1 Generalized Hodgkin-Huxley Equations\nThe method we consider applies to a class of nonlinear cable equations generalized from the\nHodgkin-Huxley equations. They differ from the linear cable equation only by the addition\nof a term, Ini(V(x,t),t), which gives the current contributed by voltage-dependent\nmembrane channels,\n(C.46)\nThe most general form we need to consider for /i«(V, x, t) includes k different ionic\ncurrents,"}
{"text": "===== Page 20 =====\n500 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\nIm(V(x, 0, 0 = gi«i(V, 0 (V(je, 0 - Ei) + · · -+gigk(V, t) (V(x, t) - Ek)(CA7)\n(see Eq. 6.20). The total current is the sum of the contributions of the k individual currents\nin the membrane patch considered. Here ~gl is the density of type 1 membrane channels at\nany point along the neuron; V(x, t) — E\\ is the driving force for this conductance; and\n0 < gi(V, t) < 1 is the fraction of total conductance of the current, that is, the fraction\nof total channels of that type that is open in one patch of membrane. The interesting term\nin this expression is the function g(V, t), because it includes the nonlinearity. It has the\ngeneral form\n(C.48)\nwhere y is a gating particle, and r is an integer corresponding to the number of identical\ngating particles that need to be simultaneously present in order for current to flow (see\nSec. 6.2). In many cases, there are two or even three gating particles, usually activating and\ninactivating particles. In this case, a slightly more general product of the form g(V, t) =\nym(V, tYmyh(V, tYh must be used. This introduces no conceptual difficulties but does clutter\nthe notation.\nWe have suppressed the spatial dependence of the currents, since this occurs only through\nV(x,t). This fact simplifies the solution, since it means that the system of equations\ngoverning g in the spatially discretized system is diagonal. The variable y (V, t) (or variables\nym and vft) in turn obeys a first-order, nonlinear differential equation of the form\n(C.49)\nHere the function yoo(V) governs the steady-state behavior of y. It is monotonic and\nbounded between 0 and 1—it must be, for g to be guaranteed to always fall in the same\nrange. The Ty(V) governs the rate at which equilibrium is reached. For numerical solution\nan equivalent but more convenient form is\n(C.50)\nThe method we describe depends on the fact that the generalized Hodgkin-Huxley\nsystem is conditionally linear (Mascagni, 1989), meaning that the system is linear in Vf+1\nif g'h is known, and likewise linear in g^+ if V* is known. Thus we can alternate between\nsolving for V' at the times V', V'+1, V'+2,..., and solving for gh at intermediate points\ng'+ 2; g'+2, \nWe do this implicitly,\n(C.51)\nWe can solve this expression explicitly for y'+? in terms of y'~i and V, both of which are\nknown. The algebra is straightforward and the details are well described in Hiñes (1984).\nC.2.2 Calcium Buffering\nChapter 11 deals with how, under certain conditions, the equations describing calcium\ndynamics are formally equivalent to the cable equation. Furthermore, over a wide range\nof parameters, the linearized equations offer an adequate approximation to the nonlinear\ndynamics. Nevertheless, in some cases it is of interest to explore the behavior of the full\nnonlinear equations. (If linear two- or three-dimensional diffusional behavior is of interest,"}
{"text": "===== Page 21 =====\nC.2 Nonlinear Cable Equations \n· 501\nthe methods of Sec. C. 1.5 can be used.) For the most part the semi-implicit method described\nabove generalizes directly to calcium dynamics. A nonlinear saturable buffer raises special\nissues that must be considered separately.\nLet us treat the common case of a second-order buffer (Sec. 11.4.1),\n(C.52)\nwhere the rate constants / and b govern the equilibrium of free and bound calcium and\n[Ca \n],· is the concentration of free, intracellular calcium. (To simplify our exposition,\nwe consider only the case of a second-order buffer, but higher-order buffers introduce no\nqualitative differences.)\nTogether with the basic diffusion equation, with a simple calcium extrusion process\nΡ ([Ca\n2+],), and a saturable nondiffusible buffer (see also Eqs. 11.37,11.43and 11.48) we\nhave,\n(C.53)\n(C.54)\nwhere the calcium concentration [Ca\n2+],(jc, i) at time t and position χ in response to the\napplied calcium current density i (x, i) depends on the partial derivatives of [Ca\n +],·, the dif-\nfusion constant D of Ca\n2+, the diameter d of the cable, and the concentration of total buffer\nTB (the sum of the free buffer and the bound buffer). The first equation specifies the rate of\nchange in concentration of calcium as a function of diffusion, extrusion, buffer dynamics\nand influx. The second equation gives the rate of change in bound buffer concentration as\na function of buffer diffusion and buffer binding and unbinding (see also Eq. 11.48).\nThe difficulty arises because [Ca\n2+]; enters into Eq. C.53 in an essentially nonlinear\nway as [Ca\n2+],-[B], and not in the conditionally linear way as in the case of the Hodgkin-\nHuxley equations. However, we can discretize Eq. C.53 in such a way that the system\nbecomes conditionally linear. That is, we can rewrite the right-hand side of Eq. C.53 in\nterms of [Ca2+]¿+1 and [B · Ca]',\n(C.55)\nand then write Eq. C.54 in terms of [Ca2+]^ and [B · Ca]'+1,\n(C.56)\nThese implicit equations can now be advanced each time step by an O(ri) step (a single\nsparse matrix inversion for the Hiñes matrix) for Eq.C.55, and another O(n) step for the\ndiagonal matrix specified in Eq. C.56.\nC.2.3 Conclusion\nAlthough this appendix has ostensibly been about efficient numerical methods for single\nneuron simulation, the real aim has been to provide a deeper understanding of the equations"}
{"text": "===== Page 22 =====\n502 \n· \nSPARSE MATRIX METHODS FOR MODELING SINGLE NEURONS\nthat govern neurodynamics. In the case of the linear cable equation our approach has been to\ncarefully analyze the properties of the discrete eigensystem corresponding to the continuous\npartial differential equations underlying the cable equation. This led to stable and efficient\nmethods for simulating the cable equation, to an understanding of why fitting passive models\nto neurophysiological data is hard, and to a different way of looking at the Green's functions.\nFor the nonlinear case our goals were more modest, namely, an overview of the best existing\nmethod for solving equations of the Hodgkin-Huxley class, and of some special problems\nthat arise in the solution of calcium dynamics."}
{"text": "===== Page 1 =====\nLINEAR CABLE THEORY\nIn the previous chapter, we briefly met some of the key actors of this book. In particular, we\nintroduced the RC model of a patch of neuronal membrane and showed an instance where\nsuch a \"trivial\" model accounts reasonably well for the input-output properties of a neuron,\nas measured at its cell body (Fig. 1.4). However, almost none of the excitatory synapses are\nmade onto the cell body, contacting instead the very extensive dendritic arbor. As we will\ndiscuss in detail in Chap. 3 (see Fig. 3.1), dendritic trees can be quite large, containing up\nto 98% of the entire neuronal surface area. We therefore need to understand the behavior\nof these extended systems having a cablelike structure (Fig. 2.1).\nFig. 2.1 CLOSEUP VIEW OF DENDRITES Two\nreconstructed dendrites of a spiny stellate cell in the\nvisual cortex of the cat. The reconstructions were\ncarried out by a very laborious serial electron mi-\ncroscopic procedure. Notice the thin elongated,\nthomlike structures, dendritic spines. The vast\nmajority of neuronal processes, whether axons or\ndendrites, possess such an elongated, cylindrical\ngeometry. Studying the spread of electrical cur-\nrent in these structures is the subject of cable the-\nory. (A) Cross section of a branching dendrite.\n(B) Three-dimensional view of another dendrite.\nThe black blobs are excitatory synapses and the\nthree clear blobs are inhibitory synapses. Reprinted\nby permission from Anderson et al. (1994).\n25\n2"}
{"text": "===== Page 2 =====\n26 • \nLINEAR CABLE THEORY\nThe basic equation governing the dynamics of the membrane potential in thin and\nelongated neuronal processes, such as axons or dendrites, is the cable equation. It originated\nin the middle of the last century in the context of calculations carried out by Lord Kelvin,\nwho described the spread of potential along the submarine telegraph cable linking Great\nBritain and America. Around the turn of the century, Herman and others formulated the\nconcept of Kemleitermodel, or core conductor model, to understand the flow of current\nin nerve axons. Such a core conductor can be visualized as a thin membrane or sheath\nsurrounding a cylindrical and electrically conducting core of constant cross section placed\nin a solution of electrolytes (see Fig. 2.2).\nThe study of the partial differential equations describing the evolution of the electrical\npotential in these structures gave rise to a body of theoretical knowledge termed cable\ntheory. In the 1930s and 1940s concepts from cable theory were being applied to axonal\nfibers, in particular to the giant axon of the squid (Hodgkin and Rushton, 1946; Davis\nand Lorente de No, 1947).1 The application of cable theory to passive, spatially extended\ndendrites started in the late 1950s and blossomed in the 1960s and 1970s, primarily due to\nthe work of Rail (1989). In an appropriate gesture acknowledging his role in the genesis\nof quantitative modeling of single neurons, Segev, Rinzel, and Shepherd (1995) edited an\nannotated collection of his papers, to which we refer the interested reader. It also contains\npersonal recollections from many of Rail's colleagues as well as historical accounts of the\nearly history of this field.\nWe restrict ourselves in this chapter to studying linear cable theory, involving neuronal\nprocesses that only contain voltage-independent components. In particular, we assume\nthat the membrane can be adequately described by resistances and capacitances (passive\nmembrane). Given the widespread existence of dendritic nonlinearities, it could be argued\nthat studying neurons under such constraints will fail to reveal their true nature. However,\nit is also true that one cannot run before one can walk, and one cannot walk before one can\ncrawl. In order to understand the subtlety of massive synaptic input in spatially extended\npassive and active cables, one first needs to study the concepts and limitations of linear\ncable theory before advancing to nonlinear phenomena.\nCable theory, whether linear or nonlinear, is based on a number of assumptions concern-\ning the nature and geometry of neuronal tissue. Let us discuss these assumptions prior to\nstudying the behavior of the membrane potential in a single, unbranched, passive cable.\n2.1 Basic Assumptions Underlying One-Dimensional Cable Theory\nIn a standard copper wire, electrons drift along the gradient of the electrical potential. In\naxons or dendrites the charge carriers are not electrons but, in the main, one of two ionic\nspecies, sodium and potassium, and, to a lesser extent, calcium and chloride. How can this\ncurrent be quantified?\n1. Starting point for any complete description of electrical currents and fields must be\nMaxwell's equations governing the dynamics of the electric field E(x, y,z,t) and the\nmagnetic field B(;c, y, z, t),2 supplemented by the principle of conservation of charge\n1. For a detailed account of all the twists and turns of this story, see Cole (1972) and Hodgkin (1976). When reading these\ndown-to-earth monographs, one becomes painfully aware of the very limited amount of real knowledge and insight gained during\ndecades of intensive experimental and theoretical research. Most of one's effort is usually spent on pursuing details that turn out\nto be irrelevant and in constructing and developing incorrect models.\n2. We follow standard convention in using boldface variables for all vector quantities."}
{"text": "===== Page 3 =====\n2.1 Basic Assumptions Underlying One-Dimensional Cable Theory\n27\nFig. 2.2 ELECTRICAL STRUCTURE OF A CABLE \n(A) Idealized cylindrical axon or dendrite at the\nheart of one-dimensional cable theory. Almost all of the current inside the cylinder is longitudional\ndue to geometrical (the radius is much smaller than the length of the cable) and electrical factors\n(the membrane covering the axon or dendrite possesses a very high resistivity compared to the\nintracellular cytoplasm). As a consequence, the radial and angular components of the current can\nbe neglected, and the problem of determining the potential in these structures can be reduced from\nthree spatial dimensions to a single one. On the basis of the bidomain approximation, gradients in the\nextracellular potentials are neglected and the cable problem is expressed in terms of the transmembrane\npotential Vm(x, t) = Vf(x, t) — Ve.(B) Equivalent electrical structure of an arbitrary neuronal process.\nThe intracellular cytoplasm is modeled by the purely ohmic resistance R. This tacitly assumes that\nmovement of carriers is exclusively due to drift along the voltage gradient and not to diffusion. Here and\nin the following the extracellular resistance is assumed to be negligible and Ve is set to zero. The current\nper unit length across the membrane, whether it is passive or contains voltage-dependent elements,\nis described by im and the system is characterized by the second-order differential equation, Eq. 2.5.\n(Feynman, Leighton, and Sands, 1964). As detailed in Rosenfalck's thesis (1969), the\nmagnetic vector potential associated with the movement of charges during an action\npotential in biological tissues only has a negligible effect (10~9) on the electric field and\ncan therefore safely be neglected. Indeed, it took the technological development of very\nsensitive quantum devices (SQUIDs) to be able to measure the magnetic field associated\nwith massive electrical activity in the brain. So the first simplification involves neglecting\nthe magnetic field."}
{"text": "===== Page 4 =====\n28 \n• \nLINEAR CABLE THEORY\n2. This leaves us with three fundamental relationships governing electrodynamics in\nneuronal structures.\na. Gauss's law, stating that the divergence of the field E is identical to the charge density\nnormalized by the electrical permittivity €. Equivalently, Poisson's equation, which\nlinks the Laplacian of the electrical potential to the negative charge density normalized\nby e, serves as well.\nb. Charge conservation, that is, the sum of the flux of current through any closed surface\nand the change of the charge over time inside this surface must be zero.\nc. An equation linking the electrical current to the electric field. In general, charged\ncarriers can move either by drift along an electric field or by diffusion, from a volume\nof high carrier concentration into one of lower concentration, and the total current flow\nis the sum of these two independent components. The mathematical expression of\nthis fact constitutes the Nernst-Planck electrodiffusion equation, treated in Sec. 11.3.\nAs discussed there, for almost all cases of interest the changes in concentration of\nthe various ions (Na+, K+, Ca2+ and CP) are too small to measurably contribute to\ncurrent flow. Only in very thin fibers of less than 1 /um diameter does longitudinal\ncurrent flow due to concentration differences begin to play any role. In other words,\nOhm's law is perfectly adequate to describe the electrical current moving within an\naxon or dendrite.3\n3. This is the starting point for most derivations of cable theory (Lorente de No, 1947; Clark\nand Plonsey, 1966,1968; Plonsey, 1969; Rail, 1969b; Eisenberg and Johnson, 1970).\na. The dominant fraction of current inside a neuronal process, such as a dendrite or\naxon, flows parallel to its longitudinal axis. Only a very small fraction of the current\nflows across the neuronal membrane. This is true both for geometrical reasons—the\ndiameter of axons and dendrites being much smaller than their longitudinal extent—\nas well as for electrical ones. As detailed in Appendix A, the neuronal membrane is\nall but impermeable to current flow. Charged carriers can only cross the membrane\nthrough the ionic channels. The high transmembrane resisitivity stands in contrast to\nthe relatively small intracellular resistivity.\nA major implication is that instead of having to solve for the voltage in three\ndimensions, our problem is reduced to one of describing the voltage along a single\nspatial dimension. In a careful comparison between the membrane potential derived\nas the solution of Laplace's equation in a three-dimensional cylindrical coordinate\nsystem and the solution of the one-dimensional cable equation, Rail (1969b) showed\nthat the radial and angular membrane potential terms typically decay 104 times faster\nthan the components of the membrane potential along the axis. Fortuitously, we can\nsafely neglect two out of three dimensions for all of the cases considered in this book.\nb. Electrical charge in the cytoplasm, no matter whether inside or outside the cell,\nrelaxes in a matter of microseconds or less. In other words, any capacitive effects\nof the cytoplasm itself can be totally ignored on the millisecond or longer time\nscale (inductive effects can be completely neglected; Scott, 1971). Thus, from an\nelectrical point of view, the extracellular as well as the intracellular cytoplasm can\nbe approximated by ohmic resistances.\n3. This is analogous to the situation prevalent in a copper wire, where the current flow due to drift down the gradient of the\nelectrical potential exceeds by many orders of magnitude the current flow due to differences in the local densities of electrons."}
{"text": "===== Page 5 =====\n2.1 Basic Assumptions Underlying One-Dimensional Cable Theory \n• \n29\nc. The solution of the equation for the electrical potential is still extremely complicated\nif all the neuronal structures and membranes outside the dendrite or axon under\ninvestigation are explicitly included. Fortuitously for modelers (but less so for\nthe electrophysiologist, who has to infer the neuronal activity of a cell from its\nextracellular signature), the extracellular potential (1) usually is small (since the\nsmall amount of current making it through the membrane encounters a relatively\nlarge extracellular volume), and (2) decays over distances which are usually much\nlarger than the diameters of the fiber itself. This implies that the extracellular space\ncan be treated as a homogeneous dielectric, averaging over local inhomogeneities.\nThe problem of computing the membrane potential is therefore reduced to two\nhomogeneous domains, the extracellular and the intracellular ones.\nThe extracellular resistivity is often defined in the case when the external medium is\na shell of conducting cytoplasm surrounding the cable, a shell that can be characterized\nby a resistance per unit length of cylinder re. For large external volumes (think of\nthe case of a single neuronal fiber placed in a bath solution) re is assumed to be\nzero. In this case, no extracellular voltage gradients exist and the entire extracellular\nspace is isopotential, Ve(x, t) = const, which we set to zero. Including a uniform\nextracellular resistivity complicates matters only slightly, and the solution of the cable\nequation is qualitatively similar to the solution for re = 0. Therefore, the membrane\npotential Vm(x,t), defined as the intracellular potential minus the extracellular\npotential (Eq. 1.1), is identical to the intracellular potential. Indeed, throughout the\nbook, we use these two variables interchangeably. Yet it should always be kept in\nmind that the membrane potential corresponds to the difference in voltage across the\nmembrane separating the inside from the outside.\nA timely research topic of considerable interest is a detailed investigation of\nelectrical coupling of realistically modeled neurons via the extracellular potential.\nLengthy experimental and theoretical studies have been carried out for the case of\ntwo parallel axons. For this geometry, any direct electrical coupling is slight (the\nextracellular potential due to a spike is in the 10 //V range; Clark and Plonsey, 1968,\n1971; Marks and Loeb, 1976; Scott and Luzader, 1979; Barr and Plonsey, 1992; Bose\nand Jones, 1995; Struijk, 1997). However, extracellular potentials recorded close to\ndendritic trees can be much larger (up to a few mV) than those next to axons. Given\nthe extremely tight packing among neurons, this type of ephaptic4 coupling could be\nof functional relevance, yet almost no theoretical work has been carried out on this\nsubject (Lorente de No, 1953; Hubbard, Llinas and Quastel, 1969; Holt, 1998).\nAt this stage, we represent the neuronal tissue with the help of a series of discrete\nelectrical circuits of the type shown in Fig. 2.2B. Without making any specific assumption\nconcerning the detailed nature of the neuronal membrane, we express the current per unit\nlength flowing through the membrane at location x as im(x, t). We can write down Ohm's\nlaw for the discrete circuit illustrated in Fig. 2.2B,\n(2.1)\nor, in the limit of an infinitesimal small interval A*, and with Vm = Vj,\n(2.2)\n4. Greek for \"touching onto,\" rather than synoptic, \"touching together.\""}
{"text": "===== Page 6 =====\n30 . \nLINEAR CABLE THEORY\nwhere ra = R/Ax is the intracellular resistance per unit length of cable with dimensions\nof ohms per centimeter. 7* is the intracellular core current flowing along the cable, assumed\nto be positive when flowing toward the right, in the direction of increasing values of x.\nKirchhoff's law of current conservation stipulates that the sum of all currents flowing into\nand out of any particular node must equal zero. Applied to the node at x in Fig. 2.2B,\nwe have\n(2.3)\nor, in differential form in the limit that AJE -> 0,\n(2.4)\nInserting the spatial derivative of Eq. 2.2 into Eq. 2.4 leads to\n(2.5)\nThis second-order ordinary differential equation, together with appropriate boundary condi-\ntions, describes the membrane potential in an extended one-dimensional cable structure with\nan ohmic intracellular cytoplasm, regardless of the exact nature of the neuronal membrane.\n2.1.1 Linear Cable Equation\nIn Sec. 1.1, we discussed the nature of a patch of passive membrane and assumed that\nthe membrane current includes a capacitive (Eq. 1.3) and a resistive (Eq. 1.4) component\n(Figs. 1.1 and 1.2). Including an external current term Iinj(x, t), the membrane current per\nunit length of the cable, im, is given by\n(2.6)\nwhere rm is the membrane resistance of a unit length of fiber, measured in units of ohms-\ncentimeter. If the electrical nature of the membrane is constant along the length of the\npassive fiber under investigation (Fig. 2.3), we can replace im(x, t) on the right-hand side\nof Eq. 2.5 with Eq. 2.6 and multiply both sides with rm to arrive at\n(2-7)\nwith the membrane time constant im = rmcm and the steady-state space constant A, =\n(rm/ra)1^2- We will discuss their significance forthwith.\nEquation 2.7 is the linear cable equation, a partial differential equation, first order in\ntime and second order in space. This type of parabolic differential equation is quite similar\nto the heat and diffusion equations. The behavior of all three is characterized by dissipation\nand the absence of any wavelike solution with constant velocity. Parabolic differential\nequations have a well-specified and unique solution if appropriate initial conditions, such\nas the voltage throughout the cable at t = 0 should be zero, or boundary conditions, such\nas no current should leak out at either end of the cable, are specified. The cable equation is\nfundamental to understanding the behavior of the membrane potential, the principal state\nvariable used for rapid intracellular communication in neurons. We will discuss its behavior\nin both this chapter and the next.\nAs expressed in Eq. 2.7, a simple, unbranched cable has a nonzero resting potential Vrest\nwhich does not vary with the position along the cable. For a homogeneous cable in the"}
{"text": "===== Page 7 =====\n2.1 Basic Assumptions Underlying One-Dimensional Cable Theory\n31\nFig. 2.3 A SINGLE PASSIVE CABLE Equivalent lumped electrical circuit of an elongated neuronal\nfiber with passive membrane. The intracellular cytoplasm is described by an ohmic resistance per unit\nlength ra and the membrane by a capacitance cm in parallel with a passive membrane resistance rm\nand a battery Vrest. The latter two components are frequently referred to as leak resistance and leak\nbattery. An external current I\\^(x, t) is injected into the cable. The associated linear cable equation\n(Eq. 2.7) describes the dynamics of the electrical potential Vm = V; — Ve along the cable.\nabsence of any input lm](x, t), the membrane potential throughout the cable will be equal\nto a constant. The amplitude of Vrest varies between —50 and —90 mV, depending on cell\ntype and other circumstances, with the inside of the neuron being at the negative potential.\nVrest need not always be constant throughout the dendritic tree (see Sec. 18.3.4).\nBecause the resting potential is simply an offset, it is often set to zero. This can be thought\nof as defining the membrane potential Vm(x, t) as relative to this resting potential. Very\noften the equations will be somewhat simplified when the potential is defined as relative to\nVrest- We use the convention that Vm(x, t) refers to the absolute membrane potential, while\nV(x, t) refers to the potential relative to Vrest-\nWe should here also allude to the vexing question of units. The three voltage-independent\ncomponents of a passive cable are commonly specified in one of two ways. If they are\nexpressed as quantities per unit length, they are conventionally labeled\n(2.8)\nin units of £2/cm,\n(2.9)\nin units of fi-cm and\n(2.10)\nin units of F/cm. Using these variables has the advantage that the cable equation contains\nno explicit terms depending on the diameter d of the cable.\nThe more common way, and the one we adopt throughout the book, is to specify these\nquantities in units that are independent of the diameter of the fiber, using capital letters: the"}
{"text": "===== Page 8 =====\n32 . \nLINEAR CABLE THEORY\nintracellular resistivity /?;, the specific membrane resistance Rm and the specific membrane\ncapacitance Cm, with dimensions of £2-cm, £2 • cm2 and F/cm2, respectively. For more\ndetails, consult Appendix A.\n2.2 Steady-State Solutions\nLet us investigate the behavior of the cable equation in response to a current 7jnj (x) injected\nat location x via an intracellular microelectrode or a synapse. We assume that the current is\nswitched on at t =0 and remains on. One frequently encounters this situation in experiments\nto investigate the cable properties of neurons and axons. After some initial transients, the\nvoltage will reach a steady-state value. To compute the steady-state membrane potential,\nwe set dV/dt — 0 and write the cable equation as\n(2.11)\nThis reduces the original partial differential Eq. 2.7 to an ordinary second-order differential\nequation depending solely on space. We now study its solutions for different neuronal\ngeometries.\n2.2.1 Infinite Cable\nWe begin by assuming that a current 7jnj of constant amplitude is injected at the origin, x = 0,\nof an infinite cable of diameter d. Mathematically, we describe this by setting /inj(jc) to\n/o<$ (x), where S (x) is the Dirac delta or impulse distribution in space. As boundary condition\nwe assume that the voltage at the two infinitely distant terminals goes to zero as \\x | —>• oo.\nUsing the theory of Fourier transforms (see Appendix B) we arrive at the solution\n(2.12)\nwith VQ = /orm/(2A.). This solution can easily be verified by placing it into Eq. 2.11. The\nstationary voltage distribution, sometimes referred to as the electrotonus, in the infinite\ncable decays exponentially away from the site of injection. The parameter controlling this\ndecay is the space constant X. The voltage decreases to e~l, that is, to 37% of its original\nvalue, at x = A and to e~2, or 13% of its original value, at x = 2A. In the derivation of the\ncable equation, the steady-state space constant is defined as\n(2.13)\nThe larger the membrane resistance Rm, the less current leaks across the membrane and the\nlarger the space constant A. Furthermore, a thick dendrite has a larger space constant than\na thin one, reflecting the fact that the spread of current is enhanced by a larger diameter.\nAnother way of deriving A involves computing the distance / over which the total resistance\nto current flowing across the membrane is identical to the total longitudinal resistance.\nPaying careful attention to the relevant units, we have rm/l — ral, or / = ^/rm/ra — A.\nFor a typical apical dendrite of a cortical cell with a 4 ju,m diameter, /?, = 200 S2-cm and\nRm — 20, 000 £2 • cm2, the space constant A comes out to be 1 mm. This large distance,\ncompared to the diameter of the dendrite, is the reason why we can neglect the radial\ncomponents of voltage along these cables."}
{"text": "===== Page 9 =====\n2.2 Steady-State Solutions \n• \n33\nGiven the importance of A. for the electrotonic spread of the potential in a neuron, we\nfrequently normalize the spatial coordinate x with respect to A., expressing it in dimension-\nless units: X = x/K. Any particular distance I can likewise be expressed in terms of the\nassociated dimensionless electrotonic distance L = t/K.\nWhat is the input resistance of the infinite cable? Operationally, it is measured by inserting\nan electrode that passes current and, at a distance that is small compared to A., an electrode\nto record the voltage. In the limit that this distance shrinks to zero, we can write\n(2.14)\nThe last equality holds because the input resistance at the location of the injecting electrode\nis, by definition, equal to the ratio of the evoked potential to the injected current causing\nthis change. It follows that\n(2.15)\nThe input resistance is-—as expected—constant throughout the infinite and homogeneous\ncable. Confirming our intuition, increasing either the membrane resistance or the intracel-\nlular resistivity will increase R-m.\nConceptually, we can think of an infinite cable as two semi-infinite cables, one going off\nto the left and one to the right. The input resistance associated with a single semi-infinite\ncable ROO must therefore be twice the resistance associated with the infinite cable (since\ncurrent can only flow in one direction); or\n(2-16)\nThis variable, rather than the resistance associated with an infinite cable, is called R^,,\nsince it corresponds to the situation of a soma with a single dendrite extending into infinity\n(Rail, 1959).\nThe input conductance of a semi-infinite cylinder is given by the inverse of Eq. 2.16,\n(2.17)\nThe input conductance decreases as the square root of the membrane resistance Rm and\nincreases as the | power of the diameter of the fiber, a relationship that will be important\nlater on.\nThe input resistance of a patch of membrane is linearly related to the membrane resistance\nRm (with the constant of proportionality given by the total membrane area). In general, as\nthe dimensionality of the space increases, the dependency of the input resistance on Rm\nlessens. Thus, Rm in an infinite! cable is proportional to the square root of Rm. For a\ntwo-dimensional resistive sheet j/?in oc log(/?m). In a three dimensional syncytium (such\nas muscle tissue) R-m oc e~lfRm \n(see Chap. 3 in Jack, Noble, and Tsien, 1975; Eisenberg\nand Johnson, 1970). Given the area- or volume-filling geometry of the dendritic tree, the\ndependency of its input resistance on Rm falls somewhere between that of an infinite cable\nand that of the resistive sheet.\n2.2.2 Finite Cable\nReal neurons certainly do not possess infinitely long dendrites, so we need to consider\na finite piece of cable of total electrotonic length L = i/X. The general solution to the"}
{"text": "===== Page 10 =====\n34 \n• \nLINEAR CABLE THEORY\nlinear second-order ordinary differential cable equation can be expressed in normalized\nelectrotonic units as\n(2.18)\nwith cosh(jc) = (e* + e~x)/2 and sinh(^) = (ex - e~x)/2. The values of a and ft depend\non the type of boundary conditions imposed at the two terminals. (What happens at the end\nof the finite cable influences the voltage throughout the fiber.) We distinguish three different\nboundary conditions.\nSealed-End Boundary Condition\nThis is the boundary condition of most relevance to neurons embedded in the living tissue.\nIt assumes that the end of the fiber is covered with neuronal membrane with resistance Rm.\nIt follows that the resistance terminating the equivalent circuit in Fig. 2.3 has the value\n4Rm/ird2. For d = 2 jitm and Rm = 10s £2 • cm2 this is about 3000 Gfi, a value so\nhigh that for all intents and purposes we can consider it to be infinite. If the terminating\nresistance is infinite, no axial current /, (X = L) will flow. And since the axial current is\ngiven by the derivative of the voltage along the cable, this implies that\n(2.19)\nat the terminal. This zero-slope or von Neumann boundary condition is referred to as a\nsealed-end boundary condition and is the one commonly adopted to model the terminals of\ndendrites or other neuronal processes. Applying Eq. 2.19 to Eq. 2.18 leads to\n(2.20)\nFigure 2.4 illustrates the voltage profile in a short and a long cable with such a sealed-end\nboundary condition. As expected from Eq. 2.19, the slope of both curves flattens out as the\nterminal is approached. Furthermore, both curves lie above the voltage decay in a semi-\ninfinite cable. In other words, the voltage in a cable with a sealed end—regardless of its\nlength—decays less rapidly than the voltage in a semi-infinite cable.\nWe compute the input resistance R-m at the origin of the cable, looking into the cable\ntoward its terminal, using the same strategy as in the previous section,\n(2.21)\nwith coth(^) = cosh(j«c)/sinh(jc). This is plotted in Fig. 2.5 (upper curve). This input\nresistance is always higher than that of the semi-infinite cable, since the intra-axial current\n/,• is prevented from leaving the cable at the endpoint of the cable.\nKilled-End Boundary Condition\nAnother type of boundary condition is of relevance when the dendrite or axon is\nphysically cut open or otherwise short-circuited. Under these conditions the intracellular\npotential at the terminal is identical to the extracellular potential, that is, the effective\npotential is set to zero,\n(2.22)\nThis Dirichlet type of boundary condition is also known as open- or killed-end boundary"}
{"text": "===== Page 11 =====\n2.2 Steady-State Solutions\n35\nFig. 2.4 STEADY-STATE VOLTAGE ATTENUATION \nSteady-state voltage attenuation in a finite piece\nof cable as a function of the normalized electrotonic distance X = x/k from the left terminal. The\npotential at the left terminal is always held fixed at V = VQ, while the normalized potential throughout\nthe cable varies with the boundary condition at the right terminal. The bold continuous line corresponds\nto the voltage in a semi-infinite cable, showing a pure exponential decay. The thin continuous lines\nshow the voltage decay for two cables that terminate in a sealed end (Eq. 2.20) at X = 1 or X = 2. This\nis the type of boundary condition used most commonly in simulations. The two thin dashed curves\nshow the same two cables, but now terminating in a short circuit (killed-end boundary condition;\nEq. 2.23). Note that either the spatial derivative of voltage (sealed-end) or the voltage itself (killed-\nend) is zero at the rightmost terminal. That the spatial voltage profile can be nonmonotonic in a passive\ncable is witnessed by the topmost bold dashed Curve, where the voltage at X — 1 is clamped to 1.1\ntimes the voltage at the origin. For the lower bold dashed curve, the voltage at the terminal is clamped\nto 0.2Vb. Reprinted in modified form by permission from Rail (1989).\ncondition and corresponds to setting the terminating resistance to zero. It follows that the\nvoltage along the cable is\n(2.23)\nand the input resistance is\n(2.24)\nwith tanh(x) = sinhC*)/ cosh(;c). The two thin dashed curves in Fig. 2.4 are the voltage\nprofiles along two cables of electrotonic length L = 1 and 2 with a killed-end boundary\ncondition. Their values are always less than the voltage at the corresponding location in a\nsemi-infinite cable. Correspondingly, the input resistance of these cables is always less than\nthat of the semi-infinite cable (Fig. 2.5). The input resistance at the origin X = 0 of the"}
{"text": "===== Page 12 =====\n36 \n• \nLINEAR CABLE THEORY\nFig. 2.5 INPUT RESISTANCE OF A FINITE CABLE Input resistance Rin looking into a cable of\nelectrotonic length L toward the right terminal. The ordinate is normalized in terms of the input\nresistance R-m of a semi-infinite cylinder (Eq. 2.16). The normalized input resistance for a sealed-end\nboundary condition (upper curve) is always larger than Rx, while the input resistance of a cable\nwith killed-end boundary condition (lower curve) is always less. In the former case, the current is\nprevented from leaving the cable at the endpoint, while the voltage is \"shorted to ground\" in the latter\ncase. For cables longer than two space constants, Rm «» Rx.\ncable is inversely proportional (see Eq. 2.2) to the slope of V. The actual input resistance as\na function of the electrotonic length of a killed-end cable is shown in Fig. 2.5 (lower curve).\nArbitrary Boundary Condition\nIn general, the terminal has neither infinite (sealed-end) nor zero (killed-end) resistance,\nbut some finite value RL. This, for instance, is the case if the cable is connected to some\nother cable or even to an entire dendritic tree. If we know the value of the voltage at this\nboundary, that is, VL, we can express the voltage as\n(2.25)\nNotice how this expression takes on the value VQ at X = 0 and VL at X — L. In Fig. 2.4\nwe show two such cases in which VL is either clamped to 0.2 VQ or to 1.1 VQ (causing the\nnon-monotonic appearance). Note that this sagged appearance is a direct consequence of\nthe unusual boundary condition.\nThe leak current through the terminal follows from Ohm's law and Eq. 2.2 as\n(2.26)"}
{"text": "===== Page 13 =====\n2.3 Time-Dependent Solutions \n• \n37\nWe can now rewrite Eq. 2.25 as\n(2.27)\nresulting in a general expression for the voltage in a finite piece of cable.\nWith the help of Eq. 2.3 and the above equation, we can derive an expression for the\ninput resistance of a cable of length L with a terminating resistance RL,\n(2.28)\nThe previous two equations allow us to obtain the values for the voltage and the input\nresistance for the sealed-end and the killed-end boundary conditions by setting RI to either\noo or 0. Furthermore we recover /?jn = 7?oo for an infinite cable (since tanh(L) goes to 1\nas L -> oo).\n2.3 Time-Dependent Solutions\nSo far, we have only been concerned with the behavior of the voltage in a cable in response\nto a stationary current injection, a situation where the voltage settles to a constant value. In\ngeneral though, we need to consider the voltage trajectory in response to some time-varying\ncurrent input. Since the time-dependent solution of the cable equation is substantially more\ncomplex than the steady-state solution treated above, we will only discuss the solution to\ntwo special cases. The interested reader is referred to the monographs by Jack, Noble, and\nTsien (1975) and by Tuckwell (1988a) for a treatment of many more cases of interest.\nBefore we do so, we will introduce a normalized version of the cable equation. Recalling\nthe definition of the neuronal time constant from Chap. 1 as\n(2.29)\nallows us to introduce dimensionless variables for both time, T = t/rm, and space,\nX = x/K. Written in these units and taking care to properly transfrom the input current\n(Sec. 4.4 in Tuckwell, 1988a), the cable equation becomes\n(2.30)\nwith Iinj(X, T) — A.Tm/inj(;c, t) and rmj(x, t) corresponds to the stimulus current density.\n2.3.1 Infinite Cable\nIn order to compute the dynamic behavior of the infinite cable in response to current\ninjections we will once again exploit the linearity of Eq. 2.30, that is, the fact that if the\nresponse of the membrane to the current I (X, T) is V(X, T), the polarization in response\nto the current <xI(X, T) is a V(X, T).\nVoltage Response to a Current Pulse\nAs discussed in the first chapter (and summarized in Appendix B), we can completely\ncharacterize the system by computing the impulse response or Green's function associated\nwith Eq. 2.30, which we do by transforming to the Fourier domain, assuming that V(X) \n—»•\n0 as | X | -*• oo, and transferring back to the time domain (Jack, Noble, and Tsien, 1975)."}
{"text": "===== Page 14 =====\n38 \n• \nLINEAR CABLE THEORY\nAssuming that a fixed amount of charge Qo is applied at X = 0 as an infinitely brief pulse\nof current /o = Qo/rm, the resultant voltage is\n(2.31)\nIn order to gain a better intuitive understanding of cable theory, let us review various\nspecial cases. If we record the voltage at the same location at which we injected the current,\ncorresponding to X = 0, the Green's function is proportional to e~T /^/T. Vg(0, T)\ndiverges at the origin and decays a bit faster than exponentially for large times (Figs. 2.6\nand 2.7B). The singularity at the origin comes about due to the infinitesimal amount of\ncapacitance for X = 0 between the site of current injection and that of the measuring\ndevice. Using L'Hopital's rule, we can see that the limit of Vg(X, T) for any value of\nX other than the origin is zero. No singularity exists for the membrane patch model,\nwhich possesses a simple exponentially decaying Green's function with a fixed amount\nof capacitance (Fig. 2.6).\nIf one waits long enough, the voltage decay throughout the cable will be identical,\napproaching more and more to the decay seen at the spatial origin. This can be observed\nFig. 2.6 IMPULSE RESPONSE AT THE ORIGIN OF AN INFINITE CABLE Comparison of the impulse\nresponse or Green's function for an infinite cable V$(X, T) (Eq. 2.31) for X = 0 and X = 1 and\nthe normalized Green's function for a patch of passive membrane (e~T; Eq. 1.17; dashed line) on a\nlogarithmic voltage scale. Time is expressed in units of Tm. The voltage in the infinite cable (solid\nlines) is measured at the location where the S pulse of current is applied or one space constant A. away.\nThe Green's function diverges at X = 0, since the amount of membrane capacitance between the\ncurrent injection and the voltage recording electrode is infinitely small, but has a constant value of C\nfor the membrane patch case."}
{"text": "===== Page 15 =====\n2.3 Time-Dependent Solutions \n• \n39\nFig. 2.7 IMPULSE RESPONSE OF THE INFINITE CABLE Impulse response or Green's function\nV$(X, T) (Eq. 2.31) at X = 0 and T = 0 as a function of normalized space (A) and time (B) using\ndifferent linear scales. At any point in time, the spatial profile of the voltage along the cable in the\nupper panel can be described by a Gaussian. One of the consequences of the low-pass nature of the\nmembrane is evident in the bottom panel: more distant locations respond with a delay.\nvery well using a logarithmic scale, as in Fig. 2.6, where the decay ultimately scales as\ne~'/rm.\nIn other words, immediately following the current injection, the voltage is sharply peaked\naround the injection site. As time goes by, the spatial voltage profile becomes broader,\nsmearing out more and more (Fig. 2.7A). At any particular instant T, the spatial distribution\nof the voltage along the cable is proportional to e~x , corresponding to a Gaussian function\ncentered at the injection site. If the time course of voltage is recorded at increasing distances\nfrom the site of the charge application, the voltage response takes longer to reach its peak\ndue to the low-pass nature of the membrane capacitance (Fig. 2.7B).\nSince the nature of the filtering carried out by the membrane does not depend on the\napplied potential across it, the system is a linear one. It follows that the voltage in response\nto an arbitrary current stimulus /mjCjT) injected at the origin is given by the superposition\nof the impulse response function with the input current. Conceptually, one can think of the\ninput /injCO as a series of delta pulses staggered in time. The final voltage is the sum of the"}
{"text": "===== Page 16 =====\n40 \n• \nLINEAR CABLE THEORY\nimpulse functions associated (and weighted) with the individual pulses. This is concisely\nexpressed by the convolution integral\nwhere * denotes convolution and the rm/<2o factor is responsible for the correct normal-\nization (converting the voltage V& into an impedance).\nVoltage Response to a Current Step\nIf a rectangular step of current is injected into the cable, such that/jn^T1) = /oforT > 0\nand 0 otherwise, the previous integral evaluates to /o(tm/<2o) /o Vs(X> T')dT'. Due to\nthe presence of the Gaussian term in Eq. 2.31, this integral has no closed-form solution and\nmust be expressed in terms of the error function erf (x) = J= f* e~y dy (as first carried\nout by Hodgkin and Rushton, 1946). While we will not discuss the full solution in all of\nits glory (see Eq. 3.24 in Jack, Noble, and Tsien, 1975), we will consider several cases of\nparticular interest to us. The transient voltage response at the site of current injection is\n(2.33)\nSince erf (1) = 0.84, the voltage in the cable at the site of the current step rises to 84%\nof its steady-state value in one time constant, compared to 63% of its peak value for the\nexponential charging in the case of a patch of membrane (see Eq. 1.9). Conceptually, this\nlatter case can be thought of as resulting from injecting a current into a cable, only that\nin the membrane patch case the entire cable has been \"space clamped\" by introducing an\nimaginary wire along its length. Let us plot the normalized membrane potential for both\ncases (Fig. 2.8A), that is, the potential relative to its steady-state value,\n(2-34)\nand plotted in Fig. 2.8 A.\nWhen considering why the potential in the cable reaches its steady-state value faster\nthan the potential across a patch of membrane with the same input impedance, it is helpful\nto consider the closely related problem (for linear cables) of why voltage decays faster\nin a cable than across a membrane patch. In the former, current can flow longitudinally\nand therefore escapes more rapidly than when it must all flow across the membrane. The\nsame argument holds for an electrode injecting current into the soma of a neuron with an\nextended dendritic tree. A significant fraction of the injected current flows onto the extensive\ndendritic membrane surface, and as a result, the buildup and decay of the somatic voltage is\nfaster compared to the isopotential membrane patch. This effect was first recognized by Rail\n(1957). Coombs, Eccles, and Fatt (1955) had fitted the experimentally observed membrane\ntransients at the motoneuron soma with a single exponential with rm = 2 msec. Rail argued\nthat the cable properties of the dendrites needed to be accounted for and—on the basis of\nEq. 2.33—estimated a rm of 4 msec.\nFrequency-Dependent Space Constant\nAs expressed by Eq. 2.12, the steady-state space constant X is defined as the distance\nin an infinite cable over which a steady-state voltage decays by l/e. This variable can\nbe generalized to a function depending on frequency /. If a sinusoidal current I(t) =\n/o sin(27r/f) is injected into an infinite cable, the theory of Fourier transforms can be"}
{"text": "===== Page 17 =====\n2.3 Time-Dependent Solutions \n• \n41\nFig. 2.8 VOLTAGE IN AN INFINITE CABLE IN RESPONSE TO A CURRENT STEP \n(A) Normalized\nvoltage W(X, T), that is, V(X, T) divided by its steady-state value at location X (Eq. 2.34), in an\ninfinite cable in response to a current step injected at the same location is compared to the voltage\nincrease in response to the same current injected into a patch of membrane. In the cable, the voltage\nrises faster to its final value than in a patch of membrane. (B) The normalized voltage in response to\na current step in an infinite cylinder at different distances X from the site of current injection. As X\nincreases, the response becomes more smeared out. The point at which the voltage at any one location\nX reaches half of its final value moves—in the limit for long times—with constant velocity 2X/rm\nalong the cable (see Sec. 2.4).\nused to introduce a generalized frequency-dependent or transient space constant A(/) (see\nAppendix B).\nIt is relatively straightforward to formulate the cable equation in a generalized linear\nsystem by representing a finite or an infinite cable as a ladder network with arbitrary\nintracellular impedance zfl(/) and membrane impedance zm(/) (Koch and Poggio, 1985a).\nIn this case, the membrane can contain any collection of linear elements, including induc-\ntances, capacitances, and resistances. Under certain conditions, explored in more detail in\nChap. 10, a nonlinear membrane can be linearized and can be expressed by a combination\nof these linear circuit elements."}
{"text": "===== Page 18 =====\n42\nLINEAR CABLE THEORY\nIf the current /o sin(27r/f) is injected into an infinite cable, the voltage at any point in\nthe cable will be proportional to sin(2nft + 0), with a phase shift </>(/)• The constant\nof proportionality is given by e~v<-^x, where x is the distance between the site of current\ninjection and the recording electrode, and the propagation constant is\n(2.35)\nFor a passive membrane, za(f) = ra and z m ( f ) — rm/(\\ + i2nfTm). By extracting the\nreal part of this function, we can define the frequency-dependent space constant A.(/) as\n(2.36)\nwith A(0) the sustained or steady-state space constant (Eisenberg and Johnson, 1970). As\nseen in Fig. 2.9, X(f) decays steeply with increasing frequency, becoming proportional\nto l/\\//Tm in the limit of 2nfTm \ny>> 1. This decay is due to the distributed membrane\ncapacitance that soaks up more and more of the current as the frequency increases. For\ninstance, at 1000 Hz (roughly corresponding to the inverse of the width of a typical action\npotential), A. has decreased to 8% of its steady-state value (assuming xm = 50 msec).\nThis emphasizes once again the low-pass nature of the passive neuronal membrane: high\nfrequencies are preferentially filtered out.\nThe frequency-dependent space constant also informs us about the limit of one-dimen-\nsional cable theory. It is clear that as A.(/) *» d, the diameter of the fiber, one can no\nlonger neglect the radial dimensions of the cable and has to treat the full three-dimensional\nproblem. In general, this is not expected to occur until very high frequencies. For instance,\nin an apical dendrite of 4-ju<m diameter with Rm = 50,000 2nf cm2, /?, = 200 2nf cm,\nand Cm = 1 (iF/cm2. The steady-state space constant X equals 1.581 mm. Under these\nconditions, one has to go to frequencies close to 1 MHz in order for A. (/) « d. We conclude\nthat under most circumstances, one-dimensional cable theory will hold.\nFig. 2.9 FREQUENCY-DEPENDENT SPACE CONSTANT If a sinusoidal current of frequency / is\ninjected at x = 0 into an infinite passive cable, the voltage at location x will also be a sinusoid of\nfrequency /, but attenuated by e~lfx<-^ and phase shifted. We here plot A.(/), normalized by the\nsteady-state space constant X(0), for rm = 50 msec. At 1 kHz, the space constant has decayed to 8%\nof its original value."}
{"text": "===== Page 19 =====\n2.3 Time-Dependent Solutions \n• \n43\n2.3.2 Finite Cable\nA number of different techniques are available for computing the Green's function in finite\ncables (Tuckwell, 1988a). One classical method, known as separation of variables, assumes\nthat the Green's function can be written as the product of two functions, one depending only\non X while the other one depends solely on T. Rail (1969a) used this technique to derive\nthe voltage in a finite cable with sealed-end boundary conditions at X = 0 and X = L. The\nvoltage in response to an arbitrary current input anywhere in the cable can be expressed as\nan infinite series,\n(2.37)\nThe Bn depend on the initial conditions chosen, such as injection of a delta pulse current or\na current step. The coefficients an are the ratios of the membrane time constant rm to the\nequalizing time constants that are associated with the redistribution of charge and with the\nreduction of voltage differences between different regions of the cable. They are defined as\n(2.38)\nThe physical intuition behind Eq. 2.37 is that each term results from a \"reflection\" of the\nvoltage at one of the terminals. Each term becomes progressively smaller as it is reflected\nback and forth an infinite number of times (Fig. 2.10). Another way to understand Eq. 2.37\nis to note that the n = 0 term, relating to the slowest decay, is constant throughout the cable\nand corresponds to an exponential decay away from the average voltage along the finite\ncable. The n — 1 term is associated with decay and rapid equalization of charge between\ntwo half-lengths of the cylinder (V(X, T) is positive for 0 < X < L/2 and negative for\nthe other half of the cylinder). Higher order terms lead to an even more rapid equalization\nof charge over shorter lengths of the cable. The sum in Eq. 2.37 can also be expressed as\nFig. 2.10 VOLTAGE RESPONSE IN A FINITE CABLE In a finite cable with sealed end boundary\nconditions at X = 0 and 1, the voltage V in response to any current input can be described as the\nsum (bold line) of infinitely many \"reflection\" terms (thin lines), each term becoming progressively\nsmaller. This leads to the convergent series in Eq. 2.41. Here, the current is injected at X = 0."}
{"text": "===== Page 20 =====\n44 \n• \nLINEAR CABLE THEORY\n(2.39)\nwhere the C,-'s depend both on the initial conditions as well as on X, and VOQ captures the\nsteady-state components of the voltage. TO equals the passive membrane time constant Tm,\nwhile all other values of rn, called equalizing time constants, are smaller than TO, with\n(2.40)\nThe voltage decay for large t is always dominated by the largest time constant TQ — Tm. In\nother words, if the logarithm of the membrane potential in response to, say, a current step,\nis plotted as a function of time, the linear slope at the tail end of the pulse is identical to the\nmembrane time constant rm (see also Fig. 2.6 and the next chapter).\nA different way to express the voltage in a finite piece of cable with sealed-end boundary\ncondition at X — 0 and X = L in response to a current pulse (with /o = Q/Tm) at X = 0\nuses the theory of Laplace transforms (Jack, Noble, and Tsien, 1975; Tuckwell, 1988a),\n(2.41)\nAs L —»• oo, we recover the impulse response function for the infinite cable (Eq. 2.31).\nThis expression has a simple graphical interpretation (as expressed in Fig. 2.10) in terms of\never reflecting and decreasing contributions, corresponding to an infinite number of virtual\nelectrodes that inject charge at ±L, ±2L, • • •\nWhile the time constants in depend on L, they are independent of the site of the current\npulse, the site of recording, or the initial conditions. Thus, they provide a convenient way\nto calculate the electrotonic length of a cable. In particular, the ratio of the first two time\nconstants provides a measure of the cable's electrotonic length,\n(2.42)\nThis expression has frequently been applied to experimental data from different preparations\nby measuring the two slowest time constants, obtained by peeling the slopes (time constants)\nof the logarithm of the transient voltage response (see Fig. 3.12). The outcome of this\nprocedure is quite dependent on the neuronal geometry of the cell recorded from and the\namount of noise in the measured voltage transient and needs to be used with great care.\nFor an overview of the advantages and limitations of this method see Rail et al. (1992) and\nHolmes, Segev, and Rail, (1992).\nA related, but more complicated, expression for the voltage can be derived if one end\nof the finite cable is terminated with an equipotential \"soma,\" consisting of a somatic leak\nand capacitance (Rail, 1962, 1969a). We refer the interested reader to the monograph by\nTuckwell (1988a) that lists the Green's functions associated with a host of other neuronal\ngeometries and initial conditions.\n2.4 Neuronal Delays and Propagation Velocity\nHow fast does the potential induced by the current step propagate along the cable? Fig-\nure 2.8B shows the relative voltage change along an infinite cable in response to a current"}
{"text": "===== Page 21 =====\n2.4 Neuronal Delays and Propagation Velocity \n• \n45\nstep. The potential is normalized at each location by its steady-state value (Eq. 2.34). This\nnormalization accounts for the effect of the exponential attenuation of V along the cable\n(Eq. 2.12). Because the membrane capacitance preferentially \"soaks\" up electrical charge\nassociated with high temporal frequencies, the farther any particular point X is away from\nthe site of current injection, the longer it takes W(X, T) to rise to a particular value, say 0.5\n(that is, half of its maximum). Is this delay proportional to the distance or, in other words,\ncan one define a propagation velocity?\nThe linear cable equation does not admit any wave solution due to the dissipation of\nenergy through the passive membrane. As long as no inductive elements are present in the\nneuronal membrane across which the current cannot change instantaneously, the voltage\nwill, in principle, respond infinitely fast to a change in the current input an arbitrary distance\naway. In other words, the answer to the above question is, \"No, in general one cannot define\na velocity in a passive cable.\" This can change if voltage-dependent nonlinear components\nare incorporated in the membrane, as witnessed by the propagation of spikes at constant\nvelocities along axons.\nYet all is not lost. Because one cannot readily define the delay of voltages in passive\ncables, Agmon-Snir and Segev (1993) used the trick of computing the propagation delay of\nthe centroid or the first moment of the voltage or the current in a passive cable. Following\nthe nomenclature of Zador, Agmon-Snir, and Segev (1995), we define the centroid of the\nsignal h(x,t) at location x as\n(2.43)\nHere h can be either a current or a voltage with a single peak or with multiple peaks. This\nmeasure is frequently also called the center of mass if t is thought of as distance and h(x,t)\nas the mass distribution, with\n(2.44)\nWe define the transfer delay as the difference between the centroid of the induced voltage\nmeasured at location y and the centroid of current that was injected at location jc,\n(2.45)\nWe define the input or the local delay in the same spirit as the difference between the\ncentroids of the voltage response and the current that gave rise to it,\n(2.46)\nIt is possible to prove a number of useful properties of these delays by multiplying the\ncable equation by t and integrating over t. This results in an ordinary linear differential\nequation, similar to the steady-state cable equation, which can be analyzed by very similar\ntechniques. Most importantly, Agmon-Snir and Segev (1993) prove that the transfer delay\nDxy is always positive and is independent of the shape of the transient input current. In\nother words, Dxy is a property of the passive cable and not of the input. Furthermore, no\nmatter what the electrical structure of the cable under consideration, the transfer delay is\nsymmetric, that is,\n(2.47)"}
{"text": "===== Page 22 =====\n46 \n• \nLINEAR CABLE THEORY\nand it does not depend on the direction of travel.\nIn the simple case of an isopotential neuron,\n(2.48)\ndue to the capacitive nature of the neuronal membrane (Fig. 2.11 A). In other words, the\ncentroid for a depolarizing potential occurs exactly one time constant later than the centroid\nfor the current underlying this potential.\nIn an infinite (or semi-infinite) cable, potentials rise and decay faster, as we saw already\nin Fig. 2.8A. Indeed, because the charge injected into the cable not only flows onto\nFig. 2.11 NEURONAL INPUT AND PROPAGATION DELAYS \nAn elegant way to define propagation\ndelays in passive cables involves tracking the centroid or center of mass of voltages or currents in\npassive cable (Agmon-Snir and Segev, 1993). This is illustrated in (A) for an isopotential patch of\nmembrane with rm = 20 msec. A brief current pulse (solid profile with (peaic = 0.5 msec) gives rise\nto a rapidly rising but very slowly decaying depolarizing potential (shown dashed using normalized\nunits). The centroids of the two signals (see arrows at 1 and 21 msec) are displaced by one time\nconstant. In (B), the same current is injected into a very long cable, and the normalized potential at\nthe same location (dashed) and at a location one space constant displaced (dotted) are plotted. In an\ninfinite cable, the transfer delay Dxy between the centroid of the current at x and the centroid of the\nvoltage at y is (1 + \\x — y\\/X)Tm/2 (see the arrows at 1, 11, and 21 msec). As witnessed already in\nFig. 2.8A, the potential decays faster in a cable than in an isopotential patch of membrane."}
{"text": "===== Page 23 =====\n2.5 Recapitulation \n• \n47\nthe capacitance at the location of the electrode but via the intracellular resistance onto\nneighboring capacitances, the input delay between any current input and the associated\npotential at the same location is only delayed by half a time constant,\n(2.49)\nas compared to Tm for an isopotential patch of membrane. The faster local response time of\nan infinite cable compared with that of a patch of membrane is obvious in Fig. 2.8A. If the\ncurrent is injected at x and the voltage recorded at y, the two centroids are displaced by\n(2.50)\nThis is clearly evident in Fig. 2.1 IB. Again, this delay does not depend on the particular\nform of the current input but holds for any input. This dependency on distance allows us to\ndefine & propagation delay Pxy as the difference between the centroids of the voltage at x\nand at y,\n(2-51>\nFor an infinite cable,\n(2.52)\nThis linear relationship between space and time is equivalent to the notion of a propagation\nvelocity in an infinite cable,\n(2.53)\nIt is important to emphasize that v is a \"pseudovelocity\" rather than the physical velocity of\na constant wave moving along the cable for which V(x, t) = V(x — vt) should hold. Yet,\ndespite the fact that any potential will decay and become smeared out as it moves along a\ncable, its center of mass travels with a fixed velocity.\nWe feel obliged to point out a serious drawback when using Dxy or Pxy in active\nstructures. The Dxy measure of a dendritic input giving rise to a somatic EPSP with\nan undershoot, that is, a hyperpolarization due to potassium current activation (as in\nFig. 18. IB), will seriously underestimate the delay due to the small, but long-lasting negative\ncontribution to the centroid, rendering it less useful for real cells than for purely passive\nstructures (Bernander, 1993). Under these conditions, Dxy can be negative.\n2.5 Recapitulation\nOne-dimensional cable theory is based on several approximations. (1) The magnetic field\ndue to the movement of charge can be neglected. (2) Changes in the concentration of the\ncharged carriers, Na+, K+, and other ions, is slight so that the current can be expressed by\nOhm's law, and the intracellular cytoplasm can be mimicked by an ohmic resistance. (3) Due\nto the wirelike geometry of dendrites and axons and the high resistivity of the neuronal\nmembrane, the radial and angular components of voltage can be neglected, reducing the\ncomplexity of the solution from three spatial dimensions to a single one. (4) The extracellular\nspace is reduced to a homogeneous resistive milieu whose resisitivity is usually set to zero."}
{"text": "===== Page 24 =====\n48 \n. \nLINEAR CABLE THEORY\nThis allows us to solve for the potential V (x, t) across the neuronal membrane on the basis\nof a single equation.\nLinear cable theory further assumes that for a limited range of voltage excursions around\nthe resting potential, the membrane properties are independent of the membrane potential,\nreducing the electrical description of the membrane to resistances and capacitances, greatly\nsimplifying analysis.\nStarting in the late 1950s and early 1960s, the linear cable equation was solved by Rail\nand others to study the dynamics of the membrane potential in dendritic trees. Several key\nconcepts associated with the linear cable equation for a single finite or infinite cylinder\nare the space constant A., determining the distance over which a steady-state potential in\nan infinite cylinder decays e-fold, the neuronal time constant Tm, determining the charging\nand decharging times of V(x, t) in response to current steps, and the input resistance /?;„,\ndetermining the amplitude of the voltage in response to slowly varying current injections.\nThe voltage in response to a current input, whether delivered by an electrode or by\nsynapses, can be expressed by convolving the input with an appropriate Green's function.\nFor passive cables, this always amounts to filtering the input by a low-pass filter function.\nWhile the class of parabolic differential equations (to which the cable equation belongs)\ndoes not admit to any wave solutions but only shows dissipative behavior, one can define\ninput, transfer, and propagation delays by computing and tracking the centroid or center of\nmass of V(x, t) relative to the centroid of the current input. In the following chapter, we\nwill apply these concepts to realistic dendritic trees."}
{"text": "===== Page 3 =====\nCENTER\nPASSIVE DENDRITIC TREES . 51\nPERIPHERY\nFig. 3.2 STRUCTURE-FUNCTION RELATIONSHIP IN RETINAL GANGLION CELLS The relationship\nbetween the extent of the receptive field of visual neurons and the size of their dendritic tree differs\ndepending on the size of the receptive field. Shown here is the functional architecture of a central\n(1 °; left column) and a peripheral (20°; right column) ft ganglion cell in the cat retina. (A) Receptive\nfield of these cells, determined by fitting a Gaussian sensitivity curve through their center (dark)\nand their surround fields. (B) Their dendritic tree, reconstructed at the electron-microscopic (left)\nand the light-microscopic (right) levels. The small cell receives excitatory input from about 170\nbipolar ribbon synapses, while the large one is excited by 1700 bipolar ribbon synapses (see Fig. 1.5).\n(C) Compartmental modeling of these two cells, assuming a passive dendritic tree, shows that voltage\nattenuation is negligible. (D) The receptive field of the two ganglion cells, obtained from the literature,\nis superimposed onto the receptive field of the presynaptic neuron, a bipolar cell. The effect of\nsynaptic input from the bipolar cell onto the ganglion cell—expressed by the dendritic tree itself—is\nsummarized by the \"synaptic weight\" curve. For a central ft cell, it contributes little to the ganglion\ncell receptive field, since the input receptive field is substantially broader than the contribution from\nsynaptic input onto the dendritic tree (solid inner curve in the left panel). Since the dendritic tree\ncontribution for a cell in the periphery (solid middle curve in the right panel) is larger than the extent\nof the receptive field of the input, it gives rise to the observed linear scaling of the receptive field with\nthe dendritic tree size for large cells. Unpublished data from P. Sterling and R. Smith, printed with\npermission."}
{"text": "===== Page 4 =====\n52 \n• \nPASSIVE DENDRITIC TREES\nFig. 3.3 RELATIONSHIP BETWEEN RECEPTIVE-FIELD SIZE AND DENDRITIC TREE \nComparison\nof the dendritic tree and the receptive field center, shape, and extent of a retinal ganglion cell in the\ncat. Morphologically, this cell type is known as a; its axon (not shown) projects to the geniculate. The\ncontinuous contour lines correspond to equal response contours and represent peak firing rates of the\nindicated amount in response to a flashing spot of light. The maximum discharge of 300 spikes per\nsecond is marked by a spot. The maintained background activity of the cell was 32 Hz. The dotted line\ncorresponds to the cell's receptive field. The physiologically defined receptive field extends roughly\n100-150 jam beyond the dendritic tree. Reprinted by permission from Peichl and Wassle (1983).\nA very similar structure-function relationship also holds for the anatomically defined ft\nclass of cat retinal ganglion cells in the periphery, where the dendritic tree of these cells is\nrelatively large (Fig. 3.2). This cell type corresponds to the physiologically defined X type\n(Saito, 1983; Stanford and Sherman, 1984). The proportionality between receptive field size\nand the extent of the dendritic tree fails for small ft type ganglion cells near the center of\nthe retina. Here, the receptive field of the presynaptic neurons providing the driving input,\nbipolar cells, is already substantially larger in extent than the dendritic tree of the ganglion\ncell, obscuring any contribution the passive dendritic tree might make.\nBesides this simple size relationship, what other functional aspects might the different\nretinal ganglion cell morphologies reflect? And what about possible structure-function\nrelationships of other cell types? In the periphery, one can expect specific structural elements\nof the nervous system to reflect specific attributes of the physical world they are designed\nto extract. This is unlikely to be the case for more central structures. Can one still say\nsomething meaningful about how information processing is related to different dendritic\ntrees (Borst and Egelhaaf, 1994)?\nIn this chapter, central to the book, we discuss how the previously introduced concepts\nfrom linear cable theory can be applied to study the electrical properties of complex, spatially\nextended dendritic trees. Why should the reader study passive dendritic trees when evidence\nfrom calcium imaging experiments, coupled with intradendritic recordings, shows that most,\nif not all, dendritic trees include substantial voltage-dependent, that is, nonlinear membrane"}
{"text": "===== Page 5 =====\n3.1 Branched Cables \n• \n53\ncomponents? This challenge can be answered by citing the truism \"You can't run before you\ncan walk.\" An understanding of information processing in active trees can only be based\nupon knowing the properties of passives ones. Furthermore, under certain well-specified\nconditions, for instance, for \"small\" synaptic inputs, voltage-dependent nonlinearities do\nnot come into play and the dendrite acts essentially like a passive one. Let us therefore press\non, delaying the confrontation with active trees until Chap. 19.\n3.1 Branched Cables\nAn extended dendritic tree can be treated as a branched cable structure, where each cable or\ncylinder can, in principle, have different diameters and membrane properties. Since there\nexists no evidence for loops within a given dendritic tree structure, that is, one dendritic\nbranch connecting—either directly or with the aid of a synapse—to another dendrite of the\nsame cell, dendritic arbors can be considered to be true trees in the graph theoretic sense,\nwith a unique path between any two points in the tree.\n3.1.1 What Happens at Branch Points ?\nThe cable equation (Eq. 2.7) as applied to a finite cable holds for any particular unbranched\ncylindrical segment with constant membrane properties and a constant diameter. How can\nwe use this equation to study the potential in two or more such segments connected together?\nThe way in which finite cable segments are linked resides in the appropriate choice of\nboundary conditions for each segment. In order to understand this better, let us focus on the\nhighly simplified dendritic tree illustrated in Fig. 3.4A. We assume that the three branches\nhave electrotonic lengths LQ, L\\, and LI, with infinite input resistances ROO,O> ^oo,i» ar|d\n/?oo,2- As discussed in Sec. 2.2.2, the infinite input resistance of a finite cable is equal to\nthe input resistance of a semi-infinite cable of the same diameter and having the same Rm\nand RJ values, looking toward the infinitely far away terminal, and is computed according\nto Eq. 2.16.\nWe will assume, for the sake of simplicity, that the two rightmost branches terminate\nwith a sealed-end boundary condition. The input resistances of the two rightmost daughter\nbranches at the branch point, looking toward their terminals, are specified by Eq. 2.21 as\n(3-1)\nand\n(3.2)\nFor an arbitrary boundary condition at the rightmost terminal of the daughter branches\n(rather than the sealed end assumed here) we replace these expressions by Eq. 2.28.\nThe right-hand side of the main cable ends at the branch point with the two daughter\ncables. Its terminal resistance RL$ is determined by the input resistances of the daughter\nbranches, by making use of the fact that the net conductance of two parallel conductances is\nsimply given by their sum. In other words, we compute the terminal resistance by effectively\nreplacing the two conductances of the daughter branches by a single one. The effective\nterminal resistance of the main cable is given by\n(3-3)"}
{"text": "===== Page 6 =====\n54 • \nPASSIVE DENDRITIC TREES\nFig. 3.4 PASSIVE BRANCHING DENDRITE \n(A) Schematic drawing of a passive cylindrical dendrite,\nwith diameter do and electrotonic length LQ — A) Ao. with 'wo daughter branches, each with its distinct\ndiameter and electrotonic length Lj = /; /A.j. A simple recursive scheme exists to compute exactly the\nvoltage in such tree structures in response to current input (Rail, 1959). All terminals are assumed to be\nsealed. (B) Compartmental-model representation of this passive dendritic tree. The voltage dynamics\nin this circuit approximate the solution to the cable equation of the continuous cable in A in the sense\nthat the cable equation (Eq. 2.7) describes the dynamics of the membrane potential in the limit that the\ngrid size becomes infinitely fine. For the sake of simplicity, we set Vrest to zero (see Fig. 2.3). Standard\nsoftware packages, such as NEURON or GENESIS, automatically solve for the voltage in these circuits.\nIn a final step, we apply Eq. 2.28 to arrive at the input resistance at the left terminal of the\nmain branch (looking toward the daughter branches),\n(3.4)\nThe same algorithm can, of course, be applied in a recursive manner to evaluate the input\nresistance of arbitrary passive trees. Moving from the outermost dendritic tips and using\neither Eq. 2.21 or Eq. 2.24 (depending on the choice of boundary conditions), we compute\nthe input resistances of the daughter branches. Adding the inverse of these input resistances\ngives the inverse of the terminal resistance of the branch of the parent generation. We now\nuse Eq. 2.28 to compute the input resistance of this branch and its sibling, add them in\nparallel, and so on. Of course, this \"folding\" algorithm needs to be carried out on both ends\nof the cylinder. (In our pedagogical example in Fig. 3.4, we did not consider the effect of one\nor more cables connecting to the left terminal of the parent cable.) In this manner, the input\nresistance at any point in the tree can be evaluated exactly, a procedure proposed by Rail\n(1959) and described in considerable detail in Chap. 7 of Jack, Noble, and Tsien (1975).\nOnce we compute the input resistance at any particular location in the dendritic tree,\nwe can derive the steady-state voltage at that location in response to a current injection 7,nj\nusing Ohm's law. For instance, the voltage at x = 0 in the parent cable is"}
{"text": "===== Page 7 =====\n3.2 Equivalent Cylinder \n• \n55\n(3.5)\nGiven VQ, we can use Eq. 2.27 to compute the sustained voltage change anywhere along\nthe main branch in response to the sustained current injection 7jnj. Furthermore, recursive\nevaluation of this equation allows us to compute the voltage at any location in the tree of\nFig. 3.4. Altogether, four linear equations (Eqs. 2.27, 2.28, 3.3, and 3.5) need to be applied\niteratively to solve for the input resistance, and therefore for the sustained voltage following\na sustained current input, in any dendritic tree structure in response to a current injection at\nany location (Rail, 1959).\nAssuming that the parent cable delivers the current /o to the two daughter branches, what\nfraction I\\ of this current will flow into one and what fraction li into the other (Fig. 3.4A)?\nUsing Eqs. 2.2 and 2.20 and the definition in Eq. 2.16, we compute the current flowing in\none of the daughter cables, assuming that its terminal is sealed (this is not necessary),\n(3-6)\nwhere V\\ is the voltage at the branch point. At this point, X = 0 (for the daughter branch),\nand the current flowing into this branch is given by\n(3.7)\nwhere we made use of Eq. 3.1. Since the same principle applies to the second daughter\nbranch and also holds true for an arbitrary boundary condition (in this case Eq. 2.20 must\nbe replaced by Eq. 2.27), it follows that\n(3-8)\nwhere G;nj = l//?in,i and likewise for G-m^- In other words, the current divides among\nthese branches according to their input conductances.\n3.2 Equivalent Cylinder\nUnder certain conditions one does not need to solve numerous equations to derive the\npotential in a branched tree, a great simplification discovered and exploited by Rail (1962,\n1964). Let us compute the voltage distribution in the simple tree shown in Fig. 3.4,\nassuming that the two daughter branches are identical (that is, LI = L2 and d\\ = d2\nwith identical membrane parameters) and that a sustained current is injected into the left\nterminal of the parent cylinder. The normalized voltage profile throughout this minitree\nis illustrated in Fig. 3.5 for three different combinations of parent and daughter branch\ndiameters: (I)d0 = 2d\\,(2)d0 \n= 2-d^ ,and(3)c?o = d\\. The upper and lower curves\nindicate the two limiting cases in which the parent cable is terminated at the right end either\nwith a sealed-end boundary condition or in a short circuit. In the cases where the parent\ndiameter has either twice the diameter or the same size as the daughter branches, close\ninspection of the curves in Fig. 3.5 reveals a discontinuity in the derivative of the voltage\nat the branch point.\nIn the third case, dQ \n= 2dl , all derivatives of the voltage profile are continuous and a\nrather interesting phenomenon occurs. In contrast to the other two cases, the voltage decay\nalong both sets of cables can be described by a single expression. To see why, let us return"}
{"text": "===== Page 8 =====\n56 \n• \nPASSIVE DENDRITIC TREES\nFig. 3.5 VOLTAGE DECAY ALONG A SIMPLE DENDRITIC TREE Steady-state voltage decay along\nthe tree shown in Fig. 3.4A, assuming that the two daughter branches are identical, with d\\ = d2\nand LI = L2 = 0.5. The three dashed curves correspond to (1) d0 - 2dt, (2) d^2 - 2d^/2, and\n(3) do = d\\. For the dotted curve, the potential at the right-hand terminal of the main branch is set\nto zero (short circuit). For the upper, continuous curve, the membrane is sealed. At the branchpoint\nx = 500 /j,m, the voltage profile has a discontinuous derivative for do = 1d\\ and for do = d\\.\nIf the input resistance of the parent at this point is matched to that of the daughter branches (as\nis the case for the second condition, dQ \n= 2d{ ), the voltage decay across the cables can be\ndescribed by a single, simple expression. This trick is exploited by Rail in his concept of the\nequivalent tree.\nto Eq. 2.27, expressing the voltage in a finite cable of length LQ with an arbitrary terminal\ncondition specified via the terminal resistance RL$,\n(3.9)\nSince both daughter cables are identical and therefore have identical input resistance,\nRin ! = Rin 2 (Eqs. 3.1 and 3.2), it follows from Eq. 3.3 that the combined input resistance\nof both cables is\n(3.10)\nWe now return to Eq. 2.16 for the input resistance of a semi-infinite cable, noticing its\ndependency on the inverse of the cable diameter to the | power\n(3.11)\nIf^/2 = 2^/2,then"}
{"text": "===== Page 9 =====\n3.2 Equivalent Cylinder \n• \n57\n(3.12)\nThat is, the infinite input resistance of the parent cable is matched to the infinite input\nresistances of the two daughter branches in parallel. Combining Eqs. 3.10 and 3.12 leads to\n(3.13)\nIf we place this back into Eq. 3.9, we have\n(3.14)\nComparing this to Eq. 2.20, we see that the above equation describes the voltage in a single,\nunbranched cylinder of length LQ + L\\ with a sealed-end boundary condition. Instead of\nhaving to model this structure as three interconnected cylinders, we can reduce it to a single\nequivalent cylinder, whose electrotonic length is given by LQ + L \\ and whose diameter is\nidentical to the diameter of the main branch. The reason we are able to carry out this sim-\nplification is that the infinite input resistance of the main branch is \"matched\" to the infinite\ninput resistance of the two daughter branches in parallel, a principle known as impedance\nmatching. In particular, the voltage and the input resistance now have continuous derivatives\nacross the branch point, as witnessed in Fig. 3.5. Note that, in general, impedance matching\ndoes not imply that the input resistance R-m of the daughter cables is matched to that of the\nmain cable (since the input resistance of the main cable at the right terminal is proportional\nto coth(Lo) while the input resistance of the daughter branches is proportional to coth(L i)).\nAs Rail (1962, 1964) first showed in the early 1960s, an entire class of dendritic trees\ncan be reduced or collapsed into a single equivalent cylinder provided the following four\nconditions are met. (1) The values of Rm and /?; are the same in all branches. (2) All terminals\nend in the same boundary condition. (3) All terminal branches end at the same electrotonic\ndistance L from the origin in the main branch, where L is the sum of the L,- values from the\norigin to the distal end of every terminal; L corresponds to the total electrotonic length of\nthe equivalent cylinder. (Note that this requirement does not necessarily include only trees\nwith perfect symmetric branching.) (4) At every branch point, infinite input resistances must\nbe matched. If all cables possess the same membrane resistance and intracellular resistivity,\nthis implies\n(3.15)\nThis last condition is known as the d3/2 law. If these four conditions are met, the equivalent\ncylinder mimics perfectly the behavior of the entire tree. In other words, injecting current\ninto the leftmost terminal of the tree will yield exactly the same voltage response as injecting\nthe identical current into its equivalent cylinder.\nIf input to any of the daughter branches is considered, an additional constraint must\nbe obeyed. (5) Identical synaptic inputs, whether current injection or conductance change,\nmust be delivered to all corresponding dendritic locations. In the case of Figs. 3.4 and 3.5,\ninjecting a current /o into both daughter branches at the same electrotonic distance X\\ from\nthe branch point corresponds to injecting the current 2/o into the equivalent cable at location\nX = LO + Xj. If we are only interested in describing the voltage in the primary branch,\nbecause this is close to the cell body where the intracellular electrode is usually located,\ninjecting the current 2/0 into only one of the two daughter branches will lead to the same"}
{"text": "===== Page 10 =====\n58 . \nPASSIVE DENDRITIC TREES\nvoltage change in the primary branch as injecting the current 2/o at the distance LQ + X\\\nin the equivalent tree (Rail and Rinzel, 1973). This ceases to be true when considering the\nvoltage profile in other parts of the equivalent tree, outside of the region X < LQ.\nThe transformation from a branching cable structure to an equivalent cylinder preserves\nmembrane area. In other words, the branching tree has the same membrane surface area\nas the unbranched cylinder that has the same diameter as the trunk of the tree and the\nsame electrotonic length as the whole dendritic tree. This implies that all variables that are\nexpressed per unit membrane area, such as channel densities, Rm, and Cm, are conserved\nin the transformation to an equivalent cylinder.\nThe principle of impedance matching is a powerful one and can be generalized to many\nsituations. While here we have only considered the case of stationary inputs, the reduction\nof a tree to a cylinder can also be carried out for transient inputs, as long as the above\nfour conditions are satisfied and Cm is constant throughout the entire tree. In other words,\nthe dynamic behavior of the voltage is identical in the two cases. Furthermore, impedance\nmatching can be generalized to tapering cables (Rail, 1962) and even holds in the presence\nof an active membrane containing voltage-dependent elements. In particular, axonal trees\nfulfilling the above conditions can similarly be reduced to a single equivalent axon.\nRail exploited this technique to great advantage in his extensive research on the electrical\nbehavior of cat a motoneurons (Rail, 1977; Rail et al, 1992; see also Fleshman, Segev,\nand Burke, 1988), where the somatic potential of the cell in response to synaptic input\nin the dendritic tree is modeled by a single equivalent cylinder with the cell body being\nrepresented by an RC element tacked onto one end of the cylinder. Under these conditions,\nthe electrical behavior of an entire neuron is characterized by a simple model with six\nfree parameters (the R and C values of the isopotential soma, the Rm,Cm, and /?, of the\nequivalent cylinder, as well as its electrotonic length L).\nReal dendritic trees rarely conform to all of the assumptions underlying the equivalent\ntree reduction. In some cases the d3/2 rule is observed to a remarkable extent at branch\npoints (e.g., relay cells in the cat's lateral geniculate; Bloomfield, Hamos, and Sherman,\n1987). In general, neither this local rule dictating the diameter of branching processes nor\nthe assumption that all dendrites terminate at the same distance from the cell body hold\n(Hillman, 1979; Burke, 1998; Larkman et al., 1992; Turner et al., 1995).\nWith a few exceptions, then, most neurons violate one or more of the constraints required\nfor reducing a tree to an equivalent tree. In fact, one can ask why should dendritic trees obey\nthe d3/2 law at all? What functional advantages can be realized for a neuron if its dendritic\ntree can be reduced to a single equivalent cylinder? Voltage \"reflections\" at branch points will\nbe absent if the d3/2 law is obeyed, but it is unclear why this should be relevant to neurons.2\n3.3 Solving the Linear Cable Equation for Branched Structures\nHistorically, much effort was spent on deriving solutions to linear differential equations\nlike the cable equation in terms of series expansions of specialized functions (such as\nBessel functions) and we will briefly touch upon these methods. Yet, given the widespread\nusage of digital computers and the complicated geometries of cells as well as their non-\nlinear membrane characteristics, the majority of research today is being carried out using\nnumerical methods.\n2. Concepts from the matched transmission lines literature in electrical engineering are germane to some of these questions.\nWe will not pursue any of the interesting analogies; see Davidson (1978) or Briihl, Jansen, and Vogt (1979)."}
{"text": "===== Page 11 =====\n3.3 Solving the Linear Cable Equation for Branched Structures \n• \n59\n3.3.1 Exact Methods\nA number of iterative techniques have been developed to solve exactly for the voltage\ntransient in treelike structures in response to an arbitrary current input I(t). All of them\nrely on the superposition principle, that is, on the linearity of the membrane (although a\nlinear membrane need not necessarily be passive; see Chap. 10).\nLinearity of the membrane implies that the injection of a sinusoidal current of frequency\n/ only gives rise to a voltage response at the same frequency. The above equations can be\nreformulated in the Laplace (or Fourier) domain to compute the potential V(f) anywhere\nin the tree in response to a sinusoidal current /(f) = /o sin(2jr/£) applied anywhere\nelse. One set of methods (Butz and Cowan, 1974; Horwitz, 1981; Koch, 1982; Koch and\nPoggio, 1985a; Holmes, 1986) uses variants of this approach to derive Vm(f) in arbitrary\ndendritic trees with the help of a small number of rales. Usually, these equations have simple\ngraphical interpretations and can be implemented recursively, leading to very efficient and\nsmall programs.\nFor instance, Koch and Poggio's (1985a) four rules specify the impedance of a single\ncylinder (Eq. 2.28), the effective impedance at a branch point (Eq. 3.3), and the voltage at\nany point in a cylinder as a function of either the injected current (by combining Eqs. 2.25\nand 2.26) or the voltage at one terminal (Eq. 2.27). One advantage of this method is that it\ncan be applied to evaluate the potential in dendritic trees with arbitrary linear membranes,\nsuch as those containing inductances as can be obtained by linearizing certain active\ntypes of membranes (see Chap. 10). The number of equations that need to be evaluated\nin these methods is proportional to the number of cylindrical segments in the dendritic tree\nconsidered. Note that these methods require the inverse Laplace (or Fourier) transform to\nobtain V(t).\nA completely different approach was pioneered by Abbott and his group (Abbott, Farhi,\nand Gutmann, 1991; Abbott, 1992; Cao and Abbott, 1993), based on a path integral approach\nof the type used so successfully in quantum mechanics and quantum field theory (Feynman\nand Hibbs, 1965). The voltage at x in response to current input at y in an arbitrary tree\n(including trees with loops) is obtained by evaluating the Green's function of the cable (for\nexample, Eq. 2.31) along all possible \"paths\" between x and y. The number of paths is\npotentially infinite, since a path, starting out at point x and heading toward y, may change\ndirection at every node or terminal it encounters on its way and may pass through x and\ny an arbitrary number of times but must begin at x and end at y. Of course, the longer\nthe path, the smaller its final contribution toward the potential. (The Green's function in\nEq. 2.31 suppresses long paths of length Lpath exponentially as e~Lf^4T.) We met such\nan approach while evaluating the potential in a single finite cylinder by summating over\ninfinitely many reflecting terms (Eq. 2.41 and Fig. 2.10). The primary advantage of this\nmethod lies in computing V(t) explicitly for short times, since under these conditions only\nas few as four paths need to be evaluated (Cao and Abbott, 1993). Furthermore, distinct\nfrom the methods working in the Laplace/Fourier domain, the path integral method yields\nthe membrane potential directly as a function of time.\n3.3.2 Compartmental Modeling\nAll the algorithms discussed so far solve for Vm(f) and Vm(t) in response to an arbitrary\ncurrent input /(/) and /(?), respectively. They come, though, with serious drawbacks. (1)\nIn the presence of n current inputs—rather than a single one—on the order of n2 additional\ncomputations have to be performed (see Sec. 3.3); that is, these methods do not at all scale"}
{"text": "===== Page 12 =====\n60 \n• \nPASSIVE DENDRITIC TREES\nwell to massive synaptic input. (2) While these methods can be adapted to treat synaptic input\nas conductance changes, it is not computationally efficient to do so. (3) Finally, and fatally,\nthey assume linearity and fail in the presence of voltage-dependent membrane components.\nThe method of choice for most work in the field is to solve the partial differential equations\nnumerically by discretizing the underlying equations. This approach, introduced by Rail\nin his landmark (1964) paper (see also Segev, Rinzel, and Shepherd, 1995), is known as\ncompartmental modeling and leads to the discrete electrical circuits illustrated in Fig. 3.4B.\nInstead of solving the continuous, linear or nonlinear, partial differential cable equation,\nit is discretized into a system of ordinary differential equations, corresponding to small\npatches of neuronal membrane that are isopotential. These compartments are then coupled\nby sparse matrices.\nThe most fundamental requirement of any numerical method is convergence, that is,\nthat the error between the solution using the numerical method and the exact solution be\nmade as small as possible. Furthermore, the method must be stable. These requirements\nplace constraints on how fine time and space must be discretized in order to arrive at an—\napproximately—correct solution. A conservative rule of thumb is to divide the cable into\nsegments equal to or smaller than one-tenth of the associated effective length constant A.\n(Segev et al., 1985). How to achieve all of this properly and in an as efficient manner\nas possible is a well-established subject within applied mathematics. Appendix C, written\nby Barak Pearlmutter and Anthony Zador, treats modern numerical techniques—using the\nmatrix formalism—for approximating the solution of these equations.\nA number of public-domain single-cell simulators with graphical interfaces, in particular\nNEURON and GENESIS, implement such methods in an efficient manner. (For an overview\nof about half a dozen of them see DeSchutter, 1992; for more comprehensive references see\nBower and Beeman, 1998 and Koch and Segev, 1998.) Indeed, the field has advanced to\nsuch a point that a set of benchmarks has been introduced to compare simulators in a more\nquantitative manner (Bhalla, Bilitch, and Bower, 1992).\nThe power of compartmental methods derives from their flexibility in permitting arbitrary\nlevels of functional resolution to be included into the model. As long as the mechanism\nat hand, say a particular membrane conductance or pump, can be described by a steady-\nstate or a differential equation, it can be incorporated into the model. Indeed, the numerical\nsimulations discussed in this book range from the submillisecond to the second scale and\nfrom the submicrometer regime to several millimeters. Ultimately, the limiting factor in\nerecting ever more complex models is the exploding number of functions and parameters\nthat need to be specified and measured as well as the speed of the computer implementing\nthe algorithm.\n3.4 Transfer Resistances\nLet us now introduce a different, and much more intuitive, manner of calculating the voltage\nchange in response to current input in passive dendritic trees of arbitrary geometry. Rather\nthan expressing the properties of some electrical network in terms of an analytical expression\nfiguring the various cellular parameters (e.g., the Green's function Eq. 2.31), the system is\n\"observed\" at one, two, or more discrete locations, and its properties are summarized in\nterms of a handful of functions called transfer functions. Their usage is based on multiport\ntheory, in which a linear or nonlinear system is characterized by n ports, or independent\npairs of current and voltage at each port (Oster, Perelson, and Katchalsky, 1971; Chua,\nDesoer and Kuh, 1987; Wyatt. 1992). Resistance, diode, capacitance, or inductance are all"}
{"text": "===== Page 13 =====\n3.4 Transfer Resistances \n• \n61\ninstances of one-port devices, since a pair of numbers, the voltage across the device and\nthe current through it, completely captures their behavior, while the simplest model of a\nbipolar transistor is a three-terminal element (collector, emittor and base) with two pairs of\ncurrent-voltage relationships. As we will see in Chap. 4, a chemical synapse can also be\napproximated as a two-port device (at a short time scale).\nThe two-port formulation of cable theory, in which voltage and current are manipulated\nat two locations in an arbitrary tree, has proven to be of great intuitive value. In particular,\nit allows the attenuation experienced by synaptic input on its way to the soma to be easily\nexpressed. In the version popularized by Butz and Cowan (1974) (see also Fig. 3.6 and\nKoch, Poggio, and Torre, 1982; Caraevale and Johnston, 1982) it assumes that the dendritic\ntree is linear (although n-port theory does not require this straight-jacket).\n3.4.1 General Definition\nAs long as one is dealing with a linear system, the voltage change Vj(t) at location j in\nresponse to an arbitrary current input /,-(f) at location i can always be expressed as\n(3.16)\nwhere K;j(t) is the impulse response or Green's function of the system (Fig. 3.6). The last\nrelationship assumes that the impulse function is causal, implying that Ki/(t) = 0 for\nt < 0 (since no effect can precede its cause) and that no current is injected prior to t = 0.\nThe subscripts ij signify that the current is injected at i and the voltage recorded\nat j, a convenient mnemonic short form for summarizing the input-output relationship.\nEquation 3.16 holds for any cable structure and simply expresses the linearity of the system.\nThe Green's function can be obtained by injecting a delta pulse of current into the cable,\nthat is, /,(?) = IoS(t), and computing V/(f) by solving the appropriate cable equation.\nThis yields\n(3.17)\nwith the dimension of a resistance. If location i coincides with location j, the equation\nreduces to\n(3.18)\nOne way to visualize the meaning of Kjj(t) is to compute its Fourier transform (see\nAppendix B) Kfj (/), where / is the temporal frequency in units of hertz. It is a complex\nFig. 3.6 TRANSFER FUNCTIONS Neuronal transfer func-\ntions are used throughout the book to characterize an ar-\nbitrarily complex, but linear neuron. The unknown system\nis observed at two points, or ports, i and j. The behavior\nat these two points can be fully described by knowledge\nof the three transfer functions KH, KI; = KJI, and KJJ. In\nparticular, the attenuation of the input ati propagating to j\ncan be easily expressed in terms of these functions. Its chief\ndrawback is that these functions are not useful for expressing\nthe voltage at some other location I. This requires knowledge\nof Kit and KU-"}
{"text": "===== Page 14 =====\n62 \n• \nPASSIVE DENDRITIC TREES\nfunction whose amplitude is measured in ohms. Any particular value of KIJ (/) can be\nmeasured by injecting a sinusoidal current of frequency / at location i, that is, /,• =\nsin(2jT/f), and recording the resultant voltage amplitude and phase at location j at the\nsame frequency /.\nThe steady-state voltage change in response to a stationary current (that is, for / = 0)\ncan be recovered with the aid of the real number\n(3.19)\nWe follow the practice of referring to the complex function Kjj(f) \nas the transfer\nimpedance, to its stationary or dc value K/j (f = 0) > 0 as the transfer resistance, and to its\ninverse as the transfer conductance. Unless otherwise noted, Ky refers to KJJ (f = 0) > 0.\nSimilarly, the amplitude of KH (/) is called the input impedance and Kti (f = 0) > 0 the\ninput resistance.\n3.4.2 An Example\nIn order to relate transfer resistances to the previously computed Green's functions, let\nus consider events in an infinite, passive cable of diameter d. Following Eq. 2.31 we can\ndirectly write\n(3.20)\nwhere |z — j\\ is a shorthand notation to represents the distance between i and j, that is,\nbetween the sites where the current is injected and the voltage is recorded, and T = t/rm\nis the normalized time. To compute KH, we set \\i-j\\to zero,\n(3.21)\nThe Fourier transform of Ktj (T) yields\n(3.22)\nwhere R&, is the input resistance of a semi-infinite cylinder (Eq. 2.16). For the transfer\nresistance at / = 0 we arrive at\n(3.23)\nand for the input resistance,\n(3.24)\nThe transfer resistance decays exponentially with the distance between the two sites and\nindicates the degree of coupling between the two sites i and j, which decreases with\nincreasing distance.\n3.4.3 Properties of KIJ\nWe will now assume that we are dealing with current inputs that either are stationary or\nchange much more slowly than the membrane time constant Tm. Since this corresponds to"}
{"text": "===== Page 15 =====\n3.4 Transfer Resistances \n• \n63\nthe solution of Eq. 3.16 with t -> oo, the convolution is replaced by multiplication and the\nfunction Ktj (t) is replaced by the dc component of its Fourier transform; that is,\n(3.25)\nfor the transfer resistance between i and j and\n(3.26)\nfor the input resistance at location z. Transfer and input resistances have several noteworthy\nproperties.\nSymmetry\nThe most important property is symmetry,\n(3.27)\nIf the current /o is injected at location i somewhere in the dendritic tree, and the voltage\nV0 = KIJ • /o is measured at location j, for instance, at the cell body, we obtain the\nsame change in voltage Vb at location i if the identical current /o is injected at location j:\nVi = Kji • Ij = K{J• • /o = VQ. This is true for any reciprocal linear system, irrespective of\nthe positions of z and j. It is important to point out that this does not mean that the attenuation\nfrom location z to location j is the same as the other way around. Most emphatically not\n(see Sec. 3.5.2)!\nSymmetry, in the electrical engineering literature also known as reciprocity, is a general\nproperty of n-ports created by interconnecting linear, two-terminal resistances. Its proof\nrequires the use of Tellegen's theorem (Penfield, Spence, and Duinker, 1970; Reza, 1971)\nand is beyond the scope of this book.\nPositivity\nA second property is\n(3.28)\nfrom which it follows that the input resistance KU is always larger than the transfer resistance\nbetween location z and any other location. Furthermore, the more removed z and j are, the\nsmaller the transfer resistance KIJ. Thus, KIJ is related to the degree of coupling of site i\nto site j.\nTransitivity\nA third property of any dendritic tree structure without loops is\n(3.29)\nwhere t corresponds to any location on the direct path between locations i and j. This\nrelationship does not hold if £ is located on a branch off the direct path between z and j\n(Koch, 1982). For instance, if we consider the transfer resistance Ka^,<s between cz3 and the\nsoma in Fig. 3.7, Eq. 3.29 holds for points aL and a 1 but not for a4, a 11, or b 1. It can be\nviewed as a form of transitivity (as long as all three points are located along a direct path).\nAll three properties remain in force not only for the dc value of the transfer impedance\nbut for any one frequency (e.g., Kjj(f) = \nKjt(f))."}
{"text": "===== Page 16 =====\n64 . \nPASSIVE DENDRITIC TREES\nFig. 3.7 A LAYER 5 NEOCORTICAL\nPYRAMIDAL CELL Morphology of\nthe neocortical pyramidal cell we use\nthroughout this book. The cell was\nstained and reconstructed by Douglas,\nMartin, and Whitteridge (1991) from\nthe visual cortex of an adult cat. The\ncell body is located in layer 5 while the\nmost distal dendrites lie in layer 1. A\nfew locations that are used as synaptic\ninput sites are indicated by letters.\nWhile these properties are true for any cable system with a linear membrane, an additional\nproperty of cables with passive membranes is that the membrane becomes increasingly more\nconductive to current flow as the temporal frequency increases due to the presence of the\ncapacitive element. Or,\n(3.30)\nfor any |/| > |/'|. \nIn particular, £^(0) > \\Kjj(f)\\. \nIn other words, the transfer and\ninput impedances show low-pass behavior. If the membrane contains inductive-like ele-\nments, caused, for instance, by the small signal behavior of a voltage-dependent potassium\nconductance activated by depolarization, \\K{j(f)\\ \ncan have a maximum at some nonzero\nfrequency, with interesting consequences for information processing (see Chap. 10).\n3.4.4 Transfer Resistances in a Pyramidal Cell\nIn order for the reader to develop his or her intuition about these transfer resistances and\nother related concepts in the following section, we will make use of our \"canonical\" model\nof a nerve cell, a pyramidal cell from the mammalian neocortex. We choose this particular\nneuron both for idiosyncratic reasons as well as for the general interest many scientists\nhave in understanding the information processing operations occurring in cortex. Most of\nthe concepts discussed in this book apply equally to other cell types.\nThe morphology of this neuron (Fig. 3.7) is derived from a layer 5 pyramidal cell\nin primary visual cortex filled with the intracellular dye HRP during experiments in the\nanesthetized adult cat (Douglas, Martin, and Whitteridge, 1991) and is translated into several"}
{"text": "===== Page 17 =====\n3.4 Transfer Resistances\n65\nhundreds of compartments, similar to the ones shown in Fig. 3.4. The dendrites are passive\nwhile eight voltage-dependent membrane conductances at the cell body enable the cell\nto generate action potentials and shape the interspike interval between them accordingly\n(for details consult Beraander, 1993; Bernander et al., 1991, 1992, 1994). In this chapter\nwe consider a linear model of the cell, obtained by replacing the active somatic membrane\nconductances by an appropriately chosen passive conductance, such that the input resistance\nand the resting potential throughout the cell are identical to the ones obtained in the full\nmodel. In other words, for small steady-state inputs this \"passified\" model behaves as the\nfull, active model.\nUnless otherwise noted, the ever-present spontaneous synaptic background activity has\nbeen incorporated into the passive membrane parameters. Because each transient opening\nof a synapse briefly increases the local membrane conductance, the random background\nactivity will affect the \"passive\" membrane resistance Rm, with the effective membrane\nresistance varying from around 12,000 £1 • cm2 at the soma to 52,000 Q • cm2 for distal\ndendrites (see Sec. 18.3). The membrane capacitance is set to 1 /zF/cm2 and the intracellular\nresistivity to 200 £2-cm.\nA number of the analytical techniques presented in Sec. 3.3 can be used to compute Ky\nin trees of arbitrary geometry, in particular the recursive techniques of Rail (1959), Butz and\nCowan (1974), and Koch and Poggio (1985a). We refer the reader to these and other papers\ndiscussed in Sec. 3.3 and to Appendix C for more details. Table 3.1 shows representative\nvalues for KU and KIS for sustained and ac inputs. The steady-state input resistance KU\nincreases dramatically as one moves along the apical tree toward more distal sites, from\n16.5 MS2 at the soma to several gigaohms in the far periphery. This is not unexpected, as\nwe can see from Eq. 2.21 that the input resistance of a finite cable of electrotonic length L\nwith a sealed end increases as 1/L as L —> 0, somewhat analogous to moving toward the\ndistal dendritic tips. Using rapidly varying sinusoidal input of 10 or 1000 Hz reduces the\ninput resistance moderately.\nThe transfer resistance to the soma K{s (by Eq. 3.27 identical to the transfer resistance\nKsj from the soma to the dendritic site i), on the other hand, only varies by about a factor\n{\n  \"table_name\": \"TABLE 3.1\",\n  \"description\": \"Transfer and Input Resistances\",\n  \"columns\": [\"Location\", \"Kti\", \"Kis\", \"|g»(10)|\", \"\\\\Kis(W)\\\\\", \"\\\\Kti(lW)0)\\\\\", \"|fr,(1000)|\", \"L\"],\n  \"rows\": [\n    {\"Location\": \"soma\", \"Kti\": 16.5, \"Kis\": 16.5, \"|g»(10)|\": 14.6, \"\\\\Kis(W)\\\\\": 14.6, \"\\\\Kti(lW)0)\\\\\": 1.71, \"|fr,(1000)|\": 1.71, \"L\": 0},\n    {\"Location\": \"bl\", \"Kti\": 388, \"Kis\": 15.5, \"|g»(10)|\": 383, \"\\\\Kis(W)\\\\\": 13.6, \"\\\\Kti(lW)0)\\\\\": 71.4, \"|fr,(1000)|\": 0.18, \"L\": 0.24},\n    {\"Location\": \"b2\", \"Kti\": 751, \"Kis\": 15.2, \"|g»(10)|\": 667, \"\\\\Kis(W)\\\\\": 13.3, \"\\\\Kti(lW)0)\\\\\": 99.3, \"|fr,(1000)|\": 0.053, \"L\": 0.43},\n    {\"Location\": \"al\", \"Kti\": 63.4, \"Kis\": 13.9, \"|g»(10)|\": 50.7, \"\\\\Kis(W)\\\\\": 11.1, \"\\\\Kti(lW)0)\\\\\": 6.48, \"|fr,(1000)|\": 0.052, \"L\": 0.33},\n    {\"Location\": \"a2\", \"Kti\": 194, \"Kis\": 12.1, \"|g»(10)|\": 156, \"\\\\Kis(W)\\\\\": 8.43, \"\\\\Kti(lW)0)\\\\\": 23.4, \"|fr,(1000)|\": \"3.3x10^-3\", \"L\": 0.52},\n    {\"Location\": \"a3\", \"Kti\": 342, \"Kis\": 10.5, \"|g»(10)|\": 258, \"\\\\Kis(W)\\\\\": 6.62, \"\\\\Kti(lW)0)\\\\\": 30.8, \"|fr,(1000)|\": \"0.11x10^-3\", \"L\": 0.75},\n    {\"Location\": \"a4\", \"Kti\": 3298, \"Kis\": 7.8, \"|g»(10)|\": 2609, \"\\\\Kis(W)\\\\\": 4.10, \"\\\\Kti(lW)0)\\\\\": 216, \"|fr,(1000)|\": \"0.053x10^-6\", \"L\": 1.31},\n    {\"Location\": \"all\", \"Kti\": 178, \"Kis\": 12.4, \"|g»(10)|\": 148, \"\\\\Kis(W)\\\\\": 8.94, \"\\\\Kti(lW)0)\\\\\": 30.9, \"|fr,(1000)|\": \"5.2x10^-3\", \"L\": 0.49},\n    {\"Location\": \"a12\", \"Kti\": 404, \"Kis\": 11.2, \"|g»(10)|\": 335, \"\\\\Kis(W)\\\\\": 7.53, \"\\\\Kti(lW)0)\\\\\": 45.2, \"|fr,(1000)|\": \"0.30x10^-3\", \"L\": 0.69}\n  ]\n}"}
{"text": "Input and transfer resistances (in megaohms) between dendritic locations and the soma of the layer 5 pyramidal cell shown in\nFig. 3.7. \\Kis\\ is computed for sustained as well as for 10 and 1000 Hz sinusoidal inputs. The voltage-dependent conductances\nat the soma were replaced by their slope conductances, rendering the cell passive but with the same steady-state input resistance\nand resting potential as the active cell. This is reflected in the low-pass behavior of | Ky \\ which decreases with frequency. The last\ncolumn lists the normalized electrotonic distance between the synapse and the soma (computed according to Eq. 3.33). Notice the\ndramatic uncoupling between dendritic sites and the soma for rapidly varying input. For a discussion of the parameters used see\nSec. 3.4.4."}
{"text": "===== Page 18 =====\n66 . \nPASSIVE DENDRITIC TREES\nof 2 between the most proximal and the most distal sites. Remember that the somatic\npotential induced by the current injection is given by the product of the injected current and\nthe transfer resistance Kis. This implies that the coupling between dendritic sites and the\nsoma depends only weakly on distance for dc or slowly varying current inputs. For rapidly\nvarying inputs, on the other hand, the coupling will effectively be zero since the distributed\ncapacitance throughout the tree will absorb the charge before it reaches the soma. We will\ndevelop this theme further in the following section.\nTo derive transfer resistances between dendritic sites we can use the third property of\nKIJ (Eq. 3.29). For instance, to obtain the transfer resistance between two points in the\napical tree, say a3 and al, we use\n(3.31)\nyielding Ka^al = 47.65 M£l\n3.5 Measures of Synaptic Efficiency\nThe introduction of the transfer impedance Kij(f), or, for stationary current input, the\ntransfer resistance A^-(O) = KIJ, allows us to characterize the voltage as well as the\ncurrent attenuation between the site / of current injection and any other location in the\ndendritic tree, for instance, the soma s. As we will see, these are but two of a number of\ndifferent measures characterizing the efficiency, or the degree of coupling, between any\nparticular synaptic input site and the cell body where the initiation of the action potential\noccurs.\n3.5.1 Electrotonic Distance\nThe simplest of all measures, yet a potentially misleading one, is an estimate of the\nelectrotonic distance between any one location i and the soma. In an infinite cylinder\nor in a single finite cylinder of diameter d, the electrotonic distance between two points i\nand j is\n(3.32)\nwhere |i — j\\ is a shorthand notation to represent the physical distance between the two\npoints and A the space constant. This expression immediately generalizes to the frequency-\ndependent electrotonic distance by replacing A. with A(/), as in Eq. 2.36. In as far as the cell\nunder investigation can be approximated by a single equivalent cylinder (see Sec. 3.2; Rail et\nal., 1992), any point along this cylinder, corresponding to one or more equivalent locations\nin the original tree, can be assigned a meaningful electrotonic distance, say, relative to one\nend of the equivalent tree. However, as discussed in Sec. 3.2, real cells rarely satisfy the\nconditions necessary for them to be reduced to an equivalent cylinder.\nOne method to estimate the \"distance\" between i and the soma is to compute the\nelectrotonic lengths of all the cylinders between i and the soma and add them up. In the\ncontinuous case, we have\n(3.33)"}
{"text": "===== Page 19 =====\n3.5 Measures of Synaptic Efficiency \n• \n67\nwhere integration takes place along the direct path between the cell body and location i. In\nan infinite cylinder, there exists a simple exponential relationship between the electrotonic\nlength and the voltage attenuation (Sec. 2.2.1). In all other structures, Lis does not provide\na measure of the efficacy of signal transfer, since the underlying space constant A. assumes\nthe convenient fiction of an infinite and constant cylinder. This can be seen graphically\nin Fig. 2.4 for the case of a single, finite cylinder. Depending on the type of boundary\ncondition imposed, the actual voltage attenuation in a cable can be stronger (for a killed-\nend) or weaker (for a sealed-end boundary condition) than in an infinite cylinder with the\nsame diameter and membrane parameters. This difference can be substantial in real trees,\ngiven the heavy load imposed by all the additional branching. Under these conditions, Lis\nseriously underestimates the attenuation experienced by dc or transient inputs on their way\nto the soma. For instance, the electrotonic distance between point a4 in the distal apical\ntree and the soma is 1.31 (Table 3.1), giving rise to the expectation that the attenuation\nfrom that point to the soma (assuming an infinite cylinder) should be e1\"4-* = 3.7, yet the\ntrue voltage attenuation is around 423 for sustained current inputs and higher for transient\ninputs. Thus, for almost any dendritic tree, L,-s fails to even be in the ballpark of the true\nvoltage attenuation.\n3.5.2 Voltage Attenuation\nOne of the most common measures is the voltage attenuation among two sites (Rail and\nRinzel, 1973), that is, the ratio of the voltage at location i to the voltage at location j,\n(3.34)\nSince we are dealing with passive structures without a voltage-dependent membrane that\ncan amplify the signal, the voltage at the input site i will always be larger than the voltage\nanywhere else in the tree: Ay > 1. For the general case of a sinusoidal input current /,• (/)\nof frequency /, we have (Koch, Poggio, and Torre, 1982)\n(3.35)\nIn general Ay (/) is a complex number. As usual, if we do not explicitly express the\ndependency of Ay on /, we refer to the dc component for / = 0.\nThis definition does not depend on whether this current injection was caused by a synapse\nor by an intracellular injection. The magnitude of Ay (/) describes by how much the voltage\nattenuates between i and j, with large values indicating large decrements. For a fixed input\nsite z, Ay depends only on the transfer resistance Kfj. Figure 3.8 illustrates this dependency\nfor the voltage attenuation between site a3 in the apical tuft (Fig. 3.7) and the rest of the cell\nas well as the attenuation from the soma to other points. Because the ordinate is specified in\nunits of KIJ, this axis is inversely proportional to Ay. This type of graph is quite instructive\nand was first used in Rail and Rinzel (1973) to demonstrate a number of important points.\n1. The attenuation from any one point toward more distal points is small. Witness this when\nconsidering the attenuation away from the soma (lower curve) or the voltage decrement\nfrom «3 backward, that is, toward a4, or in a branch away from the soma (e.g., all and\na 12). This is not surprising, since the injected current has a much easier job depolarizing\nhigh-impedance sites than low-impedance ones."}
{"text": "===== Page 20 =====\n68 \n• \nPASSIVE DENDRITIC TREES\nFig. 3.8 VOLTAGE AND CHARGE ATTENUATION IN A PYRAMIDAL CELL \nTwo curves showing the\nsteady-state transfer resistance KIJ between a site in the apical tuft (at a3 in Fig. 3.7) and other parts\nof the tree (upper trace) as well as from the soma to dendritic sites for the layer 5 pyramidal cell.\nThe upper curve is inversely proportional to the voltage attenuation from a 3 to other sites in the tree\n(Eq. 3.34), while the lower curve is inversely proportional to the charge attenuation between sites in\nthe tree and the soma (Eq. 3.47). Note the pronounced asymmetry between large voltage attenuation\nbut much smaller charge attenuation. The input resistances Ka3,a3 and Ks^s are indicated by arrows.\n2. On the other hand, the voltage decrement at sites on the way to the soma, a very low\nimpedance site, is substantial. For instance, Aaj,tS = 341.68/10.46 *» 33: if a synaptic\ninput to a 3 causes a local sustained EPSP of 10 mV, the sustained somatic EPSP will be\nonly 0.3 mV.\n3. For rapid synaptic input, the actual attenuation will be larger due to the distributed\ncapacitance that \"soaks\" up the high-frequency components on their way to the cell\nbody. When the input consists of a rapid and transient current (peaking at 0.5 msec) into\nthe dendrite at a3, the effective Aa3i-s, defined as the peak of the local potential (here\nequal to 4.86 mV) divided by the peak somatic potential (0.051 mV), is 94, about three\ntimes larger than for sustained inputs.\n4. Both sustained as well as transient voltage attenuation can be considerably larger. For\nthe distal site a4, the dc attenuation alone is already 423.\n5. The attenuation away from the soma is considerably smaller (Rinzel and Rail, 1974).\nFor instance, ASit,3 is only 1.6, about twentyfold less than the voltage attenuation in the\nreverse direction (see also the lower curve in Fig. 3.8). In fact, except for an infinite\ncylinder,\n(3.36)\nThis relationship appears to run counter to the symmetry of the transfer impedance,\nbut upon closer inspection this discrepancy is resolved: Eq. 3.27 expresses symmetry"}
{"text": "===== Page 21 =====\n3.5 Measures of Synaptic Efficiency \n• \n69\nbetween current input and voltage output, while Eq. 3.36 expresses lack of symmetry\nbetween voltages at two different sites. The symmetry of ^,3^ is expressed in the fact\nthat the location marked soma on the upper curve in Fig. 3.8 has the same numerical\nvalue as the location marked a?> on the lower one (Rail, 1989).\nThe physical reason for the pronounced asymmetry between AJS and Asi is the large\ndifference in input impedance between dendritic sites and the soma. While most of the\ncurrent flows toward the low-impedance (or high-conductance) cell body (as expressed\nby Eq. 3.8), this current causes a much smaller change in the membrane potential at the\nsoma than at high-impedance locations. This fact also explains why the voltage attenuation\nbetween a 3 and the most distal part of the apical tree (a4) is comparatively minor. Given\nthin branches and sealed-end boundary conditions, only a very small voltage gradient exists.\nThis is important to keep in mind when trying to understand the antidromic spread of the\nsomatic action potential to dendritic sites.\nBy exploiting Eq. 3.29, we can express the voltage attenuation from i to j using any\npoint t lying on the direct path between i and j as a function of the attenuation between /\nand I and between I and /,\n(3.37)\nThis property allows us to introduce a sort of pseudo attenuation metric. Following Zador\n(1993), Zador, Agmon-Snir, and Segev (1995) and Brown et al., (1992), we define the\nlog attenuation as\n(3.38)\nusing the natural logarithm. Note that to differentiate this dimensionless variable from the\nelectrotonic distance, it carries a superscript. Because | AJJ | > 1,\n(3.39)\nas expected for a distance. We now make use of Eq. 3.37 and have for any point I on the\ndirect path from i to j,\n(3.40)\nAdditivity constitutes an elegant feature of L\"j, rendering it somewhat similar to a distance\nmeasure. Note that L\". is not a true metric, since symmetry does not hold due to the\nasymmetry in the voltage attenuation,\n(3.41)\nIn the special case of an infinite cylinder, it follows from Eq. 3.23 that\n(3.42)\nand, therefore,\n(3.43)\nBecause the logarithm of the voltage attenuation between two points in an infinite cable\nequals the electrotonic distance between them, L\". can be viewed as a generalization of the"}
{"text": "===== Page 22 =====\n70 . \nPASSIVE DENDRITIC TREES\ntraditional notion of electrotonic distance to arbitrary cable structures (Zador, Agmon-Snir,\nand Segev, 1995).\nWhile voltage attenuation has been and continues to be a popular measure of synaptic\nefficiency, it should not be forgotten that it represents a relative measure. And whether or\nnot the cell spikes by exceeding a threshold voltage depends on the absolute amplitude of\nthe somatic EPSP and not on the relative ratio of voltages.\n3.5.3 Charge Attenuation\nAnother measure of synaptic efficiency is the ratio of the charge transferred across the\nmembrane at the location of the input i to the charge reaching location j (Koch, Poggio,\nand Torre, 1982). The total charge transferred at i due to the current 7,-(f) is\n(3.44)\nThis equals the steady-state value of the Fourier transform of the current\n(3.45)\nThe charge transferred across the membrane at j is\n(3.46)\nThe charge attenuation from i to j, defined by analogy to the voltage attenuation, is\n(3.47)\nOr, the charge attenuation from i to j is identical to the voltage attenuation in the reverse\ndirection (for this reason, Zador, Agmon-Snir, and Segev, 1995, also refer to the charge\nattenuation as outward going attenuation). This allows us to use the lower trace in Fig. 3.8,\nwhich is proportional to the voltage decrement As, from the soma to dendritic sites, as an\nindicator for the charge attenuation between these sites and the soma. For the parameters\nused in this model, Asi is usually below 2, even for the most distal sites in the layer 1\ndendrites. It should not be forgotten that Asi is a relative measure and does not tell us\nanything about the absolute amount of charge transferred from the synapse i to the cell\nbody. Due to the equivalence of charge attenuation and inverse voltage attenuation, the\nlogarithmic transform of Eq. 3.38 can be used to define a pseudodistance.\nThe charge attenuation has, in contrast to the voltage attenuation, the distinction that it\nis independent of the time course of the current 7,-(f)- It simply provides an index of the\nrelative amount of charge reaching the cell body. Of course, this measure also stipulates\nthat the period of integration extends to infinity, which means in practice several time\nconstants, until all ions have charged and discharged the neuronal membrane between i and\nthe soma.\nIt is also possible to define the current attenuation as the ratio of the input current flowing\nat location i and the current flowing across the neuronal membrane at j,\n(3.48)"}
{"text": "===== Page 23 =====\n3.5 Measures of Synaptic Efficiency \n• \n71\nThis implies that the steady-state current attenuation between i and j is identical to the\ncharge attenuation between these two sites. It is important to understand here that the\ncurrent /,- does not correspond to the intracellular current that flows from / to j. Rather, /,•\nis the current that needs to be injected across the membrane at location j in order to change\nthe potential at j by the same amount as the current i does (namely, by A!/,/,-).\n3.5.4 Graphical Morphoelectrotonic Transforms\nIt is more and more common that the detailed, three-dimensional morphology of recon-\nstructed neurons is available in computer readable form. It therefore becomes possible\nto reexpress the geometry of a specific tree using different measures, for instance, the\nelectrotonic length, the log attenuation pseudodistance measure, or the dendritic delay.\nAs a group, these transforms, introduced by Zador (1993; Zador and co-workers 1992,\n1995; see also Bernander, Douglas, and Koch, 1992), have been called morphoelectrotonic\ntransforms (METs). In a MET, the anatomical length of each dendritic cable segment is\nreplaced by one of these various measures, while the diameter and the orientation of the\nsegment are preserved. In this way, different aspects of the electrotonic structure of the cell\ncan be directly visualized. Color can be used as a further visual aid to identify corresponding\npoints between different METs and the original anatomy.\nFigure 3.9 illustrates these procedures without the use of color. The original anatomy,\nderived from projecting the three-dimensional digitized coordinates of the tree onto the\nimage plane, is shown in (A). In (B), the anatomical length of each compartment is replaced\nby its electrotonic length. Therefore, the distance between a point i and the soma corresponds\nto L{S (Eq. 3.33). In particular, the thin basal and distal apical dendrites are accentuated\nat the cost of the thick apical trunk, since thinner cables have shorter space constants and\nare therefore electrotonically longer. The electrotonic length does not correspond to the\nstrength of coupling between i and the cell body, since the use of L is conditioned upon the\nspace constant A. that is defined for an infinite cylinder. With the total electrotonic length\nof the cell being on the order of one A., we would expect that stationary signals from the\nperiphery decay by a factor of \\/e, while their actual decay is more substantial (Table 3.1).\nThe log transform of the voltage attenuation from the dendritic tree to the soma (Lv\nis;\nFig. 3.9C) and from the soma to dendritic sites (L\"f', Fig. 3.9D) emphasizes the large\ndifferences (about a factor of 10) between incoming and outgoing voltage decrement. The\noutgoing voltage attenuation MET in Fig. 3.9D corresponds to the logarithmic transform\nof the charge attenuation from sites in the dendritic tree to the soma. The transforms are\nfunctional, in the sense that the voltage transfer can be directly read off from the figure.\n(This is a consequence of the additivity of the L\" measure, as expressed in Eq. 3.40.) For\ninstance, the voltage attenuation from b2 at the tip of a basal dendrite is e3 9 » 50, while\nthe charge injected at the same site will only attenuate by e°'os ^ 1.09. This also means\nthat a long-lasting hyperpolarization at the cell body of 10 mV will hyperpolarize the entire\nbasal tree, an area that accounts for 62% of the cell's membrane area, by at least 9.5 mV.\nSince the most distal sites in layer 1 of the apical tree have an ASI- value of about 2.1, even\nthese parts will be hyperpolarized by about 4.8 mV.\nZador, Agmon-Snir, and Segev (1995) offer another interpretation of Lv\nsi. A frequently\nasked question relates to the cost of moving a synapse from the soma to a particular location\ni in the tree. Assuming that the synaptic input can be treated as a current and is identical\nto /o at both locations, the voltage induced by the somatic location is given by Ksslg,\nwhile the somatic potential in response to the dendritic input is A^/O. The ratio of the"}
{"text": "===== Page 24 =====\n72 . \nPASSIVE DENDRITIC TREES\nFig. 3.9 MORPHOELECTROTONIC TRANSFORMS Graphical means of visualizing different mea-\nsures of synaptic efficiency using morphoelectrotonic transforms (METs) (Zador, 1993). (A) Two-\ndimensional projection of the layer 5 pyramidal cell (see also Fig. 3.7). (B) The physical length (.\nof each dendritic compartment is replaced by the electrotonic length L = lj\\, while the orientation\nof the compartment remains constant. In an infinite cable, the voltage attenuation between two sites\na distance L apart is e~L. But because the electrical structure of the dendritic tree is quite distinct\nfrom that of an infinite cable, the voltage decay from dendritic sites to the soma is much larger than\nindicated by this measure. (C), (D) METs corresponding to the logarithmic voltage attenuation from\nsites i in the tree toward the soma, L\"v = In |A,S|, and from the soma to the tree, L\"; = In \\Asi\\,\nfor stationary input. The latter measure is equivalent to the charge attenuation, that is, to the ratio\nof the charge injected at i to the charge reaching the soma (after integrating for a time that is long\ncompared to rm). Notice the very different scales (corresponding to the distance over which the signal\nattenuates by a factor of e1 -° = 7.39 and e02 = 1.22, respectively), attesting to the fact that neurons\nare very compact from the point of view of charge but very dispersed from the point of view of voltage\nattenuation. These METs provide a snapshot of the cell's electroanatomy for a particular configuration\nof electrical parameters.\nmembrane potential induced by the somatic input to the EPSP induced by the dendritic\ninput is Kss/Kis — ASJ. As an example, moving a sustained current injection from the\nsoma to site a4 in the distal apical tree, reduces the effect of this input by only a factor\nof 2.1. Of course, for rapidly varying input, the cost of moving to the dendrites can be\nsubstantially higher (for the same synapse, a factor of about 4 for input changing at the\n10-Hz scale and a factor of 20 million for input varying at the kilohertz scale).\nIf transient input is used at the soma, the outgoing voltage attenuation is much larger. This\nis demonstrated in Fig. 3.10 for a sinusoidal input of 1000 Hz. This frequency corresponds"}
{"text": "===== Page 25 =====\n3.5 Measures of Synaptic Efficiency\n73\napproximately to the inverse of the width of a typical action potential and provides us with\nsome intuition for how far the very rapid portion of a somatic action potential will propagate\ninto a passive tree (a phenomenon known as antidromic spike invasion). While the somatic\npotential in response to current input at a distal, apical site (a4 in Fig. 3.7) is 423 times\nsmaller than the dendrite EPSP for stationary inputs (but only losing half of the injected\ncharge in the process) and 630 times smaller for a 10-Hz sinusoidal, it is attenuated by\na staggering factor of 4 billion for a 1000-Hz sinusoidal input. In other words, without\ndramatic amplification by active processes, a distal high-frequency input has no chance of\never influencing anything occurring close to the soma (see Chap. 19).\nA further feature of METs is that they reflect the momentary state of the cell, a state that\ncan be modulated by any number of different events in the brain. For instance, as discussed\nin Sec. 18.3.3, varying the synaptic background—corresponding to effectively varying the\nmembrane resistance Rm—modulates the electrotonic geometry of the cell by changing the\nLis or Lv\nis (see Fig. 18.6). Or if some brainstem afferent releases a neuromodulator, such as\nnoradrenaline or acetylcholine, onto part of the cell, its electroanatomy can shrink or grow,\nreflecting the corresponding change in the efficiency of synaptic inputs.\nFigure 3.11, from Zador, Agmon-Snir and Segev (1995), provides a graphical comparison\nof L\"(. for four different neurons with identical, albeit passive, membrane properties. The\ndifferences between them are dramatic. Given the large diameter of the Purkinje cell\ndendrites, almost no charge attenuation occurs for sustained current inputs. The electrotonic\nbehavior of the layer 2/3 pyramidal cell is very similar to the behavior of the basal tree of\nthe layer 5 cell. Finally, the apical tree of the hippocampal pyramidal cell sprouts into a\nlong, central stalk across which substantial charge attenuation occurs.\nFig. 3.10 DYNAMIC MORPHOELECTROTONIC TRANSFORMS \n(A) Logarithm of the voltage atten-\nuation, In \\AiS(f)\\ = L\"s(f) for a 1000 Hz input. For such transient input, the most distal site is 27\nunits away from the cell body (for stationary inputs, the corresponding distance is 6.7; see Fig. 3.9C).\n(B) Logarithm of the voltage attention L\"(. (/) from the soma to dendritic sites for the same 1000-Hz\ninput. This frequency has been choosen to demonstrate how far the fast components in a somatic\naction potential (with an approximate width of 1 msec) can propagate back into the dendritic tree."}
{"text": "===== Page 26 =====\n74 \n. \nPASSIVE DENDRITIC TREES\nFig. 3.11 MORPHOELECTROTONIC TRANSFORMS FOR DIFFERENT CELL TYPES Comparison of\nthe logarithm of the charge attenutation—corresponding to the log-voltage attentuation from the soma\nto dendritic sites for stationary inputs—for four different neurons: the layer 5 neocortical pyramidal\ncell used throughout, a layer 2/3 neocortical pyramidal cell, a Purkinje cell from the cerebellum of a\nguinea pig, and a CA1 pyramidal cell from rat hippocampus (Zador, Agmon-Snir, and Segev, 1995).\nThe left column corresponds to the morphology and the right one to L\"(. (0). The scale bar, indicating the\ndistance over which the voltage attenuates by e°-' « 1.1, applies to all cells. To facilitate comparison,\nall neurons are passive with identical membrane parameters (Rm = 100,000f2-cm2, /?,- = lOOS-cm).\nReprinted in modified form by permission from Zador, Agmon-Snir, and Segev (1995)."}
{"text": "===== Page 27 =====\n3.6 Signal Delays in Dendritic Trees \n• \n75\n3.6 Signal Delays in Dendritic Trees\nWe discussed in Sec. 2.3 transient inputs and their effect on the voltage in single cables.\nThe most important parameter regulating how fast the membrane potential equilibrates is\nthe neuronal time constant Tm. But how is rm measured experimentally in dendritic trees?\nFollowing Agmon-Snir and Segev (1993), we also introduced a new measure of neuronal\ndelay in terms of the difference between the centroid or center of mass of the current input\nand the voltage signal. How do these definitions fare in spatially extended structures?\n3.6.1 Experimental Determination o1rm\nA general property of the voltage response to current input into an arbitrary complex,\npassive cable structure (whether a single finite cylinder or a large dendritic tree) with\nuniform electrical parameters throughout the tree is that it can always be expressed as an\ninfinite sum of exponentials (Rail, 1969a; for more details, consult Appendix C),\n(3.49)\nwhere the Bt 's depend both on the initial conditions and on x, and Voo captures the steady-\nstate components of the voltage. The T; 's can be thought of as equalizing constants that\ngovern the rapid flow of current (and the reduction of voltage differences) between different\nregions of the cable. They are independent of the site of the current input, the site of\nrecording, or the initial voltage distribution in the tree. In general, solving for these r, 's\ninvolves extracting the root of a recursively defined transcendental equation. (For more\nrecent work on this, see Holmes, Segev, and Rail, 1992; Major, Evans, and Jack, 1993a.)\nSeveral important points need to be mentioned here. First, in a tree with uniform\nmembrane properties with sealed ends (and without a shunt or a voltage clamp) the slowest\ntime constant TO is always equal to the membrane time constant, TO = tm = RmCm.\nSecond, all time constants scale with the membrane capacitance Cm. Third, introducing a\nshunt conductance at the soma—mimicking the effect of an intracellular electrode—reduces\nall the time constants, including TQ. In other words, under realistic experimental conditions,\nthe measured slowest TO represents a lower bound on the actual value of rm. As evident in\nEq. 2.40, these T,- 's have a particularly pleasing and simple interpretation for a single finite\npassive cable with sealed-end boundaries.\nDirect measurements of Tm in neurons depended on the development of the intracellular\nelectrode, introduced by Graham and Gerard (1946) for muscles and first applied to central\nmammalian neurons (spinal motoneurons) by Woodbury and Patton (1952) and Brock,\nCoombs, and Eccles (1952). These early studies neglected the cable properties of the\ndendrites and estimated rm = 2 msec. A common numerical technique for extracting\ntime constants involves the peeling of low-order exponentials from a semilogarithmic plot\nof the voltage decay (Rail, 1969a,b; see Fig. 3.12). It was first applied to the problem of\nextracting time constants in a motoneurons by Burke and Ten Bruggencate (1971).\nThey estimated that rm ranges from 5 to 7 msec. Averaging over 50 voltage transients\nallowed them to determine not only TO = rm but also the next slowest time constant r\\, which\nthey calculated to be around 1 msec. Assuming that the dendritic tree of the motoneuron\ncan be reduced to a single, equivalent cable, allowed Burke and Ten Bruggencate (1971) to\nexploit Eq. 2.42 to estimate the electrotonic length at around 1.5.\nExperimentally, tm can be determined by injecting a brief current pulse into the soma\nand recording the voltage response at the same point. Long current steps tend to activate"}
{"text": "===== Page 28 =====\n76 . \nPASSIVE DENDRITIC TREES\nFig. 3.12 ESTIMATING THE FIRST Two TIME CONSTANTS Peeling procedure to assess the time\nconstants of voltage decay in response to a small and short current pulse. The data shown here were\nobtained by recording from a pyramidal cell located in the upper layers in the frontal cortex of the\nguinea pig. The voltage response is shown as a logarithmic function of time. Due to the nonisopotential\nnature of the dendrites, the voltage decays initially rapidly; the slowest time constant TO = Tm is only\nevident toward the tail of the response, with TO = 16 msec. Peeling it away reveals the second slowest\ntime constant r\\ = 1.7 msec. The recordings were carried out at the cell's resting potential of —65 mV\nin the laboratory of Yosef Yarom. Reprinted by permission from Koch, Rapp, and Segev (1996).\nvoltage-dependent components (e.g., the over- and undershoot dealt with in detail by Ito and\nOshima, 1965) that severely interfere with the measurement of the passive time constant.\nIn a linear system, the response to a current pulse is given by the temporal derivative of\nthe response to a step. Because the derivative of Eq. 3.49 still leaves an infinite number\nof exponential terms with the slowest term decaying as g~'/Tm, the peeling method is also\napplicable to the derivative of V(t) in Eq. 3.49 with the advantage that the steady-state\ncomponent of the voltage disappears. The slope of the tail of the decaying phase of Vm,\nwhen plotted on a semilogarithmic scale, is — l/rm. Figure 3.12 illustrates this procedure\nfor a voltage transient in a guinea pig neocortical pyramidal cell, showing how the voltage\ngradients over the cell surface equalize after about 6-8 msec; from this time on, the voltage\ndecays exponentially. The slowest time constant TO = 16 msec corresponds to the inverse\nof the slope at the tail end of the voltage distribution (see also Kim and Connors, 1993).\nSubtracting this component from the original voltage curve gives rise to a new transient\n(noisy curve at bottom) whose tail can now again be fitted with a straight line whose slope\nis — 1 /TI . In this case t\\ - 1.7 msec. If we assume that the membrane of this cell is uniform\nand that the intracellular electrode did not cause a significant shunt (injury), TO — rm.\nIt is important to emphasize that the quality of the electrical recording depends to a large\nextent on the tightness of the resistive seal around the electrode. During the impalement\nof the neuronal membrane, the intracellular electrode frequently rips a large hole in the\nmembrane, allowing ions to flow through, thereby seriously compromising the quality of\nthe recording. The introduction of the whole-cell recording of in vitro cells and the perforated\npatch clamp technique (Edwards et al., 1989; Spruston and Johnston, 1992) has dramatically"}
{"text": "===== Page 29 =====\n3.6 Signal Delays in Dendritic Trees \n• \n77\nimproved the situation with an effective electrode shunt on the order of 0.1 nS. Indeed, over\nthe last four decades the estimate for Tm in central neurons has grown significantly. In\nthe 1950s it was assumed to be only a few milliseconds. With improved averaging and\nrecording techniques, the recent estimates of rm from intracellular recordings range from\n10 to 40 msec for the major types of central neurons. (See Fleshman, Segev, and Burke,\n1988 and Clements and Redman, 1989, for a motoneurons, Brown, Fricke, and Perkel,\n1981, for hippocampal neurons, Nitzan, Segev, and Yarom, 1990, for vagal motoneurons\nand Rapp, Segev, and Yarom, 1994, for cerebellar Purkinje cells.) With tight-seal whole-cell\nrecordings, the estimates are growing even further and are approaching 100 msec in slice\npreparations (e.g., Andersen, Raastad and Storm 1990 found time constants ranging from\n50 to 140 msec in the hippocampus; Major et al., 1994 report an average value of 93 msec\nin CA3 pyramidal cells; see also Spruston, Jaffe, and Johnston, 1994). A notable exception\nto such high values is the 2-msec time constant in slices in the avian cochlear nucleus\nresponsible for processing high-fidelity sound information, measured by using whole-cell\nrecording with tight seal (Reyes, Rubel, and Spain, 1994). We conclude this section by\nnoting that rm estimates depend heavily on the composition of the physiological solution\nduring the experiment. Adding specific blockers to the solution, as is frequently done\n(e.g., caesium ions to block K+-dependent rectification or blockers of NMDA-dependent\nchannels), directly affects (typically increases) estimates of rm.\n3.6.2 Local and Propagation Delays in Dendritic Trees\nThe problem with the standard definition of Tm is twofold. Firstly, because rm is a property\nof the membrane it does not take into consideration the effect of the neuronal structure\non the dynamics of the membrane potential at the input site (which can be the soma, a\nthin dendrite, or an elongated spine). Secondly, r does not inform us about how rapidly\nthe membrane can respond to more physiological input than a current pulse or step. Both\nproblems were addressed by Agmon-Snir and Segev (1993) (see also Zador, Agmon-Snir,\nand Segev, 1995). In Chap. 2, we defined the transfer delay Dtj to be the difference in\nthe centroid of the induced voltage at location j and the centroid of the current flowing at\ni (Eq. 2.45) and the input delay DH as the difference between the centroids of the local\nvoltage and the injected current at the same location (Eq. 2.46). For an infinite cable, the\ntransfer delay Dtj = (1 + \\i - j\\/k)rm/2 (Eq. 3.50).\nThe center of mass method can be applied to calculate analytically the local delay at\nany point in an arbitrary dendritic tree. Figure 3.13 illustrates a few delays arising in\nthe pyramidal cell for fast current injections of the form expected during activation of\nfast voltage-independent synaptic input. The time course follows that of the a function\nintroduced in Eq. 1.21,\n(3.50)\n(We here approximate the synaptic input by a current source.) The current attains its peak\nvalue /peak at? = ?peak- Values for the latter are in the 0.25-1-msec range, depending on the\nexact circumstances (for more details see Sec. 4.6). The centroid of the current is located\nat 2rpeak-\nThe local delay D,, reflects the RC properties of the local membrane as well as the\nelectrotonic structure of the cell as seen from location i. It is maximal (and equal to rm) for\nthe membrane patch case (Eq. 2.48), since here the input current can only discharge through"}
{"text": "===== Page 30 =====\n78 \n• \nPASSIVE DENDRITIC TREES\nFig. 3.13 LOCAL DELAY Four examples of the local delay DH defined as the difference between the\ncentroid of the local voltage (upper curves in the four panels) and the centroid of the local current injection\nthat gave rise to this depolarization (lower curves in the four panels) for the layer 5 pyramidal cell from\nneocortex. DH reflects the contribution of the geometry of the cable structure to the voltage dynamics\nand is independent of the waveform of the current input. The input current is given by Eq. 3.50 with\n'peak — 0.5 msec and a centroid at 1 msec (the centroids are marked with arrows). At the soma (lower\nleft) DH = 17.7 msec fa ^m, while at distal sites, such as a layer 1 apical dendrite (upper right) or the\ntip of a layer 5 basal dendrite (lower right), DH can be much faster than rm. Rm = 20,000 £2 • cm2 and\nCm = I /zF/cm2throughout the cell. The amplitude of all responses have been normalized and shifted by\n0.5 msec to the right. Reprinted by permission from Koch, Rapp, and Segev (1996).\none pathway. In an infinite cable, DH = Tm/2, since current can flow through membrane re-\nsistance as well as through the intracellular resistivity (Fig. 2.11). Adding additional current\nsinks, in the form of a complex and highly branched dendritic tree, further reduces DH .\nBecause the soma is relatively large (with a radius of 20 /urn), DH at the soma is not\nvery far from that of an isopotential membrane patch, DH — 17.7 msec « rm (lower left\nin Fig. 3.13). For distal sites in the layer 5 pyramidal cell, DH « rm/10 or less (upper and\nlower right-hand panels in Fig. 3.13), while for dendritic spines DH « rm/20 (not shown).\nSuch small values of A, go hand in hand with a narrow (brief) voltage transient at the\ninput sites (Rinzel and Rail, 1974). Indeed, distal dendritic arbors, in particular the many\nfine branches of the basal tree, provide for multiple sites with a very brief (compared to rm)\nand local voltage response, provided the input is also brief. This will be taken up in more\ndetail further below as well as in Chap. 19.\nThe transfer delay between i and j really includes two components, the time it takes for\nthe current input to depolarize (or hyperpolarize) the local membrane (that is, DH) and the"}
{"text": "===== Page 31 =====\n3.6 Signal Delays in Dendritic Trees \n• \n79\ntime it takes the centroid of the local potential to propagate to j. Accordingly, Agmon-Snir\nand Segev (1993) define the propagation delay as\n(3.51)\nThis measure has a number of similarities with the log attenuation L\". measure defined in\nSec. 3.5.2. In particular, it is not symmetric,\n(3.52)\nbut it is additive for all locations t on the direct path between locations i and j:\n(3.53)\nThis last property allows us to compute the morphoelectrotonic transforms for P(y just like\nwe did for the log voltage attenuation measure (Zador, Agmon-Snir, and Segev, 1995).\nBefore we discuss the functional significance of these different delays in realistic dendritic\ntrees, we would like to note the analogies between input and transfer resistances and input\nand transfer delays. Both sets of variables are defined as quantities relating current at location\ni to voltage at location i or j and both obey the same symmetry relationships.\nThe propagation delay P, s from a dendritic site i to the soma s is obtained by subtracting\nthe local delay at i from the transfer delay from i to the soma, reflecting the actual\npropagation time of the voltage centroid from the input site to the soma. We can, of course,\nalso define a propagation delay from the soma to dendritic sites Psi (Agmon-Snir and Segev\ncall this the net dendritic delay, to indicate that PSJ measures the \"cost\" in terms of delay\nthat results from placing the input at the dendritic site i rather than at the cell body). This\nmeasure turns out to be the most useful of all. Reverting back to the definitions of P,; and\nDij given in Sec. 2.4, we have\n(3.54)\nthat is, we can think of Psi as the difference between the voltage response at i to a somatic\ninput and the voltage response at the soma to a somatic input. Because the decay phases\nof both EPSPs are relatively similar (being primarily dictated by the properties of the large\ncell body), P,,- reflects differences in the rising phases of the EPSPs. In this sense, P,,-\nhas similar values and can be interpreted as the time delay between the peak of the local\nEPSP and the peak in the somatic EPSP. Figure 3.14 illustrates the MET of the propagation\ndelay Psi for the layer 5 pyramidal cell devoid of synaptic background activity. An extreme\ncase is shown for which Rm is reduced tenfold. In an infinite cable, this leads to a \\/To\nreduction in the propagation delay (Eq. 2.52). In fact, the scaling is more complex for a\ndistributed dendritic tree. It takes the centroid of the somatic EPSP between 1 and 2 msec\nto propagate to the tips of the basal dendrites (and proximal apical dendrites) independent\nof Rm, while P,, to the tip of a typical apical dendrite decreases by about a factor of 2 to\nabout 10-15 msec.\nThe outward going propagation delay Ps; has another interesting interpretation. Using\nEq. 2.51, we have\n(3.55)\nIn other words, it corresponds to the difference between the centroids of the current at the\nsoma and at site i."}
{"text": "===== Page 32 =====\n80\nPASSIVE DENDRITIC TREES\nFig. 3.14 PROPAGATION DELAY Morphoelectrotonic transform (MET) for the outward going\npropagation delay Pst from the soma to dendritic sites (Agmon-Snir and Segev, 1993) computed\nfor the layer 5 pyramidal cell with (A) a very high value of Rm = 100, 000 £2 • cm2 or (B) a 10 times\nreduced value of 10,000 fi • cm2. P,, corresponds to the effective delay between the peak of the local\nEPSP at i and the somatic EPSP. Changing Rm by a factor of 10 should reduce the propagation by\n•s/10 for an infinite cable, while the effect is much less in a distributed tree. Pj, to the main branch\npoint of the apical tree is 6 msec for the high and 2.5 msec for the low Rm value.\n3.6.3 Dependence of Fast Synaptic Inputs on Cable Parameters\nWhile the dynamics of the membrane potential in response to a slowly varying input current\nis dictated by rm, this is most emphatically not the case for rapid inputs (Segev and Parnas,\n1983). For fast synapses onto thin dendrites, the rise time as well as the initial decay of the\nassociated postsynaptic potential is almost completely independent of Rm and is primarily\nlimited by the dynamics of the synaptic current, that is, ultimately by how fast the underlying\nionic channels open and close (e.g., DH for distal dendritic sites in Fig. 3.13).\nLet us consider how three aspects of the excitatory postsynaptic potential depend on the\nthree cable parameters, Rm, Cm, and /?,-: (1) its rise time or rate of depolarization, (2) its\npeak potential, and (3) its rate of repolarization.\nFor a fast synaptic input, the rate of depolarization in an infinite cable of diameter d is\nlimited by the temporal derivative of the membrane potential in response to a current step\nof amplitude /Q. Using Eq. 2.33 and recalling that the derivative of the error function is\n2e~~x /^/TT yields\n(3.56)\nAt very short times when the exponential term can be developed in a Taylor series, the\nderivative behaves as const/^/t, where const depends on /?;, /Q, and Cm, but not on Rm.\nWhat happens is that initially the injected current primarily charges up the membrane\ncapacitance at the site of current injection as well as at neighboring locations (via the\naxial resistance /?,). The larger the /?, or the smaller the Cm, the faster the voltage\nwill change. It is only the slow components of the decay that are affected by low-pass\nfiltering, as demonstrated graphically in Fig. 3.15. Here the dendritic potential in response\nto a single depolarizing current input (Eq. 3.50) in the pyramidal cell is computed for"}
{"text": "===== Page 33 =====\n3.6 Signal Delays in Dendritic Trees\n81\nRm - 10,000 £2 • cm2 and R = 100,000 £2 • cm2. While the dc input and transfer\nresistances are affected appropriately, almost no effect on the rise time is evident.\nSurprisingly, both the peak depolarization as well as the initial phase of the repolarization\nof the voltage change are not affected by the tenfold change in Rm. This surprising behavior\nis only seen in thin dendrites and not at the soma: because of the limited membrane area,\nall of the injected current charges up the associated capacitance. The rapid repolarization\nFig. 3.15 WEAK DEPENDENCE OF FAST DENDRITIC EPSP ON Rm \n(A) Dendritic and (B) somatic\nsynaptic potentials in response to a single fast excitatory synapse activated 5 msec into the simulation\nand located at a3 in the distal apical tree of the layer 5 pyramidal cell (see Fig. 3.7) for two different\nvalues of Rm (in units of £2 • cm2). The current peaks 0.5 msec after onset and is over within 2 msec.\nThis is so fast compared to the membrane time constant of either 10 or 100 msec that the rising phase\nas well as the peak dendritic response is more or less independent of Rm. Indeed, because the input\nis so fast compared to the time constant of the dendrite, the local voltage response approximates\nthe impulse response. Due to the low-pass filtering between the synapse and the soraa, the somatic\npotential is strongly affected by the decrease in electrotonic distance and the increase in rm. This\nshows the fallacy of using rm as an indicator for how rapidly the membrane can respond to synaptic\ninput. Arrows point to the location of the centroids."}
{"text": "===== Page 34 =====\n82 . \nPASSIVE DENDRITIC TREES\noccurs because the capacitances outside the immediate neighborhood of the site of current\ninjection (which includes the distant cell body) soak up the charge. The synaptic current is\nso rapid that all of this takes place with only little involvement of the membrane resistance.\nBecause of the much larger capacitance present at the soma, the dendrites branching from\nthe cell body fail to provide any substantive capacitive sink and repolarization is much\nslower (witness the decay of the somatic and the dendritic voltages in Fig. 3.13).\nRail (1964) first observed that this capacitive effect allows for faster than exponential\ndecay, in particular in dendrites (reviewed extensivley in Jack, Noble, and Tsien, 1975).\nSoftky (1994) derives an approximate expression for the peak potential and the rate of\nrepolarization. By neglecting any leak conductances (that is, working in the limit of\nRm -» oo), the situation is formally equivalent to treating diffusion of charge inside a\ncylinder (see Fig. 11.3). This allows Softky to find the following expression for the peak\ndepolarization in response to a very fast current pulse,\n(3.57)\n(see also Eq. 3.56). Note that Rm never appears here and that the peak potential scales with\nthe square root of fpeak- Softky quantifies the rate of repolarization by estimating the time\nTi/2 it takes for the potential to decay from Vp^ halfway to zero,\nri/2 « 6/peak. \n(3.58)\nFigure 3.16 illustrates how well this approximation works for very rapid current inputs. It\ndoes badly on the slower components of the voltage decay since it completely neglects the\neffect of the membrane resistance.\nAs enthusiastically argued by Softky (1994, 1995), such rapid membrane dynamics,\noutpacing rm by far, give rise to the possibility of submillisecond coincidence detection in\nthe thin basal and apical dendrites, in particular if Hodgkin-Huxley-like nonlinearities are\npresent. We will pick up this story in Chap. 19.\n3.7 Recapitulation\nWhile Chap. 2 is concerned with solutions to the cable equation in single cables, this one\nconcentrates on studying the membrane potential in realistic dendritic trees with the aim\nof understanding the relationship between dendritic architecture and function. A number\nof exact, recursive techniques exist to derive the membrane potential at any point in the\ndendritic tree in response to an arbitrary current injection at some other point. Because\nthese techniques do not generalize well to n simultaneous inputs and do not work in the\npresence of membrane nonlinearities, today's method of choice is the numerical solution\nof the appropriately spatio-temporal discretized cable equation (compartmental method).\nSeveral well-documented, graphics-oriented, public software packages are available that\nimplement the appropriate algorithms.\nAn alternative approach for gaining intuition about the events occurring in a dendritic\ntree is to \"observe\" the system at two points i and j, where the input is applied at i and the\noutput recorded at j. The entire system can then be characterized in terms of three frequency-\ndependent functions that, in general, take on complex values: the input impedances KH (f)\nand Kjj(f) \nand the transfer impedance Kjj(f) \n= Kjj(f). \nFor sustained current input,\nthese reduce to three real numbers, corresponding to the conventional input resistances KH\ntj/2 «3 6/peak ."}
{"text": "===== Page 35 =====\n3.7 Recapitulation\n83\nFig. 3.16 SUBMILLISECOND RISE AND DECAY OF POTENTIALS Rapid membrane depolarization\nat the center of a thin basal dendrite of the pyramidal cell (close to bl in Fig. 3.7) in response to\nlocal current injections with different rise times. The current took the form of Eq. 3.50 with fpeak\nof (A) 0.05, (B) 0.1, (C) 0.2, and (D) 0.4 msec. The approximations derived by Softky (1994) are\nindicated in bold. They describe well the peak membrane potential (Eq. 3.57) and the early part of\nthe rapid repolarization. It is only during the slow, late part of the membrane depolarization, where\nthe approximation diverges from the correct solution, that the membrane resistance plays any role.\nThe somatic depolarizations in response to these inputs are minute, typically less than 0.5 mV. The\nmorale is that brief and rapid synaptic inputs can give rise to changes in the membrane potential in\nthe submillisecond range. Reprinted in modified form by permission from Softky (1994).\nand KJJ and transfer resistance Kij (with the dimensions of a resistance). This approach\nallows us to define in a straightforward manner the voltage attenuation AJS between any\ndendritic site i and the soma s and the outgoing voltage attenuation Asi from the soma to\nsites in the dendritic tree. This last measure is identical to the charge attenuation, the ratio\nof the total charge injected at the synaptic site i to the total charge reaching the soma.\nThe chapter also introduces the logarithmic transform of the attenuation, LV. = In | A,-y |.\nAs compared to a true distance metric, this measure is not symmetric: the logarithm of the\nvoltage attenuation between i and j is different from the logarithm of the voltage decrement\nbetween j and i. Indeed, for passive trees, the voltage attenuation from any dendritic site\nto the soma is always considerably larger than in the reverse direction. The reason for this\nprofound asymmetry is that dendritic sites have high impedance while the cell body has a\nlow impedance. A further consequence is that, if integrated over sufficiently long times (in\npractice several time constants), about half of the total charge injected into distal dendritic\nsites can reach the soma. We discussed a frequently used measure of synaptic efficiency,\nthe total electrotonic distance Lis. Due to the fact that real trees are quite distinct from\ninfinite cables, this measure sharply underestimates the true voltage attenuation. All of this\ncan be visualized graphically, using the morphoelectrotonic transforms. METs are a useful\ndynamic tool for portraying different features of the electroanatomy of a cell."}
{"text": "===== Page 36 =====\n84 . \nPASSIVE DENDRITIC TREES\nWe also considered delays in the dendritic tree and between dendritic sites and the\nsoma. Using the definition of delays in terms of differences in the centroids of voltages and\ncurrents, we can show that im represents an upper bound on how slowly the voltage decays\nin response to fast inputs. At the soma, the local delay is on the order of the membrane time\nconstant, while for distal sites the local delay can be much faster, as small as 5 or 10% of\nrm, implying that events in the dendritic tree can be very rapid. This is particularly true for\nrapid synaptic events in thin dendrites, where the rise and early decay times of the dendritic\nmembrane potential are independent of Rm, allowing for the possibility of carrying out\nprecise timing relationships in these structures that are not limited by rm.\nThe take-home message is that one has to be careful in applying the concepts that\ncharacterize an infinite cable (A, Tm, and /?jn) to realistic neurons. These have finite arbors,\nwith multiple dendrites terminating at different distances and branching patterns that do\nnot obey the necessary geometrical constraints for reduction to an equivalent cylinder.\nThese considerations have significant effects on how the voltage spreads and attenuates in\ndendrites and hence in how synaptic inputs are integrated."}
{"text": "===== Page 2 =====\n86 \n• \nSYNAPTIC INPUT\nFig. 4.1 ANATOMY OF CORTICAL SYNAPSES Electron micrographs of spines and synapses on\na dendrite of a spiny stellate neuron in layer 4 of the visual cortex of an adult cat. Around 300\nmillion synapses are packed into a single cubic millimeter of this tissue, providing the substrate for\nan extraordinary degree of neuronal interconnectivity. (A) 625 ultrathin sections of part of a proximal\ndendrite of the neuron are shown here in this computer-assisted view. The lower end is close to the\nsoma. The letters and arrows correspond to the electron-microscopic views shown in the adjacent five\nphotographic panels. (B) The dendrite (d) gives rise to a spine neck (sn) and spine head (sp). The latter\nreceives an asymmetric, that is, excitatory synaptic profile at the solid arrow. These arrows always\npoint from the unstained presynaptic toward the postsynaptic site. (C) Two asymmetric synapses on\nthe dendritic shaft. (D) A symmetric, that is, inhibitory synapse (open arrow) at the branch point\nshown in A. (E) A single presynaptic element makes two asymmetric synapses with a spine and a\ndendrite of the cell. (F) An asymmetric synapse with the dendritic shaft. Scale bars equal 2 fim in A\nand 0.5 /urn in B-F. Reprinted by permission from Anderson et al., (1994).\n1996). Autapses appear to be identical to normal chemical synapses. While autapses are rare\non pyramidal cells, they are found more frequently on two subclasses of cortical inhibitory\ninterneurons, raising the possibility that they might have a functional role and are not just the\nunfortunate consequence of an imprecise developmental rule (Tamas, Buhl, and Somogyi,\n1997; Bekkers, 1998).\nThe literature on the properties of synaptic transmission and its molecular nature is\ngargantuan and keeps growing (Kandel, Schwartz, and Jessell, 1991; Jessell and Kandel,"}
{"text": "===== Page 3 =====\n4.2 Synaptic Transmission Is Stochastic \n• \n87\n1993; Stevens, 1993; for a historical overview, see Shepherd, and Erulkar, 1997). We\nrecommend Hille (1992) and Johnston and Wu (1995).\n4.1 Neuronal and Synaptic Packing Densities\nFrom the anatomical point of view, synapses in the central nervous system can be conve-\nniently classified according to the detailed morphology of the synaptic profiles in electron-\nmicroscopic images into one of two classes, Gray type I and Gray type II synapses (Gray,\n1959). Using a combination of electrophysiological, pharmacological, and anatomical\ncriteria, type I synapses, also known as asymmetrical synapses, have been found to be\nexcitatory, while type II synapses, also known as symmetrical synapses, act in an inhibitory\nmanner (White, 1989; Braitenberg and Schiiz, 1991; Douglas and Martin, 1998; see Fig. 4.1).\nSynapses are small. The area of contact between pre- and postsynaptic processes has a\ndiameter of 0.5-1.0 /zm, while the presynaptic terminal is only slightly larger. This implies\nvery high synaptic packing densities of around 7.2 x 108 synapses per cubic millimeter in\nthe mouse (Braitenberg and Schiiz, 1991). In the cat, the density has been estimated around\n3x 108 per cubic millimeter (Beaulieu and Colonnier, 1985). If the synapses were located\non a three-dimensional lattice, they would be spaced a mere 1.1 /nm apart. Braitenberg and\nSchiiz (1991) estimate the extent of neuronal processes in a 1-mm3-cube of cortical tissue,\ncoming up with the staggering amount of 4.1 km total length of axonal processes (at an\naverage diameter of 0.3 /u,m) and 456 m total length of dendrites (with a diameter of 0.9 /am)\nin this volume. In other words, the \"average\" nerve cell in the mouse cortex receives input\nfrom 7800 synapses along its 4 mm of dendrites and is connected with 4 cm of \"axonal\nwire\" to other cells.\nIn higher mammals (e.g., mouse, cat, monkey, humans) the total number of neurons in\na column of unit area that reaches across all cortical layers is constant, irrespective of what\ncortical area this column is taken from (the sole exception is the primary visual cortex with\n2.5 times more cells). Thus, while the cortical sheet thickened by a factor of 2-3 in its\nevolution from mouse to human, the number of cells below 1 mm2 of neocortex remained\nfixed, in the neighborhood of around 100,000 cells below a square millimeter of cortex\n(Rockel, Hiorns and Powell, 1980).\nGiven an approximate density of 100,000 cells per mm3 in the primate, a synaptic density\nof 6 x 108 per mm3, a total surface area of about 100,000 mm2 for one hemisphere, and an\naverage thickness of about 2 mm, the average human cortex contains on the order of 20\nbillion neurons and 240 trillion synapses (2.4 x 1014), quite impressive numbers given the\ncurrent count of about 10n transistors in the memory and central processing unit (CPU) of\na modern parallel supercomputer.1\n4.2 Synaptic Transmission Is Stochastic\nOur view of the synapse is based upon the influential work of Katz and his collaborators\n(Katz, 1969). Working on the frog neuromuscular junction, they described the basic features\n1. Of course, the connectivity among the components in the CPU is three to four orders of magnitude lower than the connectivity\nof a cortical cell. On the other hand, the cycle time of such machines, in the low nanosecond range, compares very favorably with\nneuronal time constants in the millisecond range."}
{"text": "===== Page 4 =====\n88\nSYNAPTIC INPUT\nof synaptic transmission upon which every further model has been based (Salpeter, 1987).\nThis canonical view is outlined in Fig. 4.2.\nThe neurotransmitter is prepackaged in numerous small (30-40-nm-diameter) spheres or\nvesicles which reside in the presynaptic terminal. Invasion of the presynaptic axonal terminal\nby an action potential causes a inrush of calcium ions (via voltage-dependent calcium\nchannels in the presynaptic terminal). These calcium ions—through a complex chain of\nevents—cause one or more synaptic vesicle to fuse with the membrane at special sites. Here,\nthe vesicle now releases its \"quantum\" of transmitter into the narrow synaptic cleft between\nthe pre- and postsynaptic cell membranes (Fig. 4.2), a process termed exocytosis. Calcium is\nnecessary for synaptic transmission. Reducing the extracellular calcium concentration can\ndrastically lower the efficiency of the synapse (Mintz, Sabatini, and Regehr, 1995; Borst\nand Sakmann, 1996). This might have important functional consequences (Sec. 20.3).\nThe neurotransmitter rapidly diffuses across the 20-nm-wide synaptic cleft and binds\nto postsynaptic receptors, usually ionic channels. These receptors are then responsible for\nthe great diversity of postsynaptic events that cause the postsynaptic membrane potential\nto change. Each quantum of transmitter is released probabilistically and independently of\nthe others, and their postsynaptic effects add linearly. One reason for the success of Katz's\ntheory is that its predictions have been evaluated quantitatively (Kate, 1969). If n sites exist,\nand each such site independently releases either no or only a single packet or quantum of\ntransmitter with an associated amplitude q and with probability p, then the probability of\nk quanta being released at the entire synapse is given by the binominal distribution\n(4.1)\nAt the frog's neuromuscular junction, where the theory was developed and the critical\nexperiments were performed, the probability of release can be quite low. Yet because there\nexist on the order of 100 to 1000 release sites, overall synaptic transmission from the\nFig. 4.2 ELEMENTS OF SYNAPTIC TRANSMISSION \nStandard model of synaptic transmission at a\nchemical synapse, as elucidated by Katz for the neuromuscular junction. An action potential (far\nupper left) invades the presynaptic terminal (A) and—mediated by the resultant influx of calcium\nions—causes the vesicles to fuse with the membrane and release the neurotransmitter inside the\nvesicle into the synaptic cleft separating the presynaptic membrane from the postsynaptic one (B).\nThe neurotransmitter molecules rapidly diffuse across the cleft and bind to the receptors there. In this\nillustration, they cause the opening of Na+ selective channels (C), leading to an excitatory postsynaptic\npotential (far bottom left). At central synapses, a similar chain of events occurs, except that the entire\nprocess of synaptic transmission appears to be highly stochastic. Reprinted by permission from Jessell\nand Kandel (1993)."}
{"text": "===== Page 5 =====\n4.2 Synaptic Transmission Is Stochastic \n• \n89\nnerve onto the muscle is very reliable. In a somewhat more general version of this model,\nspontaneous release of vesicles, that is, the spontaneous postsynaptic potentials, can be\naccounted for by making the probability of release time dependent such that it is very\nsmall, but nonzero, in the absence of any input and much larger following a nerve impulse\narrival (Katz and Miledi, 1965; Barrett and Stevens, 1972).\nWhile many of the same principles elucidated by Katz also appear to apply to more\ncentral synapses, several crucial differences have emerged (Walmsley, Edwards, and Tracey,\n1987; Redman, 1990; Stevens, 1993). The most important one concerns the reliability\nand variability of synaptic transmission. At central synapses, it appears that each synaptic\nbouton contains only one or a few active release zones, rather than the hundreds found\nat the neuromuscular junction (Korn and Faber, 1991). In other words, upon arrival of an\naction potential at an individual anatomically identified synapse, typically none or only\none vesicle is released (one-vesicle hypothesis; Korn and Faber, 1993). Because only one\nvesicle is released at each synapse, the probability of release p looms large in whether or\nnot a presynaptic action potential results in a postsynaptic signal.\nWe know from in vitro studies that the probability of release at an individual synapse in\nvertebrates as well as in invertebrate systems can be highly variable and is usually in the\nrange of between 0.1 to 0.9 (Korn, Faber, and Triller, 1986; Bekkers and Stevens, 1989;\nRedman, 1990; Edwards, Konnerth, and Sakmann, 1990; Raastad, Storm, and Andersen,\n1992; Laurent and Sivaramakrishnan, 1992; Hessler, Shirke and Malinow, 1993; Gulyas\netal.,1994).\nThe molecular origin of the probabilistic release is not yet clear. However, it is likely to\nrelate to the fact that exocytosis depends on a very high local calcium concentration inside\nthe synaptic terminal and that this depends on the vagaries of the exact spatial relationship\nbetween calcium channels in the membrane and the location of the vesicle (Borst and\nSakmann, 1996; Bennett, 1997).\n4.2.1 Probability of Synaptic Release p\nFailure of synaptic transmission is vividly demonstrated in Fig. 4.3 (see also Bekkers,\nRicherson, and Stevens, 1990). Using a so-called minimal stimulation paradigm, only a\nsingle Schaffer collateral axon is stimulated. These axons typically make only a single\nexcitatory synapse onto CA1 pyramidal cells (Sorra and Harris, 1993). If the presynaptic -\npostsynaptic connection were secure, each presynaptic stimulus generated by the electrode\nshould evoke a postsynaptic event. In this case, the membrane potential in the pyramidal\ncells is clamped to its resting potential and the resultant excitatory postsynaptic current\n(EPSC) is recorded. The five records in the left column and the four top records in the\nright column in Fig. 4.3 show nine trials, of which only three lead to a postsynaptic event.\nThe bottom right record corresponds to the average postsynaptic event (averaged over the\nnine records).\nAs we will discuss in Chap. 13, the probability of synaptic release p is itself subject\nto change, depending on the recent history of the presynaptic terminal. Depending on the\nnumber and timing of presynaptic action potentials, p can either decrease, as in long-term\ndepression, or increase, as in long-term potentiation. p also varies from one synapse to\nthe next. This behavior might well be different from one synaptic type to the next (e.g.,\nthalamo-cortical versus intracortical synapses; Stratford et al., 1996). Understanding the\ndynamics of p and its relationship to synaptic plasticity has been a hot research topic of the\nlast few years."}
{"text": "===== Page 6 =====\n90 \n• \nSYNAPT1C INPUT\nFig. 4.3 SYNAPTIC TRANSMISSION Is VERY UNRELIABLE \nSynaptic transmission among central\nneurons can be highly unreliable and should be thought of as a binary event with a probability of\nsuccess p, which can be as low as 0.1. This is exemplified by this recording of the excitatory inward\ncurrent caused by a single synapse, made by a Schaffer collateral ax on onto a CA1 pyramidal cell in\nthe mammalian hippocampal slice. The five records on the left and the top four records on the right\n(30 msec long) show the postsynaptic current in response to nine stimulations of the presynaptic axon\nusing an electrode. Only in three trials does the presynaptic spike lead to the release of a vesicle filled\nwith neurotransmitter that causes a postsynaptic current to flow. The amplitude of this postsynaptic\nevent, if it occurs, is also highly variable. The average postsynaptic EPSC (computed over these\nnine trials) is indicated on the lower right. It is not clear how the brain deals with the unreliability\npresent at its elementary computational units. Unpublished data from Y. Wang and C. Stevens, printed\nwith permission.\nIn the example of Fig. 4.3, the probability of release is about 0.3. This is quite remarkable,\nsince it implies that the connecting elements among neurons are binary but highly unreliable\ncircuit elements, transmitting only one out of three events. Imagine a transistor that only\nconducts, that is, switches, one out of three times that a charge is dumped onto its gate.\nSynapses made onto pyramidal cells in slices from sensory and motor cortex appear to\nbehave in an identical fashion (Smetters and Nelson, 1993; Thompson, Deuchars, and West,\n1993a,b). An important caveat is that these results were not obtained under physiological\nconditions in intact animals but in slices or other isolated preparations. It is possible that in\na behaving animal, various neuromodulators can significantly boost or reduce p."}
{"text": "===== Page 7 =====\n4.2 Synaptic Transmission Is Stochastic \n• \n91\nThe lack of reliability of synapses should be compared to a reliability of transmission\namong electronic circuits, which is many, many orders of magnitude higher.2 In principle, the\nnervous system could compensate for such unreliable components by exploiting redundancy\n(Moore and Shannon, 1956; von Neumann, 1956), that is, by using many parallel synapses.\nYet, it is common for a cortical axon to make only one or two synaptic contacts onto its target\ncell (Gulyas et al., 1993; Sorra and Harris, 1993). The brain must have found a different\nway in which it deals with such unreliable synaptic components.\nThe reliability of synaptic transmission in the central nervous system is further compro-\nmised by the fact that the variability in the amplitude of the postsynaptic response to the\nrelease of a single packet of neurotransmitters is much larger than at peripheral synapses.\nThus, even if a packet of neurotransmitters is released, a number of factors, such as the\nvariation in the size of the vesicle and the number of postsynaptic receptors, cause the\npostsynaptic response to this presynaptic event to vary from one trial to the next (see\nthe three successful instances of transmission in Fig. 4.3; Bekkers, Richerson, and Stevens,\n1990; Larkman, Stratford, and Jack, 1991). In the study by Mason, Nicoll, and Stratford\n(1991), the variance in the size of the EPSP—evoked in one rat neocortical pyramidal cell\nby electrically activating a nearby pyramidal cell —is as large as its mean (Fig. 4.4), with\nthe largest EPSP (2.08 mV) about 40 times larger than the smallest (0.05 mV). In these\nexperiments, the number of synapses between the pair of cells recorded from is not known\n(see also Stratford et al., 1996).\nWe defer a discussion of the possible functional role of the stochastic nature of synaptic\ntransmission to Sec. 13.5.5.\n4.2.2 What Is the Synaptic Weight?\nLet us briefly address the thorny issue of what exactly constitutes synaptic weight or synaptic\nstrength. In the neural network literature, this is a single scalar, usually labeled wtj or Ttj\n(see Sec. 14.4). However, synapses are very complicated devices, which are characterized\nby numerous parameters. Traditionally—as expressed by Eq. 4.1—biophysicists have used\nthree scalar variables to describe a synapse: the number of quantal release sites n, the\nprobability of synaptic release per site p, some measure of the postsynaptic effect of the\nsynapse q. Depending on the experiment, q can be the peak conductance, the maximal\nsynaptic current at some potential, or the peak EPSP. Of course, in real life, n is drawn from\nsome probability distribution (usually binominal or Poisson), p depends on the previous\nspiking history, and q itself is a function of time (such as the time-dependent conductance\nchange of Eq. 1.21).\nOne possible relationship between these state-dependent functions and the synaptic\nweight is the expected or mean postsynaptic action, computed by averaging over a Poisson\ndistributed presynaptic spike distribution with\n(4.2)\nWe shall return to this in Chap. 13. However, one should never forget that the averaging\nassumptions used to compute this weight might not be relevant to the brain under its normal\noperating conditions (for instance, since the input is oscillatory or tends to fire in bursts or\nbecause its stochastic nature is critical).\n2. Indeed, in digital CMOS technology, where the signal gain at an inverter, the elementary computational unit, is in the\nneighborhood of 15 to 20, the thermal fluctuations—on the order of 0.65 mV at room temperatures—are so much less than the\nswitching voltage, that the probability of switching is about 1 — 10~14. Given that each inverter restores the input signal, the\neffect of thermal noise does not accumulate along a cascade of inverters."}
{"text": "===== Page 8 =====\n92 . \nSYNAPTIC INPUT\nFig. 4.4 POSTSYNAPTIC AMPLITUDE Is HIGHLY\nVARIABLE Fluctuations in the amplitude of EP-\nSPs observed in a pyramidal cell by evoking an\naction potential in a nearby pyramidal cell. Both\npairs of neurons are located in layer 2/3 of brain\nslices taken from rat visual cortex. (A) Four indi-\nvidual sweeps from the same synaptic connection.\nThe EPSP amplitude over the entire population of\ncell pairs is 0.55 ± 0.49 mV. (B) Average of 2008\nsuch sweeps, together with the presynaptic record,\nshowing that the EPSP in the postsynaptic cell is\ncaused by the presynaptic spike. Due to technical\nreasons, no events less than 0.03 mV—and, in par-\nticular, no failures as in Fig. 4.3—can be recorded.\nReprinted by permission from Mason, Nicoll, and\nStratford (1991).\nIn summary, because of the highly probabilistic nature of the release of neurotransmitter,\ncompounded by the variability in the size of the postsynaptic responses to this neurotrans-\nmitter, a basic and unavoidable feature of interneuronal communications using chemical\nsynapses is their lack of reliability and consistency. As pointed out by Stevens (1994), it\nis crucial that any theory of the brain account for this fundamental property of neuronal\nhardware. We will return to the theme of stochastic computations in the brain in Chap. 15.\n4.3 Neurotransmitters\nThe plethora of time courses, amplitudes, and types of actions seen in synaptic transmission\nis the result of the interplay between different neurotransmitters, stored in vesicles at\nthe presynaptic terminal, and different synaptic receptors, inserted in the postsynaptic\nmembrane.\nThere exist three major chemical classes of neurotransmitters, amino acids, biogenic\namines, and neuropeptides (Table 4.1). Fast synaptic transmission in the central nervous"}
{"text": "===== Page 9 =====\n4.3 Neurotransmitters \n• \n93\nsystem of vertebrates is mainly mediated by amino acids, the major excitatory neuro-\ntransmitters being glutamate and aspartate, and the major inhibitory neurotransmitters\nbeing y-amino-butyric acid (GAB A) and glycine. The onset of the associated excitatory\nor inhibitory postsynaptic currents (EPSCs or IPSCs), forming the principal substrate of\nneural computation, is rapid (< 1 msec) and their durations are relatively short (<20 msec).\nAll these neurotransmitters also have slow effects.\nThe list of biogenic amines that can modulate the response of the cell to synaptic\ninput includes acetylcholine (ACh), norepinephrine (also referred to as nor adrenaline),\ndopamine, serotonin and histamine. Usually, their actions have a much slower onset than the\naction of amino acids and may persist hundreds of milliseconds to seconds. However, some\nneuroactive substances, such as acetylcholine, cause fast (e.g., nicotinic-receptor-mediated\nfast synaptic transmission at the vertebrate neuromuscular junction) as well as slow (e.g.,\nmuscarinic-receptor-mediated changes in potassium conductances) effects, depending on\nwhich receptor is present in the postsynaptic membrane.\nA long and ever-growing list ofpeptides, that is, short chains of amino acids, modulates\nthe response of neurons over very long time scales (that is, minutes). Hormones, that is\nsubstances that are transported via the bloodstream (in vertebrates) or the hemolymph (in\ninvertebrates), also affect neuronal responses, with the distinction between the actions of\nneuropeptides and hormones being a gradual one. These neuroactive substances, frequently\nalso termed neuromodulators, are usually colocalizedwith conventional fast neurotransmit-\nters in individual neurons. Indeed, sometimes two or more neuropeptides can be colocalized\nwithin a conventional terminal in different vesicles (Kupfermann, 1991). Release can be\ndifferential, in the sense that the release of the vesicles containing the modulators requires a\nhigher presynaptic firing rate than release of the vesicles harboring the fast neurotransmitter.\nDozens of peptides and hormones have been identified in small invertebrate ganglia that\nare accessible to neurochemical methods (Marder, Christie, and Kilman, 1995; see also\nFig. 20.5), and it is unlikely that the situation in the cortex will be much simpler.\nThe concentration of these substances can be thought of as the closest equivalent to a\nglobal variable in the brain. While a global variable within a computer program is defined\nfor the entire program (rather than for any particular procedure), similarly, release of a long-\nlasting neuromodulatory substance will affect all neurons within a given distance from the\nsite of release. As we will see in the next section, neurons possess receptors for a variety\nof different neurotransmitters and modulators, giving rise to a very complex and possibly\nredundant system for modulating the electrical responses of neurons over a great variety"}
{"text": "{\n  \"table_name\": \"TABLE 4.1\",\n  \"description\": \"Principal Neurotransmitters\",\n  \"columns\": [\"Amino Acids\", \"Biogenic Amines\", \"Neuropeptides\"],\n  \"rows\": [\n    {\"Amino Acids\": \"Glutamate\", \"Biogenic Amines\": \"ACh\", \"Neuropeptides\": \"Substance P\"},\n    {\"Amino Acids\": \"Aspartate\", \"Biogenic Amines\": \"Dopamine\", \"Neuropeptides\": \"Somatostatin\"},\n    {\"Amino Acids\": \"GABA\", \"Biogenic Amines\": \"Noradrenaline\", \"Neuropeptides\": \"Proctolin\"},\n    {\"Amino Acids\": \"Glycine\", \"Biogenic Amines\": \"Serotonin\", \"Neuropeptides\": \"Neurotensin\"},\n    {\"Amino Acids\": null, \"Biogenic Amines\": \"Histamine\", \"Neuropeptides\": \"Luteinizing-hormone-releasing hormone (LHRH)\"}\n  ]\n} Three main chemical classes of neurotransmitters found in the nervous systems of all animals. Amino acids are usually involved\nin fast (glutamate/aspartate) excitatory or inhibitory (GABA/glycine) synaptic traffic. Many substances, such as glutamate or\nacetylcholine (ACh), have fast and transient as well as slow and long-lasting effects, depending on the postsynaptic receptor. The\nvery large number of neurnpeptidef;, of which only a handful are listed, act on the time scale of seconds to minutes."}
{"text": "===== Page 10 =====\n94\nSYNAPTIC INPUT\nof different spatial and temporal scales (for reviews see Nicoll, 1988; Kupfermann, 1991;\nHille, 1992; Bourne and Nicoll, 1993; McCormick, 1998). This implies that cells can be\n\"addressed\" using a unique bar-code-like combination of neuromodulators and receptors\n(see Sec. 20.5).\n4.4 Synaptic Receptors\nPostsynaptic receptors come in two different flavors (Fig. 4.5). lonotropic receptors are\ndirectly coupled to ionic channels, which open and permit certain types of ions to cross\nthe postsynaptic membrane. Such channels, also called ligand-gated channels, include\nthe nicotinic acetylcholine receptor in the peripheral nervous system of vertebrates and\nmany invertebrates, the GAB AA receptor complex, and the ubiquitous glutamate W-methyl-\nD-aspartate (NMDA) and non-NMDA channel complexes (Table 4.2). Given the close\npoint-to-point connection between the presynaptic release zone and the postsynaptic re-\nceptor (Fig. 4.1) and the direct coupling between the receptor and the channel (Figs. 4.2\nA) Channel Using Intrinsic Sensor\nB) Channel Using Remote Sensor\nFig. 4.5 IONOTROPIC AND METABOTROPic SYNAPTIC ACTION \n(A) Fast excitatory and inhibitory\ninput mediated by a tightly linked ionotropic receptor-channel complex. Binding of the neurotrans-\nmitter leads to a rapid opening of the associated ionic channel. (B) In the case of a metabotropic\nreceptor, binding of the neurotransmitter leads to activation of a second messenger substance (such as\nCa2+ ions). This messenger molecule, possibly after diffusing to its site of action, binds to a particular\nionic channel and will modulate its properties. While the action of the ionotropic receptor is point\nto point and rapid, both the onset and the duration of the metabotropic mediated synaptic input are\nusually slow and its action can extend over larger distances. Both receptor types can be colocalized.\nReprinted by permission from Hille (1992)."}
{"text": "===== Page 11 =====\n4.4 Synaptic Receptors\n95\n{\n  \"table_name\": \"TABLE 4.2\",\n  \"description\": \"Synaptic Receptor Types\",\n  \"columns\": [\"Neurotransmitter\", \"Receptor\", \"Esyn (mV)\", \"Type\", \"Comments\"],\n  \"rows\": [\n    {\"Neurotransmitter\": \"Glutamate\", \"Receptor\": \"Non-NMDA\", \"Esyn (mV)\": 0, \"Type\": \"I\", \"Comments\": \"Very fast\"},\n    {\"Neurotransmitter\": \"Glutamate\", \"Receptor\": \"NMDA\", \"Esyn (mV)\": 0, \"Type\": \"I\", \"Comments\": \"Voltage dependent\"},\n    {\"Neurotransmitter\": \"GABA\", \"Receptor\": \"GABA_A\", \"Esyn (mV)\": -70, \"Type\": \"I\", \"Comments\": \"Fast inhibition\"},\n    {\"Neurotransmitter\": \"GABA\", \"Receptor\": \"GABA_B\", \"Esyn (mV)\": -100, \"Type\": \"M\", \"Comments\": \"Slow inhibition\"},\n    {\"Neurotransmitter\": \"ACh\", \"Receptor\": \"Nicotinic\", \"Esyn (mV)\": -5, \"Type\": \"I\", \"Comments\": \"Neuromuscular junction\"},\n    {\"Neurotransmitter\": \"ACh\", \"Receptor\": \"Muscarinic\", \"Esyn (mV)\": -90, \"Type\": \"M\", \"Comments\": \"Decreases K conductance\"},\n    {\"Neurotransmitter\": \"Noradrenaline\", \"Receptor\": \"α1\", \"Esyn (mV)\": -100, \"Type\": \"M\", \"Comments\": \"Increases K conductance\"},\n    {\"Neurotransmitter\": \"Noradrenaline\", \"Receptor\": \"β1\", \"Esyn (mV)\": -100, \"Type\": \"M\", \"Comments\": \"Decreases K conductance\"}\n  ]\n}"}
{"text": "List of major types of synaptic receptors and the associated neurotransmitters. The four top listings are the dominant transmitters\nused for fast communication in the vertebrate central nervous system. The synaptic reversal potential Esyn is specified in millivolts\nabsolute potential. The type corresponds to either ionotropic (T) or metabotropic (M) receptors.\nand 4.5A), the action of ionotropic receptors is rapid and transient. They implement the\ncomputations underlying rapid perception and motor control.\nBinding of neurotransmitter to metabotropic receptors, on the other hand, leads to the\nactivation of some intracellular molecules, termed second messengers, which in turn may\ninduce a conformational change in some ionic channel and therefore in its kinetic behavior.\nIt has been estimated that the mammalian genome has devoted about 1000 of its 105\ngenes to these receptors. They could encode for a truly staggering number of different\nmetabotropic receptors. At the molecular level, all of these receptors span the cell membrane\nin a snakelike fashion, crossing the bilipid membrane seven times (Chap. 8). These receptors\nconnect with so-called G proteins just inside the cell membrane. G proteins, named\nbecause they bind guanosine triphosphate (GTP), include at least 20 different proteins\nand are among the most versatile nanomachines in biology (Clapham, 1996). Acting on\nperhaps 100 different receptors throughout the brain, the muscles, the glands, and other\norgans, they recognize photons (rhodopsin) and odor molecules as well as conventional\nneurotransmitters, such as glutamate, GABA, and ACh (Ross, 1989; Hille, 1992). Activation\nof these receptors is linked via a cascade of biochemical reactions to the ionic channel that is\nmodulated (Fig. 4.5B). Functionally, these multiple intracellular steps can greatly amplify\nthe signal. Thus, a single occupied receptor might activate many G proteins, each one\nof which can, in turn, activate many other proteins, and so on. Second messengers can\nact to increase or to decrease the postsynaptic membrane conductance, in particular for\npotassium.\nAn important distinction between ionotropic and metabotropic receptors is their time\nscale. While members of the former class act rapidly, terminating within a very small fraction\nof a second, the speed of the latter class is limited by diffusion. Biochemical reactions can\nhappen nearly instantaneously at the neuronal time scale. However, if a synaptic input to a\nmetabotropic receptor induces the release of some messenger, such as calcium ions, which\nhave to diffuse to the cell body in order to \"do their thing,\" the time scale is extended to\nseconds or longer. Section 9.3 details the involvement of one such metabotropic receptor\ntype in the control of firing frequency adaptation.\nIt is difficult to overemphasize the importance of modulatory effects involving complex\nintracellular biochemical pathways. The sound of stealthy footsteps at night can set our heart\nto pound, sweat to be released, and all our senses to be at a maximum level of alertness,"}
{"text": "===== Page 12 =====\n96 • \nSYNAPTIC INPUT\nall actions that are caused by second messengers. They underlie the difference in sleep-\nwake behavior, in affective moods, and in arousal, and they mediate the induction of long-\nterm memories. It is difficult to conceptualize what this amazing adaptability of neuronal\nhardware implies in terms of the dominant Turing machine paradigm of computation.\nBecause of the indirect coupling between receptor and effector—in particular if the\nsecond messengers involved have to diffuse within the postsynaptic cell—the onset of their\neffects is usually slow but long-lasting and is not restricted to a single site. They modulate\nthe properties of groups of neurons over long time scales, that is, hundreds of milliseconds\nto seconds, minutes, and longer. Yet, this view does not do justice to the overlapping time\nscales of ionotropic and metabotropic receptors. If receptor and effector are spatially close\nto each other, both can act at the same time scale. For instance, a ligand-gated event such\nas a glutamate-induced NMDA depolarization lasts for 50-100 msec, approximately the\nsame duration as a second-messenger-mediated hyperpolarization caused by activation of\nGABAg receptors.\n4.5 Synaptic Input as Conductance Change\nActivation of an ionotropic receptor permits the associated ionic channel in the postsynaptic\nchannel to change its configuration, thereby allowing the passage of ions across the\nmembrane. Given the small diameter of the open channel, about 3-7 A, channels are highly\nselective for certain ions that diffuse through the open channel down their electrochemical\ngradient, leading to a change in postsynaptic potential. Chapter 8 will treat ionic channels\nin more detail. At this point, all we need to know is that individual channels act as binary\nelements, having zero conductance in their closed state and a fixed, nonzero conductance\nvalue in their open state (ranging between 5 and 50 pS, depending on the channel type).3\nThe graded nature of the observed postsynaptic conductance change comes from the\nsimultaneous openings of tens to hundreds of these binary elements. How long these\nchannels stay open—the determinant of the duration of synaptic input—depends on two\nfactors: (1) the presence of active uptake systems in the synaptic cleft that can remove the\nneurotransmitter and degrade or recycle it, and (2) the internal kinetics of the channel.\n4.5.1 Synaptic Reversal Potential in Series with an Increase in Conductance\nGiven the distribution of ions in the intracellular and extracellular cytoplasm and the\nspecificity of the ionic channels, each type of synaptic input has an associated Nernst\npotential, also referred to as the ionic battery, or ionic reversal potential, £syn. The origin\nof this potential—crucial to neuronal excitability— relates to the equilibrium established\nbetween the concentration gradient of the different ions across the neuronal membrane\nand the electrical force opposing this gradient. Using the Boltzmann equation of classical\nstatistical mechanics, the value of the synaptic potential at body temperature for a channel\npermeable to a single ionic species is given by the Nernst equation\n(43)\nwhere E&yn is measured in millivolts, z is the valence of the ions involved (positive\nfor Ca2+, Na+ and K+ ions, negative for Cl~ ions) and [S]0 is the extra- and [5],- the\n3. We here neglect the fact that many channels have more than one open state; this is usually described by saying that a\nchannel has conductance sublevels."}
{"text": "===== Page 13 =====\n4.5 Synaptic Input as Conductance Change \n• \n97\nintracellular concentration of the ionic species considered. The meaning of Esyn becomes\nclearer if one considers that in the absence of any other ionic species, the membrane potential\nacross the open channel would stabilize at £\"syn. At this potential, the ions on both sides of the\nmembrane are in a dynamic equilibrium, with no net flux of ions through the channel. Many\nchannels are permeable to a mixture of two or more ionic species, requiring a more complex\nexpression, such as the Goldman-Hodgkin-Katz (GHK) voltage equation (Goldman, 1943;\nHodgkin and Katz, 1949; Hille, 1992).\nAs we saw in the first chapter, the opening of a synaptic channel corresponds, from\nan electrical point of view, to an increase in the membrane conductance in series with\nthe ionic reversal battery £syn (Fig. 1.7A). The key insight, namely that the binding of\nneurotransmitters causes an increase in the conductance of the membrane, came from\nresearch carried out by Fatt, Katz, and Eccles in the 1950s as they were working on the\nneuromuscular junction (Eccles, 1964,1990).\nThe lumped effect of many ionic channels opening in response to the binding of the\ntransmitter molecules to the receptor is treated as a time-varying change in the membrane\nconductance gsyn(0 in series with the synaptic reversal potential Esyn. This conductance\nchange gives rise to a transient ionic current through the open channels, transporting charge\nacross the membrane. How does the synaptic current relate to the conductance of the channel\nand to the membrane potential?\nIn general, this relationship is a complex one. One such description is referred to as\nthe Goldman-Hodgkin-Katz current equation (Goldman, 1943; Hodgkin and Katz, 1949;\nHille, 1992; see Eq. 9.1). Assuming that ions cross the membrane without interacting with\neach other and that the potential drops linearly across the membrane, the GHK current\nequation expresses a nonlinear dependency between the ionic current and the membrane\nvoltage. This nonlinearity, referred to as rectification since current passes more easily in\none direction than in the other, is caused by the nonhomogeneous distribution of ions across\nthe membrane and increases with increasing concentration gradient.\nHowever, in general, a linear relationship is observed between synaptic current and\nthe membrane potential (with the important exception of the NMDA receptor discussed\nbelow). In fact, once the reversal potential has been accounted for, Ohm's law appears as a\nsatisfactory first-order description of the synaptic current, and this is the one we adopted at\nthe beginning of this book (e.g., Eq. 1.18),\n(4-4)\nIt is important to realize that the synaptic current depends on Vm. Only under certain\nconditions can a synaptic input be approximated as a constant current source (Fig. 1.7B).\nThis seemingly trivial observation has a number of important consequences, outlined\nfurther below as well as in Chaps. 5 and 18. Equation 4.4 is a purely phenomenological\ndescription of the current as a function of the membrane potential and is not derived\nfrom first principles. Experimentally, linearity is not necessarily assured and needs to be\nexperimentally confirmed (as, for instance, in Hestrin et al., 1990a, for the fast, non-NMDA\ninput onto hippocampal pyramidal cells).\nA synapse is excitatory if the synaptic current /syn depolarizes thepostsynaptic membrane\nby giving rise to an EPSP. Activation of an inhibitory synapse, on the other hand, can either\nclamp the potential to remain around Vrest or cause an outward current to flow, thereby giving\nrise to an IPSP that hyperpolarizes the cell. In the language of the electrophysiologist, an\nexcitatory synapse injects current into the cell; such inward currents are represented, by\nconvention, as negative currents. For synapses with gsyn > 0, an excitatory synapse has a"}
{"text": "===== Page 14 =====\n98 \n• \nSYNAPTIC INPUT\nreversal potential more positive than the resting potential, while an inhibitory synapse has\na reversal potential close to or negative to the resting potential.4\nFigure 4.6 illustrates the typical sequence of a fast EPSP followed by a fast and a slow\nIPSP seen in a cortical pyramidal cell upon electrical stimulation of the fibers projecting\ninto cortex. The hyperpolarizing potentials are caused by inhibition acting on two separate\ninhibitory receptors (GABA^ and GABAg) discussed further below.\n4.5.2 Conductance Decreasing Synapses\nIn general, the release of neurotransmitters at an ionotropic synapse increases the postsy-\nnaptic membrane conductance, that is, gsyn (t) > 0. At metabotropic receptors, this need not\nbe the case. A well-known example is the loss of firing rate adaptation seen in hippocampal\nneurons following the stimulation of fibers releasing noradrenaline. This input leads to\nthe reduction of a calcium-dependent potassium conductance (Fig. 9.9). Another example\nis the change occurring in the responsivity of thalamic relay cells when mammals wake\nFig. \n4.6 SYNAPTIC POTENTIALS\nGENERATED IN A CORTICAL CELL\n(A) Electrical stimulation of axons\nascending into the cerebral cortex\ngenerates a very rapid EPSP, termi-\nnating within 10 msec, followed by\ntwo IPSPs, an early fast one and a late\nslow one. This sequence of events is\ncommon to most cortical cells upon\nstimulation of their extracortical af-\nferents (Douglas, Martin, and Whit-\nteridge, 1991; Tseng and Haberly,\n1988). (B) and (D) The inhibition\nis caused by recruitment of cortical\ninterneurons, which release GABA\nacting on two distinct synaptic recep-\ntors. The early IPSP is due to activa-\ntion of the ionotropic GABA/i recep-\ntors, causing an increase in a chlo-\nride membrane conductance, while\nthe late IPSP is due to GABAB re-\nceptor activation. This metabotropic\nsynapse causes a hyperpolarizing\npotassium conductance to open. (C)\nApplying the substance phaclofen to\nthe slice blocks the GABAs recep-\ntors while not affecting the GABA^-\nmediated IPSP. The constant current\npulse at the beginning of the trace\n(see B) assesses the extent to which\nthe input resistance changes during\napplication of phaclofen (it does not).\nReprinted by permission from Mc-\nCormick(1998).\n4. However, under certain circumstances inhibitory input can act functionally as if it were exciting a cell and vice versa\n(Lytton and Sejnowski, 1991)."}
{"text": "===== Page 15 =====\n4.6 Excitatory NMDA and Non-NMDA Synaptic Input \n• \n99\nup (that is, the switch from slow-wave sleep to awake, attentive state). Activation of a\nvariety of different metabotropic receptors (e.g., muscarinic ACh receptors, receptors for\nadrenaline and histamine; McCormick, 1992) causes a 10-mV or greater increase in the\nresting potential. This sustained depolarization, triggered by the reduction of a potassium\n\"leak\" conductance (that is, g < 0), is sufficient to switch the cell from a bursting into a\nsingle action potential firing mode with important consequences for information processing\n(Steriade and McCarley, 1990; McCormick, 1992).\nIn summary, the postsynaptic effect of a chemical synapse involves a transient change\ngsynCO in the membrane conductance in series with a synaptic battery Esyn. Before we\ndiscuss the biophysical consequences of this further, let us summarize the key properties of\nthe most common forms of excitatory and inhibitory synaptic input.\n4.6 Excitatory NMDA and Non-NMDA Synaptic Input\nThe predominant fast, excitatory neurotransmitter of the vertebrate central nervous system is\nthe amino acid glutamate, activating synaptic receptor channels on nearly every nerve cell as\nwell as on many of the supporting glia cells. In the peripheral nervous system of vertebrates, \nglutamate synapses are nearly unknown. Here the dominant fast neurotransmitter is ACh.\nInvertebrates use ACh as well as glutamate for fast transmission of information throughout\ntheir nervous systems.\nApplying glutamate or its close relative aspartate onto central neurons causes a fast\ndepolarizing event, providing the substrate for the fast excitatory traffic in the central\nnervous system. However, receptors sensitive to glutamate turn out to be a diverse lot and\ntheir various subtypes are now being expressed using molecular techniques (Sommer and\nSeeburg, 1992; Westbrook, 1994). About two dozen glutamate receptor subunits have been\ncloned from the rat brain, some of which are ionotropic and some metabotropic receptors\n(Dingledine and Bennett, 1995). These are expressed in different locations throughout\nthe brain and differ in their kinetics, degree of voltage dependency, and so on. Thus, quite\ndifferent from commercial analog and digital CMOS highly integrated silicon circuits, where\nthe degree of specialization of transistors and circuit types across the chip is minimal, each\ncircuit in an animal may use a slightly different synaptic subtype (possibly also depending\non the history of the animal as well as on its developmental stage).\nThe study of glutamate receptors and their associated synaptic properties is currently in\na very active phase. We summarize here the pertinent facts of this unfolding story (Ascher\nand Nowak, 1988; Thomson, Girdlestone, and West, 1988; Bekkers and Stevens, 1989;\nHestrin et al, 1990a,b; Mason, Nicoll, and Stratford, 1991; Williams and Johnston, 1991;\nStern, Edwards, and Sakmann, 1992; Jonas and Spruston, 1994; Destexhe, Mainen, and\nSejnowski, 1994a).\nThe most important distinction among excitatory glutamate synapses is based on apply-\ning various pharmacological substances to the receptor and measuring their action. One set\nof these agents, called generically agonists, activates one subclass of glutamate receptors,\nwith different chemical substances binding to different receptors, like keys fitting into locks.\nThe different receptors are usually identified by the name of their agonists.\nIn the case of the glutamate-sensitive receptor, two major functional subclasses exist\n(as mentioned above, many more molecular subtypes have been cloned, but their specific\nfunctions are not yet known). One receptor binds kainate, quisqualate, and a-amino-\n3-hydroxy-5-methyl-4-isoxalone propionic acid (AMPA). The other class of glutamate"}
{"text": "===== Page 16 =====\n100 \n• \nSYNAPTIC INPUT\nreceptors can be selectively activated by NMDA. In other words, dumping NMDA onto\nthe synapse causes it to bind to the NMDA receptor and the associated synapse to open.\nIt is important to realize that NMDA, AMPA, and the other agonists do not exist within\nthe nervous system but are pharmacological substances used by biophysicists to identify\nthe receptors. In the brain, all glutamate synapses, regardless of their receptor types, are\nactivated by the presynaptic release of glutamate. Because we are not interested in the\nvarious subtypes and the proper nomenclature is still being debated, we simply refer\nthroughout this book to NMDA and non-NMDA synapses.\nOther pharmacological agents, so-called antagonists, very specifically block the various\nsubclasses of glutamate receptors, allowing us to isolate the contributions of the non-NMDA\nand the NMDA receptors to EPSPs. For instance, the substance 6-cyano-7-nitroquinoxaline-\n2,3-dione (CNQX) blocks the fast, non-NMDA synapse without interfering with the NMDA\nsynapse, while DL-2-amino-5-phosphono-valeric acid (APV) blocks the slower NMDA\ncomponent without blocking the non-NMDA component. Again, antagonists do not occur\nnaturally in the brain but are used as a tool to investigate synaptic transmission.\nOnce glutamate binds to the AMPA receptor the associated channel opens, allowing\nmonovalent cations, mainly Na+ and K+, to flow across the membrane. The synaptic\nreversal potential of this synapse is about 0 mV (absolute potential). At a non-NMDA\nreceptor, the postsynaptic channels activate very rapidly. The synaptic current peaks within\na few hundred microseconds, with an exponential decay whose time constant varies between\n0.5 and 3 msec (Hestrin, Sah, and Nicoll, 1990b; Hestrin, 1992; Trussel, Zhang, and Raman,\n1993; Jonas, Major, and Sakmann, 1993).\nThe time course of the synaptic conductance increase can in general be described by an\nnth-state Markov process with n(n — 1) time constants (Destexhe, Mainen, and Sejnowski,\n1994a; Johnston and Wu, 1995). Although efficient implementations for these schemes are\nknown (Destexhe, Mainen and Sejnowksi, 1994b), in general simplified expressions are\nused, the most common being the a function of Eq. 1.21 (Rail, 1967; Jack, Noble, and\nTsien, 1975),\n(4-5)\nThe constant is chosen such that gsyn(tpeak) — gpeak> that is, const = gpeak^Apeak- The\nfunction g(t) decays to 1% of its peak value at about t — 7.64fpeak. Using fpeak — 0.5 msec\nwith a conductance change of between 0.25 and 1 nS reproduces well the observed time\ncourse of the non-NMDA input (Fig. 4.7).\nDifferent from the non-NMDA receptor, with whom it usually appears to be colocalized,\nthe amplitude of the conductance change associated with the NMDA synapse depends on\nthe membrane potential. Figures 4.8A and 4.9A illustrate the nonlinear instantaneous /-\nV relationship. What is apparent is the negative slope-conductance region between —70\nand —40 mV. This negative slope is conferred upon the NMDA current by the action of\nMg2+ ions, which are naturally present in the extracellular environment. If the postsynaptic\npotential is at rest and glutamate is bound to the NMDA receptor, the channel opens but is\nphysically obstructed by Mg2+ ions. As the membrane is depolarized, the Mg2+ ions move\nout, and the channel becomes permeable for a mixture of Na+, K+ and a small number of\nCa2+ ions. The fraction of the current carried by Ca2+ ions through the NMDA channel\nis about 7% (at negative potentials), five times the fractional calcium current through the\nvoltage-independent glutamate channel (Schneggenburger et al., 1993). If the Mg2+ ions\nare removed from the saline solution bathing the neurons in an experimental slice setup,\nthe synapse looses its nonlinearity and becomes purely ohmic (solid line in Fig. 4.8A).\nThe synapse has a reversal potential close to zero. Another difference compared to the"}
{"text": "===== Page 17 =====\n4.6 Excitatory NMDA and Non-NMDA Synaptic Input \n• \n101\nFig. 4.7 TIME COURSE OF EXCITATORY SYNAPTIC INPUT Time course of the non-NMDA and\nNMDA excitatory inputs recorded from cells in hippocampal slices. (A) The excitatory postsynaptic\ncurrent, obtained by clamping the cell to —40 mV and applying glutamate, is shown in the absence\n(Cont) and presence of the non-NMDA channel blocker CNQX. The remaining current is mediated via\nNMDA receptors. By convention, depolarizing inward currents are represented as negative currents.\n(B) Closeup of the time course of the NMDA current (labeled CNQX) and the non-NMDA current,\nobtained by clamping the EPSC to -100 mV and thereby blocking all NMDA channels. The NMDA\ncomponent continues to rise as the non-NMDA component has started to decay already. (C) A\nsemilogarithmic plot of the NMDA-mediated current demonstrates that the decay of the NMDA\ncurrent is not well fitted by a single exponential (at 31 ° C). (D) The decay of the non-NMDA current\n(from the curve labeled \"—100 mV\" in B) is well fitted by a single exponential with a time constant\nof 7.2 msec (at 24° C). Reprinted by permission from Hestrin, Sah, and Nicoll (1990b)\nnon-NMDA input is the much slower time course of the NMDA-mediated conductance\nchange due to the intrinsic kinetics of the receptor. Both rise and decay times are at least\na factor of 10 times slower, with rise times on the order of 10 msec. Indeed, maximum\nNMDA receptor activation occurs when the non-NMDA conductance change has almost\nsubsided (Fig. 4.7C).\nThe NMDA conductance can be derived from a model in which the binding rate constant\nof Mg2+ varies as an exponential function of voltage (Ascher and Nowak, 1988; Jahr and\nStevens, 1990). Modeling the time dependency by the difference between two exponentials,\nwe have\n(4.6)\nwith n — 80 and T2 = 0.67 msec, i] — 0.33/mM, y = 0.06/mV and gn «a 0.2 to\n0.4 nS (at 35° C). The physiological concentration of Mg2+ ions is around 1 mM.\nThe synaptic current is obtained by multiplying this conductance by the membrane\npotential (since the associated Esyn ~ 0). Figure 4.8B shows the time course of an\nexperimentally measured NMDA current as compared with this model.5 Better fits can\nbe obtained by using more complex models (Clements and Westbrook, 1991).\n5. The temperature at which electrophysiological experiments occur has a significant impact on the dynamics of neuronal\nprocesses. Experiments on brain slices and cultured neurons are generally carried out around 22-25° C, significantly below the"}
{"text": "===== Page 18 =====\n102\nSYNAPTIC INPUT\nFig. 4.8 VOLTAGE DEPENDENCY OF THE NMDA CURRENT Current-voltage relationship asso-\nciated with the NMDA receptor. Because of the action of Mg2+ ions in blocking the underlying\nchannel, the associated postsynaptic conductance increase is voltage dependent, different from other\nfast sy naptic inputs. (A) Current-voltage relationship of the NMDA current in the absence and presence\nof 0.5 mM magnesium. While the current behaves relatively ohmic in the absence of magnesium,\nunder physiological concentrations of Mg2+ a strong voltage dependency is revealed. Reprinted\nby permission from Nowak et al. (1984). (B) Experimentally recorded normalized NMDA current\nin a brain slice kept at room temperature (Hessler, Shirke, and Malinow, 1993; heavy jagged line).\nSuperimposed is the current computed from Eq. 4.6, with zj = 145.5 and 12 = 4.1 msec. Unpublished\ndata from A. Destexhe, printed with permission.\n(continued) 37° C baseline in mammals. In general, the absolute conductances associated with ionic channels vary little with\nthe temperature. However, the rates with which these channels change conformation (Chap. 8) speed up considerably at higher\ntemperatures. The extent of this sensitivity is characterized by the temperature coefficient Qig, defined as the increase in rate as\nthe temperature changes by 10° C. Hestrin, Sah, and Nicoll (1990b) report an average temperature coefficient associated with the\ndecay time constant of the fast non-NMDA current of Qio = 2.7± 0.8, with a decay equal to 7.9 msec at 26.5° C. This implies\na much faster decay of 7.9 G10 \n= 2.78 msec at 37° C in the animal, which explains why the NMDA current plotted\nin Fig. 4.9B, obtained at 23° C, has larger values of T\\ and TI than those indicated here."}
{"text": "===== Page 19 =====\n4.6 Excitatory MMDA and Non-NMDA Synaptic Input \n• \n103\nFig. 4.9 EXCITATORY POSTSYNAPTIC CURRENTS \nExcitatory postsynaptic current recorded at the soma\nof inhibitory stellate cells in slices of rat visual cortex in response to the synaptic input, recorded by\nclamping the membrane potential to three different values (Stern, Edwards, and Sakmann, 1992). The\nEPSCs are measured at three different clamp potentials. (A) As discussed in the text, the EPSC mainly\nreflects the time course of the underlying synaptic channels, while the time course of the EPSP is dictated by\nthe electrotonic structure of the postsynaptic site. (B) Pharmacological dissection of the two components of\na stimulus-evoked EPSC. Here, the fast component is blocked by application of the non-NMDA antagonist\nCNQX, while in (C) the slow NMDA-mediated component is blocked by APV. Because the experiments\nwere performed at room temperatures, EPSCs in the living animal will be faster. Reprinted by permission\nfrom Stern, Edwards, and Sakmann (1992).\nAs discussed in Sec. 1.4, the voltage-clamp paradigm allows the physiologist to measure\nthe current needed to keep the membrane at a fixed potential (the clamp potential Vciamp)-\nThis technique was developed by Marmont (1949), Cole (1949), and Hodgkin, Huxley, and\nKatz (1949) to record accurately the membrane current flowing across the axonal membrane.\nUsing a high-gain feedback amplifier, enough current is supplied to the membrane to\nstabilize the potential at Vciamp» even if the membrane conductance is changing rapidly (as\nduring an action potential; see Chap. 6). The basic circuit is presented in an highly idealized\nform in Fig. 4.10. For a primer on the theory of voltage clamping, consult Appendix A in\nJohnston and Wu (1995).\nWhen voltage clamping is applied to a cell receiving synaptic input, the clamp current\n7ciamp needed to keep the membrane at Vciamp is usually termed excitatory postsynaptic\ncurrent (EPSC) or inhibitory postsynaptic current (IPSC), depending on whether the current\nis negative or positive. If the synaptic input is at, or close to, the location of the electrode,\ncable properties can be neglected and the synaptic current can be estimated via the use of\nthe voltage-clamp technique by fixing the somatic potential to the resting potential,"}
{"text": "===== Page 20 =====\n104 . \nSYNAPTIC INPUT\nFig. 4.10 VOLTAGE-CLAMP SETUP The technique of clamping the membrane potential to a\nparticular value and measuring the resultant current constitutes a key technological advance in cellular\nbiophysics. We here illustrate the vanilla flavored two-electrode voltage-clamp circuit schematically.\nThe voltage recording electrode at the bottom connects to a high impedance follower circuit that acts\nas a buffer, drawing only minimal current from the cell. Its output voltage Vb « Vm. This output is\nconnected to the negative input of an amplifier with gain A. Its output voltage V\\ = A^damp — Vb)-\nThe associated current /ciamp flows across the access resistance Ra into or out of the cell. In the limit\nof a very large gain A, the membrane potential across the cell Vm is \"clamped\" to Vdamp.\n(4.7)\nwith Vciamp = \nVrest- P°r synaptic inputs that only give rise to a small postsynaptic\npotential at the soma, the driving potential is not much changed and /ciamp constitutes a\nfair approximation to the actual postsynaptic current, defined via Eq. 4.4.\nIf the site of the synapse does not coincide with the site at which the membrane potential\nis clamped—common to most situations when one is recording from the cell body and the\nsynaptic input is located somewhere in the dendritic tree—cable properties intervene and\none no longer measures the pure synaptic current (the synapse is said to be improperly\nspace clamped). Establishing whether or not voltage clamping is complete is a challenging\nproblem with no cut and dry answers (see Smith et al., 1985).\nIn many inhibitory interneurons, excitatory input makes frequent contacts directly onto\nthe cell bodies, allowing them to be easily voltage clamped. Stern, Edwards, and Sakmann\n(1992) exploit this fact to record excitatory postsynaptic currents in inhibitory stellate cells\nin rat visual cortex (given their small size, the associated high input resistances are very\nhigh, between 0.5 and 2 G£2). Stimulating neighboring cells leads to the small unitary\nEPSCs shown in Fig. 4.9 (unitary in the sense that below a certain stimulus threshold\nno EPSC can be seen, while the peak amplitude of the EPSC remains constant once\nthe threshold has been exceeded). Making judicious use of CNQX and APV, the specific\nblockers of the non-NMDA and the NMDA receptors, Stern and his colleagues separate the\ntwo components. Clamping the membrane to near its resting potential, it can be seen that\nthe fast and large component is due to non-NMDA input, while the slow and late component\nis NMDA mediated. Clamping at progressively more depolarizing currents increases the\ncomponent remaining after application of CNQX, that is, the NMDA component, while\nthe APV-insensitive component is decreased. Because the synaptic driving potential is"}
{"text": "===== Page 21 =====\n4.7 Inhibitory GABAergic Synaptic Input \n• \n105\nprogressively reduced as Vciamp is increased, the non-NMDA-mediated current decreases,\nwhile the NMDA current increases, the decrease in driving potential being compensated for\nby the increase in the conductance in the negative slope-conductance region. Glutamate-\nmediated synaptic input usually has both NMDA and non-NMDA components, arguing for\ncolocalization of both receptors in the postsynaptic membrane (as in Fig. 4.9).\nNMDA channels are in a sense a theoretician's dream come true, since they implement\nwithin a single protein complex a molecular AND gate: a significant depolarizing current\ncan only be obtained in the presence of presynaptic neurotransmitters and postsynaptic de-\npolarization. This action is only possible because of the fact that, different from non-NMDA\nexcitation and GABAergic inhibition, the amplitude of the postsynaptic conductance change\ndepends on the postsynaptic potential. NMDA receptors have been experimentally impli-\ncated as contributing toward the conjunctive nonlinearities required for Hebb's learning\nrule (Chap. 13).\nBefore finishing this section, we need to point out a crucial difference between the EPSC\nand the resultant EPSP (the same applies for the relationship between IPSC and IPSP): the\npostsynaptic current always leads the postsynaptic voltage. Since the current flowing across\na pure capacitance is proportional to the derivative of the voltage, the voltage response to a\nsinuosidal current will also be sinusoidal but with a phase shift of 90°; the voltage lags the\ncurrent by this phase angle. No such phase shift is observed for a pure resistance. For any\ndistributed, mixed resistive and capacitive system, the current will always lead the voltage\nby a phase angle somewhere between 0° and 90°. While the EPSC is not equal to the current\nflowing during an EPSP (since in the former case the voltage is clamped to some value while\nit changes in the latter) it usually is similar if the voltage is clamped to the resting potential\nof the cell (since the voltage excursion away from Vrest is usually small relative to £syn).\nThe dynamics of the current flowing at the synapse is only limited by how fast the un-\nderlying channels can switch—in the submillisecond range for fast excitatory or inhibitory\ninput—while the evoked potential change is constrained by postsynaptic parameters (such\nas the intracellular resistance, the membrane capacity, and, to a much lesser extent, the\nmembrane resistance) as discussed at length in Sec. 3.6.3.\nOne functional reason to stress EPSCs over EPSPs is that the current flowing at the soma\nin response to synaptic input is a much more meaningful measure of synaptic efficiency\nthan the peak postsynaptic potential (see Chaps. 17 and 18).\n4.7 Inhibitory GABAergic Synaptic Input\nThe most common inhibitory neurotransmitter in the central nervous system of both\ninvertebrates and vertebrates appears to be GABA. In the thalamus and cortex, about a\nquarter of all cells utilize GABA. (For the recent physiological literature on GABA see\nMcCormick, 1990; Edwards, Konnerth, and Sakmann, 1990; LaCaille, 1991; Berman,\nDouglas, and Martin, 1992.) The second important inhibitory neurotransmitter, glycine,\nappears to mediate IPSP in the spinal cord onto motoneurons (Young and MacDonald,\n1983) and in the brainstem.\nThere are two major forms of postsynaptic receptors associated with GABA releasing\nterminals, termed A and B receptors. They are quite distinct from each other, with their\nmain commonality being that they both bind GABA. As in the case for the excitatory\nneurotransmitters, our knowledge about these different receptor classes derives from the\nexistence of specific blockers (antagonists): the pharmacological agents bicuculline and"}
{"text": "===== Page 22 =====\n106 \n• \nSYNAPTIC INPUT\npicrotoxin reversibly block GABA^ receptors while phaclofen blocks the B type (see\nFig. 4.6).\nThe GABA^ receptor is an ionotropic one, with binding of GABA leading to the direct\nopening of channels selective to chloride ions (as with the glutamate receptor, more than\na dozen subreceptor types are known). The current through these channels reverses at the\nequilibrium potential for chloride ions, in the neighborhood of —70 mV. In many cells,\nonly chloride conductances are open at rest, implying that their resting potential is close\nto £syn for GABA^ synapses, an important fact to which we will return. The associated\npostsynaptic conductance change rises very rapidly, that is, within 1 msec or less, and decays\nwithin 10-20 msec.\nGABA can also bind to a metabotropic receptor, the GAB AB receptor. With the help of a\nsecond messenger (a G protein), activation of this receptor type leads to the opening of chan-\nnels selective to potassium ions. This makes the associated synaptic battery considerably\nmore hyperpolarized than the reversal potential of the GABA^ receptor: it is between —90\nand —100 mV. Due to the indirect coupling between the release of GABA and the opening\nof the associated K+ channels, the onset and the duration of the postsynaptic conductance\nchange are much slower, with the peak not occurring until 10 msec or more after transmitter\nrelease, the total duration being on the order of 100 msec or longer.\nDifferent from the non-NMDA and the NMDA receptors, GABAyi and GABAg do not\nappear to be colocalized. In fact, pharmacological evidence argues for a segregation of these\ntwo receptor classes, with GAB A^ occurring at or close to the soma and the hyperpolarizing\nGABAg type farther away, out on the dendrites (Tseng and Haberly, 1988; Douglas and\nMartin, 1998). As we will see in the following chapter, GABA^ synapses can implement a\nmultiplication, albeit a \"dirty\" one, while the hyperpolarizing action of a GABAg synapse\nhas more similarity to a linear subtraction.\n4.8 Postsynaptic Potential\nSection 1.4 dealt with a synapse embedded within a simple RC circuit. Let us now treat\nthe more general case of synaptic input to some postsynaptic site i, characterized by the\ntime-dependent input impedance KH (t). The voltage change Vt (t) in response to a synaptic\ncurrent (Eq. 4.4) is given by Ohm's law (with all potentials relative to the resting potential\nand the sign of the synaptic current—following the convention of physiologists— inverted\nto account for the flow of electric charge),\n(4.8)\nwhere * represents convolution. Equation 4.8 is just a shorthand way of writing\n(4.9)\nThe last transformation follows from our assumption that K*;(0 and gsyD(t) are. zero for\nnegative times. Equation 4.9 is a Volterm integral equation of the second type, characterized\nby the fact that the membrane potential appears both on the left-hand and on the right-hand\nside of the equation (Poggio and Torre, 1978)."}
{"text": "===== Page 23 =====\n4.8 Postsynaptic Potential \n• \n107\nFollowing Eq. 3.16, we can express the voltage at location j caused by the synaptic\ninput at i in terms of the transfer impedance K,-7- (t),\n(4.10)\nBecause the expression for V,- depends on V,-, finding Vj necessitates the solution of a\nsystem of coupled integral equations (Eqs. 4.9 and 4.10).\n4.8.1 Stationary Synaptic Input\nIn order to develop some intuitions about these equations, let us first treat the stationary\ncase, that is, the case in which a fixed increase in synaptic conductance gsyn leads to some\nfixed change in the voltage. The convolution in Eqs. 4.9 and 4.10 is then reduced to a simple\nmultiplication, and the time-dependent input impedance K,-,-(0 is replaced by the dc part\nof the Fourier transform, that is, by the input resistance Kn(f — 0) = KU,\n(4.11)\nor,\n(4.12)\nThis nonlinear equation implies that the postsynaptic potential V/ does not increase indefi-\nnitely with gsyn, but saturates. Figure 4.11 demonstrates this in the case of transient synaptic\ninput. The reason for the sublinear behavior is the fact that the synaptic current pushes the\nmembrane potential toward E^^, reducing the driving potential Esyn — VJ-. As this becomes\nsmaller, the synaptic current will also become smaller. At Esyn — Vj, the current ceases,\nno matter how large the conductance change, since no potential difference exists across the\nsynaptic conductance gsyn.\nThe determinant of whether the synaptic input excites or inhibits the cell is primarily its\nreversal potential Esyn. If the battery is above the resting potential, an increase in gsyn(t)\ncauses an EPSP; if the converse is true, a hyperpolarizing IPSP results. In the remainder\nof this chapter, we only treat excitatory inputs. However, the same principles apply for\ninhibitory synaptic inputs.\nLet us consider the membrane potential for a small synaptic input (Rinzel and Rail, 1974).\n\"Small\" is defined here relative to the input resistance. If the dimensionless product of the\ndc input resistance and the conductance change is much less than one, that is, gsyn • KU <SC 1,\nthe input is considered small. For instance, relative to a 500 MS2 dendritic input site, a 0.2 nS\nsynaptic input is small, because 5 x 108 x 2 x 10~10 = 0.1. Under these circumstances\nwe can approximate Eq. 4.12 by\n(4.13)\nIn other words, if the synaptic input is small enough, the membrane potential changes only\nlittle (that is, EsyD — V; ^ Esyn), and the synaptic input can be approximated by a constant\ncurrent source, /syn = Esyngsyn, independent of the membrane potential. If synaptic input\ncan be treated as a current source, then doubling the amplitude of the current source will\nsimply double the postsynaptic potential change, no bothersome saturating behavior exists,\nand the output is linear in the input.\nWe discussed in Sees. 4.6 and 4.7 evidence for conductance increases at single glutamate\nand GABA synapses of 1 nS or less. This implies that as long as the postsynaptic site has"}
{"text": "===== Page 24 =====\n108 \n• \nSYNAPTIC INPUT\nFig. 4.11 SYNAPTIC SATURATION Relationship between the postsynaptic conductance change and\nthe peak somatic EPSP on a log-log plot. A single fast (fpeak = 0.5 msec) synapse of variable amplitude\ngpeak is activated at one of three different locations—the soma and at two dendritic sites (a3 and\nbl; Fig. 3.7)—and the peak somatic EPSP is shown. Given the relatively high input conductance\nat the cell body, the somatic input does not saturate within the range shown here, while the two\ndendritic synapses are saturating for inputs above 10 nS. The simultaneous or near-simultaneous\nactivation of many synapses can easily increase the membrane conductance by this amount or\nmore. Synaptic saturation has important consequences for the cell during massive synaptic input.\nRm = 20, 000 S • cm2, yrest = -65 mV and Esyn = 0 mV.\nan input resistance of less than several hundred megaohms, individual inputs can be treated\nas current injection. This is frequently the case for cortical pyramidal cells, where a single\nsynaptic input can often be treated as a current source. Only in the distal dendrites and thin\nand elongated spines will the \"current\" approximation be invalid. However, in the presence\nof massive synaptic input, as would occur under physiological conditions when hundreds\nof inputs can be active simultaneously, saturation occurs and has important functional\nconsequences (Chap. 18).\nIf the product of gsyn and KU becomes larger, but still less than 1, we can use the Taylor\nseries expansion of l/(l+;t) = 1— x+x2+O(x3) and express the postsynaptic potential as\n(4.14)\nwhere O((.. .)3) is a shorthand notation for cubic and higher order terms. Intuitively,\nwe can think of this series expansion in terms of adding higher order correction terms to\nthe linear current term: first the synapse opens and the current 7syn = EsyngSyn flows into\nthe cell, causing a change in membrane potential (given by KnESyagsyn; see Eq. 4.13).\nThis variation in membrane potential changes in turn the driving potential from Esyn to\n£Syn(l — gsynKn), which will decrease the potential by Esyn(gSynKn)2, which again leads\nto a change in the driving potential, and so on. Adding up all these terms leads to the above\nequation. Effectively, we approximate the conductance input gsyn by a current input plus a\nnumber of \"correction\" terms: V| = Kn(I^ + /$ + /$ H \n)-\nIn the other extreme case, when the product of the synaptic conductance change and the\ninput resistance is very large, that is, gsynKa 5J> 1, we have\n(4.15)\nThe synapse is saturated (Fig. 4.11). As we have seen already in Fig. 1.11 and will\ndiscuss further on in more detail, this nonlinear behavior of synapses can be exploited"}
{"text": "===== Page 25 =====\n4.8 Postsynaptic Potential \n• \n109\nto implement a number of nonlinear neuronal operations, in particular multiplication. The\nnonlinear transduction process between conductance change and membrane potential was\nfully recognized by Rail (1964), although usually disregarded by him and other early\nmodelers, both because the nonlinearity involved usually precludes analytical treatment\nand because the input resistance in the dendritic tree was thought to be relatively small.\nThe steady-state potential at any other location j in response to a constant synaptic input\nat location i (Eq. 4.10) is given by\n(4.16)\nThe denominator of this expression is identical to the one describing the potential at location\ni (Eq. 4.12). Therefore, the saturation behavior (e.g., whether or not the synaptic input can\nbe treated as a current) only depends on the electrical properties at the synapse i and not on\nthe transfer resistance Ktj. Indeed, the potential at j is given by the voltage at i divided by\nthe voltage attenuation,\n(4.17)\n4.8.2 Transient Synaptic Input\nIn the previous section, we assumed for the sake of mathematical convenience that the\nsynaptic input is so slow that it can be treated as a sustained dc input. Under most\nphysiological conditions, though, fast excitatory synaptic input is anything but stationary\nbut can be over in a time interval less than rm.\nSolving analytically for the voltage in response to transient conductance inputs is difficult.\nNumerical evaluations can be carried out in one of two different manners. Either one can\nfirst compute the appropriate input and transfer impedances using the cable equation and\nthen solve Eqs. 4.8 and 4.10, or one can directly integrate the cable equation. The first\nmethod suffers from the disadvantage that computing the potential in response to n spatially\ndistributed inputs requires the evaluation of about «2/2 input and transfer impedances\nand only works for a linear membrane, while the second method does not depend on the\nnumber of inputs and can be extended in a straightforward manner to deal with membrane\nnonlinearities.\nFor a single synaptic input gsyn(0 at location XQ onto a single passive dendrite, the\nmodified cable equation (Eq. 2.7) has the form\n(4.18)\nwith the appropriate boundary conditions.\nHistorically, it was Rail and his collaborators who first studied solutions to this equation\nfor a variety of different dendritic tree geometries (Rail and Rinzel, 1973; Rinzel and\nRail, 1974; Segev et al., 1985). This equation is difficult to solve in closed form and\nwe will not attempt to do so. Rather, the course we follow throughout the book is to\nintegrate the cable equation numerically. Because of their generality, simple numerical\nintegration algorithms are the methods of choice today for solving these partial differential\nequations (see Appendix C). We made use of the very efficient and freely distributed single-\ncell simulator program package called NEURON for generating most of the figures in this\nbook. NEURON was developed by Mines (1984, 1989, 1998) and is widely used in the"}
{"text": "===== Page 26 =====\n110\nSYNAPTIC INPUT\ncomputational neuroscience community. An even more powerful tool is the single cell\nand neuronal network simulation package GENESIS, developed by Bower and his group\n(DeSchutter, 1992; Bower and Beeman, 1998).\nFigure 4.12 shows the dendritic and somatic EPSPs due to a single non-NMDA excitatory\nsynaptic input located in the basal tree of our layer 5 pyramidal cell. Notice the small size\nof the somatic EPSP, requiring summation of many excitatory inputs before the membrane\npotential reaches the firing threshold. If the same input is located at more distal locations (not\nshown; see, however, Fig. 18.1), the somatic EPSP is even smaller and its peak occurs later\nin time. Rail has derived in the case of a single equivalent cylinder a relationship between\nthe time of peak potential and the location of synaptic input (Rail et al., 1967), which\nhas been used to infer synaptic location in the case of la synaptic input to a motoneurons\n(Smith, Wuerker, and Frank, 1967). Because pyramidal cells do not fulfill the conditions for\nreduction to an equivalent cable (Sec. 3.2), no simple analytical relationship between time\nto peak and location exists. Furthermore, because of the low-pass action of the dendritic\ntree, the peak potential becomes more and more smeared out as the synaptic input moves\naway from the soma, making the time at which the peak potential occurs more and more\ndifficult to define.\n4.8.3 Infinitely Fast Synaptic Input\nWhat occurs when we consider the other extreme, that of a conductance change that occurs\ninfinitely fast or, in practice, much faster than the neuronal time constant rml Because the\nconductance change is so rapid, it does not have time to change the membrane potential,\nFig. 4.12 EXCITATORY SYNAPTIC INPUT \nSimulated synaptic input in response to activation of a\nsingle, fast non-NMDA excitatory input in the basal tree of our pyramidal cell (at location b\\ in\nFig. 3.7). We here show the synaptic conductance change gsyn(t) (activated at 1 msec and peaking\n0.5 msec later) and the local as well as the somatic EPSP (the 5-mV scale bar applies to V,-, while\nthe 0.5-mV scale applies to Vsoma). Notice the small amplitude of the somatic EPSP, in agreement\nwith experiments (Fig. 4.4). The bottom trace shows the clamp current in response to this input, when\nthe soma is clamped to —65 mV. This approximates the current at the soma when the synaptic input\nis activated."}
{"text": "===== Page 27 =====\n4.9 Visibility of Synaptic Inputs \n• \n111\nand therefore the driving potential, appreciably before it is over. Thus, we expect a very fast\nconductance input to act like a current input. This can be shown quite easily by assuming\ngsyn(t) = gpeak<5(0- Replacing this into the convolution Eq. 4.9 yields\n(4.19)\nassuming that Vt(t = 0) = 0 (relative to Vrest). The resulting EPSP faithfully follows the\nimpulse response function K;,-(£)- From this we expect that very fast synaptic inputs will\nshow little evidence of conductance changes, even if they are large.\n4.9 Visibility of Synaptic Inputs\nA question of experimental interest is to what extent a synaptic conductance change can be\ndetected at the cell body using an intracellular electrode. This issue is of paramount concern\nwhen trying to resolve the question of the existence and strength of conductance changes,\nin particular those associated with inhibition, during visual stimulation of neurons in both\nretinal ganglion cells (Marchiafava, 1979; Watanabe and Murakami, 1984) and cortical\nneurons (Douglas, Martin, and Whitteridge, 1988; Pei et al., 1991; Ferster and Jagadeesh,\n1992; Borg-Graham, Monier, and Fregnac, 1998).\nIt has been proposed that shunting or silent inhibition vetoes the response of the cell when\nthe visual stimulus, usually a spot or bar of light, moves in the cell's null direction (Torre and\nPoggio, 1978; Koch, Poggio, and Torre, 1982; Koch and Poggio, 1985b; Sec. 5.1). These\ntheoretical ideas can be tested experimentally by inferring the existence of synaptically\nmediated shunting inhibition, raising the general issue of the conditions under which a\nsynaptic input somewhere in the dendritic tree can be seen by an electrode at the cell body\n(Rail, 1967).\nOne problem is that an intracellular electrode cannot record conductance inputs directly;\nrather they have to be inferred from their shunting effect on the voltage. A sustained\nconductance change is measured by injecting a steady-state current Is at the soma and\nrecording the resultant stationary voltage Vs; the ratio of these two corresponds to the input\nconductance Gss = l/Kss. The same procedure is repeated during synaptic stimulation.\nThe difference between the old and the new values of the somatic input conductance then\ncorresponds to the change in input conductance AG^ due to the synaptic input gsyn. The\nvoltage at the postsynaptic site i is the sum of the synaptic contribution and the current\nspread from the somatic current injection,\n(4.20)\nThe somatic EPSP in the presence of both synaptic input and injected current is\n(4.21)\nIt is now straightforward to show (Koch, Douglas, and Wehmeier, 1990) that the change in\nsomatic input resistance AKSS is given by\n(4.22)\nAs expected for any input with gsya > 0, AA^j is always negative; physically, this\ncorresponds to the input resistance decreasing in response to an increase in the membrane"}
{"text": "===== Page 28 =====\n112 \n• \nSYNAPTIC INPUT\nconductance. Furthermore, it can be proven that the change in input conductance AGSS is\nalways bound by the total synaptic change, that is, 0 < AGSS < gsyn.\nThe equation itself is simple to understand. The current injected at the soma Is induces\na voltage change K{SIS at the location of the synapse. This voltage provides a synaptic\ndriving force, which converts the synaptic conductance input gsyn into a voltage change at\nlocation /, that is, (KisIs)KiigSyn/(l + gsynKa)- This is propagated to the soma, causing\na voltage change gsyaKfs!s/(l \n+ gsynKn). Dividing this voltage by the current Is leaves\nthe resistance change.\nThe higher the input resistance KU at the synapse, or the further removed the synapse\nis from the cell body (Kjs —> 0), the less visible the synaptic input becomes. Even if the\nsynaptic conductance change is very large, its effect on the soma will always be limited\n(as gsyn —>• oo, the change in input resistance will converge to —K?s/Kn), except if the\ninput is located directly at the soma, since in that case AKSS —>• —Kss and the new input\nconductance goes to zero.\nNotice that AKSS does not depend on the synaptic battery. In principle, this implies\nthat hyperpolarizing, shunting, or excitatory synaptic inputs are all equally visible from\nthe recording electrode. Due to the capacitive nature of the neuronal membrane, transient\nconductance changes are more difficult to see from the soma than sustained ones (since\nadditional charge between the synapse and the soma flows onto the distributed capacitances).\nTherefore, Eq. 4.22 represents an upper bound on what can be seen of a conductance change\nat the cell body.\n4.9.1 Input Impedance in the Presence of Synaptic Input\nIn the previous chapter, we defined the input impedance in a quiescent system without any\nsynaptic input. If a sustained synaptic input occurs at i, the new value of the somatic input\nresistance\n(4.23)\nfollows simply from Eq. 4.22.\nIn the presence of a time-dependent conductance change, the system is a nonstationary\none and the new input conductance is not simply the sum of the time-varying conductance\nchange gsyn (t) and the old conductance change. In the case of a patch of membrane without\nspatial extent characterized by the input impedance K (t), the new input impedance K'(t,t')\nis a two-dimensional function depending on the time the synaptic input arrived,\n(4.24)\nThe exact value of the conductance change depends on the relative timing between the onset\nof the synaptic input and the onset of the current pulse used to measure the input impedance.\nFor stationary input, we obtain the familiar l/K' = gsyn + I/A\".\n4.10 Electrical Gap Junctions\nAs we mentioned, neurons are also coupled using direct electrical connections (Dermietzel\nand Spray, 1993). This point-to-point coupling occurs at specialized channels spanning\nthe pre- and postsynaptic membranes called gap junctions. Such a junction provides a"}
{"text": "===== Page 29 =====\n4.10 Electrical Gap Junctions \n• \n113\ndirect, high-conductance pathway between between neurons and eliminates the possibility\nthat the gap junction current would be shunted by the extracellular space. The strength\nof the coupling can be modulated by a number of factors, such as high internal calcium\nconcentration (a feature thought to protect the surrounding tissue from the death of any one\ncell) or intracellular pH (Bennett and Spray, 1987).\nMany cells are coupled by gap junctions, giving rise to a syncytium of two- or three-\ndimensional cellular networks. The most dramatic example is the cardiac muscle. Gap\njunctions allow single action potentials, originating in a group of pacemaker cells, to sweep\nthrough all cells in a wavelike manner, generating the rhythmic squeeze and relaxation that\nis the stuff of life (Noble, 1979).\nGlial cells are another example. These cells, thought to play mainly a supporting,\nmetabolic role in the nervous system—such as maintaining the ionic distributions and\ngradients necessary to homeostasis—lack conventional chemical synapses. They commu-\nnicate instead via an extensive grid of electrical gap junctions with each other (Giaume and\nMcCarthy, 1996).\nOne way to study gap junctions is to record from two coupled cells using pipettes,\nand measuring the current flow between the two as a function of the voltage gradient\n(Fig. 4.13A). In general, one observes a linear I-V curve, whose slope depends on the\nnumber of gap junction channels among the pair of cells (Fig. 4.13B). Typically, each\nchannel contributes about 100 pS of conductance. Gap junctions are usually symmetrical,\nwith a depolarization in one cell leading to a depolarization in the other one, albeit of\nless value, and a hyperpolarization leading to a less pronounced hyperpolarization in the\nother cell. From an electrical point of view, this situation can be mimicked by postulating\na coupling conductance gc among the two cells (Fig. 4.13C).\nWhen current is injected into cell 1, as shown in the figure, the application of Kirchhoff's\ncurrent law to the two nodes and the neglect of all transient behavior leads to the following\nexpression for the transfer resistance:\n(4.25)\nwhere gi and g2 correspond to the input conductance of each cell. The inverse of the slope\nof the I—V curve in Fig. 4.13B corresponds to this coupling resistance. The input resistance\nof cell 1 is\n(4.26)\nEquation 4.25 implies that injecting some current into cell 1 induces the same voltage in\ncell 2 as injecting the current in cell 2 and measuring the voltage in cell 1. In order to see\nthe asymmetry inherent in direct electrical coupling, we compute the voltage attenuation\nexperienced by going from cell 1 to cell 2,\n- \n(4.27)\nand in the opposite direction,\n(4.28)\nLet us assume a coupling conductance of gc = InS and an input conductance of g2 = lOnS\nfor the second cell. If the first cell has the same input conductance, A \\2 = A|j = 1.1, that"}
{"text": "===== Page 30 =====\n114 \n• \nSYNAPTIC INPUT\n(A) MEASURING COUPLING\n(B) OHMIC COUPLING\nFig. 4.13 ELECTRICAL COUPLING A second type of specific cell-to-cell coupling occurs via gap\njunctions or electrical synapses. (A) They can be studied by applying a membrane potential across\ntwo adjacent cells and measuring the junctional current. Different from a chemical synapse, the\ncurrent flows instantaneously, with no delay. The inset provides a graphical rendition of the molecular\nnature of the voltage-independent channels underlying this coupling. Reprinted by permission from\nHille (1992). (B) The resultant I—V curve, here taken from the gap junction among axons in the\ncrayfish motor system (Watanabe and Grundfest, 1961). The inverse of the slope corresponds to the\ntransfer resistance K\\i of Eq. 4.25. (C) The majority of such junctions can be modeled by a coupling\nconductance gc. Current flows from one cell to the next without being shunted by the extracellular\ncytoplasm. The capacitive nature of the pre- and postsynaptic membranes has been neglected.\nis, anEPSP would be attenuated by about 10% in either direction. However, if cell 1 is much\nsmaller, with a tenfold lower conductance of g\\ = 1 nS, then the voltage attenuates by a\nfactor of 2 when going from the low-input to the high-input conductance cell, while it still\nonly attenuates by 10% in the opposite direction. Attenuation is symmetrical when the two\ninput conductances g\\ and g2 are identical. It should be pointed out that attenuation across\nan electrical synapse is in marked contrast to the high degree of amplification possible at\nchemical synapses.\nElectrical coupling has been studied in detail in the outer layers of the vertebrate retina.\nGap junctions among rods as well as among horizontal cells give rise to extended two-\ndimensional sheets of cells (Kaneko, 1976; Copenhagen and Owen, 1976; Attwell and\nWilson, 1980). Theoretical calculations show that a spatio-temporal filter can be ascribed\nto this syncytium (Torre, Owen, and Sandini, 1983; Torre and Owen, 1983). Indeed, the\nexperimental data suggest that the high-pass filtering properties of the rod network serve to\noptimize the signal-to-noise ratio by integrating visually evoked signals over a large area"}
{"text": "===== Page 31 =====\n4.11 Recapitulation \n• 115\nfor rapid signals and over a small area for slowly changing ones (Detwiler, Hodgkin, and\nMcNaughton, 1980).\nIn the adult thalamus and cortex, gap junctions have only occasionally been reported. At\nthis point in time it is safe to say that the mere existence as well as any possible functional\nrole of electrical synapses in cortex and associated structures remain unknown.\nElectrical synapses do not have any intrinsic delay, unlike chemical ones. Thus, one\nfrequently finds them in time-critical pathways. The classical example is electrical synapses\nmade by presynaptic fibers onto axons of giant motor neurons that mediate the emergency\ntail flips in the crayfish, the basis of its escape reaction during danger (Furshpan and Potter,\n1959). Interestingly, here the connection is rectifying, with a diode-like relationship between\nthe voltage across and the current flowing through this junction.\nA further example is the part of the electrosensory system in weakly electric fish that\nconveys information about the time of occurrence of the zero crossing of the electrical wave\nsignal that the fish can send out and also sense (Heiligenberg, 1991). At multiple locations\nin this pathway, electric synapses mediate cell-to-cell coupling. For instance, spikes in the\nso-called T afferents from the periphery carry temporal information in action potentials\nwith a time jitter of 30 ± 25 /zsec. A number of these afferents make gap junctions with a\nspherical cell in the electrosensory lateral line lobe, such that a randomly occurring input in\nany one input fiber does not bring the cell to threshold. Using this conjunctive logic together\nwith synapses that do not introduce any additional temporal smear, reduces the temporal\njitter of the output spikes to 11 ± 3 /usec (Carr, Heiligenberg, and Rose, 1986).\nOne functional role of gap junctions most certainly has to do with the fact that small\nmolecules, such as cAMP (or intracellular dye in an experiment), can pass through these\nchannels, enabling them to diffuse across many cells. This might explain the more wide-\nspread distribution of gap junctions during early development (DeHaan and Chen, 1990).\n4.11 Recapitulation\nFast communication among nerve cells occurs at specialized junctions called synapses. They\nare very compact: between several hundred million and one billion synapses can be packed\ninto one cubic millimeter of neuronal tissue. Of the two types, chemical and electrical, we\nfocus on the former since they are much more frequent and make very specific point-to-point\nconnections.\nIt is useful to distinguish fast ionotropic chemical synapses, acting on a millisecond time\nscale, from metabotropic chemical synapses, acting on a time scale of a fraction of a second\nto minutes. Conceptually and cum grano sails, ionotropic synapses are of the essence in\nthe rapid forms of neuronal communication and computations underlying perception and\nmotor control.\nIn response to a presynaptic change in membrane potential at a synapse, neurotransmitters\nare released and diffuses within a fraction of a millisecond across the cleft separating pre-\nand postsynaptic terminal. At the postsynaptic terminal, neurotransmitter molecules bind to\nspecific receptors, which usually—either directly or indirectly, via involvement of a second-\nmessenger system—open specific ionic channels. Depending on the neurotransmitter-\nreceptor kinetics, these channels remain open for some time and a synaptic current flows\nacross the membrane. Synaptic transmission at central synapses appears to be stochastic.\nA presynaptic action potential has a probability p of causing a release of a vesicle and a\npostsynaptic response, where p can be as small as a few percent and depends on the spiking"}
{"text": "===== Page 32 =====\n116 . \nSYNAPTIC INPUT\nhistory of the synapse. The amplitude of the postsynaptic signal is variable as well. These\nfactors need to be taken into consideration when thinking about neural computation.\nAn electrical engineer would be justified in treating a chemical synapse (at the time\nscale of tens of milliseconds) as a nonreciprocal, two-port device (see the introduction to\nSec. 3.4). A two-port description is necessary, since a pair of equations (for both the pre-\nand the postsynaptic current and voltage changes) is required to completely characterize\nits behavior; it is non-reciprocal since changes at the postsynaptic side have no (fast) effect\non the presynaptic side. Synapses serve to decouple neuronal elements that can each have\nvery different electrical impedances, rather like a follower-amplifier circuit.\nAt the macroscopic and phenomenological level, a fast synaptic input induces a time-\ndependent increase in conductance gsyn(0 in series with a battery Esya. The sign of Esyn\nrelative to the membrane potential at the postsynaptic terminal determines whether synaptic\ninput causes an EPSP (excitatory synapse), an IPSP (inhibitory synapse), or no change in\nmembrane potential (silent or shunting inhibition). The five dominant types of fast synaptic\ninputs are (1) non-NMDA or AMPA voltage-independent excitation; (2) ACh-mediated\nexcitation; (3) the voltage-dependent and slower NMDA excitatory input that is thought to\nbe crucially involved in synaptic plasticity; (4) the GABA^ type of silent inhibition; and\n(5) the slower GABAg hyperpolarizing inhibition. The fact that synaptic input increases the\npostsynaptic membrane conductance in series with a battery has important consequences.\nIn particular, synaptic inputs can saturate, will influence the input conductance of the cell,\nand can interact with each other nonlinearly. Only if the amplitude of the synaptic input\nconductance change is small relative to the local input impedance can synaptic input be\ntreated as a constant current source.\nThe immense variety of neurotransmitiers and postsynaptic receptors gives rise to a\nstaggering combination of possible pairings that act on all possible time scales, and across\ndifferent spatial scales, from a single synapse to a single ganglion or an entire neural system,\nsuch as the thalamus. Synapses are responsible for the most salient difference between\nnervous systems and even our most advanced digital computers: while the former adapt\nand learn—a subject we will cover in depth in Chap. 13—the latter do not.\nElectrical synapses allow for direct current flow among adjacent neurons. A gap junction\ncan usually be modeled by a fixed conductance. Different from chemical synapses where\namplification between the pre- and postsynaptic sites can occur, here the signal is always\nattenuated. One advantage of this mode of cellular communication is speed, since no\nsynaptic delay occurs. Thus, electrical synapses are frequently found in neuronal pathways,\nwhich subserve information that needs to be communicated very rapidly and faithfully. In\nthe retina, gap junctions among photoreceptors and horizontal cells create vast, electrically\ninterconnected networks that filter the incoming visual signal."}
{"text": "===== Page 6 =====\n122 • \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nFig. 5.2 INTERACTION AMONG AN EXCITATORY AND AN INHIBITORY SYNAPSE \nHow does the\ninteraction between an excitatory synapse (at location e) and an inhibitory synapse (at i) in a passive\ndendritic tree depend on their spatial positions? And what role do the synaptic architecture and the\ndendritic morphology play? In general, the potential at the soma s is not simply the sum of the\nindividual IPSP and EPSP but can be much less. If the inhibition is of the shunting type, with a\nreversal potential close to the resting potential of the cell, inhibition by itself leads to no significant\npotential change while still being able to veto the EPSP, as long as the inhibitory synapse is either\nclose to the excitatory one or \"on the direct path\" between excitation and the soma s (shaded area).\nThe effectiveness of shunting inhibition drops substantially outside this zone.\n(5.11)\nwith\nWhether the somatic potential is positive or negative, that is, whether it corresponds to an\nEPSP or to an IPSP, depends on the relative magnitude of the two contributions.\nIn order to arrive at a qualitative picture of the behavior of the system, let us assume\nthat the conductance inputs ge and g, are small, such that geKee <g 1, giKu <& I, and\ngegiK* -C 1, and that higher than second-order terms in ge and g; can be neglected (e.g.,\ngl & 0 and geg? & 0, etc.). Similarly to Eq. 4.14, we can use the Taylor series expansion\nof Eq. 5.11 to arrive at\n(5.12)\nThe three components in the first bracket are a shorthand version of the currents flowing at\nlocations e that are propagated to the cell body, and the three currents in the second bracket\nare propagated from the site of inhibition to the soma. Ie is the current generated under\nthe assumption that the driving potential remains unperturbed (that is, Ee — Ve & Ee). Iee\n\"corrects\" the first term by approximating the driving potential as the difference between"}
{"text": "===== Page 7 =====\n5.1 Nonlinear Interaction among Excitation and Inhibition \n• \n123\nEe and the voltage change KeegeEe associated with the first term. Iei results from the\ninteraction between the two synapses. It can be interpreted as follows: the first-order\napproximation to the inhibitory current is g, E,, causing an IPSP at e of amplitude Kiegi Et,\nwhich will affect the driving potential at e. The appropriate correction current is given by\nthe IPSP multiplied by the local conductance change —geKiegi E,. By analogy, the currents\nat the site of the inhibition can likewise be explained by the superposition of a zero-order\ncurrent plus the first two correction terms.\n5.1.3 Location of the Inhibitory Synapse\nSo far we did not discuss any specific spatial arrangements between excitation and inhibition.\nWe are interested in finding the location where synaptic inhibition can be maximally\neffective in reducing the amplitude of the excitatory input. Specifically, given an excitatory\nsynapse (of amplitude ge and battery Ee) at location e, where should the inhibitory synapse\n(of amplitude g, and battery £,) be located such that it maximally reduces the EPSP?\nThis question was the subject of an investigation into the relationship between synaptic\narchitecture and dendritic morphology (Koch, Poggio, and Torre, 1982,1983). It is tedious\nbut straightforward (Koch, 1982) to prove the following:\nOn-the-path theorem. For arbitrary values of ge > 0, gj > 0, Ee > 0, andEi < 0,\nthe location where inhibition is maximally effective is always on the direct path from\nthe location of the excitatory synapse to the soma (Fig. 5.2).\nWhere exactly the optimal location is on this path depends on the details of the system\nand can shift from the location of the excitatory synapse to somewhere on the path to the\ncell body. As inhibition is moved toward the soma, its specificity decreases, since the same\ninhibition now reduces not only the EPSP coming from synapse e but also those from other\nlocations. This is most pronounced at the soma, where an excitatory input anywhere in the\ndendritic tree will be attenuated. Thus, mapping synaptic architecture onto the dendritic\nmorphology can give rise to different classes of computations.\nThree useful properties concerning the optimal location of inhibition (see also Jack,\nNoble, and Tsien, 1975; Rail, 1967, 1970) that are valid for stationary conductance inputs\nin arbitrary, passive dendritic trees are:\n1. For small synaptic inputs ge and g,-, the most relevant parameter is the distance between\nthe two synapses. It makes only little difference whether inhibition is behind (with respect\nto the cell body) excitation or on the path. The strength of the interaction among synapses\ndecreases as the amplitudes of the conductance changes are decreased.\n2. As ge increases while gi remains constant, the optimal location of inhibition moves\nalong the direct path toward the soma.\n3. For very large excitatory inputs (ge -*• oo), all inhibitory synapses located behind the\nexcitatory synapse are completely ineffective.\nThese properties are a direct consequence of Eq. 5.11, the fact that there exists a unique\npath between any two points in a dendritic tree, and properties of the transfer resistances\n(Sec. 3.4)."}
{"text": "===== Page 8 =====\n124 \n• \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nExperimental verification of the specific nature of synaptic inhibition comes from a\nstudy by Skydsgaard and Hounsgaard (1994) carried out on the large dendritic arbor\nof motoneurons. Using three independent electrodes that can release either glutamate or\nGABA (so-called iontophoresis electrodes) as well as a fourth recording electrode at the\ncell body, they showed in 10 out of 12 experiments a spatially specific reduction in the\nglutamate-induced excitatory response. In other words, the shunting action of GABA was\nprimarily effective in reducing the excitatory action of glutamate when the two iontophoresis\nelectrodes were close to each other. No or little effect was observed on the response due to\na more distal glutamate-releasing electrode. This constitutes prima facie evidence for the\nspatial selectivity of shunting inhibition.\n5.1.4 Shunting Inhibition Implements a \"Dirty\" Multiplication\nLet us consider the specific case when inhibition has a reversal potential close or equal to\nthe resting potential of the cell. The GABA^ mediated increase in chloride conductance\napproximates such a shunting inhibition. In this case, the nonlinear interaction between\nexcitation and inhibition is most pronounced, because activation of the inhibition by itself\ndoes not cause any change in potential (hence its proper—but rarely used—name of silent\ninhibition), while inhibition during synaptic excitation can greatly reduce the EPSP. For\nsmall enough inputs Eq. 5.11 reduces to\n(5.13)\nHere the somatic potential is given by two terms in ge in addition to a crossterm involving a\nmultiplicative interaction between ge and g,-. Conceptually, one can think of this as a dirty\nor approximate multiplication between two synaptic inputs ge and g,-, with the value of the\noffset (geKes — g^KeeKes) depending in a nonlinear manner on one of the inputs (Poggio\nand Torre, 1978).\nIn order to quantify the effectiveness of shunting inhibition, Koch, Poggio, and Torre\n(1982) introduced the F factor as the ratio of the somatic EPSP in the absence of any\ninhibition to the somatic EPSP in the presence of inhibition; the larger this number, the\nstronger the effect of inhibition. On the basis of Eq. 5.11 this can be expressed as\n(5.14)\nwhere K+ = 0 for inhibition located on the path between e and s. A large F value\ndescribes an effective inhibition, while 1/F indicates the relative decrease of the somatic\nEPSP by inhibition. As expected, F > 1 for all cases. Table 5.1 illustrates some typical F\nvalues obtained in a numerical simulation of the effect of inhibition in a retinal ganglion\ncell responding to a single excitatory input at location 1 (Fig. 5.3A). Let us summarize\nthese results.\nEffectiveness of Shunting Inhibition\n1. For small synaptic inputs the on-the-path effect is weak and the strength of the inter-\naction depends mainly on the distance between excitation and inhibition. Under these\nconditions, all locations close to the excitatory synapse are equally effective in reducing\nexcitation."}
{"text": "===== Page 9 =====\n5.1 Nonlinear interaction among Excitation and Inhibition\n125\nFig. 5.3 SPATIO-TEMPORAL SPECIFICITY OF SYNAPTIC INTERACTION Effectiveness of shunting\ninhibition (Ej = Vrest) in vetoing an EPSP in a retinal ganglion cell. (A) Excitation is always at location\n1, while the position of the inhibitory synapse varies. The cell's morphology was reconstructed based\non Golgi material from Wassle, Illing, and Peichl (1979). (B) The time course of the excitatory and\nthe inhibitory conductance changes reflects the slow retinal dynamics (Baylor and Fettiplace, 1979),\nwith identical time courses but different peak conductance changes (10 nS for excitation and 100 nS\nfor inhibition). (C) The F factor, that is, the reduction in the peak somatic EPSP due to inhibition, as\na function of the delay between the onset of ge(t) and gt(t). The strength of the interaction (see also\nTable 5.1) is specific in space and time. This result holds over a large parameter range. In particular,\nRm (here set to 14,000 Q • cm2) can be increased over several orders of magnitude. Reprinted by\npermission from Koch, Poggio, and Torre (1983)."}
{"text": "===== Page 10 =====\n126\nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\n{\n  \"table_name\": \"TABLE 5.1\",\n  \"description\": \"Effectiveness of Shunting Inhibition\",\n  \"columns\": [\"Inhibition\", \"ge = 1, gi = 1\", \"ge = 1, gi = 10\", \"ge = 1, gi = 100\", \"ge = 10, gi = 100\"],\n  \"rows\": [\n    {\"Inhibition\": \"3\", \"ge = 1, gi = 1\": 1.16, \"ge = 1, gi = 10\": 2.06, \"ge = 1, gi = 100\": 3.43, \"ge = 10, gi = 100\": 1.96},\n    {\"Inhibition\": \"1\", \"ge = 1, gi = 1\": 1.17, \"ge = 1, gi = 10\": 2.70, \"ge = 1, gi = 100\": 1.80, \"ge = 10, gi = 100\": 7.72},\n    {\"Inhibition\": \"2\", \"ge = 1, gi = 1\": 1.14, \"ge = 1, gi = 10\": 2.40, \"ge = 1, gi = 100\": 15.00, \"ge = 10, gi = 100\": 8.72},\n    {\"Inhibition\": \"4\", \"ge = 1, gi = 1\": 1.12, \"ge = 1, gi = 10\": 1.72, \"ge = 1, gi = 100\": 2.47, \"ge = 10, gi = 100\": 1.87},\n    {\"Inhibition\": \"5\", \"ge = 1, gi = 1\": 1.08, \"ge = 1, gi = 10\": 1.46, \"ge = 1, gi = 100\": 1.84, \"ge = 10, gi = 100\": 1.62},\n    {\"Inhibition\": \"s\", \"ge = 1, gi = 1\": 1.11, \"ge = 1, gi = 10\": 2.06, \"ge = 1, gi = 100\": 11.57, \"ge = 10, gi = 100\": 9.18}\n  ]\n}"}
{"text": "F values, describing the effectiveness of shunting inhibition in reducing a somatic EPSP, for stationary synaptic inputs\nin the retinal ganglion cell model of Fig. 5.3A. The excitatory synaptic input ge (with Ee = 80 mV relative to Vrest) is\nalways at location 1, while inhibition gi is either located behind excitation (3), at the same location as excitation (1),\nbetween excitation and the soma (2), in abranch just off the path (4), in a very different part of the tree (5), or at the soma\n(s). The amplitudes of ge and #,- are in nanosiemens. The somatic EPSP in the absence of any inhibition for ge = 1 nS\nis 6.56 mV. Km = 14, 000 Q • cm2 and K; = 70 £2-cm. From Koch (1982).\n2. If the two synapses coincide anywhere in the dendritic tree (i = e), F reduces to\n(5.15)\nThis is also the F factor obtained when dealing with a patch of passive membrane in the\npresence of two synapses (if we identify R with KH). The difference between Eqs. 5.14\nand 1.34 (or 5.15) is due to the spatially distributed tree. As discussed in Sec. 5.1.1,\nexcitation can always overcome the effect of fixed but large inhibition.\n3. If the amplitude of inhibition is above a critical value (about 50 nS in the case of the\nretinal ganglion cell simulation; Koch, Poggio, and Torre, 1983), F values can be quite\nhigh (2-8), even if the excitatory input is much larger than the inhibitory one, as long as\nthe inhibition is between excitation and the soma. Comparing this against the F factor\nobtained in a lumped circuit model (Eq. 5.15), we ascertain that these high F values are\ndue to the cable properties of the tree. Inhibition behind excitation or on a neighboring\nbranch (about 10 or 20 /j,m for the retinal ganglion cell considered here) off the direct\npath is ineffective in reducing excitation significantly.\n4. The specificity for high values of gi persists even for very large values of the membrane\nresistivity, when the cell is relatively compact (in terms of electrotonic distance or of\nthe logarithm of the voltage attenuation L\"). For instance, for Rm — 1 M£2 • cm2,\nthe electrotonic size of the ganglion cell in Fig. 5.3A shrinks to 0.03A. and its input\nresistances increase to over 1 GSl. Yet for ge = 10 nS and g, = 100 nS, we obtain\nF = 10.5, 14.5, 21.0, 2.4, 2.7, and 2.7 for inhibition at the location of excitation (1),\non the path (2), at the cell body (s), behind excitation (3), on a neighboring branch (4),\nand in a different part of the tree (5; Fig. 5.3A). These effects can be visualized with\nthe aid of a loose analogy between dendrites and water pipes and reservoirs. Shunting\ninhibition corresponds to opening a hole in a pipe which is coupled to a water reservoir\nwith the same pressure as in the quiescent pipe system. If this hole is between the site\nwhere water is being injected and the central pool, it is clear that more water will leave\nthe hole on its way to the large somatic sink than if the hole were to be opened upstream\nfrom excitation."}
{"text": "===== Page 11 =====\n5.1 Nonlinear Interaction among Excitation and Inhibition \n• \n127\nWe conclude that if gi is large enough, the on-the-path condition becomes very specific\nsuch that blocking an EPSP is only possible if inhibition is either in the close neighborhood\nof the excitatory synapse or between excitation and the cell body. Rail (1964) observed\nearlier that in a single unbranched cable, shunting inhibition effectively vetoes more distal\nbut not more proximal excitation.\nTemporal Specificity\nSo far we only considered the case of stationary synaptic inputs. In the general time-\ndependent case, the algebraic Eqs. 5.8-5.10 are replaced by integral equations,\n(5.16)\nTypically, these equations are solved numerically (Segev and Parnas, 1983; Koch, Poggio,\nand Torre, 1983). The efficacy of inhibition is characterized by a slight generalization of the\nF factor of Eq. 5.14 (F is defined as the ratio of the peak amplitude of the somatic EPSP\nwithout inhibition to the peak of the somatic EPSP in the presence of inhibition).\nFigure 5.3 shows the dependency of F on the relative timing between the onset of\nexcitation and inhibition. Negative delays correspond to inhibition preceding excitation,\nwhile positive delays are associated with inhibition following excitation. As can be seen, the\nspecificity of shunting inhibition in vetoing EPSPs generalizes from space to time: on-the-\npath inhibition can effectively veto excitation if it occurs within about 10 msec of the onset\nof excitation, about two-thirds of the membrane time constant. As we discussed in Sec. 3.6,\nthe local delay DU (Agmon-Snir and Segev, 1993) specifies the window during which\nsynaptic inputs can interact with each other. Different locations have different temporal\nwindows associated with them (e.g., Fig. 3.13), allowing for an operationalized definition\nof what is meant by two simultaneous inputs.\nThe delay between the onset of excitation and optimal inhibition increases with increasing\ndistance between the two synapses, as expected from the definition of the propagation delay\nin Eq. 2.51.\nDecreasing the duration of inhibition below that of excitation decreases its effectiveness,\nin some cases dramatically. In no case did the F factor for transient inputs exceed the F\nfactor for the corresponding stationary case. That is, inhibition appears to be at its most\neffective if temporal effects can be discounted.\nFigure 5.3 demonstrates the on-the-path effect very vividly. Locating inhibition between\nthe site of the excitatory synapse and the soma can shunt the somatic EPSP by a maximal\nfactor of 9, while moving inhibition beyond excitation (approximately conserving the\ndistance between the two synapses) renders inhibition much less effective, with the F factor\ndropping to 1.3 (a reduction of about 7). Yet this specificity only occurs in the presence of\nlarge conductance changes, possibly outside the physiological range (see Table 5.1), and\nbecomes much less pronounced for smaller values of g,, when the interaction depends more\non the distance between excitation and inhibition.\nIn summary, the veto effect of inhibition can be strong and specific with respect to spatial\nlocation and relative timing provided the following requirements are fulfilled: (1) inhibition\nmust have a reversal potential close to the resting potential of the cell, (2) inhibition should\nbe close to excitation or on the direct path between excitation and the cell body, (3) peak"}
{"text": "===== Page 12 =====\n128 \n. \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\ninhibition must be large enough, and (4) inhibition must last (at least) as long as excitation\nand their time courses should overlap substantially.\n5.1.5 Hyperpolarizing Inhibition Acts Like a Linear Subtraction\nAs we saw, in the limit of small synaptic inputs (ge Kee <$C 1 and gt Kit <g 1), the action of\nshunting inhibition on excitation can be characterized as a multiplication with an offset. Even\nthough the interaction between hyperpolarizing inhibition, such as the GABAg complex,\nwhich increases the postsynaptic potassium conductance (reversing close to —100 mV),\nand excitation also contains such multiplicative effects, it is more linear due to the direct\ncontribution the hyperpolarizing battery makes toward the membrane potential. Inspection\nof Eq. 5.12 reveals that the first two terms in Kis are proportional to the reversal potential\nof the inhibitory synapses. The more negative the inhibitory reversal potential (relative to\nVrest), the more these terms will dominate and the more the overall operation will resemble\na linear subtraction.\nA physiological instance of such a subtractive inhibition that can be linked to a specific\ncomputation occurs in the fly's motion detection system. Hassenstein and Reichardt (1956;\nReichardt, 1961) first suggested that the key mechanism underlying the optomotor response\nof insects to moving stimuli is mediated by a correlation-like operation. In this scheme, the\noutput from one receptor is multiplied by the temporally delayed (e.g., low-pass filtered)\noutput from a neighboring receptor and then temporally averaged. The component that is\ndirection selective can be extracted from the component that is independent of the direction\nof motion by subtracting the output of two mirror-symmetric pairs of Reichardt detectors\nfrom each other.\nA computational analysis (Egelhaaf, Borst, and Reichardt, 1989) of this system predicts\nthat as the subtraction stage between oppositely oriented motion detectors is blocked, the\npower of the second-order component of the membrane potential in the cellular output\nstage increases. Experiments with picrotoxin, a blocker of GABA/i receptors, confirm this\n(Egelhaaf, Borst, and Pilz, 1990). Intracellular recordings from large tangential cells in the\nlobula plate in the blowfly in combination with pharmacological blockers show that the\nexcitatory input is mediated by fast cholinergic synapses while the subtraction relies on\nGABAergic synapses that reverse between —70 mV and —80 mV (Borst, Egelhaaf, and\nHaag, 1995; Brotz and Borst, 1996). A stable resting potential around -50 mV (Fig. 5.4)\ngives the membrane enough maneuvering room in both directions to implement a linear\noperation of importance to optomotor behavior in flies.\n5.1.6 Functional Interpretation of the Synaptic Architecture\nand Dendritic Morphology: AND-NOT Gates\nThe specificity of the interaction between excitation and shunting or silent inhibition\nprovides in principle the substrate for implementing different classes of analog computations\nin different dendritic trees. This situation is portrayed in a highly idealized but suggestive\nmanner in Fig. 5.5 using a logical metaphor (Koch, Poggio, and Torre, 1982), bearing in\nmind that these interactions are continuous and not all or none.\nIn an unbranched cable (Fig. 5.5A), input /1 can inhibit more distal excitatory input e\\, €2\nand £3, while inhibition ij, can only significantly affect e^. If we adopt the logic AND-NOT\nall-or-none operation to characterize this interaction (an output only occurs in the presence\nof excitation and no inhibition), such a cable implements the logic expression"}
{"text": "===== Page 13 =====\n5.1 Nonlinear Interaction among Excitation and Inhibition\n129\nFig. 5.4 LINEAR SUBTRACTION IN THE MOTION PATHWAY OF THE FLY Intracellular recording\nfrom a VS cell in the third optic ganglion of the blowfly Calliphora during motion of a whole-\nfield grating. This stimulus is moved for 1 sec in the preferred (first black bar) and for 1 sec in the\ncell's opposite, null direction (second black bar) past the fly. The resting potential is stable around\n—50 mV, explaining the more or less symmetric response pattern driven by opposing pairs of Reichardt\ncorrelation detectors (Borst, Egelhaaf, and Haag, 1995). Excitation is mediated by fast cholinergic\nand inhibition by GABAergic synaptic input onto this cell. Unpublished data from J. Haag and A.\nBorst, printed with permission.\n(5.18)\n[e-} AND-NOT (i\\ OR 1*2 OR ^3)] OR \\Ci AND-NOT (z'l OR Z^)] OR (fii AND-NOT z'l) \n(5.17)\nIn other words, the soma receives a significant signal only if e3 is high and not blocked by\nz'i, z-2,or /3,orif e^ is high and not vetoed by eitherz'i orz'2,orifei is high and not vetoed by\nz'i. Due to the on-the-path effect, the more branched dendritic tree in Fig. 5.5B implements\na different expression,\nI\nThus, the combination of dendritic morphology coupled with specific synaptic circuits\nconspires to create a rich class of nonlinear operations, with different dendritic trees carrying\nout different computations. These interactions would be based on voltage-independent\nAMPA excitatory synapses and GABA^ inhibitory synapses.\nThe \"dendritic tree as logic network\" metaphor has been a popular concept, which has\nbeen extended to a number of other biophysical situations and logical operators (Segev and\nRail, 1988; Shepherd and Brayton, 1987; Shepherd, 1992; Zador, Claiborne, and Brown,\n1992; see Chaps. 12 and 19).\nYet as succinctly summarized by Mel (1994), this simile can be criticized on several\ngrounds. The principal argument against postulating that specific arrangements of individual\nsynapses act as logical or analog gates is the great demand that this places on developmental\nprocesses. In order to achieve the precise type of wiring idealized in Fig. 5.5, some learning\nmechanism has to guide individual synapses to individual branches in the dendritic tree to\nachieve the precise spatial arrangements required. For instance, moving i-j in Fig. 5.5B to\nthe adjacent branch significantly changes the associated logical expression. Thus, the great\nspecificity of this interaction represents at the same time its own Achilles heel (Mel, 1994)."}
{"text": "===== Page 14 =====\n130 \n. \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nFig. 5.5 IMPLEMENTING AND-NOT LOGIC IN A DENDRITIC TREE \nIdealized binary view of the\ncontinuous nonlinear interactions occurring between excitation and shunting inhibition. Inhibitory\ninputs (rectangles) veto more distal excitatory inputs (circles), but have only a marginal effect on inputs\nmore proximal to the soma, thereby approximating a logic AND-NOT gate. The two distinct dendritic\narchitectures, coupled with specific synaptic wiring, implement very different logical expressions (see\nEqs. 5.17 and 5.18). Reprinted by permission from Koch, Poggio, and Torre (1982).\nIt is possible, of course, that on-the-path effects are exerted not by individual synaptic\ninputs but by groups of synapses, as is the case in the distinction between absolute and\nrelative suppression (Vu and Krasne, 1992). This spatially less precise degree of spatial\ninteractions places a much reduced burden on developmental processes, since it only\nspecifies that inhibitory synapses should be either adjacent to the excitatory ones or in\na different part of the dendritic tree.\nOn biophysical grounds, it is unclear how specific the interaction between excitatory and\ninhibitory synapses actually is. As illustrated in Table 5.1, if the amplitude of the inhibitory\nconductance change is small, little specificity results. Furthermore, during physiological\nconditions, the cell potential may never be at Vrest because the cell receives tonic excitatory\ninput. Under these conditions, the distinction between hyperpolarizing and shunting inhi-\nbition is much smaller than we assume here, since both reverse negative to the \"effective\"\nresting potential.\n5.1.7 Retinal Directional Selectivity and Synaptic Logic\nDirectional selectivity of retinal ganglion cells is one example of a complex nonlinear\noperation that appears to use synaptic logic (Koch, Poggio, and Torre, 1986). A subset\nof ganglion cells in the vertebrate retina fire vigorously in response to the motion of a\nspot of light in one direction, the preferred one, but are silent to motion in the opposite,"}
{"text": "===== Page 15 =====\n5.1 Nonlinear Interaction among Excitation and Inhibition \n• \n131\nnull, direction. The classical experiments of Barlow and Levick (1965) inferred that\nsynaptic inhibition—now known to be GABAergic—plays a critical role by preventing\nthe cell from responding in the null direction. More recent work on rabbit ganglion cells\nreveals an additional facilitatory direction-selective component (Grzywacz and Amthor,\n1993).\nA popular model for the biophysical basis of directional selectivity is nonlinear synaptic\ninteraction between cholinergic excitation and GABAergic inhibition mediated by GABA^\nreceptors in the dendritic tree of ganglion cells (Torre and Poggio, 1978; Koch, Poggio, and\nTorre, 1982, 1983, 1986). Intracellular recordings from turtle and frog direction-selective\nganglion cells support activation of shunting inhibition in the null direction (Marchi-\nafava, 1979; Watanabe and Murakami, 1984). The very complex dendritic morphology\nof direction-selective ganglion cells (Oyster, Amthor, and Takahashi, 1993) provides an\nideal substrate for numerous local and nonlinear veto operations of the type schematized\nin Fig. 5.5B.\nYet in some ganglion cells, direction selectivity is maintained or even reversed in the\npresence of pharmacological blockers of GABA. Patch clamping the dendrites of turtle\nganglion cells revealed an excitatory input that is already direction selective (Borg-Graham,\n1991b). Furthermore, motion in the preferred direction causes a much larger increase in the\ninput conductance than motion in the null direction, in conflict with the models presented\nhere, in which the exact temporal relationship between excitation and inhibition shapes the\npostsynaptic response but the total conductance change remains invariant to the direction\nof motion (e.g., Fig. 5.3).\nIt thus appears that at least some fraction of the direction-selective response is computed\npresynaptically, most likely in cholinergic neurons possessing a unique dendritic branching\npattern called starburst amacrine cells (Masland, Mills, and Cassidy, 1984). In the rabbit,\nstarburst amacrine cells are well situated to provide synaptic input from their distal dendrites\nto directionally selective ganglion cells via graded, dendro-dendritic synapses (Vaney, Collin\nand Young, 1989). Their distinctive dendritic branching pattern is especially suited for\nlocal generation of directionally selective outputs, such that directional selectivity could\nbe implemented using a combination of nonlinear synaptic interaction and inhibitory\nkinetics that are slower than those for excitation (Borg-Graham and Grzywacz, 1992).\nIn the second scheme, excitation is always trailed by a slower inhibitory wave—if the\nactivated synaptic input is moving toward a site of synaptic output (in this case a dendritic\ntip), excitation reaches the output before the inhibition catches up. On the other hand,\nif the input is moving away from the output, the inhibition, interposed between the\nexcitation and the output, effectively shunts the excitatory current, preventing it from\ndepolarizing the output zone (see also Rail, 1964). Yet, biophysical simulations of amacrine\ncells suggest that the nonlinear interaction of excitation and inhibition probably is the\ndominant mechanism generating a direction-selective response under normal conditions\n(Borg-Graham and Grzywacz, 1992).\nThe final verdict is not in. It appears likely that even such a simple operation as\ndistinguishing the direction of a moving stimulus is implemented using a plurality of\nbiophysical mechanisms acting at several sites, some requiring inhibition and some not\n(Amthor and Grzywacz, 1993). Such redundancy might be necessary in the face of demands\nthat the circuitry wire itself up during development and retain its specificity in the face of\na constantly varying environment."}
{"text": "===== Page 16 =====\n132 • \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\n5.2 Nonlinear Interaction among Excitatory Synapses\nWhat about the interaction expected between two excitatory voltage-independent synaptic\ninputs (with synaptic reversal potential Ee and stationary conductance increases g\\ and\ngzf! If both synapses are colocalized, the somatic EPSP will be of amplitude\n(5.19)\nIn the opposite case, when the two sites 1 and 2 are electrotonically decoupled, that is,\nKYI ~^ 0, the somatic potential is given by\n(5.20)\nAssuming that Kee ~ K\\\\ ~ £22 and Kes ^ K\\_s ^ KIS (that is, 1 and 2 have the\nsame electrotonic properties), it is clear that the somatic EPSP is smaller if both synapses\ncoincide (Eq. 5.19) than if both are far apart (Eq. 5.20). The same results holds true even\nif the coupling term K\\2 is nonzero and transient inputs are considered (Koch, Poggio, and\nTorre, 1982). The reason for this sublinear addition is simply the fact that as the postsynaptic\npotential increases, the amount of current flowing through voltage-independent excitatory\nsynaptic channels decreases (Rail, 1977).\nExperimental studies of these predictions have been scant, with the large motoneurons\nproviding a favorite preparation. Two earlier studies (Burke, 1967; Kuno and Miyahara,\n1969) had concluded, based on an indirect inference, that excitatory inputs interact sublin-\nearly when located somewhere in the distal dendritic tree. In the Skydsgaard and Hounsgaard\n(1994) experiment mentioned, glutamate was applied locally to different parts of the\ndendritic tree of turtle motoneurons from two independent iontophoresis electrodes. At\nthe cell's resting potential they observed linear summation at the soma (see also Langmoen\nand Andersen, 1983), which turned supralinear as the membrane was depolarized by the\nrecording electrode (most likely due to amplifying voltage-dependent membrane conduc-\ntances). Yet basic biophysics implies nonlinear saturation due to the conductance-increasing\nnature of synaptic input. Is it possible that synaptic saturation is always compensated for\nby voltage-dependent outward currents distributed in the dendritic tree? And if so, why?\n5.2.1 Sensitivity of Synaptic Input to Spatial Clustering\nAs emphasized in Chap. 4, in the central nervous system many, if not most, neurons receive\nsynaptic input from a mixture of voltage-independent AMPA and voltage-dependent NMDA\nionotrophic synapses.\nFor instance, a major role for NMDA input in cat visual cortex is supported by experi-\nments that blocked NMDA responses using the pharmacological agent APV. This procedure\ncaused the visual response of neurons whose cell bodies are located in the upper three layers\nto be either strongly attenuated or eliminated altogether, while neurons in input layer 4 could\nstill be visually activated (Miller, Chapman, and Stryker, 1989; Fox, Sato, and Daw, 1990;\nDaw, Stein, and Fox, 1993). Thus, NMDA distribution is most likely not homogeneous\nbut layer specific.\nSuch a voltage-dependent excitatory input can implement a multiplicative-like interac-\ntion: while the EPSP produced by a single NMDA synapse might not be strong enough to\nallow current to flow through the associated channels, two or more simultaneously active"}
{"text": "===== Page 17 =====\n5.2 Nonlinear Interaction among Excitatory Synapses \n• \n133\nNMD A inputs could depolarize the potential sufficiently to relieve the Mg2+-mediated\nblockage and lead to a much larger depolarization (Fig. 4.8; Koch, 1987). Motivated by the\npossibility that such interactions can instantiate a specific class of computations underlying\ncertain forms of learning, Mel (1992, 1993, 1994; Mel and Koch, 1990; see also Brown\net al., 1992) initiated a detailed biophysical investigation of synaptic integration in the\ndendritic tree of cortical pyramidal cells on the basis of three different mechanisms: NMDA\nreceptors and calcium- and sodium-mediated active dendritic membrane conductances. In\nthis chapter we focus on the interaction occurring in a passive dendritic tree among NMDA\nsynapses, deferring the more complex set of events that can occur in a dendritic tree with\nvoltage-dependent conductances to Sec. 19.3.4.\nThe basic question Mel addresses is the following: what is the sensitivity of excitatory\ninput to spatial clustering in the postsynaptic neuron? In particular, are spatially adjacent\nsynapses in a passive dendritic tree more or less effective than the same number of synapses\nspread throughout the tree?\nTo answer these questions, Mel (1992) models the layer 5 pyramidal cell (Fig. 3.7) from\ncat visual cortex with a passive dendritic tree and a cell body that contains the two basic\nHodgkin and Huxley currents /Na and /K necessary for spike initiation (and which are\ndescribed exhaustively in the following chapter). Mel randomly distributes 100 synapses\nin clusters of k over the entire dendritic tree, like sprinkling salt over food. Figure 5.6\nillustrates this procedure for cluster sizes k — 1 and 7. In the first case 100 synapses\nare placed at random within the dendritic tree, while in the second case clusters of seven\nsynapses each (plus one cluster of two synapses) are assigned randomly to 14 locations in\nthe tree. In the absence of any NMDA input, each synapse is placed onto a dendritic spine\nand treated as a fast, glutamergic input of the AMPA type. In the alternative case, 90% of the\npostsynaptic conductance increase is of the NMDA type, using the membrane potential and\ntime dependence of Eq. 4.6 (Fig. 5.7). The NMDA-mediated synaptic current, the product of\nthe driving potential (which decreases for increasing levels of membrane depolarization) and\nthe conductance change (which first increases and subsequently saturates with increasing\nVm), increases to about —30 mV potential, after which it decreases and reverses sign at the\nsynaptic reversal potential of 0 mV. The remaining 10% of the synaptic conductance is of\nthe AMPA type.\nAll synapses (whether belonging to the same cluster or not) are activated independently\nof each other at a mean rate of 100 Hz. As a measure of cell response, Mel considers the\nnumber of spikes triggered over a 100-msec period. Using the peak somatic potential or the\ntime integral of the somatic potential (in the absence of a somatic spiking mechanism) gives\nresults that are qualitatively very similar. In the passive cell with voltage-independent inputs,\nthe somatic response following activation of 100 synapses randomly located throughout the\ndendritic arbor is much bigger than the response following activation of 10 clusters of 10\nneighboring synapses (Fig. 5.7). Indeed, the former situation triggers four spikes, while the\nlatter none (even though here the local response is larger, but, of course, fewer clusters\ncontribute toward the somatic excitability).\nIf the cell's membrane is endowed with NMDA channels, it will respond selectively\nto patterns of stimulation in which activated synapses are spatially clustered rather than\nuniformly distributed across the tree (Fig. 5.7). The cell fails to spike in response to random\nactivation of 100 NMDA \"isolated\" synapses (firing at 100 Hz), while it fires at an effective\nrate of 20 Hz if the same 100 synapses are clustered in 10 locations (Mel, 1993).\nThis preference for spatial clustering as compared to a more uniform synaptic distribution\nfor NMDA input is confirmed statistically by generating a large number (either 50 or 100)"}
{"text": "===== Page 18 =====\n134\nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nFig. 5.6 CELLULAR RESPONSE TO CLUSTERED AND NONCLUSTERED INPUT \nIn the study by Mel\n(1992), the layer 5 pyramidal cell is festooned with 100 fast excitatory synapses that are either spread out\nover 100 randomly selected individual locations (left) or clustered at 14 randomly chosen locations of 7\nsynapses each (right). In a passive dendritic tree and in the absence of NMDA input, clustered synaptic\nactivity leads to a reduced somatic response compared to the case when synapses are dispersed. For\nsynaptic input of the NMDA type, cooperativity exists, such that spatial clustering causes an enhanced\nsomatic response. Unpublished data from B. Mel, printed with permission.\nof randomized synaptic distributions of the type shown in Fig. 5.6 and averaging the results\nfor cluster sizes ranging between 1 and 15. The effect is clear and unambiguous (Fig. 5.8).\nDue to synaptic saturation, non-NMDA input will always be more effective when spread out\nin space, while the converse is true for NMDA input: as the size of the cluster is increased\n(and even though the number of cluster sites goes down since the total number of synapses\nremains fixed) the cellular response increases. For very large cluster sizes the attenuating\neffects of synaptic saturation begin to offset the excitatory effects of the NMDA voltage\ndependence.\nWhile the exact shape of the curve in Fig. 5.8B, such as the most effective cluster size,\ndepends on the details of the biophysical setting, the overall shape does not. Its convex\nshape is due to cooperativity for small cluster sizes and to synaptic saturation for large\ncluster sizes (that is, the subsynaptic potential approaches the synaptic reversal potential)\nand is extremely robust to parameter variations (Mel, 1992).\nWe conclude that due to the voltage dependency of the NMDA receptor complex,\ncooperativity pays off, in the sense that a cluster of adjacent synapses will lead to a larger\npostsynaptic response than if the synapses are randomly distributed throughout the tree.\nAs opposed to the more demanding spatial accuracy in positioning excitation and shunting\ninhibition to obtain the AND-NOT effect discussed in Sec. 5.1, cluster sensitivity does not\ndepend on the exact positioning of synapses within a cluster. Thus, it places much less\npremium on development mechanisms to carefully \"wire\" up the synaptic architecture of the\ncell. Mel (1992,1993) made no special assumptions about temporal synchronization among\nsynaptic inputs: if input to a cluster arrives as a highly synchronized wave of excitation, it"}
{"text": "===== Page 19 =====\n5.2 Nonlinear Interaction among Excitatory Synapses\n135\nFig. 5.7 DETAILS OF THE CELL'S RESPONSE TO NMDA SYNAPTIC INPUT Membrane potential at\nthe synapses (panels AI, BI) and at the soma (panels A3,83) in response to 100 uniformly distributed\nsynapses (upper panels) and to 10 clusters of 10 synapses. 90% of the synaptic conductance change is\nof the NMDA type, the remainder being voltage independent. Each synapse, whether within a cluster\nor not, is independently activated by a Poisson process of 100-Hz rate. Because in the first case,\nsynapses act \"alone,\" the average synaptic conductance (A2> and synaptic current (A4> remain modest.\nIf, however, synapses can \"cooperate\" due to spatial proximity, the effective synaptic conductance\n(82) and synaptic current flowing (84) can be much larger, inducing the cell to fire at 20 Hz. The\nsynaptic conductance changes scale inversely with the local input resistance, with high gpeak values\nat the soma and low ones in the distal dendritic tree. Reprinted by permission from Mel (1993).\ncan be much more powerful than if spread out over time. In this sense, his analysis represents\na worst-case scenario.\nThe sensitivity to spatial clumping of synaptic input is present under a broad parameter\nregime, in particular in the presence of calcium or sodium conductances in the dendritic\ntree (as we will discuss in more detail in Chap. 19). Cluster sensitivity appears to represent\na neurobiologically plausible mechanism to confer nonlinear, multiplicative properties to\nlocal submits in the dendritic tree. Such a subunit could be more rigorously defined as a\nregion of the dendritic tree within which synaptic interaction is strong and nonlinear, while\ninteraction between two or more subunits would be linear (Koch, Poggio and Torre, 1982)."}
{"text": "===== Page 20 =====\n136\nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nFig. 5.8 CLUSTER SENSITIVITY IN A PASSIVE DENDRITIC TREE Average number of spikes within\na 100-msec period using (A) zero NMDA and (B) high NMDA conditions as a function of cluster size.\nIn each case, 100 synapses are placed on spines in 100/k clusters randomly distributed throughout the\npassive tree (with uniform probability density over the dendritic length). This procedure is repeated\neither (A) 100 or (B) 50 times for cluster sizes between 1 and 15, and the cellular responses are\naveraged. Due to the cooperativity of NMDA synapses, spatial clustering increases the postsynaptic\nresponse. This basic effect is very robust to parameter variations. Reprinted in modified form by\npermission from Mel (1993).\n5.2.2 Cluster Sensitivity for Pattern Discrimination\nMel (1992) provides an illustrative example of how the nonlinear cooperative interaction\nof NMDA synapses can implement a high-level computation, discriminating one pattern\namong many others.\nThe basic idea is straightforward: if a set of synaptic afferents coding for pattern A\nterminates in a synaptic cluster on a particular neuron, and the synapses expressing some\npattern B are randomly spread throughout the cell, the cell can be said to recognize or\ndiscriminate pattern A, since activating its associated synaptic cluster makes the cell fire,\nwhile the cellular response to activation of the randomly distributed synapses will be\nnegligible. This is illustrated by showing how a neuron can discriminate among a set of 100\nblack-and-white photos from a summer vacation (Mel, 1992).\nThe coding of the gray-scale pictures is accomplished in the following manner. First 50\nrandomly chosen images, designated as training set, are convolved with four oriented Gabor\nfilters on a 64- by 64-pixel grid, resulting in a population of 16,384 orientation-selective\nvisual units. Each one of these oriented units is assumed to make a single all-or-none\noutput synapse on the layer 5 pyramidal cell in Fig. 5.6. The activity of one such unit\nwould represent the presence of a particular oriented edge at that location in the image.\nOut of this large population, the 80 most active units are selected—the output of all other\nunits being suppressed—and are mapped onto 10 cluster of eight neighboring synapses\nknown to trigger at least one spike. This process is repeated for each of the 50 training\nimages, except that a visual unit, once mapped onto the cell, is not remapped if it occurs\nin a subsequent training image. Following this \"learning\" procedure, the remaining unused\norientation-selective units (about 12,000) are mapped randomly onto individual synaptic\nsites. Although a caricature of what is expected to occur in visual cortex, this procedure\nassures that each of the 50 images is mapped onto at most 80 clustered synapses (because\nof the potential overlap among images).\nAs seen in Fig. 5.9, the cell responds to any one of the 50 training images with an\neffective spike frequency of 12.5 Hz, while the 50 randomly assigned test images fail"}
{"text": "===== Page 21 =====\n5.2 Nonlinear Interaction among Excitatory Synapses\n137\nto bring the cell above threshold. Mel (1992) also uses three sets of partially corrupted\ntraining images as input to the pyramidal cell. In the \"half/half\" stimulation paradigm, half\nof the vector components coding for one training image were swapped with half of the\ncomponents encoding another training image. In a linear perception, the superposition of\ntwo training patterns cannot be distinguished from a single training pattern. In this case the\ncell's response is significantly reduced compared to its response to a full training pattern (by\nabout 33%; Fig. 5.9). The same performance is seen if 20% of a training image is randomly\ncorrupted. If much more noise is added, the cell only responds weakly.\nThe performance of the pyramidal cell endowed with NMDA synapses is quite re-\nmarkable. The cell can be trained to respond to any one of 50 high-dimensional vectors,\ndiscriminating them from random input or vectors not beloging to the training set. The\nprobability of misclassification is 14%, with the majority being false negatives, that is,\ntraining images not recognized as such. Given the highly dispersed three-dimensional\ngeometry of dendritic and axonal arbors and the rich combinatorics possible in choosing\nparticular spatial arrangements among 10,000 afferent synapses, the true discrimination\ncapacity of even a single pyramidal cell could be much greater than demonstrated here\n(Mel, 1993, 1994; see also Brown et al., 1991a). The thread of this story is picked up\nin Sec. 14.4.2 in the context of understanding the computational capabilities of neu-\nral networks.\nFig. 5.9 NMDA ENDOWED PYRAMIDAL CELL AS PATTERN DISCRIMINATOR \nAverage response\n(over a 100-msec period) of the pyramidal cell to five types of images mapped onto its synapses.\nThe largest response is to the 50 training images and the smallest to the remaining 50 test images.\nIn the intermediate cases, training images are partially corrupted. In the \"half/half\" case, half of the\nfeatures of one image are combined with half of the features of another image. Note the significantly\nreduced response to the superposition of two training patterns, direct evidence of the nonlinear pattern\ndiscrimination ability of a neuron endowed with NMDA input. In the two other cases, either 20% or\n50% of the image is replaced by random features. Reprinted by permission from Mel (1992)."}
{"text": "===== Page 22 =====\n138 \n• \nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nTo what extent pyramidal cells in cortex implement such a scheme is difficult to ascertain\nsince the detailed microanatomy of the origin of all synapses would, by itself, not be\nsufficient to answer the question. Yet this mechanism predicts the existence of a learning\nrule that favors the placement of simultaneously active synapses in clusters on the dendritic\ntree, while uncorrelated synapses should have no priviliged spatial relationship to each other.\nBiophysical experiments involving synaptic plasticity should uncover such a clustering rule.\nThe degree of specificity required is less than that of the AND-NOT type of interaction\ndiscussed at the beginning of this chapter. Yet similar to the interaction between excitation\nand inhibition, clustering effectively fractionates the dendritic tree into many small subunits,\nwithin which nonlinear interactions take place.\n5.2.3 Detecting Coincident Input from the Two Ears\nOne situation where sublinear addition is exploited is in aiding individual neurons to detect\ncoincident inputs. In both birds and mammals, sound localization involves comparing the\ntravel time of inputs from the two ears (Konishi, 1992). If the sound source is located\nstraight ahead, there will be no delay between inputs from the two ears, while shifting the\nsource to one side will cause the auditory input from one side to be delayed relative to\ninput from the opposite side (Carr and Konishi, 1990). Individual cells can perform quite a\nremarkable job in detecting the resultant small interaural time differences. For instance, a\nneuron specialized to sounds in the 5 kilohertz auditory range reduces its firing range when\nthe delay between the two ears changes by a mere 20 /x,sec (Moiseff and Konishi, 1981).\nThis remarkable sensitivity goes hand in hand with a very low passive time constant of less\nthan 2 msec (Reyes, Rubel, and Spain, 1994).2\nAuditory brainstem cells receive binaural inputs segregated onto their two dendrites\n(Fig. 5.10), a mechanism that enhances the discriminability of sounds that arrive in phase\n(straight ahead) relative to those arriving 180° out of phase. Numerous synaptic inputs con-\nverge onto each dendrite and the associated synaptic amplitude has a significant stochastic\ncomponent (Sec. 4.2). Suppose that six simultaneous synaptic events are just enough to\ntrigger a spike in a point neuron without a dendrite. Since twice as many presynaptic\ninputs are active simultaneously when both inputs arrive in a coincident manner, the neuron\nresponds more strongly to in-phase than to out-of-phase input. In the bipolar neuron with\nsegregated inputs from the two ears to its two dendrites, the amplitude of the synaptic\nconductance is adjusted. The resulting three synaptic inputs on one dendrite, coupled with\nthree synaptic events on the other, give rise to the same firing rate as in the point neuron.\nIf—due to the stochastic nature of how many inputs arrive per sound cycle—four, five, or\nmore synaptic events were to occur on one dendrite (and not on the other), the neuron would\nstill not fire due to the synaptic saturation that limits how much current each dendrite by\nitself can deliver to the soma (see also Sec. 18.4.1). As long as the voltage threshold for\nspike initiation is set sufficiently high, synaptic current from two independent dendrites is\nrequired to trigger a spike and temporal discrimination is enhanced compared to a point\nneuron with no dendrites (Agmon-Snir, Carr, and Rinzel, 1998).\n5.3 Synaptic Microcircuits\nExcitation and inhibition by individual synapses usually have little computational signif-\nicance by themselves. It is the assembly of synapses into specific patterns of connectivity\n2. Since these cells fire at high rates, their membrane is rarely at rest and the effective time constant is probably in the\nsubmillisecond range (Gerstner et al., 1996)."}
{"text": "===== Page 23 =====\n5.3 Synaptic Microcircuits\n139\nFig. 5.10 COINCIDENCE DETECTION IN\nBIPOLAR NEURONS Two auditory brain\nstem neurons from the guinea pig that\nreceive segregated inputs from the two\nears onto their two bipolar dendrites. In a\nmodel of these cells (Agmon-Snir, Carr, and\nRinzel, 1998), the average firing rate evoked\nby the two inputs arriving synchronously\n(/max) or maximally out of phase (/min)\nis plotted as the amplitude of the synaptic\nconductance is varied. The least amount of\ndiscrimination (smallest difference between\n/max and /mi,,) is obtained in a point neuron\n(dashed curve). Synaptic saturation in the\nlong and thin dendrites (dotted curve) pre-\nvents randomness in the number of inputs\nto each side from firing the cell when inputs\narrive only to one side without coincident\ninputs from the other ear. Thicker dendrites\n(solid curve) perform at an intermediate\nlevel. Thus, dendrites enhance the ability of\nan individual cell to detect coincident input.\nReprinted by permission from Agmon-Snir,\nCarr, and Rinzel, (1998).\nwhich gives rise to identifiable building blocks, which can be found throughout the nervous\nsystem. Shepherd (1972,1978) has termed such specific synaptic arrangements, which have\na spatial extent measured in micrometers, synaptic microcircuits (Fig. 5.11). Any particular\nneuron may have dozens or hundreds of such microcircuits. Very often they will include\ndendro-dendritic synapses, that is, fast excitatory or inhibitory chemical synapses from one\ndendrite of the presynaptic cell onto a dendrite from a different, postsynaptic, cell. Examples\nof such circuits abound in the central nervous systems of both vertebrates and invertebrates.\n(For an excellent and detailed account of microcircuits in the mammalian brain see the\nmonograph by Shepherd, 1998.)\nWell-known examples include dendro-dendritic interactions among spiking and nonspik-\ning interneurons in the sensory-motor system of the locust (Laurent and Burrows, 1989;\nLaurent, 1990), synaptic arrangements among the nonspiking neurons in the vertebrate\nretina (Dowling, 1979, 1987; Sterling, 1998; Fig. 1.5), the reciprocal dendro-dendritic\ninhibition occurring among the dendrites of the mitral cells and the spines of the granule\ncells in the mammalian olfactory bulb (Rail et al., 1966; Rail and Shepherd, 1968; Woolf,\nShepherd, and Greer, 1991a,b) and the spine-triad circuit in the thalamus (Hamos et al.,\n1985; Koch, 1985; Fig. 12.8). What characterizes these synaptic arrangements is that they\ninvolve at least one excitatory and one inhibitory synapse on a very compact spatial scale,\noperate at the millisecond time scale, and exist in very large numbers (Shepherd, 1972,\n1978; Shepherd and Koch, 1998).\nCuriously, synaptic microcircuits do not appear to be numerous in mature cortical\nstructures. To a good first approximation, fast excitatory and inhibitory traffic is mediated\nfrom the presynaptic axon of one cell to the postsynaptic dendrites of another cell without\nthe involvement of higher order synaptic arrangements nor the dendro-dendritic synapses.3\n3. It could be argued that an exception to this rule are the small fraction of dendritic spines that carry both an excitatory and\nan inhibitory synaptic profile; it is doubtful, though, that such arrangements have a specific function (Sec. 12.3.3)."}
{"text": "===== Page 24 =====\n140\nSYNAPTIC INTERACTIONS IN A PASSIVE DENDRITIC TREE\nFig. 5.11 SYNAPTIC MICROCIRCUITS MEDIATING DIFFERENT TYPES OF INHIBITION \nTwo types\nof synaptic microcircuits that mediate two types of inhibition in the vertebrate nervous system. (A) The\nspine triade consists of an afferent a making an excitatory synapse onto a dendrite or spine b as\nwell as onto an interneuron c. The interneuron in turn inhibits, via a dendro-dendritic synapse, the\npostsynaptic neuron b. This feedforward inhibition is common in the retina and the thalamus (see\nSec. 12.3.4 and Fig. 12.8). (B) Recurrent inhibition is implemented by a relay cell a that excites\nan interneuron b. This interneuron inhibits, via a dendro-dendritic synapse, the original relay cell.\nDendro-dendritic reciprocal inhibition occurs between mitral and granule cells in the olfactory bulb.\nSuch microcircuits, which are specified at the micrometer level, are common in structures outside the\ncortex and in invertebrates. Reprinted by permission from Shepherd and Koch (1998).\n5.4 Recapitulation\nThe fact that a synaptic input changes the conductance in the postsynaptic membrane in\nseries with a synaptic battery and does not correspond to a constant current source ultimately\nimplies that synaptic inputs interact with each other via the membrane potential. In particular,\nthe somatic potential in a passive tree in response to two or more inputs is not equal to the\nsum of the individual synaptic components. We explored the computational consequences\nof this in two cases.\nThe interaction between voltage-independent non-NMDA excitatory input and shunting\ninhibition (that is, when £\",- reverses close to the cell's resting membrane potential) in\nthe subthreshold domain can mediate a veto operation that is specific in space and time.\nIf inhibition is adjacent to the excitatory synapse or on the path between excitation and\nthe cell body and if their time courses overlap, inhibition can effectively suppress the\neffect of excitation. This implies that specific synaptic arrangements in a dendritic tree\ncan implement logic-like AND-NOT operations, possibly one of the crucial nonlinearities\nunderlying direction selectivity in retinal ganglion cells.\nThe specificity of synaptic interaction represents at the same time also its greatest\nweakness, in the sense that it places great demands on developmental mechanisms to\nprecisely guide synapses and dendrites during development. A more plausible synaptic\narrangement could be implemented at the level of synaptic populations: if excitatory and\ninhibitory synapses are colocalized onto the same part of the dendritic tree, excitatory input\ncan always override any inhibitory influence. Conversely, if inhibition is at a different site,\nfor instance, close to the spike initiating zone, and excitation at more distal sites, then any\nexcitatory input can always be vetoed by inhibition. These two types of synaptic placements\nmight instantiate different kinds of suppressive behaviors. Under certain conditions the\nthreshold for initiating a reflex should be elevated (relative suppression) while under others\nthe behavior needs to be totally abolished (absolute suppression)."}
{"text": "===== Page 25 =====\n5.4 Recapitulation \n• \n141\nIf GABAergic inhibition has a reversal potential much below the membrane resting\npotential, as in the fly tangential interneurons, inhibition tends to act akin to a linear\nsubtraction.\nA more plausible mechanism to implement multiplicative behavior involves NMDA\nsynapses clustered over the dendritic tree. When clusters of adjacent non-NMDA synapses\nare randomly sprinkled around the dendritic tree of a pyramidal cell, their activation\ncauses a smaller cellular response than when the synapses are isolated from each other,\na consequence of synaptic saturation. A very different behavior is obtained with clusters of\nvoltage-dependent NMDA synapses. Because of their cooperative nature, clusters of 6 to\n10 adjacent NMDA synapses are much more effective than the same number of synapses\nby themselves. A dendritic tree endowed with such synapses can be used to implement\na very efficient nonlinear pattern discriminator that is robust to the presence of dendritic\nnonlinearities, while also serving as a plausible biophysical mechanism for multiplication,\nwhich is required for a host of computations, such as motion, binocular disparity, and tabular\nlook-up storage.\nBoth case studies imply that the dendritic tree, rather than just performing a filtering\noperation onto the synaptic input, as suggested by linear cable theory presented in Chaps. 2\nand 3, can be partitioned into numerous spatial subunits. Within each such subunit, synaptic\ninputs interact nonlinearly, while the interaction between two or more subunits is approxi-\nmately linear.\nFinally, we mentioned the concept of a synaptic microcircuit, pioneered by Shepherd\n(1978, 1998). These usually involve a combination of one or several excitatory and\ninhibitory synapses with a predilection for a specific arrangement among two or three\nneurons and are common in extracortical structures such as the retina, the olfactory bulb,\nand the thalamus and in invertebrates."}
{"text": "===== Page 7 =====\n148\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nFig. 6.3 VOLTAGE DEPENDENCY OF THE GATING PARTICLES Time constants (A) and steady-\nstate activation and inactivation (B) as a function of the relative membrane potential V for sodium\nactivation m (solid line) and inactivation h (long dashed line) and potassium activation n (short,\ndashed line). The steady-state sodium inactivation hx is a monotonically decreasing function of V,\nwhile the activation variables nx and m^ increase with the membrane voltage. Activation of the\nsodium and potassium conductances is a much steeper function of the voltage, due to the power-law\nrelationship between the activation variables and the conductances. Around rest, Gfla increases e-fold\nfor every 3.9 mV and GR for every 4.8 mV. Activating the sodium conductance occurs approximately\n10 times faster than inactivating sodium or activating the potassium conductance. The time constants\nare slowest around the resting potential.\n6.2.2 Sodium Current /Na\nAs can be seen on the left hand side of Fig. 6.4, the dynamics of the sodium conductance\nthat we will explore now are substantially more complex.\nIn order to fit the kinetic behavior of the sodium current, Hodgkin and Huxley had to\npostulate the existence of a sodium activation particle m as well as an inactivation particle h,\n(6.13)\nwhere GNH is the maximal sodium conductance, G^a = 120 mS/cm2, andENa is the sodium\nreversal potential, E^a = 115 mV, relative to the axon's resting potential, m and h are\ndimensionless numbers, with 0< m, h < 1. By convention the sodium current is negative,\nthat is, inward, throughout the physiological voltage range (for V < E^a; see Fig. 6.5).\nThe amplitude of the sodium current is contingent on four hypothetical gating particles\nmaking independent first-order transitions between an open and a closed state. Since these"}
{"text": "===== Page 8 =====\n6.2 Activation and Inactivation States\n149\nFig. 6.4 K+ AND NA+ CONDUCTANCES DURING A VOLTAGE STEP Experimentally recorded\n(circles) and theoretically calculated (smooth curves) changes in G^a and GK in the squid giant\naxon at 6.3° C during depolarizing voltage steps away from the resting potential (which here, as\nthroughout this chapter, is set to zero). For large voltage changes, GNS briefly increases before it\ndecays back to zero (due to inactivation), while GK remains activated. Reprinted by permission from\nHodgkin (1958).\nparticles are independent, the probability for the three m and the one h particle to exist\nin this state is nr'h. Notice that h is the probability that the inactivating particle is not in\nits inactivating state. Formally, the temporal change of these particles is described by two\nfirst-order differential equations,\n(6.14)\nand\n(6.15)\nEmpirically, Hodgkin and Huxley derived the following equations for the rate constants:\n(6.16)\n(6.17)\n(6.18)\n(6.19)\nThe associated time constants and steady-state variables are plotted in Fig. 6.3 as a function"}
{"text": "===== Page 9 =====\n150 \n• \nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nFig. 6.5 HODGKIN-HUXLEY AC-\nTION POTENTIAL Computed ac-\ntion potential in response to a 0.5-\nmsec current pulse of 0.4-nA am-\nplitude (solid lines) compared to\na subthreshold response following\na 0.35-nA current pulse (dashed\nlines). (A) Time course of the two\nionic currents. Note their large sizes\ncompared to the stimulating current.\n(B) Membrane potential in response\nto threshold and subthreshold stim-\nuli. The injected current charges\nup the membrane capacity (with an\neffective membrane time constant\nr =0.85 msec), enabling sufficient\n/Na to be recruited to outweigh the\nincrease in /K (due to the increase in\ndriving potential). The smaller cur-\nrent pulse fails to trigger an action\npotential, but causes a depolariza-\ntion followed by a small hyperpo-\nlarization due to activation of /K-\n(C) Dynamics of the gating parti-\ncles. Sodium activation m changes\nmuch more rapidly than either h or\nn. The long time course of potas-\nsium activation n explains why the\nmembrane potential takes 12 msec\nafter the potential has first dipped\nbelow the resting potential to return\nto baseline level.\nof voltage. Similar to before, both rm and TH are bell-shaped curves,2 but with a tenfold\ndifference in duration. While m^ is a monotonically increasing function of V, as expected of\nan activation variable, h^ decreases with increasing membrane depolarization, the defining\nfeature of an inactivating particle. Without inactivation, the sodium conductance would\nremain at its maximum value in response to a depolarizing voltage step.\nThe fraction of the steady-state sodium conductance open at rest is less than 1% of\nthe peak sodium conductance. Inspection of Fig. 6.3 immediately reveals the reason: for\nvoltages below or close to the resting potential of the axon, the activation variable m is\nclose to zero while at positive potentials the inactivation variable h is almost zero. Thus,\n2. Note that the voltage-dependent membrane time constant for the activation variable Tm has the same symbol as the passive\nmembrane time constant. When in doubt, we will refer to the latter simply as T."}
{"text": "===== Page 10 =====\n6.3 Generation of Action Potentials \n• \n151\nthe steady-state sodium current G^am^h^V — E^a), also known as the window current,\nis always very small. The secret to obtaining the large sodium current needed to rapidly\ndepolarize the membrane lies in the temporal dynamics of m and h. At values of the\nmembrane close to the resting potential, h takes on a value close to 1. When a sudden\ndepolarizing voltage step is imposed onto the membrane as in Fig. 6.4, m changes within\na fraction of a millisecond to its new value close to 1, while h requires 5 msec or longer\nto relax from its previous high value to its new and much smaller value. In other words,\ntwo processes control the sodium conductance: activation is the rapid process that increases\nGNa upon depolarization and outpaces inactivation, the much slower process that reduces\nGNS upon depolarization.\n6.2.3 Complete Model\nSimilar to most other biological membranes, the axonal membrane contains a voltage-\nindependent \"leak\" conductance Gm, which does not depend on the applied voltage and re-\nmains constant over time. The value measured by Hodgkin and Huxley, Gm =0.3 mS/cm2,\ncorresponds to a passive membrane resistivity of Rm — 3333 £2 • cm2. The passive\ncomponent also has a reversal potential associated with it. Hodgkin and Huxley did\nnot explicitly measure Vrest, but adjusted it so that the total membrane current at the\nresting potential V = 0 was zero. In other words, Vrest was denned via the equation\nGNa(0)£Na + GK(O)£K + Gm Vrest = 0» and came out to be +10.613 mV. The membrane\ncapacity Cm = I /iF/cm2. At the resting potential, the effective membrane resistance due\nto the presence of the sum of the leak, the potassium, and the (tiny) sodium conductances\namounts to 857 £2 • cm2, equivalent to an effective \"passive\" membrane time constant of\nabout 0.85 msec.\nWe can now write down a single equation for all the currents flowing across a patch of\naxonal membrane,\n(6.20)\nwhere /jnj is the current that is injected via an intracellular electrode. This nonlinear\ndifferential equation, in addition to the three ordinary linear first-order differential equations\nspecifying the evolution of the rate constants (as well as their voltage dependencies),\nconstitutes the four-dimensional Hodgkin and Huxley model for the space-clamped axon\nor for a small patch of membrane. Throughout the book, we shall refer to Eq. 6.20, in\ncombination with the rate constants (Eqs. 6.7, 6.14, and 6.15) at 6.3° C as the standard\nHodgkin-Huxley membrane patch model. In our simulations of these equations, we solve\nEq. 6.20 for an equipotential 30 x 30 x n /iim2 patch of squid axonal membrane and\ntherefore express lm in units of nanoamperes (nA), and not as current density.\nWe will explain in the following sections how this model reproduces the stereotyped\nsequence of membrane events that give rise to the initiation and propagation of all-or-none\naction potentials.\n6.3 Generation of Action Potentials\nOne of the most remarkable aspect of the axonal membrane is its propensity to respond in\neither of two ways to brief pulses of depolarizing inward current. If the amplitude of the\npulse is below a given threshold, the membrane will depolarize slightly but will return to the"}
{"text": "===== Page 11 =====\n152 \n• \nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nmembrane's resting potential, while larger currents will induce a pulse-like action potential,\nwhose overall shape is relatively independent of the stimulus required to trigger it.\nConsider the effect of delivering a short (0.5-msec) inward current pulse /inj (t) of 0.35-\nnA amplitude to the membrane (Fig. 6.5). The injected current charges up the membrane\ncapacitance, depolarizing the membrane in the process. The smaller this capacitance, the\nfaster the potential will rise. The depolarization has the effect of slightly increasing m and n,\nin other words, increasing both sodium and potassium activation, but decreasing h, that is,\ndecreasing potassium inactivation. Because the time constant of sodium activation is more\nthan one order of magnitude faster than rn and TV, at these voltages, we can consider the\nlatter two for the moment to be stationary. But the sodium conductance GNS will increase\nsomewhat. Because the membrane is depolarized from rest, the driving potential for the\npotassium current, V — £K, has also increased. The concomitant increase in /K outweighs\nthe increase in /Na due to the increase in G^a and the overall current is outward, driving\nthe axon's potential back toward the resting potential. The membrane potential will slightly\nundershoot and then overshoot until it finally returns to Vrest. The oscillatory response\naround the resting potential can be attributed to the small-signal behavior of the potassium\nconductance acting phenomenologically similar to an inductance (see Chap. 10 for further\ndiscussion).\nIf the amplitude of the current pulse is increased slightly to 0.4 nA, the depolarization\ndue to the voltage-independent membrane components will reach a point where the amount\nof /Na generated exceeds the amount of /K. At this point, the membrane voltage undergoes a\nrunaway reaction: the additional 7Na depolarizes the membrane, further increasing m, which\nincreases /Na, causing further membrane depolarization. Given the almost instantaneous\ndynamics of sodium activation (rm ~ 0.1-0.2 msec at these potentials), the inrushing\nsodium current moves the membrane potential within a fraction of a millisecond to 0 mV\nand beyond. In the absence of sodium inactivation and potassium activation, this positive\nfeedback process would continue until the membrane would come to rest at E^a. As we\nsaw already in Fig. 6.4, after a delay both the slower sodium inactivation variable h as well\nas the potassium activation n will turn on (explaining why /K is also called the delayed\nrectifier current /DR)- Sodium inactivation acts to directly decrease the amount of sodium\nconductance available, while the activation of the potassium conductance tends to try to\nbring the axon's membrane potential toward EH by increasing /K. Thus, both processes\ncause the membrane potential to dip down from its peak. Because the total sodium current\nquickly falls to zero after 1 msec, but /K persists longer at small amplitudes (not readily\nvisible in Fig. 6.5), the membrane potential is depressed to below its resting level, that is,\nthe axon hyperpolarizes. At these low potentials, eventually potassium activation switches\noff, returning the system to its initial configuration as V approaches the resting potential.\n6.3.1 Voltage Threshold for Spike Initiation\nWhat are the exact conditions under which a spike is initiated? Does the voltage have to\nexceed a particular threshold value V^, or does a minimal amount of current 7^ have to\nbe injected, or does a certain amount of electrical charge Qth have to be delivered to the\nmembrane in order to initiate spiking? These possibilities and more have been discussed in\nthe literature and experimental evidence exists to support all of these views under different\ncircumstances (Hodgkin and Rushton, 1946; Cooley, Dodge, and Cohen, 1965; Noble and\nStein, 1966; Cole, 1972;Rinzel, 1978; for a thorough discussion see Jack, Noble, and Tsien,\n1975). Because the squid axon is not a good model for spike encoding in central neurons,"}
{"text": "===== Page 12 =====\n6.3 Generation of Action Potentials\n153\nwe will defer a more detailed discussion of this issue to Sees. 17.3 and 19,2. We here\nlimit ourselves to considering spike initiation in an idealized nonlinear membrane, without\ndealing with the complications of cable structures (such as the axon).\nTo answer this question, we need to consider the I-V relationship of the squid axonal\nmembrane. Because we are interested in rapid synaptic inputs, we assume that the risetime\nof the synaptic current is faster than the effective passive time constant, T = 0.85 msec,\nand make use of the observation that the dynamics of sodium activation m is very rapid (the\nassociated time constant is always less than 0.5 msec) and at least a factor of 10 faster than\nsodium inactivation h and potassium activation n (see Fig. 6.3). With these observations\nin mind, we ask what happens if the input depolarizes the membrane very rapidly to a new\nvalue VI Let us estimate the current that will flow with the help of the instantaneous I-V\nrelationship Io(V) (Fig. 6.6).\n/o is given by the sum of the ionic and the leak currents. We approximate the associated\nsodium and potassium conductances by assuming that h and n have not had time to change\nfrom the value they had at the resting potential V — 0, while m adjusts instantaneously to\nits new value at V. In other words,\n(6.21)\nFigure 6.6 shows the inverted U-form shape of /o in the neighborhood of the resting\npotential, as well as its three ionic components /Na, /K» and I\\ea^.\nIn the absence of any input, the system rests at V = 0. If a small depolarizing voltage\nstep is applied, the system is displaced to the right, generating a small, positive current. This\ncurrent is outward since the increase in m (increasing the amplitude of /Na) is outweighed\nby the increase in the driving potential V — EK (increasing 7K) and the decrease in /leak-\nFig. 6.6 CURRENT-VOLTAGE RELATIONSHIP AROUND REST Instantaneous I-V \nrelationship,\n/o, associated with the standard patch of squid axon membrane and its three components: /o =\n/Na + IK. + /leak (Eq. 6.21). Because m changes much faster than either h or n for rapid inputs, we\ncomputed G^ and GK. under the assumption that m adapts instantaneously to its new value at V,\nwhile h and n remain at their resting values, /o crosses the voltage axis at two points: a stable point at\nV = 0 and an unstable one at Va, « 2.5 mV. Under these idealized conditions, any input that exceeds\nFO, will lead to a spike. For the \"real\" equations, m does not change instantaneously and nor do n and\nh remain stationary; thus, /o only crudely predicts the voltage threshold which is, in fact, 6.85 mV\nfor rapid synaptic input. Note that /o is specified in absolute terms and scales with the size of the\nmembrane patch."}
{"text": "===== Page 13 =====\n154 \n. \nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nThis forces the membrane potential back down toward the resting potential: the voltage\ntrajectory corresponds to a subthreshold input. Similarly, if a hyperpolarizing current step\nis injected, moving the system to below V = 0, a negative inward current is generated,\npulling the membrane back up toward Vrest. The slope of the I-V curve around the resting\npotential dl/dV, termed the membrane slope conductance, is positive (for a substantial\ndiscussion of this concept, see Sec. 17.1.2). That is, the point V = 0 is a stable attractor.\n(For these and related notions, we defer the reader to the following chapter.)\n/o( V) has a second zero crossing at V = V& « 2.5 mV. If an input moves the membrane\npotential to exactly Vth, n° current flows and the system remains at Vth (Fig- 6.6). Because\nthe slope conductance is negative, the point is unstable, and an arbitrarily small perturbation\nwill carry the system away from the zero crossing. A negative perturbation will carry the\nsystem back to Vrest- Conversely, a positive voltage displacement, no matter how minute,\ncauses a small inward current to flow, which further depolarizes the membrane (due to the\nnegative slope conductance), leading in turn to a larger inward current, and so on. The\nmembrane potential rapidly increases to above absolute zero, that is, an action potential is\ntriggered. During this phase, very large inward currents are generated, far exceeding the\namplitude of the modest stimulus current. (Recall that around these potentials, /Na increases\ne-fold every 3.9 mV.) For the patch of squid membrane simulated here (where the current\nscales linearly with the area of the patch), the peak of /Na is about 23 nA.\nThis qualitative account of the origin of the voltage threshold for an active patch of\nmembrane argues that in order for an action potential to be initiated, the net inward current\nmust be negative. For rapid input, this first occurs at V = Vth- This analysis was based on the\nrather restrictive assumption that m changes instantaneously, while h and n remain fixed. In\npractice, neither assumption is perfect. Indeed, while our argument predicts Vth = 2.5 mV,\nthe voltage threshold for spike initiation for rapid EPSPs for the Hodgkin-Huxley equation\nis, in fact, equal to 6.85 mV (Noble and Stein, 1966). As discussed in Sec. 17.3, reaching\na particular value of the voltage for a rapid input in a single compartment is equivalent to\nrapidly dumping a threshold amount of charge £)th into the system.\nApplying a current step that increases very slowly in amplitude—allowing the system to\nrelax always to its stationary state—prevents any substantial sodium current from flowing\nand will therefore not cause spiking. Thus, not only does a given voltage level have to\nbe reached and exceeded but also within a given time window. We take up this issue in\nSec. 17.3 in the context of our full pyramidal cell model and in Sec. 19.2 to explore how\nVth is affected by the cable structure.\n6.3.2 Refractory Period\nOnce the rapid upstroke exceeding 0 has been generated, the membrane potential should\nbe pulled back to its resting potential, that is, repolarized, as rapidly as possible in order to\nenable the system to generate the next impulse.3\nThis is accomplished by inactivating GNJJ and by increasing a potassium conductance,\nGK- This conductance remains activated even subsequent to spike polarization (for up to\n12 msec following the peak of the action potential in Fig. 6.5), causing the membrane\nto undergo a hyperpolarization. During this period, it is more difficult to initiate an action\npotential than before; the membrane remains in a refractory state. The reason for the reduced\nability of the membrane to discharge again is the inactivation of 7Na (that is, h is small) and\nthe continuing activation of /K (n only decays slowly).\n3. Given a specific membrane capacitance of 1 /ctF/cm2, the 100 mV spike depolarization amounts to transferring about 6,000\npositively charged ions per n,m of membrane area."}
{"text": "===== Page 14 =====\n6.3 Generation of Action Potentials\n155\nThis refractory period can be documented by the use of a second current pulse (Fig. 6.7).\nAt t = 1 msec a 0.5-msec current pulse is injected into our standard patch of squid axonal\nmembrane. The amplitude of this pulse, I\\ = 3.95 nA, is close to the minimal one needed\nto generate an action potential. The input causes a spike to be triggered that peaks at\naround 5 msec and repolarizes to V = 0 at t = 1 msec. This time, at which the membrane\npotential starts to dip below the resting potential (Fig. 6.5), is somewhat arbitrarily assigned\nto Af = 0. Following this point, a second 0.5-msec-long current pulse of amplitude /2 is\napplied A? msec later. The amplitude of /2 is increased until a second action potential is\ngenerated. This first occurs at At = 2 msec (that is, 2 msec after the membrane potential\nhas repolarized to zero). At this time, h/I\\ = 23.7, that is, the amplitude of the second\npulse must be 23.7 times larger than the amplitude of the first pulse in order to trigger a\nspike. Since such large current amplitudes are unphysiological, the membrane is de facto\nnot excitable during this period, which is frequently referred to as the absolute refractory\nperiod. The threshold for initiation of the second spike is elevated up to 11 msec after\nrepolarization of the membrane due to the first spike (relative refractory period; Fig. 6.7).\nThis is followed by a brief period of mild hyperexcitability, when a spike can be elicited by\na slightly (15%) smaller current than under resting conditions.\nFrom a computational point of view, it is important to realize that the threshold behavior\nof the Hodgkin-Huxley model depends on the previous spiking history of the membrane.\nIn the squid axon, as in most axons, the threshold rises only briefly, returning to baseline\nlevels after 20 msec or less. As warming the axon to body temperatures speeds up the rates\nof gating two- to fourfold,4 the minimal separation time is expected to be only 1-2 msec\nFig. 6.7 REFRACTORY PERIOD A 0.5-msec brief current pulse of I\\ = 0.4 nA amplitude causes\nan action potential (Fig. 6.5). A second, equally brief pulse of amplitude /2 is injected At msec after\nthe membrane potential due to the first spike having reached V = 0 and is about to hyperpolarize\nthe membrane. For each value of Af, /2 is increased until a second spike is generated (see the inset\nfor Af = 10 msec). The ratio h/h of the two pulses is here plotted as a function of At. For\nseveral milliseconds following repolarization, the membrane is practically inexcitable since such large\ncurrents are unphysiological (absolute refractory period). Subsequently, a spike can be generated,\nbut it requires a larger current input (relative refractory period). This is followed by a brief period of\nreduced threshold (hyperexcitability). No more interactions are observed beyond about A? = 18 msec.\n4. A crucial parameter in determining the dynamics of the action potential is the temperature. As first mentioned in footnote\n5 in Sec. 4.6, if the temperature is reduced, the rate at which the ionic channels underlying the action potential open or close\nslows down, while the peak conductance remains unchanged. Hodgkin and Huxley recorded most of their data at 6.3° C and\nthe rate constants are expressed at these temperatures (Eqs. 6.10, 6.11 and 6.16-6.19). To obtain the action potential at any other\ntemperature T, all a's and /8's need to be corrected by Q\\0 \n,with 210 between 2 and 4 (Hodgkin, Huxley, and Katz, 1952;"}
{"text": "===== Page 15 =====\n156 • \nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nfor axons in warm-blooded animals. Nerve cells—as compared to axons—often display a\nmuch longer increase in their effective spiking threshold, depending on the number of action\npotentials generated within the last 100 msec or longer (Raymond, 1979). Section 9.3 will\ntreat the biophysical mechanism underlying this short-term firing frequency adaptation in\nmore detail.\n6.4 Relating Firing Frequency to Sustained Current Input\nWhat happens if a long-lasting current step of constant amplitude is injected into the space-\nclamped axon (Agin, 1964; Cooley, Dodge, and Cohen, 1965; Stein, 1967a)? If the current\nis too small, it will give rise to a persistent subthreshold depolarization (Fig. 6.8). Plotting\nthe steady-state membrane depolarization as a function of the applied membrane current\n(Fig. 6.9A) reveals the linear relationship between the two. If the input is of sufficient\namplitude to exceed the threshold, the membrane will generate a single action potential\n(Fig. 6.8). The minimal amount of sustained current needed to generate at least one action\npotential (but not necessarily an infinite train of spikes) is called rheobase (Cole, 1972).\nFor our standard membrane patch, rheobase corresponds to 0.065 nA. (This current is\nobviously far less than the amplitude of the brief current pulse used previously.) After the\nspike has been trigged and following the afterhyperpolarization, V(t) stabilizes at around\n2 mV positive to the resting potential, limiting the removal of sodium inactivation as well\nas enhancing /K. As the current amplitude is increased, the offset depolarization following\nthe action potential and its hyperpolarization increases until, when the amplitude of the\ncurrent step is about three times rheobase (0.175 nA), a second action potential is initiated.\nAt around 0.18 nA (/i in Fig. 6.9A), the Hodgkin-Huxley equations will start to generate\nan indefinite train of spikes, that is, the membrane fires repetitively. After the membrane\npotential goes through its gyrations following each action potential, V creeps past Vth and\nthe cycle begins anew: the system travels on a stable limit cycle. In a noiseless situation, the\ninterval between consecutive spikes is constant and the cell behaves as a periodic oscillator\nwith constant frequency.\nFigure 6.9A shows the associated steady-state I—V relationship. Experimentally, it can\nbe obtained by clamping the membrane potential to a particular value V and measuring\nthe resultant clamp current 7. The equations generate infinite trains of action potentials for\nI > /i (dashed line in Fig. 6.9A).\nA mathematical curiosity of uncertain relevance is the observation that for a range of\namplitudes of the current step, the Hodgkin-Huxley equations can display several solutions,\nincluding the stable and periodic oscillation emphasized here, a stable but steady-state\ndepolarizing solution, and an unstable periodic solution (Troy, 1978; Rinzel and Miller,\n1980). Which one is actually realized depends on the initial conditions.\nIf the current amplitude is further increased, the interspike intervals begin to decrease and\nthe spiking frequency increases. Figure 6.9B shows the relationship between the amplitude\nof the injected current and the spiking frequency around threshold, and Fig. 6.10A over\na larger current range. It is referred to as the frequency-current or /-/ \ncurve. Overall,\n(continued) Beam and Donaldson, 1983; for a definition of Q\\Q see footnote 5 in Sec. 4.6). The Q\\Q for the peak conductances is\na modest 1.3. As the temperature is increased, the upstroke, that is, the rate at which the voltage rises during the rapid depolarizing\nphase of the action potential, increases, because the speed at which /Na is activated increases. At the same time, both sodium\ninactivation and potassium activation increase. Altogether, the total duration of the spike decreases. At temperatures above 33° C no\nspike is generated (Hodgkin and Katz, 1949; of course, the squid axon lives in far more frigid waters than these balmy temperatures)."}
{"text": "===== Page 16 =====\n6.4 Relating Firing Frequency to Sustained Current Input \n• \n157\nFig. 6.8 REPETITIVE SPIKING \nVoltage trajectories\nin response to current steps of various amplitudes in\nthe standard patch of squid axonal membrane. The\nminimum sustained current necessary to initiate a\nspike, termed rheobase, is 0.065 nA. In order for the\nmembrane to spike indefinitely, larger currents must\nbe used. Experimentally, the squid axon usually stops\nfiring after a few seconds due to secondary inactiva-\ntion processes not modeled by the Hodgkin-Huxley\nequations (1952d).\nFig. 6.9 SUSTAINED SPIKING IN THE HODGKIN-HUXLEY EQUATIONS \n(A) Steady-state I-V\nrelationship and (B) /-/ or discharge curve as a function of the amplitude of the sustained current /\nassociated with the Hodgkin-Huxley equations for a patch of squid axonal membrane. For currents\nless than 0.18 nA, the membrane responds by a sustained depolarization (solid curve). At /i, the\nsystem loses its stability and generates an infinite train of action potentials: it moves along a stable\nlimit cycle (dashed line). A characteristic feature of the squid membrane is its abrupt onset of firing\nwith nonzero oscillation frequency. The steady-state I-V curve can also be viewed as the sum of all\nsteady-state ionic currents flowing at any particular membrane potential Vm.\nthere is a fairly limited range of frequencies at which the membrane fires, between 53 and\n138 Hz. If a current at the upper amplitude range is injected in the axon, the membrane fails\nto repolarize sufficiently between spikes to relieve sodium inactivation. Thus, although"}
{"text": "===== Page 17 =====\n158\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nFig. 6.10 HODGKIN-HUXLEY /-/ \nCURVE AND NOISE (A) Relationship between the amplitude\nof an injected current step and the frequency of the resultant sustained discharge of action potentials\n(/-/ \ncurve) for a membrane patch of squid axon at 6.3° C (solid line) and its numerical fit (dashed\nline) by / = 33.2 log / + 106. Superimposed in bold is the /-/ curve for the standard squid axon\ncable (using normalized current). Notice the very limited bandwidth of axonal firing. (B) /-/ \ncurve\nfor the membrane patch case around its threshold (rheobase) in the presence of noise. White (2000-\nHz band-limited) current noise whose amplitude is Gaussian distributed with zero mean current is\nadded to the current stimulus. In the absence of any noise (solid line) the /-/ curve shows abrupt\nonset of spiking. The effect of noise (dotted curve—standard deviation of 0.05 nA; dashed curve—\n0.1 n A) is to linearize the threshold behavior and to increase the bandwidth of transmission (stochastic\nlinearization). Linear /-/ curves are also obtained when replacing the continuous and deterministic\nHodgkin-Huxley currents by discrete and stochastic channels (see Sec. 8.3).\nthe membrane potential does show oscillatory behavior, no true action potentials are\ngenerated.\nIn the laboratory, maintained firing in the squid axon is not that common (Hagiwara and\nOomura, 1958; see, however, Chapman, 1963). This is most likely due to secondary inacti-\nvation mechanisms which are not incorporated into the Hodgkin-Huxley equations. Yet for\nshort times, the theoretical model of Hodgkin and Huxley makes reasonably satisfactory\npredictions of the behavior of the space-clamped axon (for a detailed comparison between\nexperimental observations and theoretical predictions see Guttman and Barnhill, 1970 as\nwell as Chap. 11 in the ever trustworthy Jack, Noble, and Tsien, 1975), in particular with\nrespect to the small dynamic range of firing frequencies supported by the axonal membrane\nand the abrupt onset of spiking at a high firing frequency. The /-/ \ncurve can be well\napproximated by either a square root or a logarithmic relationship between frequency and\ninjected current (Agin, 1964; see Fig. 6.10).\nThe /-/ \ncurves of most neurons, and not just the squid axon, bend over and saturate\nfor large input currents. This justifies the introduction of a smooth, sigmoidal nonlinearity"}
{"text": "===== Page 18 =====\n6.5 Action Potential Propagation along the Axon \n• \n159\nmimicking the neuronal input-output transduction process in continuously valued neural\nnetwork models (Hopfield, 1984). It is important to keep in mind that the paradigm under\nwhich the /-/ \ncurves are obtained, sustained current input* represents only a very crude\napproximation to the dynamic events occurring during synaptic bombardment of a cell\nleading to very complex spike discharge patterns (see Chap. 14).\nAn important feature of the Hodgkin-Huxley model is that the frequency at the onset of\nrepetitive activity has a well-defined nonzero minimum (about 53 Hz at 6.3° C; Fig. 6.10B).\nThe membrane is not able to sustain oscillations at lower frequencies. This behavior,\ngenerated by a so-called Hopf bifurcation mechanism, is generic to a large class of oscillators\noccurring in nonlinear differential equations (Cronin, 1987; Rinzel and Ermentrout, 1998)\nand will be treated in more detail in the following chapter.\nAs first explicitly simulated by Stein (1967b), adding random variability to the input can\nincrease the bandwidth of the axon by effectively increasing the range within which the\nmembrane can generate action potentials. If the input current is made to vary around its mean\nwith some variance, reflecting for instance the spontaneous release of synaptic vesicles, the\nsharp discontinuity in the firing frequency at low current amplitudes is eliminated, since even\nwith an input current that is on average below threshold, the stimulus will become strong\nenough to generate an impulse with a finite, though small, average frequency. Depending\non the level of noise, the effective minimal firing frequency can be reduced to close to\nzero (Fig. 6.1 OB). A similar linearization behavior can be obtained if the continuous,\ndeterministic, and macroscopic currents inherent in the Hodgkin-Huxley equations are\napproximated by the underlying discrete, stochastic, and microscopic channels (Skaugen\nand Walloe, 1979; see Sec. 8.3).\nAdding noise to a quantized signal to reduce the effect of this discretization is a standard\ntechnique in engineering known as dithering or stochastic linearization (Gammaitoni, 1995;\nStemmler, 1996).\nA large number of neurons can generate repetitive spike trains with arbitrarily small\nfrequencies. As first shown by Connor and Stevens (1971c) in their Hodgkin-Huxley-like\nmodel of a gastropod nerve cell, the addition of a transient, inactivating potassium current\n(termed the I A current) enables the cell to respond to very small sustained input currents\nwith a maintained discharge of very low frequency. (This topic will be further pursued in\nSec. 7.2.2.) Such low firing frequencies are also supported by pyramidal cells (Fig. 9.7).\n6.5 Action Potential Propagation along the Axon\nOnce the threshold for excitation has been exceeded, the all-or-none action potential can\npropagate from the stimulus site to other areas of the axon. The hypothesis that this\npropagation is mediated by cable currents flowing from excited to neighboring, nonexcited\nregions was suggested already around the turn of the century by Hermann (1899). It was\nnot until Hodgkin (1937) that direct experimental proof became available. A quantitative\ntheory of this propagation had to await Hodgkin and Huxley's 1952 study. Because this\nhas been a very well explored chapter in the history of biophysics, we will be brief here,\nonly summarizing the salient points. Chapter 10 in Jack, Noble, and Tsien (1975) provides a\ndeep and thorough coverage of nonlinear cable theory as applied to the conduction of action\npotentials. Section 19.2 will deal with how cable structures, such as an infinite cylinder,\naffect the voltage threshold for spike initiation."}
{"text": "===== Page 19 =====\n160\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\n6.5.1 Empirical Determination of the Propagation Velocity\nThe equivalent electrical circuit replicates the patch of sodium, potassium, and leak conduc-\ntances and batteries (Fig. 6.2) along the cable in a fashion we are already familiar with from\nthe passive cable (Fig. 6.11). Equation 2.5 specifies the relationship between the membrane\ncurrent (per unit length) and the voltage along the cable,\n(6.22)\nIn Eq. 6.20, we derived the membrane current (per unit area) flowing in a patch of axonal\nmembrane. Combining the two with the appropriate attention to scaling factors leads to\nan equation relating the potential along the axon to the electrical property of the active\nmembrane,\n(6.23)\nwhere d is the diameter of the axon. Hodgkin and Huxley (1952d) used a d = 0.476 mm\nthick axon in their calculations and a value of /?, = 35.4 Q-cm. This nonlinear partial\ndifferential equation, in conjunction with the three equations describing the dynamics of\nm, h, and n and the appropriate initial and boundary conditions, constitutes the complete\nHodgkin-Huxley model.\nThis type of second-order equation, for which no general analytical solution is known,\nis called a reaction-diffusion equation, because it can be put into the form of\n(6.24)\nwith D > 0 constant. We will meet this type of equation again when considering the\ndynamics of intracellular calcium (see Chap. 11). Under certain conditions, it has wave-like\nsolutions.\nBecause Hodgkin and Huxley only had access to a very primitive hand calculator,\nthey could not directly solve Eq. 6.23. Instead, they considered a particular solution\nto these equations. Since they observed that the action potential propagated along the\naxon without changing its shape, they postulated the existence of a wave solution to this\nequation, in which the action potential travels with constant velocity u along the axon,\nthat is, V(x, t) = V(x — ut). Taking the second spatial and temporal derivative of this\nFig. 6.11 ELECTRICAL CIRCUIT or THE SQUID GIANT AXON One-dimensional cable model of\nthe squid giant axon. The structure of the cable is as in the passive case (Fig. 2.2B), with the RC\nmembrane components augmented with circuit elements modeling the sodium and potassium currents\n(Fig. 6.2)."}
{"text": "===== Page 20 =====\n6.5 Action Potential Propagation along the Axon \n• \n161\nexpression and using the chain rule leads to a second-order hyperbolic partial differential\nequation,\n(6.25)\nReplacing the second spatial derivative term in Eq. 6.23 with this expression yields\n(6.26)\nwith K = 4Riu2Cm/d and 7j0nic denned in Eq. 6.2. Equation 6.26 is an ordinary second-\norder differential equation, whose solution is much easier to compute than the solution to\nthe full-blown partial differential equation. It does require, though, a value for u. By a\nlaborious trial-and-error procedure, Hodgkin and Huxley iteratively solved this equation\nuntil they found a value of u leading to a stable propagating wave solution. In a truly\nremarkable test of the power of their model, they estimated 18.8 m/sec (at 18.3° C) for the\nvelocity at which the spike propagates along the squid giant axon, a value within 10% of the\nexperimental value of 21.2 m/sec. This is all the more remarkable, given that their model\nis based on voltage- and space-clamped data, and represents one of the rare instances in\nwhich a neurobiological model makes a successful quantitative prediction.\nWe can establish the dependency of the velocity on the diameter of the fiber using the\nfollowing argumentation. Because both /,• and Cm are expressed as current and capacitance\nper unit membrane area, their ratio is independent of the fiber diameter. The voltage across\nthe membrane and its temporal derivatives must also be independent of d. This implies that\nthe constant K in Eq. 6.26 must remain invariant to changes in diameter. Assuming that\nCm and Rj do not depend on d, we are lead to the conclusion that the velocity u must be\n(6-27)\nIn other words, the propagation velocity in unmyelinated fibers is expected to be proportional\nto the square root of the axonal diameter.5 Indeed, this predicted relationship is roughly\nfollowed in real neurons (see Fig. 6.16; Ritchie, 1995).\nThis implies that if the delay between spike initiation at the cell body and the arrival of\nthe spike at the termination of an axon needs to be cut in half, the diameter of the axon needs\nto increase by a factor of 4, a heavy price to pay for rapid communication. The premium\nput on minimizing propagation delay in long cable structures is most likely the reason the\nsquid evolved such thick axons. As we will see further below, many axons in vertebrates\nuse a particular form of electrical insulation, termed myelination, to greatly speed up spike\npropagation without a concomitant increase in fiber diameter.\nIt was more than 10 years later that Cooley, Dodge, and Cohen (1965; see also Cooley\nand Dodge, 1966) solved the full partial differential equation (Eq. 6.23) numerically using\nan iterative technique. Figure 6.12 displays the voltage trajectory at three different locations\nalong the axon; at x = 0 a short suprathreshold current pulse charges up the local membrane\ncapacitance. This activates the sodium conductance and Na+ ions rush in, initiating the\nfull-blown action potential (not shown). The local circuit current generated by this spike\nleads to an exponential rise in the membrane potential in the neighboring region, known\nas the \"foot\" of the action potential. This capacitive current in turn activates the local\nsodium conductance, which will increase rapidly, bringing this region above threshold:\n5. Notice that we derived a similar square-root relationship between diameter and \"pseudovelocity\" for the decremental wave\nin the case of a passive cable (Eq. 2.53)."}
{"text": "===== Page 21 =====\n162\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nthe spike propagates along the axon. Different from the space-clamped axon, where the\ncapacitive current is always equal and opposite to the ionic currents once the stimulus\ncurrent has stopped flowing (Eq. 6.20), the time course of current is more complex\nduring the propagated action potential due to the local circuit currents. Because some\nfraction of the local membrane current depolarizes neighboring segments of the axonal\ncable (the so-called local circuit currents; see Fig. 6.13), the current amplitude required\nto trigger at least one action potential is larger than the current amplitude in the space-\nclamped case\nIf the voltage applied to the squid membrane is small enough, one can linearize the mem-\nbrane, describing its behavior in terms of voltage-independent resistances, capacitances, and\ninductances. This procedure was first carried out by Hodgkin and Huxley (1952d) and will\nbe discussed in detail in Chap. 10. Under these circumstances, a space constant A can be\nassociated with the \"linearized\" cable, describing how very small currents are attenuated\nalong the axon. At rest, the dc space constant for the squid axon is A. = 5.4 mm, about 10\ntimes larger than its diameter.\nWhen long current steps of varying amplitudes are injected into the axon, the squid axon\nresponds with regular, periodic spikes. However, the already small dynamic range of the\n/-/ curve of the space-clamped axon (Fig. 6.10A) becomes further reduced to a factor of\nless than 1.7 when the sustained firing activity in the full axon is considered (from 58 to\n96 Hz at 6.3° C). Thus, while the Hodgkin-Huxley model describes to a remarkable degree\nthe behavior of the squid's giant axon, the equations do not serve as an adequate model for\nimpulse transduction in nerve cells, most of which have a dynamic range that extends over\ntwo orders of magnitude.\nAs predicted by Huxley (1959), Cooley and Dodge (1966) found a second solution to the\nHodgkin-Huxley equations. When the amplitude of the injected current step is very close\nto the threshold for spike initiation, they observed a decremental wave propagating away\nFig. 6.12 PROPAGATING ACTION POTENTIAL \nSolution to the complete Hodgkin-Huxley model\nfor a 5-cm-long piece of squid axon for a brief suprathreshold current pulse delivered to one end\nof the axon. This pulse generates an action potential that travels down the cable and is shown here\nat the origin as well as 2 and 3 cm away from the stimulating electrode (solid lines). Notice that\nthe shape of the action potential remains invariant due to the nonlinear membrane. The effective\nvelocity of the spike is 12.3 m/sec (at 6.3° C). If the amplitude of the current pulse is halved, only\na local depolarization is generated (dashed curve) that depolarizes the membrane 2 cm away by a\nmere 0.5 mV (not shown). This illustrates the dramatic difference between active and passive voltage\npropagation."}
{"text": "===== Page 22 =====\n6.5 Action Potential Propagation along the Axon\n163\nFig. 6.13 LOCAL CIRCUIT CURRENT IN THE SQUID AXON Illustration of the events occurring in\nthe squid axon during the propagation of an action potential. Since the spike behaves like a wave\ntraveling at constant velocity, these two panels can be thought of either as showing the voltages and\ncurrents in time at one location or as providing a snapshot of the state of the axon at one particular\ninstant (see the space/time axes at the bottom). (A) Distribution of the voltage (left scale) or the number\nof open channels (right scale) as inferred from the Hodgkin-Huxley model at 18.5° C. (B) Local circuit\ncurrents that spread from an excited patch of the axon to neighboring regions bringing them above\nthreshold, thereby propagating the action potential. The diameter of the axon (0.476 mm) is not drawn\nto scale. Reprinted by permission from Hille (1992).\nfrom the source. This solution quickly dies away to zero potential as x increases and is only\nobserved if the amplitude of the current step is within 0.1 % of the threshold current needed to\nobtain at least one spike. This phenomenon reveals the fact that the Hodgkin-Huxley model\ndoes not possess a strict threshold in the true sense of the word. In other words, there exists\na continuous transformation between the subthreshold and the threshold voltage response.\nYet in order to reveal these intermediate solutions the excitation must be adjusted with\na degree of accuracy impossible to achieve physiologically. Practically speaking, given\nunavoidable noise in any neuronal system, only the propagating wave solution (with its\nassociated threshold) plays a significant role in propagating information along the axon.\nCooley and Dodge (1966) also considered what happens if the density of voltage-\ndependent channels underlying GNa and GK is attenuated by a factor of r\\ (with 0< r\\ < 1;\nthe value of Vrest and Gieak were adjusted so that the resting potential and resting conductance"}
{"text": "===== Page 23 =====\n164 \n« \nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nwere held constant). Reducing these conductances is somewhat analogous to the action of\ncertain local anesthetics, such as lidocaine or procaine as used by dentists, in blocking\naction potential propagation. As rj is reduced below 1, the velocity of propagation as well\nas the peak amplitude of the spike are reduced. For r\\ < 0.26, no uniform wave solution is\npossible and the \"action potential\" decrements with distance.\n6.5.2 Nonlinear Wave Propagation\nSpikes moving down an axon are but one instance of a nonlinear propagating wave—\nnonlinear since in a linear dispersive medium, such as a passive cable, the different Fourier\ncomponents associated with any particular voltage disturbance will propagate at a different\nvelocity and the disturbance will lose its shape. This is why propagating spikes and the like\nare frequently referred to by mathematicians as resulting from nonlinear diffusion. Other\nexamples include sonic shock waves or the digital pulses in an optical cable.\nScott (1975) argues for a broad classification of such phenomena into (1) those systems\nfor which energy is conserved and which obey a conservation law and (2) those for which\nsolitary traveling waves imply a balance between the rate of energy release by some\nnonlinearity and its consumption.\nWaves associated with the first type of systems are known as solitons and are always\nbased on energy conservation (Scott, Chu, and McLaughlin, 1973). Solitons emerge from a\nbalance between the effects of nonlinearity, which tend to draw the wave together, fighting\ndispersion, which tends to spread the pulse out. This implies that solitons can propagate\nover a range of speeds. Furthermore, they can propagate through each other without any\ninterference. Solitons have been observed in ocean waves and play a major role in high-\nspeed optical fibers.\nAction potentials are an example of the second type of propagating wave, similar to\nan ordinary burning candle (Scott, 1975). Diffusion of heat down the candle releases wax\nwhich burns to supply the heat. If P is the power (in joules per second) necesssary to feed\nthe flame and E the chemical energy stored per unit length of the candle (joules per meter),\nthe nonlinear wave in the form of the flame moves down the candle at a fixed velocity u\ngiven by\n(6.28)\nIn other words, the velocity is fixed by the properties of the medium and does not depend\non the initial conditions. Were we to light flames at both ends of a candle, the flames would\nmove toward each other and annihilate themselves. This is also true if action potentials\nare initiated at the opposite ends of an axon. When they meet, they run into each other's\nrefractory period and destroy each other. Thus, spikes are not solitons.\n6.6 Action Potential Propagation in Myelinated Fibers\nThe successful culmination of the research effort by Hodgkin and Huxley heralded the\ncoming of age of neurobiology. While we will deal in Chap. 9 with their methodology as\napplied in the past decades to the ionic currents found at the cell body of nerve cells, let\nus here briefly summarize spike propagation in myelinated axons (for more details, see\nWaxman, Kocsis, and Stys, 1995; Ritchie, 1995; Weiss, 1996).\nAxons come in two flavors, those covered by layers of the lipid myelin and those that are\nnot. The squid axon is an unmyelinated fiber, common to invertebrates. In vertebrates, many"}
{"text": "===== Page 24 =====\n6.6 Action Potential Propagation in Myelinated Fibers\n165\nfibers are wrapped dozens or even hundreds of times with myelin, the actual diameter of the\naxon itself being only 60% or 70% of the total diameter (Fig. 6.14). This insulating material\nis formed by special supporting cells, called Schwann cells in the peripheral nervous system\nand oligodendrocytes in the central nervous system.\nA second specialization of myelinated fibers is that the myelin sheet is interrupted at\nregular intervals along the axon by nodes, named for their discoverer nodes of Ranvier.\nHere, the extracellular space gains direct access to the axonal membrane. Typically, the\nlength of a node is very small (0.1%) compared to the length of the internodal segment\n(Fig. 6.15). In the vertebrate, single myelinated fibers range in diameter from 0.2 to 20 /^m,\nwhile unmyelinated fibers range between 0.1 and 1 //,m. In stark contrast, the diameters of\nunmyelinated invertebrate fibers range from under 1 jiim to 1 mm.\nIn myelinated axons, conduction does not proceed continuously along the cable, but\njumps in a discontinuous manner from one node to the next. This saltatory conduction\n(from the Latin saltus, to leap) was clearly demonstrated by Huxley and Stampfli (1949)\nand Tasaki (1953). What these and similar experiments on frog, rabbit, and rat myelinated\nfibers made clear is that ionic currents are strikingly inhomogeneously distributed across the\naxonal membrane (Fig. 6.15; FitzHugh, 1962; Frankenhaeuser and Huxley, 1964; Stampfli\nand Hille, 1976; Rogart and Ritchie, 1977; Chiu et al., 1979; Chiu and Ritchie, 1980).\nSpike generation essentially only takes place at the small nodes of Ranvier, which are\nloaded with fast sodium channels (between 700 and 2000 per square micrometer). In\nmammalian myelinated nerves, the repolarization of the spike is not driven by a large\noutward potassium current, as in the squid axon6 but is achieved using a rapid sodium\ninacti vation in combination with a large effective leak conductance. Indeed, action potentials\ndo not show any hyperpolarization (Fig. 6. IB), unlike those in the squid giant axon. The\norigin of the large voltage-independent leak might involve an extracellular pathway beneath\nthe myelin that connects the nodal and internodal regions (Barrett and Barrett, 1982; Ritchie,\nFig. 6.14 MYELINATED AXONS\nElectron micrograph of across sec-\ntion through a portion of the optic\nfiber in an adult rat. The complete\ntransverse section through a sin-\ngle myelinated axon is shown in\nclose neighborhood to other axons.\nAbout four wrappings of myelin\ninsulation are visible. The circu-\nlar structures inside the axonal\ncytoplasm are transverse sections\nthrough microtubules. \nReprinted\nby permission from Peters, Palay,\nand Webster (1976).\n6. Pharmacological blockage of potassium channels has no effect on the shape of the action potential in the rabbit fibers\n(Ritchie, Rang, and Pellegrino, 1981)."}
{"text": "===== Page 25 =====\n166\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nFig. 6.15 ELECTRICAL CIRCUIT FOR A MYELINATED AXON Geometrical and electrical layout\nof the myelinated axon from the frog sciatic nerve (Frankenhaeuser and Huxley, 1964; Rogart and\nRitchie, 1977). The diameter of the axon and its myelin sheath is 15 /um, the diameter of the axon\nitself 10.5 [im, the difference being made up by 250 wrappings of myelin. The myelin is interrupted\nevery 1.38 mm by a node ofRanvier that is 2.5 /u,m wide. The total distributed capacitance for the\ninternode (2.2 pF) is only slightly larger than the capacitance of the much smaller node (1.6 pF). The\nsame is also true of the distributed resistance. At each node, the spike is reamplified by a fast sodium\ncurrent and is repolarized by a potassium current. Little or no potassium current is found at the nodes\nof Ranvier in mammalian myelinated axons. There, repolarization is accomplished by rapid sodium\ninactivation in conjunction with a large effective \"leak\" current."}
{"text": "===== Page 26 =====\n6.6 Action Potential Propagation in Myelinated Fibers \n• \n167\n1995). Potassium channels are present under the myelin sheet along the internodal section,\nalthough their functional role is unclear (Waxman and Ritchie, 1985).\nThe function of the numerous tightly drawn layers of myelin around the internodal\nsegments is to reduce the huge capacitive load imposed by this very large cable segment,\nas well as to reduce the amount of longitudinal current that leaks out across the membrane.\nThe effective membrane capacitance of the entire myelin sheath, which in the case of the\nfrog axon illustrated in Fig. 6.15, is made up of 250 myelin layers, is Cm/250, with Cm\nthe specific capacitance of one layer of myelin (similar to that of the axonal membrane),\nwhile the effective resistance is 250 times higher than the Rm of one layer of myelin.\nEven though the length of the interaxial node is typically 1000 times larger than the node,\nits total capacitance has the same order of magnitude (Fig. 6.15). This allows the action\npotential to spread rapidly from one node to the next, \"jumping\" across the intervening\ninternodal areas and reducing metabolic cost (since less energy must be expended to restore\nthe sodium concentration gradient following action potential generation). There is a safety\nfactor built into the system, since blocking one node via a local anesthetic agent does\nnot prevent blockage of the impulse across the node (Tasaki, 1953). Detailed computer\nsimulations of the appropriately modified Hodgkin-Huxely equations (based on the circuit\nshown in Fig. 6.15) have confirmed all of this (Frankenhaeuser and Huxley, 1964; Rogart\nand Ritchie, 1977).\nSingle axons can extend over 1 m or more,7 making conduction velocity of the electrical\nimpulses something that evolution must have tried to maximize at all cost. Measurements\n(Huxley and Stampfli, 1949) and computations indicate that the time it takes for the currents\nat one node to charge up the membrane potential at the next node is limited by the time it takes\nto charge up the intervening internodal membrane. This is determined by the time constant of\nthe membrane r, which is independent of the geometry of the axon. In this time the spike will\nhave moved across the internodal distance, making the propagation velocity proportional\nto this distance divided by r. Since anatomically the internodal distance is linearly related\nto the diameter of the axon, the velocity of spike propagation will be proportional to the\nfiber diameter,\n(6.29)\nrather than the square-root dependency found for unmyelinated fibers (Eq. 6.27). Rushton\n(1951) gave this argument a precise form using the principle of dimensional scaling. If,\nhe argued, axons had the same specific membrane properties, then in order for points\nalong two axons with different diameters to be in \"corresponding states,\" certain scaling\nrelationships must hold. In particular, space should increase in units of the internodal\nlength and velocity should be roughly proportional to the fiber diameter (for more details,\nsee Weiss, 1996). The latter is actually the case (Fig. 6.16). When comparing the fiber\ndiameter against the propagation velocity for myelinated cat axons, a roughly linear rela-\ntionship can be observed (Hursh, 1939; Rushton, 1951; Ritchie, 1982). With the exception\nof a l.l-/zm-thick unmyelinated mammalian fiber that propagates action potentials at\n2.3 mm/msec (Gasser, 1950), spike velocity in very small fibers has, so far, been difficult\nto record.\nThe functional importance of myelinated fibers is clear. They provide a reliable and rapid\nmeans of communicating impulses at a much reduced cost compared to unmyelinated fibers\n(at the same conduction velocity, a myelinated fiber can be up to 50 times thinner than an\n7. Think about the spinal nerve axons of an elephant or of the extinct Brontosaurus."}
{"text": "===== Page 27 =====\n168\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\nFig. 6.16 DIAMETER AND PROPA-\nGATION VELOCITY Relationship be-\ntween (internal) diameter d of adult cat\nperipheral myelinated fibers and prop-\nagation velocity u of the action poten-\ntial. The data are shown as dots (Hursh,\n1939) and the least-square fit as a line.\nPeripheral myelinated fibers are bigger\nthan 1 fim, while myelinated fibers in\nthe central nervous system can be as\nthin as 0.2 /xm, with an expected veloc-\nity in the 1-mm/msec range. Reprinted\nby permission from Ritchie (1982).\nunmyelinated fiber). This large (x 2500) factor in packing allows the brain to squeeze more\nthan a million axons into a single nerve that supplies the brain with visual information. The\nprimary cost of this insulation is the added developmental complexity and the possibility\nthat demyelinating diseases, such as multiple sclerosis, can incapacitate the organism.\n6.7 Branching Axons\nThe all-or-none nature of action potentials has lead to the idea that the axon serves mainly\nas a reliable transmission line, making a highly secure, one-way point-to-point connection\namong two processing devices. Furthermore, because of its high propagation velocity,\nthe action potential is thought to arrive almost simultaneously to all of its output sites.\nIndeed, both properties have been used to infer that spikes propagating parallel fibers in\nthe cerebellum serve to implement a very precise timing circuit (Braitenberg and Atwood,\n1958; Braitenberg, 1967).\nIt will not come as a surprise that the axon-as-a-wire concept is not quite true and needs\nto be revised. Experimentally, it is known that trains of action potentials show failure at\ncertain regions along the axon, most likely at the branch points. In other words, the train\nof spikes generated at the soma may have lost some of its members by the time it reaches\nthe presynaptic terminals, with individual spikes \"deleted\" (Barron and Matthews, 1935;\nTauc and Hughes, 1963; Chung, Raymond, andLettvin, 1970; Parnas, 1972; Smith, 1983).\nFor instance, conduction across a branching point in a lobster axon fails at frequencies\nabove 30 Hz (Grossman, Parnas, and Spira, 1979a). This conduction block first appears\nin the thicker daughter branch and only later in the thinner branch, most likely due to a\ndifferential buildup of potassium ions (Grossman, Parnas, and Spira, 1979b). Physiological\nevidence indicates that such a switching mechanism might subserve a specific function in\nthe case of the motor axon innervating the muscle used for opening the claw in the crayfish\n(Bittner, 1968). Depending on the firing frequencies, spikes are routed differentially into\ntwo branches of the axon going to separate muscle fibers."}
{"text": "===== Page 28 =====\n6.7 Branching Axons \n• \n169\nThese experimental studies have shown that action potentials may fail to invade the\ndaughter branches of a bifurcating axon successfully. As the theoretical analysis by Gold-\nstein and Rail (1974) pointed out, the single most important parameter upon which propa-\ngation past the bifurcation depends is its associated geometric ratio\n(6.30)\nwhere the d's are the fiber diameters and it is assumed that the specific membrane properties\nare constant in all three branches. This should remind us, of course, of the analysis of the\nbranching passive cables (Sees. 3.1,3.2, and Eq. 3.15) and, indeed, the reasoning is identical.\nGR equals the ratio of the input impedances if all cables are semi-infinite.\nGoldstein and Rail's (1974) and subsequent analytical and modeling investigations\n(Khodorov and Timin, 1975; Parnas and Segev, 1979; Moore, Stockbridge, and Wester-\nfield, 1983; Luscher and Shiner, 1990a,b; Manor, Koch, and Segev, 1991) established the\nfollowing principles. For GR = 1, impedances match perfectly and the spike propagates\nwithout any perturbation past the branch point (indeed, electrically speaking, for GR = 1\nthe branching configuration can be reduced to a single equivalent cable, albeit an active\none; see Sec. 3.2). If GR < 1, the action potential behaves as if the axon tapers and it will\nslightly speed up. The far more common situation is GR > 1, that is, the combined electrical\nload of the daughters exceeds the load of the main branch. As long as GR is approximately\n< 10, propagation past the branch point is assured, although with some delay (that can\nbe substantial for large values of GR). If GR> 10, propagation into both branches fails\nsimultaneously, since the electrical load of the daughters has increased beyond the capacity\nof the electrical current from the parent branch to initiate a spike in the daughter branches.\nParnas and Segev (1979) emphasize that for each constant geometric ratio, changes in the\ndiameter ratio between the daughter branches never yields differential conduction into one\nof the daughters if the specific membrane properties are identical in both. This implies that\nthe experimentally observed differential conduction (Bittner, 1968; Grossman, Parnas, and\nSpira, 1979a) must be due to other factors, such as a rundown in the ionic concentration\nacross the membrane or saturation of the ionic pumps that are sensitive to the ratio of area\nto volume (and would thus be expected to occur earlier in larger fibers). Note that all of\nthese modeling studies have assumed unmyelinated fibers and that axonal branch points\nappear to be devoid of myelin.\nUp to 10-12 bifurcations (see the heavily branched axonal terminal arbor in Fig. 3.1M)\ncan occur before the action potential reaches its presynaptic terminal where it initiates\nvesicular release. The delay at branch points with GR >1, in conjunction with other\ngeometrical inhomogeneities, such as the short swellings at sites of synaptic terminals called\nvaricosities, might add up to a considerable number, leading to a substantial broadening of\nspike arrival times at their postsynaptic targets.\nThe degree of temporal dispersion was simulated in the case of an axon from the\nsomatosensory cortex of the cat (Manor, Koch, and Segev, 1991). Since it is almost entirely\nconfined to cortical gray matter, it was taken to be unmyelinated (Fig. 6.17). In the absence\nof better data, Hodgkin-Huxley dynamics (at 20° C) were assumed. About 1000 boutons\nwere added to the axon and the propagation time between spike initiation just beyond the\ncell body and these boutons is shown in the histogram of Fig. 6.17. The first peak (with\na mean of 3.8 ± 0.5 msec) is contributed from terminals along the branches in cortical\nareas 3a and 3b (see the inset in Fig. 6.17A) and the more delayed one from those in area"}
{"text": "===== Page 29 =====\n170\nTHE HODGKIN-HUXLEY MODEL OF ACTION POTENTIAL GENERATION\n4 (mean of 5.8± 0.4 msec). Of the total delay, about 22-33% is due to the branch points\nand geometrical inhomogeneities; the majority is simple propagation delay. Manor, Koch,\nand Segev (1991) conclude that temporal dispersion in the axonal tree will be minor, on the\norder of 0.5-1 msec.\nLet us conclude with one observation. Computer simulations of branching axons by the\nauthor have shown that a strategically located inhibitory synapse of the shunting type onto\none branch of the axon, following the on-the-path theorem (Sec. 5.1.3), can selectively\nveto an action potential from invading this branch while not affecting spike invasion\ninto the second branch. This would allow for very fast synaptic switching or routing of\ninformation in an axonal tree (similar to a telephone network). While inhibitory synapses\ncan be found directly at the axon initial segment (Kosaka, 1983; Soriano and Frotscher,\n1989), no synapses, whether excitatory or inhibitory, have been observed on or around\naxonal branching points. It is anybody's guess why the nervous system did not avail itself\nof this opportunity to precisely (in space and time) filter or gate action potentials.\nFig. 6.17 PROPAGATION DELAYS ALONG A BRANCHING AXON Delays that action potentials incur\nas they propagate through a highly branching axonal tree (A), simulated in the case of an HRP\nlabeled axon originated in layer 5 of the somatosensory cortex of the adult cat (see inset). Reprinted\nby permission from Schwark and Jones (1989). (B) Histogram of the delay incurred between action\npotential initiation just beyond the cell body and the 977 terminals distributed in the terminal branches\nof the axon. The two humps correspond to locations in the proximal and distal parts of the axonal tree.\nOver the 3.5 mm of the tree, the total delay is 6.5 msec and temporal dispersion is minimal. Reprinted\nby permission from Manor, Koch, and Segev (1991)."}
{"text": "===== Page 30 =====\n6.8 Recapitulation \n• \n171\n6.8 Recapitulation\nThe Hodgkin-Huxley 1952 model of action potential generation and propagation is the\nsingle most successful quantitative model in neuroscience. At its heart is the depiction\nof the time- and voltage-dependent sodium and potassium conductances G^a and GK in\nterms of a number of gating particles. The state of Gna is governed by three activation\nparticles m and one inactivating particle h, while the fate of the potassium conductance\nis regulated by four activating particles n. The dynamics of these particles are governed\nby first-order differential equations with two voltage-dependent terms, the steady-state\nactivation (or inactivation), and the time constant. The key feature of activating particles is\nthat their amplitude increases with increasing depolarization, while the converse is true for\ninactivating particles. For rapid input to a patch of squid axonal membrane, spike initiation\nis exceeded whenever the net inward current becomes negative, that is, when a particular\nvoltage threshold Va, is exceeded.\nInclusion of the cable term leads to a four-dimensional system of coupled, nonlinear\ndifferential equations with a wave solution that propagates at a constant velocity down\nthe axon. This wave, the action potential, is due to the balance between dispersion and\nrestoration caused by the voltage-dependent membrane. When injecting sustained currents\ninto the axon, the equations predict two important aspects of the squid axon: the abrupt\nonset of sustained firing with a high spiking frequency and the very limited bandwidth of\nthe firing frequency.\nThe Hodgkin-Huxley formalism represents the cornerstone of quantitative models of\nnerve cell excitability, and constitutes a remarkable testimony to the brilliance of these\nresearchers. It should be remembered that their model was formulated at a time when the\nexistence of ionic channels, the binary, microscopic, and stochastic elements underlying\nthe continuous macroscopic, and deterministic ionic currents, was not known.\nWrapping axons in insulating material, such as the many layers of myelin observed\nin myelinated fibers that are found in all vertebrates, leads to a dramatic speedup over\nunmyelinated fibers. Conversely, at the same spike propagation speed, myelinated fibers\ncan be up to 50 times thinner than unmyelinated fibers. In mammals, axons above 1 /w,m are\nusually myelinated, with speeds in the 5-mm/msec range, and rarely exceed 20 /^m. When\naxons reach their target zone, they branch profusely, enabeling them to make thousands\nof contacts on postsynaptic processes. As trains of spikes attempt to propagate past these\npoints, they can be slowed down, depending on the exact geometry of the junction. In the\nmore extreme cases, individual spikes can fail to propagate past branch points.\nWe conclude that pulses can communicate along axons reliably, rapidly (at speeds\nbetween 1 and 100 mm/sec) and with little temporal dispersion. The main exception to\nthis appears to be the propagation of trains of spikes past branching points. Here, due\nto a variety of phenomena, conduction block can occur, which will differentially route\ninformation into one of the daughter branches or prevent conduction altogether."}
{"text": "===== Page 3 =====\n174\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.1 REDUCING THE HODGKIN-HUXLEY MODEL TO THE FITZHUGH-NAGUMO SYSTEM\nEvolution of the space-clamped Hodgkin-Huxley and the FitzHugh-Nagumo equations in response to\na current step of amplitude 0.18 n A in A and B and of amplitude I = 0.35 in C and D. (A) Membrane\npotential V(t) and sodium activation m(t) (see also Fig. 6.8). Sodium activation closely follows\nthe dynamics of the membrane potential. (B) Sodium inactivation 1 — h and potassium activation n\nof the Hodgkin-Huxley system. (C) \"Excitability\" V (t) of the two-dimensional FitzHugh-Nagumo\nequations (Eqs. 7.1) with constant parameters has a very similar time course to V and m of the squid\naxon (notice the different scaling). (D) The \"accommodation\" variable W shows modulations similar\nto 1 — h and n of the Hodgkin-Huxley equations.\nWe will let Fig. 7.1 (and others to follow) be sufficient justification for this reduction.\nWe refer those of our readers who are not satisfied by the somewhat ad hoc nature of\nthis procedure to Kepler, Abbott, and Marder (1992), who describe a general method for\nreducing Hodgkin-Huxley-like systems using \"equivalent potentials.\"\nHistorically, the equations underlying the FitzHugh-Nagumo model have their origin\nin the work of van der Pol (1926), who formulated a nonlinear oscillator model (termed\na relaxation oscillator) and applied it to the cardiac pacemaker (van der Pol and van der\nMark, 1928). Using the phase space method explored by Bonhoeffer (1948) in the context\nof a chemical reaction bearing some similarities to nerve cell excitation, FitzHugh (1961,\n1969) and, independently, Nagumo, Arimoto, and Yoshizawa (1962) derived the following\ntwo equations to qualitatively describe the events occurring in an excitable neuron:\n(7.1)\nwhere we use the conventional shorthand of writing V for the temporal derivative of"}
{"text": "===== Page 4 =====\n7.1 The FitzHugh-Nagumo Model \n• \n175\nV, dV/dt. Equations 7.1 describe the so-called FitzHugh-Nagumo model or, in deference\nto earlier work by van der Pol (1926) and Bonhoeffer (1948), the Bonhoeffer-van der\nPol model. The parameters a, b, and <j> are dimensionless and positive, with a number of\ndifferent versions of these equations in usage. In this chapter, we assume a = 0.7, b = 0.8,\nand (p = 0.08 (Cronin, 1987). The amplitude of 0, corresponding to the inverse of a time\nconstant, determines how fast the variable W changes relative to V. With <j> = 0.08, V\nchanges substantially more rapidly than W (since V is typically 1/0.08 = 12.5 times\nlarger than W).\nEquations 7.1 represent an instance of a singularly perturbed system, where one variable\n(or set of variables) evolves much faster than the other variable (or set of variables). This\nhas the consequence that the evolution of the system consists of \"rapid\" segments, in\nwhich the fast variable V evolves so rapidly that the slow variable W can be considered\nstationary, interconnected with \"slow\" segments, during which the slow variable will not\nremain constant, as during the fast segments, but will be an instantaneous function of the\nfast variable W = W(V(t)) (for more details, see Cronin, 1987).\nBecause of the nonlinear nature of these differential equations, closed-form solutions\nare difficult to derive and we are forced to integrate the equations numerically. We can,\nhowever, deduce the qualitative topological properties of these equations without explicitly\nsolving them. The key concept here is to consider the evolution of the system, specified by\nthe vector r(t) = (V(t), W(t)), in the associated two-dimensional phase space, spanned\nby V and W (Fig. 7.2). For every state or phase point r in this plane, Eqs. 7.1 assign a\nvector r = (V, W) to this point, specifying how the system will evolve in time. Indeed,\nthese equations can be thought of as a vector field in phase space, akin to the motion of an\nimaginary laminar fluid on the plane. While only a few representative vectors are drawn in\nFig. 7.2, they completely fill the plane. Such plots are known as phase plane portraits and\nprovide an intuitive and geometrical way to understand certain qualitative aspects of the\nsolution of the associated ordinary differential equations.\nIn cases such as Eqs. 7.1, where the derivatives on the right-hand side do not depend\nexplicitly on time, different trajectories can never cross, since if two would intersect, there\nwould be two different solutions starting from the same (crossing) point. This is ruled\nout by the existence and uniqueness theorem associated with a set of coupled differential\nequations, such as these, that are smooth enough. Thus, from any point in the phase space,\nthe system can only evolve in one way, giving phase portraits their orderly looks.\n7.1.1 Nullclines\nIn order to understand how the system evolves in time, let us consider two of its isoclines.\nAn isocline is a curve in the (V, W) plane along which one of the derivatives is constant.\nIn particular, the null isocline, or nullcline, is the curve along which either V or W is zero.\nThe nullcline associated with the fast variable V, defined by V = 0, is the cubic function\nW = V — V3/3 + /. If the system currently is located on the V nullcline, its imminent\nfuture trajectory must be vertical, pointing either upward (for W > 0) or downward (for\nW < 0). Furthermore, for all points in the plane above this cubic polynomial, V < 0,\nwhile the converse is true for all points below the curve W = V — V3/3 +1.\nThe nullcline associated with the slow variable, W = 0, is specified by the linear equation\nW = (V + a)/b. Thus, if the evolution of the system brings it onto the W nullcline, its\ntrajectory in the immediate future must be horizontal, for only V, but not W, can change.\nFurthermore, all phase points in the half-plane to the left of W = (V + a)/b have a"}
{"text": "===== Page 5 =====\n176\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.2 PHASE PLANE PORTRAIT OF THE FITZHUGH-NAGUMO MODEL \nPhase plane associated\nwith the FitzHugh-Nagumo Eqs. 7.1 for 7 = 0. The fast variable V corresponds to membrane\nexcitability while the slower variable W can be visualized as the state of membrane accommodation.\nThe nullcline for the V variable, that is, all points with V = 0, is a cubic polynomial, and the W\nnullcline (all points with W = 0) is a straight line. The system can only exist in equilibrium at the\nintersection of these curves. For our choice of parameters and for / = 0, a single equilibrium point\nexists: (V, W) = (—1.20, —0.625). The arrows are proportional to (V, W) and indicate the direction\nand rate of change of the system: V usually changes much more rapidly than W.\nnegative W derivative and therefore move downward, while points in the right half-plane\nare associated with increasing W values.\nAs mentioned above, the FitzHugh-Nagumo equations are an example of an autonomous\nsystem whose derivatives do not explicitly depend on time (assuming that / has some\nconstant value for t > 0). If the right-hand side of Eqs. 7.1 would contain terms that\nexplicitly depended on time t (such as a time-dependent forcing term / (t)), the nullclines\nwould change with time. The trajectory (V(t), W(t)) of the system in phase space can now\ncross itself and our simple analysis no longer applies.\n7.7.2 Stability of the Equilibrium Points\nLet us first analyze the resting states of the system and their stability. The fixed, critical, or\nsingular points of the system r* are all those points (V,, W,) at which both derivatives are\nzero, that is, for which (V(V{, Wi), W(Vt, Wf)) = (0, 0). For Eqs. 7.1 with our choice\nof parameter values and 7 = 0, the two nullclines meet at a single point r* = (V, W) —\n(— 1.20, —0.625). If the system started out exactly at this point, it would not move to any\nother point in phase space. This is why it is known as an equilibrium point. However, in the\nreal world, the system will never be precisely at the critical point but in some neighborhood\naround this point. Or, if the system started exactly at the critical point (V, W), any noise\nwould perturb it and carry it to a point in the immediate neighborhood (V +8V, W + 8W).\nThe subsequent fate of the perturbed system depends on whether^ the_fixed point is stable\nor not. In the former case, the system will eventually return to (V, W). If the equilibrium\npoint is not stable, the system will diverge away from the singular point.\nThe stability of the equilibrium point can be evaluated by linearizing the system around\nthe singular point and computing its eigenvalues. Conceptually, this can be thought of\nas zooming into the immediate neighborhood of this point. The linearization procedure"}
{"text": "===== Page 6 =====\n7.1 The FitzHugh-Nagumo Model \n• \n177\ncorresponds to moving the origin of the coordinate system to the singular point and\nconsidering the fate of points in the immediate neighborhood of the origin (Fig. 7.3A).\nThe associated eigenvalues of the linear system completely characterize the behavior in\nthis neighborhood. Following Eqs. 7.1, we can write for any perturbation S r around the\nfixed point r*,\n(7.2)\nRemembering that (~V -V\"3/3 - W + I) = 0 and </> (V + a - bW) = 0 (the definition of\nthe critical point) as well as V = W = 0 and neglecting higher order terms in 8 V (since\nthe perturbation is assumed to be small), we arrive at\n(7.3)\nor, in obvious vector notation,\n(7.4)\nwith the matrix M given by\n(7.5)\nThis corresponds to the matrix of partial derivatives associated with Eqs. 7.1 and evaluated\nat the critical point.\nEquation 7.4 describes how any perturbation <5r evolves in the neighborhood of the\nsingular point r*. We can characterize the behavior around this point by finding the\neigenvalues of M. The associated characteristic equation is\nFig. 7.3 BEHAVIOR \nAROUND THE EQUILIBRIUM\nPOINT The behavior of the FitzHugh-Nagumo equa-\ntions in a small neighborhood around the fixed point is\ndetermined by linearizing these equations around their\nfixed point and computing the associated pair of eigen-\nvalues. (A) Evolution of (S V, S W) in a coordinate system\ncentered at the equilibrium point r* for 7=0. Because\nthe real part of both eigenvalues is negative, a small\nperturbation away from the fixed point will decay to zero,\nrendering this point asymptotically stable. Any point in\nthis plane will ultimately converge to the fixed point at\nthe origin. (B) Similar analysis for the equilibrium point\nr*' for a sustained input with / = 1 (Fig. 7.5A). The\nfixed point is unstable."}
{"text": "===== Page 7 =====\n178 \n. \nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\n(7.6)\nThe eigenvalues correspond to the solutions of this equation,\n(7.7)\nThe evolution of the system therefore takes the following form:\n(7.8)\nwith FI and r2 the two eigenvectors associated with the eigenvalues AI and \"ki and c\\ and\n€2 depending on the initial conditions of the system.\nIn the two-dimensional case considered here, the different types of solutions of Eqs. 7.1\nare easy to classify and understand geometrically in terms of the eigenvalues. In order to\nstudy the ultimate fate of the fixed points, the exact values of A^ and A.2 do not matter. What\ndoes matter is whether or not the eigenvalues are real or imaginary and the sign of their real\nparts. (For a thorough discussion of this see Hirsch and Smale, 1974, and Strogatz, 1994.)\n1. If AI and A.2 are both real and at least one of them is positive, the perturbation 8r(t)\nwill grow without bounds and the solution will diverge from the fixed point: the critical\npoint is unstable. If both eigenvalues are positive, the point is called a source; if one of\nthem is positive and the other negative, the point is called a saddle.\n2. When the eigenvalues are real and both negative, 8r(t) -> 0 and the system will decay\nback to its resting state: the critical point is stable and termed a sink.\n3. If the expression under the square root in Eq. 7.7 becomes negative, the two eigenvalues\nform a complex conjugate pair with AI = a + iu> and A 2 = a — ico, and we have\n(7.9)\nNote that the eigenvectors r are complex, since the A's are. Exploiting Euler's formula,\neia>t _ cosft)f _|_ i sinoif, we can reexpress the evolution of the perturbation as a\ncombination of terms involving e°\" cos(fttf) and e°\" sin(otf).\nThis evolution equation has two parts; the first one, e°\", specifies whether the system\nwill converge back to its resting state or whether it will diverge, depending on the sign of\na, that is, on the sign of the real part of the eigenvalues. The second part can be thought\nof as a rotation around the origin, with the oscillation frequency given by / = a)/(2n).\nFor &> > 0, the trajectories will be traversed counterclockwise and clockwise if a) < 0. If\na — 0, the system will always remain at a fixed distance from the center and will neither\napproach nor recede from this periodic trajectory. For a < 0, the system will display\ndamped oscillations and converge toward the equilibrium state. Here, the fixed point\n(V, W) is termed a stable spiral or attractor. Conversely, if a > 0, the oscillations will\ngrow exponentially in amplitude and the equilibrium point is an unstable spiral or focus.\nReturning to the FitzHugh-Nagumo model in the case of no current input, the two eigen-\nvalues associated with the equilibrium point at rest (7 = 0) are —0.50 ± 0.42z. According\nto our argumentation, this implies that any perturbation <5r decays in an exponential manner\nback to zero, that is, back to the equilibrium point, since the real part of the eigenvalues is"}
{"text": "===== Page 8 =====\n7.1 The FitzHugh-Nagumo Model \n• \n179\nnegative (see Fig. 7.3A). Because the solution will also oscillate, the equilibrium point is a\nstable spiral or attractor.\nHow large is the neighborhood around the equilibrium point within which any perturba-\ntion will ultimately die out? The size of the largest such neighborhood is called the basin of\nattraction associated with this stable point, that is, the set of initial conditions TO = (Vo> Wo)\nin phase space that will ultimately \"fall into\" this stable point. In our case, the basin is the\nentire plane. In other words, no matter what the initial conditions, in the absence of any\nstimulus current / the system will ultimately always come to rest at r*.\nThis illustrates the power of phase space analysis. The stability of any nonlinear au-\ntonomous system of differential equations can be analyzed in terms of the stability of\nthe linearized system; the signs of the real part of the eigenvalues then determine the\nqualitative behavior of the system in some neighborhood around the equilibrium point\n(Hirsch and Smale, 1974). By looking for the intersections of the nullclines when changing\nthe differential equations characterizing the system, one can immediately understand in an\nintuitive manner the steady-state properties of the system (see Strogatz, 1994, for many\nbiologically inspired examples of this).\n7.7.3 Instantaneous Current Pulses: Action Potentials\nHow will the FitzHugh-Nagumo model respond if an instantaneous current pulse / (t) =\nQS(t) (with Q > 0) is applied? The initial value of V will jump by Q, thereby moving\nthe system in phase space a certain horizontal distance away from the equilibrium point\n(dashed line in Fig. 7.4A). The direction of the movement is determined by the sign of the\ncurrent input.\nIf the amplitude of the current input is small, the system will almost immediately return\nto rest following a tight trajectory around the equilibrium point. In the process, V will over-\nand undershoot the resting state V = —1.2 (since the eigenvalues are complex), typical of\na system with an inductance-like behavior (see Chap. 10). However, in order to see these\noscillations around r*, Fig. 7.4B would have to be enlarged. The trajectory of V reminds us\nof the subthreshold response of the Hodgkin-Huxley membrane model (Fig. 6.5B). There,\na short current pulse also depolarizes the membrane, activating some potassium current and\nslightly hyperpolarizing the membrane, before finally settling down at the resting potential.\nBoth in the Hodgkin-Huxley and in the FitzHugh-Nagumo systems, graded \"potentials\"\ncan be obtained by varying the amplitude of the initial displacement over a small range\nof values.\nWhat happens if the amplitude of the current pulse is made larger? If V is moved\ninstantaneously past —0.64, the evolution of the system sharply veers away from the V\nnullcline, with V rapidly increasing while W remains more or less stationary until the\nrightmost branch of the V nullcline is encountered, defining the maximal value of V. The\nreason for this rapid change is that V is large, while W, due to the small size of $, is\nsmall (the slope dW/dV of the trajectory is on the order of $; Eqs. 7.1). During the next\nphase, the system slowly \"crawls\" upward along the V nullcline until it reaches the knee\nat the top of this curve. This slow phase is dictated by the fact that V is very small in the\nneighborhood of this curve, causing W to be an instantaneous function of V (the slope\nof the trajectory dW/dV is large here). A third \"fast\" phase follows, where V is rapidly\nreduced, undershooting the resting value of V in the process. Finally, the system slowly\nloses its accommodation W, crawling back to the resting state along the left branch of the\nnullcline. This trajectory is very much reminiscent of a Hodgkin-Huxley action potential"}
{"text": "===== Page 9 =====\n180\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.4 RESPONSE or THE FITZHUGH-NAGUMO MODEL TO CURRENT PULSES \n(A) The qui-\nescent system is excited by a current pulse of different amplitudes I = Q8(t), displacing the system\nfrom its resting state r* along the dashed horizontal line. Following Eqs. 7.1, this briefly increases\nV, in agreement with physical intuition, since a brief current pulse will cause a transient capacitive\ncurrent C dV/dt to flow. The evolution of the voltage V and of the adaptation variable W is plotted in\n(B) and (C). Changing V from its initial value of — 1.2 to —0.8 or —0.65 only causes quick excursions\nof the voltage around the equilibrium point with the system rapidly returning to rest (the oscillatory\nmanner in which the system does so is not readily apparent at the scale of these panels). If the current\npulse is large enough so that V exceeds —0.64, a stereotyped \"all-or-none\" sequence is triggered: V\nrapidly increases to positive values but then dives below its resting value V before finally coming to\nrest again at r*. Notice that the trajectory in this case consists of \"fast\" segments, where V changes\nrapidly but W remains essentially constant (upper and lower segments), interconnected by \"slow\"\nsegments, where the system changes so slowly that V is always in equilibrium (the \"slow\" segments\nclosely coincide with the V nullcline).\n(Fig. 6.5), with V and sodium activation m changing rapidly, while sodium inactivation h\nand potassium activation n vary at a more leisurely pace.\nWhile the system is \"hyperpolarized,\" that is, traveling along the leftmost branch of the\nV nullcline, it is clear that the inactivation variable W must be brought down below its"}
{"text": "===== Page 10 =====\n7.1 The FitzHugh-Nagumo Model \n• \n181\nresting value of about —0.625 before the system can cross over the W nullcline and once\nagain execute its standard trajectory. This is equivalent to a refractory period, during which\nmuch larger current pulses are needed in order to trigger another spike.\nAs emphasized above, this behavior, with \"fast\" segments along which accommodation\nvaries little but V changes rapidly, connected by \"slower\" segments where V is in equi-\nlibrium as W changes slowly, is typical of singularly perturbed systems (Cronin, 1987).\nInjecting larger current pulses will not change this basic trajectory, even though each value\nof / is associated with its own unique trajectory in phase space.\nAlthough we stated above that the system goes through its stereotypical trajectory once\nV exceeds —0.64, the FitzHugh-Nagumo system shows no true threshold behavior. From\na numerical point of view, the amplitude Q of the current pulse can be varied by minute\namounts between —0.65 and —0.64, giving rise to all possible intermediate trajectories\nbetween sub- and suprathreshold (filling in the available phase space between the two\nassociated trajectories in Fig. 7.4A). Indeed, it is known both computationally (Cooley and\nDodge, 1966) and experimentally (at higher temperature; Cole, Guttman, and Bezanilla,\n1970) that the squid action potential does not behave in a true all-or-none manner. In both\nsystems, no true saddle exists in the sense that all points to one side of the saddle will\nremain below and all points beyond the saddle will be above threshold. However, if </> is\nsmall, points close to the middle part of the V nullcline act approximately as a separatrix,\nfrom which neighboring paths diverge sharply to the left or to the right. The \"width\" of the\nseparatrix over which intermediate forms between sub- and suprathreshold responses exist is\nso small that from a physiological point of view (given the ambient level of variability in the\nmembrane potential), the system does show threshold behavior, with the horizontal distance\nbetween the resting point and the separatrix corresponding to the amplitude of the threshold.\n7.1.4 Sustained Current Injection: A Limit Cycle Appears\nLet us stimulate the quiescent system with a sustained current step of amplitude / at t = 0.\nThis input will, different from a current pulse, change the associated phase space diagram.\nWe can visualize the new phase diagram by noticing that for a positive current step the V\nnullcline is shifted upward, while the W nullcline does not change (Fig. 7.5A). This, of\ncourse, changes the position of the equilibrium point. In the presence of a positive current\nup to three fixed points now coexist, since the cubic V nullcline can intersect the linear W\nnullcline at up to three different locations. However, both geometrical intuition (the slope\nof the W nullcline is too steep to intersect the V nullcline more than once) as well as an\nalgebraic criterion (Cardan's solution) tell us that for b < 1 only a single solution exists.\nThe solution is stable as long as the real part of the two eigenvalues is negative. Once\nthe real part becomes zero and then positive, even infinitesimal small perturbations will\nbecome amplified and diverge away from the equilibrium point. Following Eq. 7.7, the real\npart changes sign at the two locations where\n(7.10)\nThus, the equilibrium point is stable whenever the W nullcline meets the cubic nullcline\nalong its right- and leftmost branches. Here the slope is negative and | V \\ > 1. However,\nalong the central part of the V nullcline, | V \\ < ^\\ — b(j>, and the eigenvalues will acquire\na positive real part, rendering the fixed point unstable.\nAssuming that the system is in its quiescent state r* = (V, W) = (—1.20, —0.625),\nthen as long as the amplitude of the current step is small enough, the real part of the"}
{"text": "===== Page 11 =====\n182 \n. \nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.5 RESPONSE or THE FITZ-\nHUGH-NAGUMO MODEL TO CUR-\nRENT STEPS (A) The quiescent sys-\ntem is subject to a current step of am-\nplitude /. For / < 0.32, the new equi-\nlibrium r*' is stable: the system depolar-\nizes but remains subthreshold. For larger\nsteps (here / = 1), the new equilibrium\npoint lies along the middle portion of\nthe V nullcline and is unstable. Because\nthe system has a stable limit cycle, it\nwill not diverge; rather, it generates a\ntrain of \"action potentials\" whose time\ncourse is shown in (B). Regardless of the\ninitial state of the system, it will always\nconverge rapidly onto this limit cycle.\neigenvalues associated with the new equilibrium point r*' remains negative and r*' is stable.\nBecause of the upward shift of the V nullcline, the new resting state of the membrane\npotential V is more positive than V, that is, the system is depolarized.\nFor a critical value of the input current /_, the nullclines intersect along the central\npart of the V nullcline at V- (defined by Eq. 7.10) and local stability breaks down (as\nin Fig. 7.5B). Due to the cubic nonlinearity in Eqs. 7.1, the system does not diverge2 but\nfollows a stereotypical trajectory around r*'. As demonstrated in Fig. 7.5, the system moves\nfrom its starting point rapidly to the right (\"the membrane depolarizes\") until it meets the\nrightmost branch of the V nullcline where it will slowly creep upward (adaptation builds\nup). Subsequently, V decays rapidly by moving horizontally toward the right and down the\nleftmost branch of the V nullcline. So far, the behavior of the equations is very similar to\nthat seen earlier, with fast phases alternating with phases during which the system crawls\nalong the V nullclines (the arrows in Fig. 7.2 give a good feeling for the relative speed of\nthese processes).\nDifferent from before, the system does not return to its new equilibrium point r*', since\nthis is unstable. Rather the trajectory of the system undershoots r*' and traces a path almost\nparallel to the V axis until it meets its previous trajectory, which it then follows. As\nlong as the input /o persists, the system continues to evolve along the same trajectory,\nproducing a constant and infinite stream of action potentials (Figs. 7.1C and 7.5B). Since\nthese oscillations are stable, but the term \"steady-state\" usually refers to an unchanging\nand not to a changing state, another label is needed. The French mathematician Poincare,\nto whom we owe phase space analysis of differential equations with two variables, called\nsuch steady oscillations a stable limit cycle.\n2. As it would in a purely linear system once the system looses stability."}
{"text": "===== Page 12 =====\n7.1 The FitzHugh-Nagumo Model \n• \n183\nIf the system starts out at a location in phase space inside the limit cycle, its trajectory\nwill spiral outward until it meets the limit cycle. Likewise, for any point outside the limit\ncycle, the system will spiral into the limit cycle. Thus, the basin of attraction of the limit\ncycle is the entire phase plane (with the exception of the equilibrium point). The limit cycle\nis stable, since small perturbations away from the cycle will be quickly suppressed and\nthe system will return to the limit cycle. In order for a closed, that is, oscillatory, solution\nto be a limit cycle is must have an isolated trajectory. This implies that any neighboring\ntrajectories must either spiral toward (as they do here) or away from the limit cycle (unstable\nlimit cycle). This is where the sense of the word \"limit\" in limit cycles comes from.\nLimit cycles are inherently nonlinear phenomena; they cannot occur in a linear system.\nObviously, linear systems can oscillate, as shown by the harmonic oscillator, that is they\nhave closed orbits. Yet these trajectories are not isolated. If r(0 is an oscillatory solution,\nso will cr(f )> for any value of c. For such a system, the initial conditions will always dictate\nthe exact orbit chosen by the system. The system never forgets this, no matter how long one\nwaits. This is quite distinct from a limit cycle in a nonlinear system, such as the Hodgkin-\nHuxley or the FitzHugh-Nagumo equations, where, given a strong enough input, the system\nwill ultimately converge to the limit cycle. The cubic nonlinearity in Eqs. 7.1 is essential\nfor the overall behavior of the system.\nLimit cycles cannot exist in systems with only one degree of freedom that are governed\nby a first-order differential equation, no matter whether linear or nonlinear. Such systems\neither converge toward their fixed points or they diverge to infinity. No overshooting or\ndamped oscillations can occur.\n7.1.5 Onset of Nonzero Frequency Oscillations: The Hopf Bifurcation\nWhat governs the onset of oscillations? Is it possible to obtain spiking with arbitrarily low\nfrequencies? The coupled nonlinear differential equations that arise within a biophysical\ncontext can display two fundamentally different types of onset of oscillations, one related to\na Hopf bifurcation and one related to the creation of a saddle. Both behave in very different\nways, which are of functional relevance. We will study the first type of bifurcation here,\ndeferring the second one to the following section.\nAs discussed in the previous section, increasing the amplitude of the injected current /\nfrom zero to more positive values shifts the intersection of the two nullclines (Fig. 7.5). As\n/ increases, so does the real part a of the eigenvalue associated with the equilibrium point\nat its resting value. Breakdown of stability occurs when ce changes sign, since for a > 0,\nsmall perturbations in the neighborhood^of the equilibrium point will fail to die out. Based\non Eq. 7.7, a = 0 occurs if and only if V = ±^/l — b<f>. These two voltages are associated\nwith two different values of the input current, /_ = 0.33 and /+ = 1.42.\nAt these two values of the input current, the equilibrium point ceases to be stable and\ndevelops into an unstable source. The system starts to oscillate, with a frequency determined\nby the imaginary part of the associated eigenvalues (Fig. 7.6B). Whether or not these\noscillations develop into a stable limit cycle, as here or as in the case of the Hodgkin-Huxley\nequations, depends on the global character of the equations and cannot be determined based\non a local criterion.\nThis type of dynamical phenomenon, in which stable large-amplitude oscillations arise\nabruptly as one particular parameter, known as the bifurcation parameter (here /), varies\nsmoothly, is called hard excitation, or subcritical Hopf bifurcation. Hard excitation is feared\nin engineering applications, since one of its salient properties is that the amplitudes of the"}
{"text": "===== Page 13 =====\n184\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.6 DISCHARGE CURVE OF THE FrrzHucH-NAGUMO MODEL \n(A) Steady-state membrane\npotential and (B) oscillation frequency of the limit cycle for the FitzHugh-Nagumo equations as a\nfunction of a sustained current /. As the membrane is depolarized in response to increasing injection of\ncurrent, it loses stability at /_ = 0.33 (arrow) and moves on a stable limit cycle. An important property\nof this type of bifurcation phenomenon, known as hard excitation or subcritical Hopf bifurcation, is\nthat oscillations occur with nonzero frequency. This behavior is also characteristic for the Hodgkin—\nHuxley equations. Between /_ and /+ = 1.42, the system moves along the limit cycle. The frequency\nof the oscillation is a function of / (dashed line in panel A). Beyond /+, the equilibrium point becomes\nstable again, and the system remains \"locked\" at a depolarized level (not shown).\noscillations are nonzero and can be fairly large. A soft excitation—also known as supercrit-\nical Hopf bifurcation—occurs when oscillations arise with arbitrarily small amplitudes and\ngradually increase as the bifurcation parameter is ramped up. Hard excitation constitutes\none of the generic mechanisms for the onset of oscillations in coupled nonlinear differential\nequations (Glass andMackey, 1988; Strogatz, 1994). A subcritical Hopf bifurcation requires\nthat a stable spiral or attractor change into an unstable spiral and is surrounded by a stable\nlimit cycle.\nA key feature of a Hopf bifurcation is the onset of a stable limit cycle with a nonzero\noscillation frequency (since u> is finite). It is a well-known feature of the Hodgkin-Huxley\nmodel that the minimal stable spiking frequency is about 50 Hz (at 6.3°; see Fig. 6.9B).\nExperimentally, this is true for most axonal membranes as well as for the generation of\nspike trains at the cell body of certain neuronal types, for instance, the nonadapting cortical\ninterneurons. As we keep on increasing /, the frequency of these stable oscillations first\nincreases before the /-/ curve peaks and decreases back to 22.8 Hz at /+.\nAlso plotted in Fig. 7.6 is the steady-state voltage V as a function of the sustained\ncurrent Iss flowing across the membrane. Iss can be obtained solving for / in Eqs. 7.1 under\nsteady-state conditions. Similar to the steady-state I—V relationship for the squid axon\nmembrane (Fig. 6.9A), this curve is a monotonically increasing function. Operationally, it"}
{"text": "===== Page 14 =====\n7.2 The Morris-Lecar Model \n• \n185\ncan be obtained by injecting a current step of amplitude / into the system and plotting the\nresulting membrane potential.\n7.2 The Morris-Lecar Model\nIt has been argued that the FitzHugh-Nagumo equations do not faithfully represent any\nparticular neuronal membrane and that phase space analysis therefore is not a particular\nuseful tool to understand the dynamics of \"real\" neurons. Because we think otherwise, we\nwill apply phase space analysis to a second set of equations, which is meant to capture\nthe dynamics of the membrane potential in the muscle of barnacles. Here all terms can be\ndirectly identified with various voltages and conductances. These equations also allow us\nto demonstrate a very different way in which the onset of oscillations can occur. In the\nso-called saddle bifurcation, oscillations first emerge with an arbitrarily small frequency.\nAs the amplitude of the injected current is smoothly increased, the frequency of the spike\ntrains also increases.\nBarnacle muscle fibers that are subject to constant current inputs respond with a variety of\noscillatory voltage patterns. In order to describe these, Morris and Lecar (1981) postulated\na set of coupled, ordinary differential equations (neglecting any spatial dependencies)\nincorporating two ionic currents: an outward going, noninactivating potassium current and\nan inward going, noninactivating calcium current. Because the Ca2+ current responds much\nfaster than the K+ current, we assume that /ca (0 is always in equilibrium for the time scales\nwe are considering. This allows us to neglect the dynamics of the calcium current and we\nonly need to treat the two-dimensional reduced version of the equations. Different variants\nof the Morris-Lecar equations are in usage. We will follow the presentation in Rinzel and\nErmentrout (1998) which demonstrates best the two distinct manners in which a system can\nstart to oscillate.\n7.2.1 Abrupt Onset of Oscillations\nIn their reduced form, the equations showing this generic type of behavior are\n(7.11)\nwhere Vm is the absolute membrane potential (in millivolts), Cm = 1/xF/cm2, ID is the\nactivation variable for potassium, all currents are in units of microamperes per square\ncentimeter (/nA/cm2) and time t is measured in milliseconds. The ionic current has three\ncomponents,\n(7.12)\n*As stated above, we assume that the calcium current is always at equilibrium, with its\nactivation curve given by\n(7.13)\nThe potassium activation variable follows the standard first-order Eq. 6.7 with steady-state\nactivation"}
{"text": "===== Page 15 =====\n186 \n. \nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\n(7.14)\nand time constant\n(7.15)\nThe other parameters are GCa = 1.1, GK = 2.0, Gm = 0.5, ECa - 100, EK = -70,\nand Vrest = —50 (all conductances are in units of millisiemens per square centimeter and\nthe reversal potentials in millivolts).\nFollowing the methods described in the first part of this chapter, we characterize the phase\nspace associated with these equations by the two nullclines (Fig 7.7A). For the parameters\nused here, the nullclines intersect at a single location, no matter what the amplitude of\nthe injected current /. We obtain a physiological picture of events in a muscle when we\ndepolarize the membrane by injecting a sustained current 7 = 15 /iiA/cm2. Under these\nconditions, the equilibrium point is stable and the resting membrane potential is — 31.7 mV.\nApplying current pulses that briefly depolarize the system from its resting state demon-\nstrates the existence of a threshold and a limit cycle. As long as the membrane is depolarized\nto less than —14.8 mV, the system quickly returns to its resting state (after a brief hyperpo-\nlarization; see Fig. 7.7). Depolarizing the membrane past —14.8 mV triggers a stereotypical\nbehavior, following a very similar limit cycle to the one shown by the FitzHugh-Nagumo\nmodel. Indeed, different initial conditions (in the case of Fig. 7.7 depolarizing the membrane\nto either —14.7 or to —12 mV) has little effect on the phase space trajectory (besides\nspeeding up initiation of the spike).\nFigure 7.8 summarizes the behavior of the system to constant current steps. Panel 7.8A is\na plot of the steady-state membrane potential as a function of the applied current. Following\nEqs. 7.11, this is identical to the steady-state ionic current /ionic(Vm, u>oo(Vm))- In this\nsystem, the sustained I—V relationship is strictly monotonic. As more and more current is\ninjected, the membrane is depolarized from rest at —31.7 mV (in the presence of the stabi-\nlizing 15-/MA/cm2 current injection), and the w nullcline intersects the Vm nullcline along its\nmiddle portion (Fig. 7.7A). Similar to the FitzHugh-Nagumo equations which experience\nthe onset of oscillations via a subcritical Hopf bifurcation, the real part of the two conjugate\neigenvalues goes to zero (at /_ = 24.9 /zA/cm2) and then becomes positive, indicating loss\nof stability. At the same time, a stable limit cycle appears: the system generates an infinite\ntrain of action potentials. For a limited region of current amplitudes, the equilibrium point\nremains unstable (as indicated by dashes in Fig. 7.8A) and the system spikes. All of this\nis more or less identical to what happens in the FitzHugh-Nagumo equations. Let us now\nchange these equations in a minor way to obtain quite a different behavior.\n7.2.2 Oscillations with Arbitrarily Small Frequencies\nA key property of a Hopf bifurcation is that if the system oscillates, it will oscillate at a\nwell-defined minimum frequency, as the squid axonal membrane and indeed most axons.\nHowever, numerous cell types, such as pyramidal cells in cortex, can generate trains of\nspikes with large interspike intervals, that is, with very low oscillation frequencies upon\ncurrent injection (see Fig. 9.7; Connor and Stevens, 1971c).\nHere, the onset of oscillations is governed by a dynamical mechanism different from\nthe Hopf bifurcation, which can be observed in a slightly modified version of the Morris-\nLecar equations. What has to occur is that the nullclines must intersect several times. We"}
{"text": "===== Page 16 =====\n7.2 The Morris-Lecar Model \n• \n187\nFig. 7.7 RESPONSE OF THE MORRIS-LECAR MODEL TO CURRENT PULSES Equations 7.11,\ndescribing electrical events in the muscle cells of barnacles (Morris and Lecar, 1981), show a\nqualitatively similar behavior to the squid axon membrane and to the FitzHugh-Nagumo equations.\n(A) Phase plane portrait (here membrane potential Vm versus potassium activation ti>) for different\nstimulus conditions. The nullclines are plotted in bold. In the presence of a stabilizing current injection,\nthe resting potential is —31.7 mV. From this state, the system is stimulated by brief current pulses,\ninstantly depolarizing the membrane (thin lines). If the stimulus depolarizes the muscle to either —18\nor to —14.8 mV, the system responds with a subthreshold excursion around the resting potential (two\ninnermost loops around the resting potential). For larger values, the system moves along a limit cycle:\nit spikes. (B) Evolution of the membrane potential Vm for this stimulus paradigm.\ncan achieve this by assuming, for example, that the potassium activation is a much steeper\nfunction of voltage than before. Following Rinzel and Ermentrout (1998), we modify the\npotassium activation as well as its dynamics,\n(7-16)\nand\n(7.17)\nWe also decrease the amount of calcium conductance by 10% to Gca = 1. All other\nparameters remain unchanged.\nAs witnessed by Fig. 7.9, this fundamentally changes the character of the phase portrait.\nNow, three equilibrium points exist. If we were to carry out a linear stability analysis, we\nwould find that the lower point is a stable sink (that is, both eigenvalues are real and negative),"}
{"text": "===== Page 17 =====\n188\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.8 SUSTAINED SPIKING IN THE MORRIS-LECAR MODEL (A) Steady-state voltage and\n(B) oscillation frequency as a function of the amplitude of the sustained current / for the reduced\nMorris-Lecar model (Eqs. 7.11). The steady-state /-V curve can also be viewed as the cumultative,\nsteady-state ionic current flowing at any particular membrane potential Vm. At /_ = 24.9 /LtA/cm2\n(arrow), the single equilibrium point (Fig. 7.7A) loses stability via hard excitation (a subcritical Hopf\nbifurcation) when the real part of the two conjugate eigenvalues goes through zero and becomes\npositive. As in the case for the Hodgkin-Huxley and FitzHugh-Nagumo equations, this type of onset\nof oscillations implies a nonzero oscillation frequency (here a minimum of 50 Hz).\nthe middle one is an unstable saddle (one eigenvalue is positive and the other negative),\nwhile the upper equilibrium point is an unstable spiral (the eigenvalues are complex, with\ntheir real part being positive; Fig. 7.9A). Indeed, the lower resting state at Vm — —29.2 mV\nand w = 0.0045 is a globally attracting rest state. A local perturbation in its neighborhood\nleads to a prompt decay back to rest, while a large input pulse will cause the system to\nmove on its usual trajectory around the phase space, a trajectory that ultimately ends at rest\nagain: the system responds to a brief current pulse with a spike before settling down again.\nHowever, different from the previous systems we analyzed, this one shows a true threshold\ndue to the presence of the unstable saddle.\nThe middle equilibrium point, associated with the negative and the positive real eigen-\nvalues, is called a saddle because it comes with a curve, called the separatrix curve, which\nsharply distinguishes between sub- and suprathresholds: any phase point to the left of this\ncurve will veer away from equilibrium toward the stable sink, while any point to the right\nof the separatrix will move in that direction and generate a spike (Fig. 7.9B). If the initial\ncondition places the system near the threshold separatrix, the system will display a long\nlatency before decaying back or spiking, because being close to both nullclines implies that\nboth variables Vm and w change only slowly.\nWhat happens if the amplitude of the injected current is increased? This has the effect of\nlifting the Vm nullcline closer to the w nullcline and moving the two lower equilibrium points"}
{"text": "===== Page 18 =====\n7.2 The Morris-Lecar Model \n• \n189\nFig. 7.9 SYSTEM WITH MULTIPLE EQUILIBRIUM POINTS The Morris-Lecar equations were\nmodified by changing the dynamics of potassium activation and making its voltage dependency\nsteeper (Eqs. 7.16 and 7.17). (A) Under these conditions, the two nullclines can intersect up to three\ntimes. Here, the lower equilibrium point is a globally attracting, stable sink, the middle one an unstable\nsaddle, and the upper one an unstable spiral. (B) Phase space portrait around the two lower equilibrium\npoints. The separatrix curve (thin line) at the saddle strictly separates sub- and suprathreshold regions\nof phase space. Points to the left of this curve will decay back to the stable sink, while points to\nthe right will lead to a spike. If the current injected into the system is further increased, these two\nequilibrium points will move toward each other, coalesce, and disappear. If the initial state of the\nsystem lies in the neighborhood of these two curves, it will evolve very slowly since, by definition,\nVm = 0 along the Vm nullcline and w = 0 along the w nullcline, explaining why the system is able\nto spike at very low frequencies.\ncloser and closer to each other. For a critical value I\\ = 8.326 /xA/cm2, the stable sink\nand the unstable saddle meet and coalesce and—upon further increases of /— disappear\nentirely. A saddle-node bifurcation has just occurred (Rinzel and Ermentrout, 1998), with\nthe system losing two of its equilibrium points. At the point when the two critical points\nmeet, the two eigenvalues are identical to zero.\nIf the state of the system is in the neighborhood of these two points (as in Fig. 7.9B), it\nwill take a long time to move into a different region of phase space. (After all, by definition,\nthe change in one or the other variable along the nullclines is zero.) Indeed, the closer the\nsystem is to the nullclines, the slower it will move. At I\\, that is, when the two equilibrium\npoints have coalesced, the period of the oscillations is infinite. It can be shown under some\nfairly generic conditions (Strogatz, 1994) that the oscillation frequency is proportional to\nVT^Ti\" (Figs. 7.10 and 7.11B).\nA property of systems with a saddle-node bifurcation is an N-shaped relationship between\nthe membrane potential and the injected current, or between the potential and the sustained\ncurrent (Fig. 7.11 A). In other words, for a range of membrane currents three different steady"}
{"text": "===== Page 19 =====\n190\nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nFig. 7.10 SPIKING AT Low FREQUENCIES \nResponse of the modified Morris-Lecar equations\n(Eqs. 7.16 and 7.17) using current steps (starting at t =0) of variable amplitude. In response to\na current step of 7.9 /u.A/cm2 amplitude, the membrane depolarizes. Close to l\\ = 8.326 ;u.A/cm2, the\nonset of spiking can be delayed almost indefinitely, similar to the delay observed in pyramidal cells\ndue to the presence of the A-like current (Fig. 9.7). This is not caused by the very slow time constant\nof any one ionic current, but it is due to the structure of the nullclines in phase space (Fig. 7.9B).\nFig. 7.11 SUSTAINED SPIKING IN THE\nMODIFIED MORRIS-LECAR MODEL \n(A)\nSteady-state voltage and (B) oscillation fre-\nquency as a function of / for the modified\nMorris-Lecar model (Eqs. 7.16 and 7.17).\nDifferent from Fig. 7.8, the I-V relationship\nis N-shaped. Spiking first occurs at I\\ (arrow)\nwhen the slope of this curve is infinite and\nthen becomes negative. This happens when\nthe two lower equilibrium points in the phase\nportrait of Fig. 7.9 merge, creating oscilla-\ntions with arbitrarily long interspike intervals\nvia a saddle-node bifurcation.\nstate voltages exist, of which only one, however, is stable (solid portion of the curve). When\nthe injected current is slowly increased, oscillations first occur when the slope of the I-V\ncurve becomes infinite (for / = I\\) and then negative. If very large stimulus currents are\nused, the system locks up at very depolarized levels and spiking ceases."}
{"text": "===== Page 20 =====\n7.4 Recapitulation \n• \n191\nIn this model, the transition from a stable equilibrium point to a stable limit cycle is\nmarked by arbitrarily low firing rates without having to rely on arbitrarily slow activation\nrates. Having a saddle is a necessary condition for such a continuous /-/ relationship. In our\ncase, this is achieved via a steep nonlinear relationship between the membrane potential and\nactivation of the potassium conductance. As we will see in Chap. 9, this is the characteristic\naction of a transient potassium current (the I A current of Connor and Stevens, 197 Ic), which\nhas a steep voltage dependency in the neighborhood of the resting state of the cell.\n7.3 More Elaborate Phase Space Models\nSo far, we reviewed models of spiking that are constrained to a two-dimensional phase\nspace. However, a variety of higher dimensional models have been studied to characterize\nmore complex behavior, in particular bursting and intrinsic rhythmic behavior.\nThalamic cells can respond to input by either firing a few fast action potentials or\ngenerating a slow calcium, all-or-none event, on top of which ride a series of fast, con-\nventional sodium spikes. Which response type occurs depends on the level of membrane\ndepolarization (Jahnsen and Llinas, 1984a,b; Fig. 9.4). Researchers have generated detailed\nbiophysical models of the various ionic currents that are responsible for this behavior\n(McCormick and Huguenard, 1992). Given the potential significance of bursting as a special\nmeans of communicating a privileged symbol, we dedicated Chap. 16 to this topic.\nRose and Hindmarsh (1985) use phase space models with three variables: one for\nthe membrane potential, one for the degree of accommodation, and one to mimic slow\nadaptation. They proved that it is this slow variable that is responsible for determining\nwhether the neuron fires in a tonic or in a burst mode in response to sustained current input.\nIndeed, a classification of bursting based on this type of phase-space analysis is beginning\nto emerge (Wang and Rinzel, 1995).\nAs reviewed extensively by Llinas (1988), many neurons do not behave at all like the\nsquid axon membrane, but have a great deal of diversity of electrical behaviors. For instance,\nsubsets of thalamic cells express complex intrinsic oscillations of the membrane potential in\nvery different frequency bands (from a few to 40 Hz or more) depending on the behavioral\nstate of the animal (sleep, arousal, etc.; see Steriade, McCormick, and Sejnowski, 1993). In\norder to understand the interplay between intrinsic cellular properties and network properties\nthat generate these period phenomena, networks of thalamic cells with more than half a\ndozen ionic conductances have been simulated (Destexhe, McCormick, and Sejnowski,\n1993; Wang, 1994; Contreras et al., 1997). Although the underlying phase spaces are\nhigh-dimensional, their projection onto suitable two-dimensional subspaces has shown the\nusefulness of analyzing the behavior of these systems in terms of the concepts introduced\nin this chapter.\n7.4 Recapitulation\nThe theory of nonlinear dynamics represents a powerful tool to characterize the generic\nmechanisms giving rise to the threshold response, the stereotypical shape of action poten-\ntials, or the onset of oscillations. It allows us to understand why these phenomena occur even\nwhen we have insufficient information concerning the detailed kinetics or the exact shape of\nthe various rate and time constants. In this chapter, we apply the theory of dynamical systems"}
{"text": "===== Page 21 =====\n192 \n. \nPHASE SPACE ANALYSIS OF NEURONAL EXCITABILITY\nto analyze two-dimensional systems with the help of the phase portrait. In particular, we\nfocus on the FitzHugh-Nagumo (FitzHugh, 1961,1969; Nagumo, Arimoto, and Yoshizawa,\n1962) and the Morris-Lecar (1981) models of spiking.\nThe FitzHugh-Nagumo model was based on the observation that the membrane potential\nand the sodium activation in the four-dimensional Hodgkin-Huxley equations evolve on a\nsimilar time scale, while the sodium inactivation and the potassium activation also share\nsimilar behavior, albeit on a slower scale. This feature can be exploited by expressing the\nmembrane excitability via the two-dimensional FitzHugh-Nagumo equations with constant\ncoefficients, with V corresponding to the excitability of the system and W its degree of\naccommodation. Linear stability analysis allows us to understand why the resting state is\nstable and when spiking first occurs. It is also helpful to explain the all-or-none shape of the\naction potential as an example of a limit cycle, upon which the system will rapidly converge\nonce threshold is exceeded.\nWe also acquainted the reader with a qualitativly similar system of equations that\ndescribes muscle fiber excitability in terms of a leak, a calcium, and a potassium current\n(Morris and Lecar, 1981). Following the lead of Rinzel and Ermentrout (1998), we assume\nthat the calcium current is so rapid that it is always in the steady-state with respect to\nthe membrane potential and the potassium activation, and we discuss two variants of\nthese equations. The first behaves similar to the squid axon and to the FitzHugh-Nagumo\nequations, generating oscillations with a nonzero oscillation frequency (via a subcritical\nHopf bifurcation). Modifying the potassium conductance to be a steeper function of Vm\nallows the system to spike at very low frequencies via a saddle-node bifurcation, similar\nto what occurs in cells that have a marked delay between the onset of current injection and\nthe first spike (due to the presence of /A).\nThe advantage of using models based on a very small number of variables—rather than\nrelying on biophysical very detailed models with an excuberance of variables—is that they\noffer us a qualitative, geometrical way to understand why a cell spikes and why it switches\nbetween two modes of firing without necessarily having to know every single detail of\nthe system.\nGiven the enormous numerical load involved in simulating the dynamics of hundreds\nor thousands of neurons, such a simplified single-cell model will allow us to study neural\nnetworks containing realistic numbers of neurons. The price one pays for this reduced\ncomplexity is a lack of quantitative predictions."}
{"text": "===== Page 4 =====\n196 • \nIONIC CHANNELS\nFig. 8.2 VOLTAGE DEPENDENCY OF IONIC CHANNELS (A) Current-voltage relationship of a single\nnicotinic ACh-activated ionic channel. Since the concentration of the permeant ion on both sides of\nthe membrane is identical (here set to either 150 mM NHj (downward pointing arrowhead) or Li+\n(upward pointing arrowhead)), the reversal potential is zero. As the voltage gradient across the channel\nincreases, so does the current through the channel (up to a limit), in accordance with Ohm's law. The\nsingle-channel conductance y is either 17 pS (in Li+) or 79 pS (in NH|). Reprinted in modified form\nby permission from Dani (1989). (B) I-V relationship for a single voltage- and calcium-dependent\npotassium channel in a symmetrical 160-mM potassium solution. The slope of this curve around the\norigin is 265 pS. Deviations from Ohm's law are apparent for large voltage gradients. Reprinted in\nmodified form by permission from Yellen (1984).\nNMDA channel (Fig. 4.9A). Another difficulty is that many channels show subconductance\nlevels; that is, they have two or more conductive states, each with its own characteristic\nconductance and with random voltage-dependent transitions among these sublevels. Indeed,\nin the long time series from which Fig. 8.1 was excerpted, occasionally the channel opens\nto a 4.3 pA level (Sigworth, 1983). Many, if not all, channels show evidence of some\nsublevels; this appears to be particularly true for GABA and glycine channels (Bormann,\nHamill, and Sakmann, 1987). Yet another complication is that the current through an open\nchannel sometimes briefly and transiently goes to zero. This rapid closing and opening of\nthe channel is called flickering and is thought to be caused by the transient blocking of the\npore by some ion or molecule. In some cases this happens so frequently that the current\nrecord has the appearance of a comb rather than a rectangular pulse. If the flickering is"}
{"text": "===== Page 5 =====\n8.1 Properties of Ionic Channels\n197\n{\n  \"table_name\": \"TABLE 8.1\",\n  \"description\": \"Properties of Different Types of Ionic Channels\",\n  \"columns\": [\"Channel Type\", \"Preparation\", \"γ (pS)\", \"τ (μs)\"],\n  \"rows\": [\n    {\"Channel Type\": \"FastNa+\", \"Preparation\": \"Squid giant axon\", \"γ (pS)\": 14, \"τ (μs)\": 330},\n    {\"Channel Type\": \"FastNa+\", \"Preparation\": \"Rat axonal node of Ranvier\", \"γ (pS)\": 14.5, \"τ (μs)\": 700},\n    {\"Channel Type\": \"FastNa+\", \"Preparation\": \"Pyramidal cell body\", \"γ (pS)\": 14.5, \"τ (μs)\": \"4-5\"},\n    {\"Channel Type\": \"Delayed rectifier K+\", \"Preparation\": \"Squid giant axon\", \"γ (pS)\": 20, \"τ (μs)\": 18},\n    {\"Channel Type\": \"Ca2+-dependent K+\", \"Preparation\": \"Mammalian preparation\", \"γ (pS)\": \"130-240\", \"τ (μs)\": null},\n    {\"Channel Type\": \"Transient A current\", \"Preparation\": \"Insect, snail, mammal\", \"γ (pS)\": \"5-23\", \"τ (μs)\": null},\n    {\"Channel Type\": \"Nicotinic ACh receptor\", \"Preparation\": \"Mammalian motor endplate\", \"γ (pS)\": \"20-40\", \"τ (μs)\": 10000},\n    {\"Channel Type\": \"GABA_A Cl- receptor\", \"Preparation\": \"Hippocampal granule cells\", \"γ (pS)\": [14, 23], \"τ (μs)\": null}\n  ]\n}"}
{"text": "Single channel conductance -y (in pS) and average channel density i\\ if known (per /ira2) for some ionic channels. The exact value\nof y depends on many variables, in particular the temperature and the composition of the extracellular fluid.\n1 In the absence of external divalent ions at 5° C; Bezanilla (1987).\n2 At 20° C; Neumcke and Stampfli (1982).\n3 Hippocampal pyramidal cell body and initial segment at 24° C; Colbert and Johnston (1996).\n\"•Between 13-25° C; less frequent are channels with 7 = 10 and 40 nS; Llano, Webb, and Bezanilla (1988).\n5 At 22° C; Latorre and Miller (1983).\n6 At 22° C; Adams and Nonner (1989).\n7 Peak density; the density decreases with distance from active zone; for a review, see Hille (1992).\n8 At 22° C; Edwards, Konnerth, and Sakmann (1990).\ntoo rapid to be adequately resolved by the recording apparatus, it effectively lowers the\napparent conductance of the channel. Nevertheless, to a good first approximation one can\ntreat ionic channels as conductances (with the appropriate reversal potential depending on\nthe concentration gradient across the membrane; see Eq. 4.2 and Fig. 8.2A; for more details,\nsee Hille, 1992).2\nEstimates of the single-channel conductance y based on Ohm's macroscopic law and\nthe geometry of the pore put the upper limit at about 300 pS and no channel with larger\nconductance has been reported. The current through single channels is usually larger than\n1 pA, corresponding to about 6000 monovalent ions passing through the channel each\nmillisecond; indeed most pass more current, with the record somewhere around 27 pA. The\ndiffusion equation places an upper limit of around 108 ions per second diffusing through\nthese small pores, whose width is only one or two atomic diameters at the most restricted\npart of the channel (Hille, 1992; Doyle et al., 1998).\nThe density of these channels can vary widely depending on the exact preparation used\n(Table 8.1). In the squid axon, there are on the order of 300 sodium and 18 potassium\nchannels per square micrometer. Similar numbers hold for other rapidly conducting systems\nwithout myelin insulation. In the frog or the rat node of Ranvier, a membrane specialization\nin myelinated axons where all the sodium channels appear to be concentrated (Sec. 6.6),\nthe density of Na channels can be as high as 2000 per square micrometer. Even at these\ndensities, channel proteins do not constitute major chemical components of most neuronal\nmembranes. In principle, between two and four orders of magnitude additional channels\ncould be packed into the membrane before the maximum molecular crowding sustainable\nby a membrane is achieved (Hille, 1992). Given ionic fluxes of between 107 and 108 ions\nper second per channel and a membrane capacitance of around 1 /nF/cm2, these densities\n2. Modern patch-clamp amplifiers have such high resolution that they are limited by the thermal noise of the recorded system,\nrather than by instrument noise. As is evident in Fig. 8.1, the current measurements fluctuate in both opened and closed states.\nThe standard deviation of these fluctuations—dictated by statistical physics—is typically less than 0.1 pA."}
{"text": "===== Page 6 =====\n198\nIONIC CHANNELS\nare more than sufficient to lead to the rapid and pronounced changes in membrane potential\nevidenced in action potential initiation and propagation and other electrical events.\nIt should be emphasized that the properties of ionic channels discussed here, in particular\ntheir microscopic, all-or-none and stochastic nature, hold true regardless of whether we are\ndealing with ligand-gated NMDA or GAB A receptors, voltage-dependent sodium, calcium,\nor potassium channels or ionic channels in nonneuronal systems, such as in T lymphocytes,\ncardiac pacemaker cells, muscle cells, mechanosensitive calcium channels in vertebrate\nhair cells or channels in the unicellular organism Paramecium. Furthermore, there appears\nto be very little systematic difference between channels recorded from molluscs, flies,\nsquids, or mammals. All these channels are to a good, first approximation activated by\nthe same stimuli, respond with the same kinetics, and are blocked by the same agents.\nIn particular, ionic channels in more highly developed animals, such as vertebrates, are\nno more complex than channels in less complex animals, such as molluscs. Evidence from\nmolecular cloning studies suggests that the major channel families evolved into their present\nform with the appearance of multicellular organisms about 700 million years ago (Hille,\n1992). If these speculations concerning the evolution of channels bear out, nature seems\nto have converged on a small number of basic circuit elements—aqueous pores built using\nprotein technology—a long time ago and has remained faithful to this basic design across\nall species.\n8.1.2 Molecular Structure of Channels\nFigure 8.3A shows an electron-microscopic image of an actual ACh-gated channel within\nits natural habitat, while Fig. 8.3B summarizes in a graphic manner the current working\nhypothesis for the structure of a voltage-dependent ionic channel. The entire macromolecule\nFig. 8.3 MOLECULAR STRUCTURE OF AN IONIC CHANNEL \n(A) Electron-microscopic image using\ncrystallographic methods of the axial section of one of the nicotinic acetylcholine receptors in the\nelectric fish Torpedo. This channel has the shape of a tall hour glass, with an overall length of 110 A.\nMost of the protein lies outside the bilipid membrane. The complex extends about 20 A into the\ncytoplasmic medium and about 60 A into the extracellular environment. The most narrow portion of\nthe channel is about 8 A wide. The blob at the bottom of the image is not part of the channel. Reprinted\nby permission from Toyoshima and Unwin (1988). (B) Schematic view of a generic voltage-dependent\nionic channel. The molecular details of such channels are now being charted by structural studies (e.g.\nDoyle et al., 1998). Reprinted by permission from Hille (1992)."}
{"text": "===== Page 7 =====\n8.1 Properties of Ionic Channels \n• \n199\nmaking up the channel is sitting in the lipid bilayer of the membrane, anchored to elements of\nthe intracellular cytoskeleton. It is quite large, consisting of several thousand amino acids\n(and associated sugar residues), with a molecular weight in excess of 300,000 daltons.\nWhen the channel is open, it forms a water-filled pore extending across the membrane. The\nselectivity for certain ions, that is, the fact that the sodium channel is primarily permeable\nto Na+ ions and not to K+ ions, is conveyed to the channel by the most narrow portion of\nthe channel (termed the selectivity filter) with a diameter of around 3 A(Hille, 1973).\nThe voltage-dependent gating of the channel is achieved by a conformational change in\nthe three-dimensional shape of part of the molecule, such that the channel is either open or\nblocked. As discussed already by Hodgkin and Huxley (1952d), this requires the movement\nof hypothetical gating particles able to sense the electric field across the membrane. In fact,\nat a resting potential of —80 mV an electric field of about 200,000 V/cm is set up across the\n40 Athin membrane. Changes in this field move these gating charges through the membrane\nand change the configuration of the channel. The steepness of the voltage dependency of\nthe sodium channel (that is, the steepness of m^ in Sec. 6.2.2) yields an estimate of six\nelementary gating charges that need to move across the entire membrane in order for a\nsingle sodium channel to open. This gating current, caused by the channel changing its\nconfiguration so that it can conduct current, is tiny compared to the 107 or so ions moving\nthrough the open channel each second but has been measured (Armstrong and Bezanilla,\n1973). Qualitatively, the gating current can be thought of as a nonlinear capacitive current,\nproportional to Qmdm/dt, where Qm is the total charge associated with the transition of\nthe m particle from close to open.\nRecombinant DNA technology—in parallel with classical methods such as protein\nchemistry, X-ray diffraction, and electron microscopy—has given us the opportunity to\nimage channels as well as to derive their amino acid sequence and to clone, express, and\nmanipulate them. (For overviews of this very active research area see Catterall, 1995,\n1996 and Doyle et al., 1998.) These methods have allowed the activation and inactivation\nprocesses of the sodium channel to be characterized at the molecular level.\nThe key culprit in voltage-dependent activation are four homologous segments (termed\nS4) of the sodium channel protein that span the membrane in an a helix. These S4 regions are\nhighly conserved between Na+ channels from different species and are also homologous to\nspecific regions of the voltage-dependent K+ and Ca2+ channels. Even before the molecular\nsequence of these channels was known, Armstrong proposed that two helices, one bearing\nmultiple positive charges and the other multiple negative charges, move relative to each\nother, perhaps by rotating, so that relative translation by one turn would be equivalent to\nmoving one charge across the membrane. Subsequently, Catterall (1988, 1992) outlined\nthe sliding helix model for the action of each S4 segment in activating the channel, a\nmodel for which much experimental support is now available (Fig. 8.4). Upon membrane\ndepolarization, each S4 segment rotates by 60°, moving outward by 5 A. Every third\namino acid residue making up this helix is positively charged and is about 5 A away from\nthe next charged residue. In other words, the rotating movement of the helix effectively\nmoves one charge across the membrane. All four S4 segments, acting in concert, cause\nthe conformational change that leads to the opening of the channel. A similar mechanism,\nacting also via S4, is thought to confer voltage-dependency onto certain potassium channels\n(Larsson et al., 1996).\nInactivation is thought to arise by a different process, usually explained by invoking\na mechanistic analogy, the ball-and-chain model (Armstrong and Bezanilla, 1977; Hoshi,\nZagotta, and Aldrich, 1990). Here, a part of the channel molecule on the inner, cytoplasmic"}
{"text": "===== Page 8 =====\n200 • \nIONIC CHANNELS\nFig. 8.4 VOLTAGE-DEPENDENT ACTIVATION OF\nTHE NA+ CHANNEL \nSliding helix model postu-\nlated by Catterall (1988, 1992) to account for the\nvoltage dependency of activation of the fast sodium\nconductance underlying action potential generation.\nThe S4 segment of the channel protein that is ex-\ntended as an a helix throughout the membrane is\nelectrically neutral at rest. In the presence of sufficient\ndepolarization, the helix rotates by 60° in a screw-like\noutward movement, until it moves into a new stable\nposition, where locally the charges are balanced. The\nmovements of four such S4 segments that are part of\nthe channel molecule are thought to induce a confor-\nmational change that leaves the channel in its open\nstate. Reprinted by permission from Catterall (1992).\nside of the membrane is thought to act like a tethered gate. Under the right conditions, it\nswings shut, physically occluding the pore. Zagotta, Hoshi, and Aldrich (1990) showed in\na series of elegant site-directed mutagenesis experiments that deletion of the appropriate\ncytoplasmic part of the inactivating, transient A-type K+ channel molecule does, indeed,\neliminate, or greatly reduce, inactivation. Furthermore, if this cytoplasmic region was added\nas a synthetic soluble peptide, inactivation was restored.\nBecause of the existence of four S4 segments in the Na+ channel, it can be argued that\na better model of sodium current activation would be m4, rather than the m3 term favored\nby Hodgkin and Huxley (see Eq. 6.13). Yet it needs to be remembered that the Hodgkin-\nHuxley equations are purely phenomenological fits of the data; the cubic m expression was\nintroduced in order to make a first-order differential equation account for the sigmoidal\nrise of/Na-\nOnce the detailed molecular structure of the sodium and other channels is worked out,\nwe will be in a better position to derive the relevant equation relating the voltage across\nthe channel to the time-varying current through it. It remains unclear, though, whether\nmore complex models will increase our qualitative understanding of the dynamics of the\nmacroscopic current.\nOne last comment on the molecular nature of ionic channels. These proteins are so\namazingly specific that substituting one amino acid at one of two positions in the several\nthousands long amino acid sequence coding for the sodium channel with another amino acid\nwill alter the ion-selection properties of the sodium channel to resemble those of calcium\nchannels (Heinemann et ah, 1992).\n8.2 Kinetic Model of the Sodium Channel\nIn Chap. 6, we saw that Hodgkin and Huxley postulated a number of fictive \"gating particles\"\nto satisfactorily describe their macroscopic current records. For any current to flow, several\n(four in the case of /Na) such particles have to come together simultaneously. Given the\nevidence alluded to above concerning internal repeats in the channel protein, m and h can\nbe reinterpreted in molecular terms as the conformation of these internal repeats (Fig. 8.4).\nSimilar activation and inactivation variables have been postulated for the myriads of sodium,"}
{"text": "===== Page 9 =====\n8.2 Kinetic Model of the Sodium Channel\n201\npotassium, calcium, and chloride channels described over the last two decades (see the\nfollowing chapter).\nClassical kinetic theory specifies the manner in which one can go from a description\nof these particles and their underlying states to the final, overall gating scheme. Two key\nassumptions underlie classical kinetic theory: (1) Gating is a Markov process, implying that\nthe rate constants, regulating the transitions from one state to the next, do not depend on\nthe previous history of the system. A sufficient description of the system is its present state,\nwithout knowing its previous state. (2) All transitions are assumed to be characterized by a\nfirst-order differential equation, with a single associated time constant. The applications of\nthese methods to probabilistic ionic channels are summarized by Neher and Stevens (1977),\nColquhoun and Hawkes (1981) and DeFelice (1981).\nThese two simplifying assumptions are used to construct the overall state diagram of the\nsystem, as well as its dynamic behavior, as we will now do for the fast sodium channel.\nFigure 8.5 illustrates the simplest symmetrical eight-state model of this channel. Here three\nidentical m particles can be either in an \"open\" or in a \"closed\" position. am can be thought\nof as proportional to the probability for an m particle to make the transition from closed\nto open (and fim proportional to the probability for the opposite transition). The fourth\nh particle can be either in an \"inactivated\" (top row of Fig. 8.5) or in a \"not inactivated\"\n(bottom row) position, with a/, the probability that the h particle will switch from inactivated\nto open. Alternatively, m can represent the fraction of m gates in their open state and h the\nfraction of h gates in their noninactivating state. Now, the a's and /3's need to be interpreted\nas rate constants, governing the rate transitions from one state to the other.\nIf we assume that the system is in a state where no particle is in its open position, any one\nof the three identical m particles can open. Since any of these three particles can open, the\nrate constant is 3oim if we assume that the m particles act independent of each other. For this\none open particle to switch back to its closed position, the rate constant is fim. To make the\nnext transition from one to two open particles, any one of the two remaining closed particles\ncan switch and the appropriate rate constant is 2am, and so on. For the system to be in its\nFig. 8.5 KINETIC DIAGRAM OF THE FAST SODIUM CHANNEL Simplest kinetic diagram associated\nwith the fast voltage-dependent Na+ channel. In order for the channel to be in its conductive state, all\nthree m particles as well as the inactivating h particle need to be in their open position. Because we\nhere assume that all three m particles are identical, eight different states exist. am is the rate constant\nat which the m particles switch from their closed to their open positions (that is, moving toward the\nright in this diagram), while /3m is the rate constant for the reverse operation (transition toward the\nleft). Similarly, a/, is the rate constant governing the transition from inactivating to noninactivating\nstate (downward) and fa for the reverse operation. Many more complex gating schemes have been\ndiscussed in the literature."}
{"text": "===== Page 10 =====\n202 \n• \nIONIC CHANNELS\nconductive state, the h particle has to switch into the appropriate configuration; thus, of the\neight possible different states (from zero m particles open and h closed to all three m as\nwell as h open), only a single one—in the lower right corner in Fig. 8.5—corresponds to\nthe open state.\nAssuming the Markov property and first-order transitions among states, leads us to the\ncalculus used by Hodgkin and Huxley in 1952. Following the logic laid out in Chap. 6, if\nthe membrane potential is stepped from its initial value V0 to Vi, the fraction of m particles\nin their activated state follows a simple exponential law,\n(8.2)\nwith moo(V) = am(V)/(am(V) + ^m(V)) and rm(V) = l/(am(V) + 0m(V)). With our\nusual assumption of independence of the gating particles, the fraction of channels populating\nthe conductive m^h\\ state is\n(8.3)\nBy multiplying out the exponentials, this equation can be expressed as a sum of seven\nexponentially decaying terms,\n(8.4)\nwith seven associated time constants r, = rm, 2rm, 3rm, T/,, TmTh/(Tm+Th), Tmrft/(rm+\n2r/,), and Tmr/,/(rm + 3t>,). For a system with eight independent states and first-order\ntransitions among these, seven time constants are expected.\nExperimentally, most of these time constants are difficult to resolve, since only the total\nchannel current can be recorded, making the identification of these states difficult. If the\nthree m subunits are assumed to differ from each other in some minor ways, the Na+ channel\nneeds to be characterized by 15 closed and a single open state and by 15 time constants,\nfurther compounding the problem. The rate constants between the different states can also\nbe more complex than in the simple scheme illustrated in Fig. 8.5. If cooperativity exists,\nthe transitions between states are not independent of each other. For instance, it has been\npostulated that the rate of inactivation depends on the number of open subunits and is much\nfaster than in the standard Hodgkin-Huxley model (so-called coupled models; Armstrong\nand Bezanilla, 1977; Aldrich, Corey, and Stevens, 1983). A great deal of ingenuity continues\nto be spent deciphering the correct kinetic gating scheme with the help of highly refined\nmeasuring techniques. It remains an open question whether working out all the details of\nthe underlying microscopic transitions will lead to a description of the ionic current at\nthe macroscopic level substantially different from that in use today. Furthermore, as we\nwill illustrate now, the large number of channels underlying most signaling in neurons\nallows us to average over individual stochastic channels and to only consider deterministic\nionic currents.\n8.3 From Stochastic Channels to Deterministic Currents\nIn Chap. 6 we learned how Hodgkin and Huxley successfully described the dynamics of the\nmembrane voltage in terms of deterministic and graded ionic currents. Yet as detailed above,"}
{"text": "===== Page 11 =====\n8.3 From Stochastic Channels to Deterministic Currents\n203\nwe now know this to be caused by the current through stochastic all-or-none molecular pores.\nUndoubtedly, the macroscopic current is constituted by the summation of the microscopic\ncurrents sweeping through many, many channels. But what is their exact relationship?\nFigure 8.6 expresses the general idea: although the precise time each channel switches into\nits open state is a random occurrence, the average duration of channel opening has a well-\ndescribed temporal onset, depending systematically on the membrane voltage. On each of\nthe 10 trials in Fig. 8.6, the exact sequence of openings and closings of a single delayed-\nrectifier potassium channel differs, yet averaging over a sufficient number of trials leads to\nthe smoothly varying current expected from the continuous Hodgkin-Huxley formalism.\nIndeed, since h^ is close to 1 and m^ is close to 0 at —80 mV (absolute potential; Fig. 6.4),\nthe time course of Fig. 8.6B can be approximated by (1 — e~'/rm)3e~'/rh.\nA number of researchers have explored the relationship between microscopic channels\nand macroscopic currents (VerveenandDerksen, 1968; Lecar andNossal, 1971a,b; Skaugen\nand Walloe, 1979; Skaugen, 1980a, b; Clay and DeFelice, 1983; DeFelice and Clay, 1983;\nStrassberg and DeFelice, 1993; Fox and Lu, 1994; Chow and White, 1996). Let us delve\ninto this topic in a bit more detail.\n8.3.1 Probabilistic Interpretation\nLet us go back to the idea expressed in the nv'h formalism of Hodgkin and Huxley. Ions are\nonly free to drift down the potential gradient of the channel if all four gating particles are\nA) Unitary Sodium Currents\nFig. 8.6 STOCHASTIC OPEN-\nINGS OF INDIVIDUAL SODIUM\nCHANNELS \n(A) \nRandom\nopening and closing of a hand-\nful of fast sodium channels\nin a mouse muscle cell. The\nmembrane potential was stepped\nfrom -80 to -40 mV; the first\ntrial reveals the simultaneous\nopening of two Na+ channels,\nwhile on all other trials, only\na single channel was open. (B)\nAveraging over 352 such trials\nleads to a smoothly varying cur-\nrent in accordance with the m3h\nmodel of Hodgkin and Huxley.\nExperiment carried out at 15° C.\nReprinted by permission from\nPatlak and Ortiz (1986).\nB) Ensemble Average"}
{"text": "===== Page 12 =====\n204 • \nIONIC CHANNELS\nin their open position. Let m(t) be the fraction of activation particles in their open position\nand /3m(Vm) the rate at which transitions occur from open to closed. In the case of the\nHodgkin-Huxley model (Eq. 6.17), each 18-mV depolarization increases this rate e-fold.\nIn the absence of any new open states being created, the fraction of open states must decay\naccording to\n(8-5)\nIf at the beginning of the experiment at t = 0, the fraction of particles in their open position\nwas mo, the temporal evolution of m is given by\n(8.6)\nSince we assume that the gating particles have no memory, in other words, that the\nprobability of a given particle to change its state does not depend on its previous history\n(Markovian property), we can express the probability that a single particle will remain in\nits open position after an observation period T as\n(8.7)\nIf the m variable is in its closed position it can switch into its open state at the voltage-\ndependent rate um(Vm). The total change in the fraction of particles open is the difference\nbetween the fraction of closed particles making the transition into open states and the\nfraction of open states decaying away into closed states,\n(8.8)\nNote that this is the equation we encountered in Chap. 6 in the context of the deterministic and\nmacroscopic Hodgkin-Huxley formalism, while here we interpret m(t) in a probabilistic\nmanner.\nLet us apply this interpretation to the fast sodium channels of the squid giant axon.\nAdopting the simplest kinetic scheme illustrated in Fig. 8.5, we need to compute the\nprobabilities for each sodium channel to be in one of its eight distinct states. For instance, the\nprobability of the mih\\ state (Fig. 8.5; two activation particles and the inactivation particle\nall open) is given by the product (since all four particles are assumed to act independently)\nof the probability of rinding two open activation particles (m2), one closed particle (1 — m)\nand one open inactivating particle (h). Because any two of the three m particles can be\nopen, the final probability for this state is\n(8.9)\nwhere VQ is the voltage at rest. Similar expressions can be derived for all other seven\nstates. In a Monte-Carlo computer simulation of the probabilistic openings and closings\nof single channels (as first carried out by Skaugen and Walloe, 1979, and Skaugen,\n1980a,b), the actual initial state is computed by assigning each of the probabilities for\nthe eight states a portion of the real line between 0 and 1. The length of this parti-\ntioning is given by the associated probability (with the sum of all probabilities equal\nto 1, of course) and by generating a random number r\\ (drawn from a uniform prob-\nability distribution between 0 and 1). The part of the line on which r\\ falls is the ini-\ntial state."}
{"text": "===== Page 13 =====\n8.3 From Stochastic Channels to Deterministic Currents \n• \n205\nAssume now that the membrane potential changes instantaneously from Vb to V\\. How\nlong will the channel remain in its initial state? The probability that the channel remains in\nthis state after a period T is\n(8.10)\nThe three constants in the exponential account for the fact that the third subunit could open\n(with rate am), or either one of the two subunits could close again (2/?„,), or the channel could\ninactivate («A). To determine the actual lifetime, another random number r-i is generated;\nthe duration of state mih\\ is then — (am + a/, + 2/6m)~1ln(r2). The probability for the\nchannel to make a transition from m^h \\ to the state where all four subunits are in their open\nstate and the channel conducts (state m?,h\\) is given by\n(8.11)\nLikewise, the probability for the channel to switch from m^hi into m^h^ is\n(8.12)\nand from m-ih\\ into m\\h\\ it is\n(8.13)\nPartitioning the unit line into three intervals, rj is generated to decide into which state the\nchannel has switched: if 0 < r3 < p3, the open state m3h\\ is chosen; if j?3 < rj, < p^ + p^,\nit will be m2/io» and if p^ + p* < r^ < 1, the state m\\h\\ is chosen. In this manner, the\nlife history of one particular sodium channel is simulated as the membrane is voltage-\nclamped (Fig. 8.7). A patch of membrane with more than one channel, where the channels\nare assumed to act independenty—no substantial evidence for coupling among neighboring\nchannels has emerged—can be simulated with a more efficient procedure that keeps track\nof what fraction of the population is in what state (Skaugen and Walloe, 1979; Chow and\nWhite, 1996).\nAs witnessed by the bottom trace in Fig. 8.7, the changing conductance associated with a\nhandful of opening and closing channels can be quite different from that of the macroscopic\nconductance expected by the continuous Hodgkin-Huxley equation. Yet for 600 channels,\nthe behavior approximates that of the continuous deterministic m3(t)h (?) formulation. This\nis not surprising, since the central limit theorem tells us that the standard deviation of the\nmean of a population of n independent variables increases like «Jn as n -> oo. Thus,\nthe relative deviation from the mean decreases as l/^/n for large values of n. For 600\nsodium channels, corresponding to a few square micrometer of squid membrane, the total\nconductance differs little from that obtained by the continuous m3/i formulation. Suitably\nnormalized, this is also the conductance obtained when experimentally averaging over\nhundreds of single-channel openings, as shown empirically in Fig. 8.6B,\nA similar Monte-Carlo procedure can be used to calculate the behavior of a piece\nof squid axon membrane in the presence of a constant stimulus current (Strassberg and\nDeFelice, 1993). Here, the potassium channels are modeled using the simplest possible\nlinear and noncooperative kinetic scheme with five states (four closed and one open) with\nthe appropriate values of an (Vm) and fin (Vm) (with all rate constants as specified in Chap. 6).\nThis patch of membrane is assumed to be studded with a constant density of channels (18"}
{"text": "===== Page 14 =====\n206\nIONIC CHANNELS\nFig. 8.7 SIMULATED LIFE HISTORY OF INDIVIDUAL SODIUM CHANNELS The membrane po-\ntential in a simulated membrane patch containing a variable number of Na+ channels was stepped\nfrom VQ = 0 to V\\ = 50 mV at 5 msec (arrow). The normalized conductance associated with the\neight-state Markov model shown in Fig. 8.5 was evaluated numerically for several trial runs (see\nStrassberg and DeFelice, 1993). As the number of channels is increased from 6 to 600, the graded and\ndeterministic nature of the (normalized) sodium conductance emerges from the binary and stochastic\nsingle-channel behavior. The top trace shows the conductance computed using the continuous time-\ncourse (approximating (1 — e-'/r\"')3\ne~'/T») formalism of Hodgkin and Huxley (1952). This figure\nshould be compared against the experimentally recorded sodium current through a few channels in\nFig. 8.6B. Reprinted in modified form by permission from Strassberg and DeFelice (1993).\npotassium and 60 sodium channels per square micrometer) each characterized by its own\nstate. As the size of the patch is increased from 1 to 104 p,m2 (always assuming that all\nchannels see the same membrane potential) in the presence of a constant injected current\ndensity (to normalize for the membrane area) a steady progression from random openings\nto a highly regular train of action potentials can be observed (Strassberg and DeFelice,\n1993). For patches containing on the order of 1000 binary channels, repetitive spikes with\napproximately the same firing frequency (90 ± 10 Hz) as in the continuous Hodgkin-\nHuxley model occur (top trace in Fig. 8.8).\nWe saw in Fig. 6.10 how adding noise to the input current linearized the abrupt discharge\ncurve associated with the space-clamped Hodgkin-Huxley system. A similar linear behavior\nhas been obtained in computer simulations of the complete /-/ \ncurve in the presence\nof 1000 and fewer discrete Na+ channels (Skaugen and Walloe, 1979). It is only when\nincreasing the number of Na+ channels to 10,000 (while keeping Gj\\ja constant) that the\n/-/ curve with the sudden onset of the firing characteristic of the deterministic Hodgkin-\nHuxley equations is recovered.\nThe simulations discussed here were carried out at Vrest. At more depolarized potentials\nthe amplitudes of the channel fluctuations are larger. Indeed, it is plausible to imagine a\nscenario where an extended and deterministic input hovering just around firing threshold"}
{"text": "===== Page 15 =====\n8.3 From Stochastic Channels to Deterministic Currents \n• \n207\nFig. 8.8 ACTION POTENTIALS AND SINGLE CHANNELS Computed membrane potential (relative\nto Vrest indicated by horizontal lines) in different size patches of squid axon membrane populated\nby a constant density of Na+ and K+ channels. The space-clamped membrane is responding to a\ncurrent injection of 100 pA//xm2. The transitions of each all-or-none channel are described by its\nown probabilistic Markov model (the eight-state model in Fig. 8.5 for the Na+ channel and the\nsimplest possible five-state linear model for the K+ channel). For patches containing less than 100\nchannels (bottom trace), firing can be quite irregular. For patches containing dozens or fewer channels\nit becomes impossible to define action potentials unambiguously, since the opening of one or two\nchannels can rapidly depolarize the membrane (not shown). As the membrane potential acts on 1000\nor more binary and stochastic channels, the response becomes quite predictable, and merges into\nthe behavior expected by numerical integration of the Hodgkin-Huxley equations for continuous and\ndeterministic currents (top trace). The density is set to 60 Na+ channels and 18 K+ channels per square\nmicrometer, each with a single channel conductance y of 20 pS. All other values are as specified in\nthe standard Hodgkin-Huxley model. Reprinted in modified form by permission from Strassberg and\nDeFelice (1993).\ncauses stochastic spiking by virtue of the fact that even in a spatially extended system,\nfluctuations in a small number of channels are enough to exceed Vth and initiate firing.\nIndeed, pyramidal cells responding to just suprathreshold, maintained current injections are\nknown to show much more jitter in their exact timing compared to a temporally modulated\ncurrent input (Mainen and Sejnowski, 1995). As sustained inputs are unlikely to occur\nunder ecological conditions, it remains unclear to what extent the stochastic nature of ionic\nchannels influences computations performed on physiological input.\nFor simulated squid axon patches of less than 1 f^m2, the opening of any one channel\nhas an appreciable effect on the membrane potential. The all-or-none behavior of the action\npotential is less and less evident in these cases, since the simultaneous opening of a few\nchannels depolarizes the membrane significantly. Given this variability in the amplitude of\nthe membrane response, it is difficult to define a firing frequency for a very small number\nof channels."}
{"text": "===== Page 16 =====\n208 \n• \nIONIC CHANNELS\nWe should emphasize here the obvious, namely, that individual channels do not show\nany threshold behavior. They do not change their configuration abruptly at some fixed\nvoltage level. Rather, the probability of opening increases continuously with voltage. The\nmacroscopically observed voltage (or current) threshold occurs when the sum of all inward\ncurrents through the Na+ channels just about exceeds the sum of the outward currents\ngenerated by the potassium and leak conductances and by the local circuit current.\n8.3.2 Spontaneous Action Potentials\nMuch more tantalizing behavior can be seen in the absence of any direct stimulus. Fig-\nure 8.9 illustrates the flat voltage trajectory associated with the continuous Hodgkin-Huxley\nequations when no external current is supplied. This stationary behavior also occurs for\nFig. 8.9 SPONTANEOUS ACTION POTENTIALS The same model as is Fig. 8.8 is rerun, but now\nin the absence of any current stimulus. The bottom three traces show Monte-Carlo sample trials for\naxonal membrane patches of different surface areas A populated with a constant density of Na+\nand K+ channels (60 and 18 channels per square micrometer, respectively). Because the opening of a\nsingle sodium channel (of conductance y) can give rise to a very large depolarization, the spontaneous\nopenings of two sodium channels is enough to trigger the opening of the remaining sodium channels\nand the membrane generates a spike. These spikes are Poisson distributed (once a refractory period has\nbeen accounted for; Chow and White, 1996). The top trace shows the solution of the continuous and\ndeterministic Hodgkin-Huxley equation: V in the absence of a current input remains flat. The circuit\ninset provides the basic intuition why a single channel, in the presence of a small leak conductance\nG, can cause a large membrane depolarization. These simulations predict that in high-impedance\nsystems with an active membrane, spontaneous spiking can be observed under certain conditions\nin the absence of any synaptic input. The membrane leak conductance G for these three conditions\ncorresponds to 3 pS, 300 pS, and 30 nS, as compared to a single-channel conductance of y =20 pS for\nboth channel types. For more details see legend to Fig. 8.8. Reprinted in modified form by permission\nfrom Strassberg and DeFelice (1993)."}
{"text": "===== Page 17 =====\n8.3 From Stochastic Channels to Deterministic Currents \n• \n209\nsimulations with several hundred thousand discrete probabilistic ionic channels (third trace\nfrom the bottom in Fig. 8.9). As the number of channels is reduced—by reducing the size of\nthe space-clamped patch—a much more interesting behavior emerges: the axonal membrane\n\"spontaneously\" generates a train of remarkably regular action potentials.\nWhat causes microscopic events, such as the openings of a few channels, to be so\ndrastically amplified that they can lead to observable macroscopic phenomena? Upon\ninspection of the V(t) trace for the smallest 1-jU.m2 patch at the bottom of Fig. 8.9, we\nsee many binary transitions from a value close to £R to a value a few millivolts above Vrest.\nThese events are caused by the abrupt opening of single sodium channels. That one ionic\nchannel can have this large effect can be understood in terms of a very simple circuit (inset\nin Fig. 8.9). We assume the membrane to be at rest, with a steady-state membrane leak\nconductance G = AGm, where A is the surface area of the patch and Gm — 3 pS//xm2.\nEach channel opening contributes y = 20 pS toward the total conductance, an amount\nindependent of the surface area. The voltage trajectory following the opening of a single\nchannel is given by\n(8-14)\nwith AV = Efia/(l + AGm/y) and r = ACm/(AGm + y). For this simple model, AV\nin response to a single sodium channel opening is 100 mV. However, V (?) in the lower trace\nin Fig. 8.9 hovers just above £K> due to a few open potassium channels. They add enough\nbackground conductance for A V to be on the order of 20 mV, with the membrane potential\nreaching its equilibrium within a fraction of a millisecond. When two sodium channels (out\nof the 60 present in the l-/xm2-large patch) open coincidentally, they rapidly depolarize\nthe membrane beyond Vih associated with the deterministic equations, thereby initiating an\naction potential.\nThe fact that the random opening of channels can induce \"spontaneous\" spikes is\nultimately caused by the fact that while the leak conductance (and the membrane ca-\npacitance) is assumed to scale with the membrane area A, the channel conductance is\nindependent of A. The probability that two out of 60 independently acting sodium channels\nare open is so high, even at E&, that the membrane is either always refractory or spiking,\nleading to an average firing frequency of around 90 Hz. And all of this in the absence\nof any synaptic or current input. For larger values of A, the membrane depolarization\nproduced by the simultaneous opening of a small number of channels becomes so small\nthat the membrane potential effectively remains always around zero (third voltage trace in\nFig. 8.9).\nIn an elegant study demonstrating how concepts from physics can be applied to neu-\nrobiological problems, Chow and White (1996) derive an analytical expression for the\nprobability of spontaneous spiking in analogy to the classical problem of determining the\nprobability that a particle in a double-well potential makes the transition from one well to\nthe other. Under the assumption that sodium activation occurs at a much faster time scale\nthan changes in V, n, or h, they incorporate fluctuations in m into a stochastic conductance\nterm that is added to the conventional deterministic gNa term. Chow and White find that\nthe probability of spontaneous firing decays exponentially as the area of the membrane\npatch increases. Furthermore, the interspike intervals are exponentially distributed once the\nrefractory period is accounted for (Chap. 15). In other words, the spontaneous spikes are\nPoisson distributed.\nUnder what circumstances could the random opening of one or a few ionic channels\ncause such spontaneous action potentials to occur? From an electrical point of view, this"}
{"text": "===== Page 18 =====\n210 . \nIONIC CHANNELS\nrequires a small input conductance (on the order of the single channel conductance or less)\nas well as a low membrane capacitance. Assuming a relatively conservative value of the\nmembrane resistance of Rm — 30,000 £2-cm2, implies that the entire leak conductance of a\nspherical 10-/Mm-diameter cell is about five times the open-channel conductance of a single\nsodium channel.\nThe occurrence of action potentials following the opening of a single ionic channel in\ncells with very high input resistances (tens of gigaohms) has been experimentally observed\nin chromaffin cells of the adrenal medulla (Fenwick, Marty, and Neher, 1982), rat olfactory\nreceptor neurons (Lynch and Barry, 1989), and in hippocampal cultured neurons (Johansson\nand Arhem, 1994). Whether or not the opening of these channels is truly spontaneous, that\nis, due to thermal fluctuation in the configuration of the channel protein, or triggered by\nsome factor in the extra- or intracellular milieu is very difficult to establish experimentally.\nSimilar phenomena might well occur in thin spines, distal dendrites, or even the cell body\nof small interneurons, in particular if shielded by membrane invaginations or the cellular\nnucleus. Does the nervous system exploit such local sources of microscopic noise for its\nown, functional purposes?\n8.4 Recapitulation\nUnderlying the entire gamut of electrochemical events in the nervous system are proteins\ninserted into the bilipid membrane, so-called ionic channels, which allow specific ions\npassage across the bilipid membrane. Given their small electrical conductance (between 5\nand 200 pS), it required the development of the giga-seal technique by Neher and Sakmann\nto readily observe their behavior. From a functional point of view, the key properties of\nchannels are that they possess one or a very small number of open, conductive states\nand that the transitions among closed, open, and inactivated states are governed in a\nprobabilistic manner by the amplitude of the applied membrane potential (for voltage-\ndependent channels) or the presence of various agonists (for ligand-gated channels).\nA very large effort in the field is directed toward identifying and characterizing the\nmolecular sequence of these channels and in relating specific structural features of the\nchannel protein to its voltage and ionic selectivity and, ultimately, to its function. Such\na detailed molecular understanding goes hand in hand with the construction of ever more\ncomplex kinetic models, describing the transitions of a single channel among a large number\nof internal states in terms of probabilistic Markov models.\nNumerical studies have related the microscopic, stochastic chatter of individual ionic\nchannels opening and closing to the observance of macroscopic currents changing in a highly\ndeterministic and graded manner. A membrane at rest and studded with a few thousand\ndiscrete channels behaves in general little different from the deterministic Hodgkin-\nHuxley equations. Deviations are only expected to occur when the membrane potential\nis close to threshold or when the channels are embedded into a small patch of neuronal\nmembrane with a low leak conductance. In the latter case, the single-channel conductance\ncan have the same magnitude as the passive leak conductance, and the opening of one\nor a few channels can depolarize the membrane beyond Vth- The resultant spikes are\nPoisson distributed. In other words, the microscopic behavior of individual molecules is\namplified and causes a macroscopic event, an action potential. Such a mechanism, known to\noccur in certain experimental preparations, could be of functional relevance in very small"}
{"text": "===== Page 19 =====\n8.4 Recapitulation \n• \n211\ncells or in electrically decoupled, distal parts of the dendritic tree as a \"random event\"\ngenerator.\nIn the remainder of this book we will typically deal with large enough membrane areas—\nand therefore channel numbers—that we are usually justified in treating electrical events in\nterms of deterministic, continuous currents, rather than in terms of probabilistic, all-or-none\nionic channels."}
{"text": "===== Page 1 =====\nBEYOND HODGKIN AND HUXLEY:\nCALCIUM AND CALCIUM-\nDEPENDENT POTASSIUM\nCURRENTS\nThe cornerstone of modern biophysics is the comprehensive analysis by Hodgkin and\nHuxley (1952a,b,c,d) of the generation and propagation of action potentials in the squid\ngiant axon. The basis of their model is a fast sodium current /Na and a delayed potassium\ncurrent /K (which here we also refer to as /DR)- The last 40 years of research have shown\nthat impulse conduction along axons can be successfully analyzed in terms of one or both\nof these currents (see Sec. 6.6). Nonetheless, their equations do not capture—nor were they\nintended to capture—a number of important biophysical phenomena, such as adaptation\nof the firing frequency to long-lasting stimuli or bursting, that is, the generation of two\nto five spikes within 5-20 msec. Moreover, the transmission of electrical signals within\nand between neurons involves more than the mere circulation of stereotyped pulses. These\nimpulses must be set up and generated by subthreshold processes.\nThe differences between the firing behavior of most neurons and the squid giant axon\nreflect the roles of other voltage-dependent ionic conductances than the two described\nby Hodgkin and Huxley. Over the last two decades, more than several dozen membrane\nconductances have been characterized (Hagiwara, 1983; Llinas, 1988; Hille, 1992). They\ndiffer in principal carrier, voltage, and time dependence, dependence on the presence of\nintracellular calcium and on their susceptibility to modulation by synaptic inputs and\nsecond messengers. Our knowledge of these conductances and the role they play in\nimpulse formation has accelerated rapidly in recent years as a result of various technical\ninnovations such as single-cell isolation, patch clamping, and molecular techniques. We\nwill here describe the most important of these conductances and briefly characterize each\none. In order to understand more completely the functional role of these conductances\nin determining the response of the cell to input, empirical equations that approximate\ntheir behavior under physiological conditions must be developed and compared against\nthe physiological preparations. In a remarkable testimony to the power and the generality\n212\n9"}
{"text": "===== Page 2 =====\n9.1 Calcium Currents \n• \n213\nof the Hodgkin-Huxley approach, the majority of such phenomenological models has used\ntheir methodology of describing individual ionic conductances in terms of activating and\ninactivating particles with first-order kinetics (see Chap. 6). Theirs is also our method of\nchoice for quantifying the membrane conductance that we use throughout this book.\nBecause of the large number of ionic conductances that have been described throughout\nthe nervous systems of different animals, we would certainly exhaust the patience of the\nreader by listing and describing them all (as Llinas, 1988, has done). Instead, we focus on\na few key conductances, in particular those that appear to be common to many neurons in\nmost animals, from slugs and flies to mammals.\n9.1 Calcium Currents\nIn Chaps. 6 and 8, we characterized two currents, an inward one carried by sodium ions and\nan outward one mediated by a potassium current. The missing member in the triumvirate of\nions crucial for understanding the dynamics of the membrane potential is calcium. Ionized\ncalcium is the most common signal transduction element in all of biology. Ca2+ is required\nfor the survival of cells; yet an excess of calcium ions can lead to cell death (Johnson,\nKoike, and Franklin, 1992). Ca2+ plays a crucial role in triggering synaptic release upon\nthe invasion of an action potential at a presynaptic site, it is the key determinant for axonal\ngrowth and muscle contraction, and it constitutes the crucial signal that initiates synaptic\nplasticity. Indeed, Ca2+ can be thought of as coupling the membrane potential to cellular\noutput such as secretion, contraction, growth, and plasticity; that is, linking computation—\nas expressed by means of rapid and highly localized changes in Vm (t)—to action. Whenever\na neuron decides to do something, Ca2+ is the key.\nScores of mechanisms exist to precisely adjust the concentration of free intracellular\ncalcium [Ca2+],. As we will discuss in more detail in Chap. 11, the influx of Ca2+ into\nthe intracellular cytoplasm is primarily regulated by voltage-dependent calcium channels\nand by the NMDA synaptic receptor complex (see Sec. 4.6) in the membrane as well as\nby release from intracellular storage sites in the endoplasmic reticulum. These organelles\nare spread like a vast three-dimensional spider web throughout neurons and are thought\nto warehouse Ca2+ ions. Because the release from these intracellular storage sites usually\noccurs on a slow time scale, we will not discuss it here, referring the reader to Clapham\n(1995) for further references.\nWhat are some of the important functional properties shared by all calcium conductances?\nMost importantly, the associated calcium current is always activated by depolarization, is\nalways inward, and has some degree of inactivation that occurs on a much slower time\nscale than does inactivation of the sodium current. What is strikingly different among\nthe different calcium currents is their sensitivity to depolarization. Some currents activate\nin response to a depolarization of a few millivolts, while others require 20 mV or even\nmore. Some show rapid and some very slow voltage-dependent inactivation. At least\nthree distinct flavors of voltage-dependent calcium currents have been reasonably well\ncharacterized, although as the molecular structure of calcium channels becomes better\nunderstood, we will probably be able to distinguish many more subtypes (Yang et al., 1993;\nMcCleskey, 1994; Catterall, 1995,1996). Finally, calcium currents are no aberrations found\nonly in a few exceptional neurons. All nerve cells show evidence of calcium currents, many\nsimultaneously expressing two or more types (Tsien et al., 1988). While calcium currents"}
{"text": "===== Page 3 =====\n214 \n• \nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nappear to be largely absent in axons, they can be found throughout the dendritic tree, the\ncell body, and at presynaptic sites.\n9.1.1 Goldman-Hodgkin-Katz Current Equation\nUp to now, we have used Ohm's law to describe the relationship between sustained\ncurrent flow and applied membrane potential. However, given the extreme imbalance\nbetween the typical extracellular calcium concentration, in the low millimolar range, and\nthe intracellular concentration, ranging between 10 and 100 nM, it can be difficult to\nmeasure a reversal potential. Indeed, one would hardly expect calcium channels to generate\nmuch outward current beyond the calcium equilibrium potential. Even though the effective\ndriving potential might be very large at very depolarizing values, only a few calcium ions\nare avilable to move into the extracellular space, making the outward current very close\nto zero.\nThe standard theoretical account for the current flowing under these conditions, based\non the electrodiffusion model, was introduced into the literature by Goldman (1943) and\nHodgkin and Katz (1949) (see also Sees. 4.5 and 11.3). It makes no assumption about pores,\nrather hypothesizing that ions move independently through the membrane along a constant\nelectrical field (that is, a linear drop in membrane potential). Using this phenomenological\nmodel, the ionic current flowing across the membrane can be derived (see Jack, Noble, and\nTsien, 1975 or Hille, 1992) to be\n(9.1)\nwhere v = 2VmF/RT is the normalized membrane potential, with F = 96, 480 C/mol\nbeing Faraday's constant, R the gas constant, and T the absolute temperature, /ta is the\npermeability of the membrane to calcium and is defined as the constant of proportionality\nbetween the molar flux density of calcium across a unit area of membrane S and the\nconcentration difference across the membrane\n(9-2)\nwhere the membrane permeability has units of cm/sec. (This is essentially Pick's first law\nof diffusion; see Eq. 11.14.) Equation 9.1 is known as the Goldman-Hodgkin-Katz (GHK)\ncurrent equation.\nIf the ionic milieu on both sides of the membrane is the same, the GHK current equation\nis linear in the membrane potential, with a reversal potential of zero. As the external\nconcentration increases relative to the intracellular one, the I—V curve becomes shallower\nand shallower for positive potentials, since it becomes increasingly more difficult for the\nCa2+ ions to move against their concentration gradient.\nFor large, positive values of v, the current is linear in v, as for very negative values (e.g.,\nVm < —15 mV in the case of Fig. 9.1). The ratio of the slopes in the two domains is simply\nthe ratio of the concentration values across the membrane.\nMatching Eq. 9.1 against a real calcium current requires further considerations since,\nfor instance, calcium channels are mildly permeable to potassium. (The ratio of the two\npermeabilities is 1/1000; see Lee and Tsien, 1982, and Fig. 9.1.)\nWhether or not the calcium current should be described by Ohm's law or by the\nGHK current equation has yet to be carefully evaluated from an experimental point of\nview. The various numerical models that match simulations against experimental records"}
{"text": "===== Page 4 =====\n9.1 Calcium Currents \n• \n215\nFig. 9.1 THEORETICAL /—V RELATIONSHIP FOR A CALCIUM CURRENT \nDue to the 40,000-fold\nconcentration difference between extracellular Ca2+ (around 2 mM) and intracellular Ca2+ (around\n50 nM), the phenomenological constant field equation of Goldman, Hodgkin, and Katz (Eq. 9.1)\npredicts a strongly rectifying I-V relationship for calcium. If one takes account of the fact that\ncalcium channels are also weakly selective for potassium ions, the reversal potential shifts to somewhat\nlower values and the curve becomes less steep, although still rectifying. A linear relationship, based\nupon approximating the slope of the I-V curve for values Vm < —20 mV is superimposed. Under\nphysiological conditions, the membrane potential almost always resides in this range. Reprinted in\nmodified form by permission from Hille (1992).\nhave used either GHK or variants thereof (Llinas, Steinberg and Walton, 1981a,b; Lytton\nand Sejnowski, 1991; Huguenard and McCormick, 1992; DeSchutter and Bower, 1994a;\nBorg-Graham, 1997) or Ohm's law (Hudspeth and Lewis, 1988b; Traub and Miles, 1991;\nMcCormick, Huguenard, and Strowbridge, 1992; Yamada, Koch, and Adams, 1998). In\ndefense of the linear model it should be noted that under physiological conditions the cell\nspends almost all of its time at membrane potentials < — 15 mV, a region in which the I-V\nrelationship can be approximated very well by a linear one (Fig. 9.1). Cells only briefly\ntransgress to more positive potentials.\nWith this background, let us now review three of the most important calcium currents.\nThese are not the only types of calcium currents that have been dissected out of the\nphysiological responses of neurons. (Other currents include the P and the R calcium\ncurrents; Mintz, Adams, and Bean, 1992; Randall and Tsien, 1995.). Indeed, it remains\nunclear how many distinct types of ionic channels permeable to calcium ions exist in the\nnervous systems. This number could, potentially, be much larger than we assume today.\n9.1.2 High-Threshold Calcium Current\nThis current, labeled L for long lasting, was for many years the only known calcium current.\nDifferent from the fast sodium current of the squid axon, /ca(L) is only activated at the very\ndepolarized levels expected during action potentials. Its halfway activation point lies around\n10-0 mV (Fig. 9.2), implying that around Vrest. ^Ca(L) is totally deactivated. The L current\ndoes show inactivation that is not, however, dependent on the membrane potential but\ndepends, instead, on the intracellular calcium concentration just below the membrane. It is\nhalfway inactivated at quite low values of [Ca +],- (tens of micromolars). Because activation"}
{"text": "===== Page 5 =====\n216\nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nof /ca(L) does not follow the onset of depolarization instantaneously but appears with a\ndelay, the activation particle m is usually taken to the second, third, or even higher power\n(Hagiwara and Byerly, 1981; Johnston and Wu, 1995). Recalling the GHK equation 9.1,\nwe can express the current as\n(9.3)\nwith k an integer > 2, and /ca the GHK expression for the current as a function of the\nvoltage (see also Kay and Wong, 1987; Hille, 1992).\nBecause of the existence of specific pharmacological L channel blockers, it is known\nthat such L channels live in proximal dendrites and at the cell body of cortical pyramidal\nand thalamic cells (Galvan et al., 1986; Llinas, 1988; Fisher, Gray and Johnston, 1990;\nWestenbroek, Ahlijanian and Catterall, 1990).\n9.1.3 Low-Threshold Transient Calcium Current\nThe low-threshold transient or T calcium current is activated at lower voltages than the\nL current. Typically, its half-activation point is at —40 mV. At normal resting levels, this\ncurrent is turned off. It does inactivate slowly but strongly as a function of voltage, as\nevident in Fig. 9.3A. Under physiological conditions, the T current can be triggered by\nhyperpolarizing the membrane potential—thereby completely removing inactivation—and\nsubsequently allowing synaptic input (or a depolarizing current injection) to activate it.\n/Ca(T) can generate an all-or-nothing electrical event (usually called an LTspike; see below)\nsimilar to a classical action potential, but much broader, on top of which two or more sodium\nspikes are riding (e.g., Fig. 6.1H).\nIn thalamic relay cells, this current has been modeled (McCormick, Huguenard,\nand Strowbridge, 1992; McCormick and Huguenard, 1992; Huguenard and McCormick,\n1992) as\n(9.4)\nFig. 9.2 ACTIVATION AND INACTIVATION RANGES FOR CALCIUM CURRENTS Steady-state acti-\nvation (leftmost plot) and inactivation (middle plot) variables as a function of the membrane potential\nfor the three calcium currents discussed in the text. Inactivation of the L calcium current does not\ndepend on voltage but on the intracellular calcium concentration. The rightmost plot shows their\nI-V relationships. It is likely that intermediate forms of calcium current exist without specific\npharmacological blockers to uniquely identify them. Reprinted by permission from Johnston and\nWu (1995)."}
{"text": "===== Page 6 =====\n9.1 Calcium Currents \n• \n217\nFig. 9.3 ACTIVATION AND INACTIVATION OF THE TRANSIENT CALCIUM CURRENT Properties of\nthe transient low-threshold calcium current found in rat thalamic relay cells as modeled by Huguenard\nand McCormick (1992) using a standard Hodgkin-Huxley-like formalism (Eq. 9.4). Data for six\nthalamic cells are shown. (A) Steady-state activation and inactivation as a function of Vm. (B) IT is\nactivated by stepping the potential for 30 msec to various potentials, followed by repolarization to\n—80 mV (1—experimental data; 2—model). (C) Time constant of activation and (D) of inactivation\nand recovery. Reprinted by permission from Huguenard and McCormick (1992).\nThe steady-state activation moo and inactivation h^ as well as their associated time\nconstants rm and i), are illustrated in Fig. 9.3 as a function of the membrane potential\nfor the thalamus current. (For an alternate formulation see Wang, Rinzel, and Rogawski,\n1991.) Different from 7Na in the squid axon, /ca(T) does not possess a window current.\nThe window current refers to current flowing at steady-state, determined by the amount\nof overlap between activation and inactivation at any particular membrane potential. For\nthe T current, the window current /3ca(T)w^0'zoo^Ca> is always zero or close to zero for\nany sustained voltages; hence its name (Fig. 9.3A). Physiological data point toward the T\ncurrent as playing a prominent role in distal dendrites (Christie et al., 1995).\nA significant difference between low- and high-threshold calcium currents is that the\nlatter require depolarization beyond 0 m V, something only expected during action potentials,\nwhile the former are activated at much more modest levels of depolarization.\n9.1.4 Low-Threshold Spike in Thalamic Neurons\nThe role that the T current might play in information transmission was first elucidated in\ntwo classical papers by Jahnsen and Llinas (1984a,b). Thalamic relay cells occupy a unique\nposition in the central nervous system. With the exception of olfaction, all sensory systems\naccess cortex by projecting through the thalamus (for a review see Sherman and Koch, 1986,\n1998). Furthermore, about one-half of all synaptic contacts onto thalamic cells originate\nin the lower layers of the cortex. The thalamus is also the recipient of diffusely projecting"}
{"text": "===== Page 7 =====\n218\nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\ninput from the brainstem. The various systems have more than ample opportunity to control\nwhat is transmitted through the thalamus (Crick, 1984a).\nIntracellular recordings from thalamic cells in slice as well as in the intact animal (Jahnsen\nand Llinas, 1984a,b; Coulter, Huguenard, and Prince, 1989; Hernandez-Cruz and Pape,\n1989; Guido et al., 1995; Guido and Weyand, 1995) have revealed two functional, quite\ndistinct modes of operation which are characterized by whether or not the transient calcium\ncurrent is activated (Fig. 9.4).\nIf the membrane potential at the soma is around —75 mV or below—held there by modu-\nlatory input from the brain stem that is known to hyperpolarize thalamic cells tonically—the\ninactivation of /ca(T) is removed and any sufficiently large depolarizing synaptic input will\nactivate the low-threshold calcium current (Fig. 9.4A). Similar to the fast Hodgkin-Huxley-\nlike action potential, this further depolarizes the membrane, activating additional /ca(T), and\nso on. The result is an all-or-none action potential, the LT spike, which is relatively broad,\naround 50 msec. This is due to a two order of magnitude difference between fast activation\nand slow inactivation (Fig. 9.3D). As can be seen in Fig. 9.4A, this broad depolarization\ntriggers a series of very fast conventional sodium spikes riding on top of the LT spike. The\ncell is said to burst; consequently, this manner of operating is known as the burst mode.\nSubsequent to the LT spike, the membrane is profoundly refractory for a period of 100 msec\nor longer before it can generate the next calcium spike. During the intervening period, any\nsynaptic input will have little effect on the cell. If, conversely, the membrane is at rest or\nslightly depolarized, /ca(T) is inactivated and any input will cause either only a subthreshold\ndepolarization (Fig. 9.4B) or a series of conventional action potentials (Fig. 9.4C). Stronger\ndepolarization is encoded in a higher firing frequency: the cell operates in its relay mode.\nSteriade and his colleagues (Steriade and McCarley, 1990; Steriade, McCormick, and\nSejnowski, 1993) have provided much evidence that the relatively minor dc shifts in Vm\nFig. 9.4 BURSTING IN THALAMIC NEURONS \nThe majority of thalamic cells can be in one of two\nfunctionally quite distinct states. (A) If the membrane potential is tonically hyperpolarized to —75 mV\nor beyond—due to modulatory activity from the brainstem or inhibitory synaptic input—a transient\nlow-threshold calcium current is de-inactivated. A depolarizing current step (indicated in the lower\ntrace) triggers a slow all-or-none electrical event due to inward calcium current, the so-called LT spike.\nRiding on top of the LT action potential are a number of conventional, fast sodium spikes. The LT\nspike concludes with a profound hyperpolarization that prevents excitatory synaptic input during this\ntime from generating output spikes. (B) In an intermediate state, the cell responds to the same input\nin a linear fashion, with a slowly rising and decaying depolarization. (C) If the membrane is tonically\ndepolarized to —60 mV or above, the same input causes the cell to fire a series of conventional fast\nsodium action potentials. This constitutes the so-called relay mode. Reprinted in modified form by\npermission from Jahnsen and Llinas (1984a)."}
{"text": "===== Page 8 =====\n9.1 Calcium Currents \n• \n219\nwhich bring about the switch from one mode into the other are controlled by modulatory\ninput from a small number of discrete sites in the brainstem. It is known that thalamic cells\nin anesthetized as well as in awake behaving cats can respond to visual stimuli using burst\nas well as isolated spikes, preferentially generating bursts during the early phase of fixation\n(Guido and Weyand, 1995; Guido et al., 1995). We will return in Sec. 16.3 to the possible\nsignificance of bursting for information processing.\n9.1.5 N-Type Calcium Current\nThis is a current that has properties intermediate between the L and the T calcium currents,\nhence its name N for neither L nor T. It is activated at potentials intermediate between\nthe low- and high-threshold currents (Fig. 9.2). It does show slower, voltage-dependent\ninactivation dynamics than the T current and has a specific pharmacological channel blocker.\nThe N current is modeled as\n(9.5)\n(see Sutor and Zieglgansberger, 1987; Lytton and Sejnowski, 1991).\nWestenbroek and colleagues (1992) stained the entire rat brain with antibodies to a\npart of the N channel. As expected, most gray matter through the central nervous system\nwas stained, in particular proximal and distal dendrites and presynaptic terminals. Thus,\nN channels appear to complement the locations of L channels, which are restricted to the\ncell bodies and the proximal dendrites of pyramidal cells (Westenbroek, Ahlijanian, and\nCatterall, 1990).\n9.1.6 Calcium as a Measure of the Spiking Activity of the Neuron\nNo matter through which type of calcium channel Ca2+ ions enter the neuron, they\nwill now diffuse throughout the available space and interact with enzymes and proteins\nin the membrane and the cytoplasm. We defer until Chap. 11 an in-depth account of\nthe mathematics of diffusion and binding. Nonetheless, let us here briefly allude to a\ncomputationally interesting role of intracellular calcium.\nAround the intracellular mouth of calcium channels, Ca2+ exit the 5 A pore at rates\nexceeding one million per second. These ions diffuse throughout the neuron, binding at\navailable sites in the cytoplasm and the intracellular organelles (this binding can induce\nrelease of Ca2+ ions from these organelles). While the majority of Ca2+ ions is bound,\na small amount remains unbound and slowly accumulates inside the cytoplasm. Each\naction potential admits further calcium ions which further increases [Ca2+],. This buildup\nis illustrated in numerical simulations of the spherical cell body of bullfrog sympathetic\nganglion cells (Fig. 9.5; see Gamble and Koch, 1987, and Sec. 9.4 below). The Ca2+ ions\nrushing in through calcium channels for the few milliseconds that these channels are open\nduring an action potential, increase [Ca +],- in the core of the cell by between 3 and 4 nM.\nTo a first order, this increase is relatively independent of the spiking frequency (here varying\nbetween 10 and 50 Hz). While 3 nM might appear as a tiny increase, it is averaged over the\nentire cell body and will be much larger in local regions. The increase needs to be judged\nagainst the low resting level of [Ca2+],-, between 10 and 50 nM.\nExperimental calcium measurements in cell bodies of the sea snail Aplysia (Gorman and\nThomas, 1980), of bullfrog sympathetic ganglion cells (Smith, MacDermott, and Weight,\n1983; Hernandez-Cruz, Sala, and Adams, 1990) and of cortical pyramidal cells (Fig. 9.6;\nHelmchen, Imoto, and Sakmann, 1996) have confirmed this hypothetical buildup, which"}
{"text": "===== Page 9 =====\n220\nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nFig. 9.5 INTRACELLULAR CALCIUM ACCUMULATION FOLLOWING SPIKING ACTIVITY \nIncrease\nin intracellular free calcium in simulated bullfrog sympathetic ganglion cells in response to repetitive\nfast synaptic input at different input frequencies. For a more detailed description of this model see\nSec. 9.4. The increase of [Ca2+],- in the central 19-jU.m-radius core of the 20-jiim-radius cell body\nis relatively independent of the spiking frequency (each synaptic input triggers one spike). In other\nwords, the calcium concentration provides an index for the spiking activity of the cell in recent times.\n(The time constant of this integrated measure depends on the cellular geometry; the larger the volume,\nthe longer the effective time constant; see Sec. 11.7.2.) This \"activity\" measure can be read out by\nany calcium-dependent enzyme or protein. Reprinted by permission from Gamble and Koch (1987).\nFig. 9.6 CALCIUM BUILDUP PROPORTIONAL TO\nNEURONAL ACTIVITY \nFollowing each action\npotential in a layer 5 pyramidal neuron, calcium\nrushes into the cell and accumulates, here recorded\nusing a calcium-dependent fluorescent dye in the\nproximal apical dendrite (Helmchen, Imoto, and\nSakmann, 1996). This increase in calcium reaches\nan equilibrium with a time constant of 200 msec.\nThe final level (in nM) is linearly related to the\nfiring frequency of the cell (evoked by current\ninjections), with a slope of 15 nM/Hz (solid line).\nReprinted in modified form by permission from\nHelmchen, Imoto, and Sakmann (1996).\nreaches a plateau between 0.1 and 1 sec, depending on the geometry of the cell. It gradually\nsubsides due to extrusion processes that remove the excess calcium from the cytoplasm.\nThe integrated measure of past neuronal activity can be read out by any enzyme or protein\nthat is sensitive to calcium, such as a calcium-dependent potassium channel (see below), or\nany of the myriad cellular processes that are so exquisitely sensitive to calcium concentration\n(see Chaps. 11 and 20). This allows the neuron or a local circuit to up or downregulate its own\nactivity (a sort of gain control operation). Indeed, experimental evidence has accumulated\nin favor of this calcium set-point hypothesis, in which the concentration of free, intracellular\ncalcium determines the amplitude of various ionic currents (Johnson, Koike, and Franklin,\n1992; Turrigiano, Abbott, and Marder, 1994). An elegant modeling study has shown us how"}
{"text": "===== Page 10 =====\n9.2 Potassium Currents \n• \n221\na simple neuronal system, here modeled by the Morris-Lecar equations (Sec. 7.2), can be\nmodified to adjust its maximal conductances G as a function of integrated calcium activity\nuntil the system expresses stable spiking patterns (LeMasson, Marder, and Abbott, 1993).\n9.2 Potassium Currents\nA variety at least as great as among the calcium channels, if not greater, is displayed\nby ionic channels permeable to potassium ions. Under physiological conditions, current\nflow through these channels is outward, that is, upward in the normal convention. Given the\nphysiological reversal potential of K+ in the neighborhood of —100 mV, potassium currents\ncan be thought of as stabilizing the membrane potential, since activating a potassium current\npulls the membrane potential to hyperpolarizing levels. As we saw already in the case of /DR,\nthe delayed rectified potassium current in the squid axon (Sec. 6.2.1), potassium currents\nserve to keep the action potential brief. In the absence of such a potassium current, which\nrapidly repolarizes the membrane, the action potential would be much longer and would\nfollow the time course of inactivation of the sodium channel.1 As we will see, potassium\ncurrents serve to delay the generation of an action potential following synaptic or current\ninput as well as to reduce the firing frequency in the face of a constant input.\nLike the majority of ionic currents, potassium currents—with the noticeable exception of\nthe anomalous or inward rectifier current (Hagiwara, 1983; Doupnik, Davidson, and Lester,\n1995)—require depolarization to activate. Around Vresl, they—as well as all sodium and\ncalcium currents—are very small, minimizing the possibly wasteful antagonistic inward\nand outward current flow and, thereby, metabolic cost. In a sense, biology might have\ndiscovered a principle that designers of portable computers only implemented within the\nlast few years: when not in use, reduce the clock rate of the microprocessors as much as\npossible in order to conserve precious battery power.\n9.2.1 Transient Potassium Currents and Delays\nNext to /DR, arguably the most important potassium current is a transient, inactivating\npotassium current, known as 7A. First characterized by Connor and Stevens (1971b), it lives\nin a rather narrow voltage range between —70 and —40 mV, activating within 5-10 msec.\nDifferent from the delayed rectifier, /A inactivates on a slower time scale, typically on\nthe order of 100 msec. Based on their voltage-clamp data, Connor and Stevens (1971c)\ndescribed 7A on the basis of Ohm's law,\n(9.6)\nThe fourth power yields the observed sigmoidal activation curves, while the single inacti-\nvation particle reflects the exponential decay of the current. The time constants of activation\nand inactivation depend only weakly on the membrane potential.\nThe functional relevance of /A (Connor and Stevens, 197 Ic) can be understood by\nreference to Fig. 9.7. As we learned in Chaps. 6 and 7, the squid axonal membrane shows\nan abrupt onset of spiking: in the standard model, the minimum spiking frequency is\n53 Hz. We can understand why either by looking at the exact equations or by considering\nphase space analysis. In terms of biophysics, the Hodgkin-Huxley equations are unable to\n1. An exception to this are the nodes of Ranvier of myelinated fibers, in which a vigorous leak current serves to partially\nrepolarize the membrane; Sec. 6.6."}
{"text": "===== Page 11 =====\n222 \n• \nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\ngenerate spikes at very low frequencies, since too slow a rate of depolarization would lead\nto inactivation of the sodium current. In Chap. 7 we used phase space analysis to argue\nthat onset of oscillations with an infinitely long period requires a saddle-node bifurcation\nthat can be obtained with the help of an inactivation variable that depends steeply on the\nmembrane potential (witness the w = 0 nullcline in Fig. 7.9).\n/A exactly fulfills this condition: it is active in a narrow subthreshold range. In cortical\npyramidal cells—such as the one modeled in Fig 9.7—this current will try to hyperpolarize\nthe membrane potential subsequent to a current step (hence the bump in Fig. 9.7A; Segal\nand Barker, 1984; Schwindt et al., 1988; Zona et al., 1988). In this voltage range, /A slowly\ninactivates, thereby contributing less and less outward current: Vm slowly drifts upward until\nthe spiking threshold is reached and a spike is triggered. A similar mechanism generates\ndelays of up to 3-4 sec in molluscan neurons (Getting, 1983). It has also been hypothetized\nthat a class of cells in the cat lateral geniculate nucleus, lagged cells, which show a much\ndelayed response to a visual stimulus compared to nonlagged geniculate cells, implement\nthis delay via a transient A current (Saul and Humphrey, 1990; McCormick, 1991).\n/A can exert another profound effect on the firing behavior of cells. As is apparent upon\ninspection of the /-/ curve of apatch of squid axon membrane (Fig. 6.10A), the bandwidth\nFig. 9.7 DELAYED SPIKING DUE TO\nTRANSIENT POTASSIUM CURRENT \nThe\nA current implements a delay element.\n(A) Somatic membrane potential in the\nstandard pyramidal cell model in response\nto a 0.32-nA current step injected at\n100 msec. Vm depolarizes and activates\nthe transient potassium /A current. At\nthese potentials /A slowly inactivates, as\nevident in (B), showing the normalized\nA conductance. (An m h instead of the\nm4h formalism of Eq. 9.6 was used.)\nThis gradual inactivation of a potassium\ncurrent allows the membrane potential to\ncreep upward until, 1 sec after the current\ninjection, an action potential is generated.\nWithout /A the membrane potential would\neither remain subthreshold or generate a\nspike within a few tens of milliseconds.\n(C) Relationship between the injected cur-\nrent and the delay between current onset\nand spike initiation."}
{"text": "===== Page 12 =====\n9.2 Potassium Currents \n• \n223\nof firing is very limited because of the high onset firing frequency. The remedy for this is\nthe introduction of /A, allowing the membrane to generate action potentials across a much\nlarger firing frequency range than before.\nThe A current is just one member of a larger class of transient inactivating potassium\ncurrents that share similar pharmacological and functional properties (Rudy, 1988). Other\nmembers of this club include /D (D for delay) described in hippocampal cells (Storm,\n1988) and /AS, found in thalamic neurons (McCormick, 1991). Both currents have very\nlong activation times, causing a delay in the onset of spiking in response to a current\ninjection of up to 10 sec.\nWe conclude that I\\ and its siblings subserve at least two functions: (1) to allow cells to\nfire at very low frequencies, thereby increasing the bandwidth of the firing frequency code\nsignificantly, and (2) to implement a delay element that does not require ultraslow intrinsic\ntime constants.\n9.2.2 Calcium-Dependent Potassium Currents\nThe amplitude of several potassium currents is influenced by [Ca2+],, the intracellular\nconcentration of Ca2+ ions. These currents are collectively referred to as /K(Ca) to emphasize\nthat they are modulated by calcium but that the current itself is carried by K+ ions. The\nfunction of these currents is to shape the membrane potential following individual action\npotentials (Fig. 9.8), to read out the previous spiking history of the cell, shaping adaptation\naccordingly, and to instantiate, in conjunction with a calcium current, a resonance-like\nbehavior (Hudspeth and Lewis, 1988b; Sec. 10.4.3).\nWe here discuss two potassium currents: /c and /AHP (Latorre et al., 1989).\n/c is a fairly large, outward current whose activity depends on [Ca2+],- as well as on Vm.\n/c quickly activates upon the entry of calcium through calcium channels during an action\npotential. Similar to the delayed rectifier, it rapidly repolarizes the membrane potential,\nshutting off in the process in readiness for the next spike. It is not easy to study the dynamics\nFig. 9.8 CALCIUM AND THE ACTION POTENTIAL \nEffect of blocking calcium channels on\ncomputer-simulated and observed action potentials in type B bullfrog sympathetic ganglion cells\n(Yamada, Koch, and Adams, 1998). In both cases, the standard action potential (induced by a brief\ncurrent pulse) repolarizes more quickly and is followed by a longer afterhyperpolarization than the\naction potential (dotted lines) in the absence of calcium currents (achieved in the experiment by adding\ncadmium to the bathing solution and in the model by reducing Gca to 5% of its normal value). The\neffect of blocking Ca2+ on the peak amplitude of the spike is minimal: the spike itself is mediated\nby Na+ and not by Ca2+ ions. Reprinted in modified form by permission from Yamada, Koch, and\nAdams (1998)."}
{"text": "===== Page 13 =====\n224 • \nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nof this, or any other calcium-dependent potassium current, since its time course is highly\nsensitive to the calcium concentration in the immediate neighborhood of the underlying\nchannel. Because Ca2+ will decay very differently in thin dendrites than in a large cell\nbody, /c will also have a different time course, even if the dynamics of the membrane\npotential are the same in both cases.\nThe microscopic gating of the ionic channels underlying this current has been studied in\nsome detail, giving rise to complex kinetic gating schemes with two or more calcium ions\nnecessary for the channel to open (McManus and Magleby, 1988). A much simpler gating\nscheme for the underlying conductance gc that reproduces much of the time and voltage\ndependency involves calcium ions binding to a single activation particle, which effects the\ntransition from closed to open channel in a voltage-dependent manner (Adams et al., 1986;\nYamada, Koch, and Adams, 1998):\n(9.7)\nIf /DR and /c were the only currents activated by a single action potential, the spike\nafterhyperpolarization (AHP) would promptly return to rest with a time course dictated\nby the membrane time constant. Yet neurons throughout the nervous system (Connors,\nGutnick, and Prince, 1982; Pennefather et al., 1985; Lancaster and Adams, 1986) show a\nsecond component of the AHP, reflecting a much smaller and slower calcium-dependent\npotassium conductance. The associated current, termed /AHP> is small and depends only on\nthe concentration of intracellular calcium just below the membrane. The secret to its success\nis that it is sustained: when calcium rushes in during an action potential, /AHP turns on and\neffectively subtracts from the depolarizing current from an electrode or from synaptic input.\nIn hippocampal pyramidal cells, the amplitude of the hyperpolarization following spiking\nactivity increases with increasing number of action potentials, showing that /AHP increments\nwith each spike (Madison and Nicoll, 1984). Blocking this current prevents firing frequency\nadaptation from occurring. The current is described as\n(9-8)\nExperimentally, /AHP can be distinguished from /c by the use of naturally occurring toxins\nthat are added to the solution bathing the neurons: while /c is blocked in many cells by\ncharybdotoxin, a peptide from scorpion venom, /AHP can be blocked by injecting a toxin\nfrom bee venom, apamin. This reveals the tight coevolution of predators and the nervous\nsystems of the animals they hunt.\n9.3 Firing Frequency Adaptation\nThroughout neocortex, hippocampus, and thalamus, cells receive a clearly denned input\nfrom a single site in the pontine brainstem, the locus coeruleus. Each of the small number\nof coeruleus neurons (about 2 x 1600 in rat and 2 x 13,000 in humans) projects in a\nvery diffuse manner throughout the brain. A single neuron might enervate frontal cortex,\nthalamus as well as occipital cortex (Foote and Morrison, 1987). Excitation in these cells\nhas been related to increased vigilance and arousal in both rats and monkeys (Foote, Aston-\nJones and Bloom, 1980).\nUpon stimulation, the terminals of these neurons release the neurotransmitter nora-\ndrenaline (also termed norepinephrine) onto their postsynaptic targets (see Table 4.1). This\nleads to a quite pronounced and long-lasting change in the excitability of these cells, an effect"}
{"text": "===== Page 14 =====\n9.3 Firing Frequency Adaptation \n• \n225\nFig. 9.9 SLOW MODULATION OF FIRING FREQUENCY Change in firing frequency adaptation in\nCA1 pyramidal cells from a hippocampal slice during application of noradrenaline. A brief just-\nthreshold current ramp is applied first, followed by a 650-msec-long, suprathreshold current pulse.\nThe principal action of noradrenaline, mediated by a second messenger, is to reduce firing frequency\nadaptation by blocking the slow calcium-dependent potassium conductance /AHP- A side effect is a\nslight increase in the firing threshold of the cell (that is, the brief current ramp does not trigger an\naction potential in the presence of noradrenaline). This modulation in the firing properties can persist\nfor seconds or even minutes. Hippocampal pyramidal cells receive a well-defined noradrenergic input\nfrom brainstem neurons. Reprinted by permission from Madison and Nicoll (1986).\nthat has been studied in isolation in pyramidal cells in the neocortical and the hippocampal\nslice preparation (Madison and Nicoll, 1986; Nicoll, 1988; Schwindt et al., 1988).\nApplication of noradrenaline to pyramidal cells causes two changes. One is a small\n(about 2-3 mV) hyperpolarization, leading to a measurable increase in the spiking threshold\n(Fig. 9.9). A much more dramatic effect can be observed when the cell is sufficiently\nexcited to fire trains of spikes. Normally, these cells adapt, that is, their firing rate in\nresponse to a constant current step slows down (as in \"Control\" in Fig. 9.9). In the\npresence of noradrenaline, spike discharge accommodation is almost completely blocked.\nIts action is specific in that neither the shape of the action potential itself nor the fast\nafterhyperpolarization phase is affected. Indeed, experiments have shown (Madison and\nNicoll, 1986) that noradrenaline specifically blocks /AHP for many tens of seconds. This\neffect is mediated by a metabotropic receptor (Sec. 4.4 and Table 4.2). In this case,\nnoradrenaline binds to fi\\ adrenergic receptors, releasing a second messenger inside the\ncell which directly or indirectly phosphorylates the channel proteins underlying /AHP- An\nadded complication is that the small hyperpolarizing action of noradrenaline—caused by\nan increase in a potassium conductance— is mediated by a adrenergic receptors. Any\nneuroactive substance in the brain will often initiate a series of changes in the electrical\nbehavior of its postsynaptic targets via multiple actuators (here receptors).\nNoradrenaline is not alone in being able to control spike rate adaptation in cortical\ncells. Acetylcholine, mediated by a dense cholinergic input to the hippocampus from\ncells in the medial septal nucleus, has a similar effect, although its action is even more\ncomplex than that of noradrenaline (it appears to modify at least four distinct potassium\ncurrents; Cole and Nicoll, 1984; McCormick and Prince, 1986; Nicoll, 1988; Schwindt et\nal., 1988). The pyramidal cells that receive such a cholinergic input can have their spike\nrate accommodation switched off for tens of seconds in the absence of any direct change\nof the membrane potential.\nThe lesson is that multiple, diffusely projecting systems in the brain can independently\ncontrol the excitability of neurons via slow modulation of membrane conductances. Their\nmode of action is somewhat similar to a broadcast that is sent out to all processors in a"}
{"text": "===== Page 15 =====\n226 \n• \nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nmassively parallel computer. In other words, all processors will receive the same signal.\nBecause a single neuron in the locus coeruleus projects widely throughout the brain, this\nblockage of spike rate adaptation is not very precise in terms of space nor in terms of\ntime (since it typically requires a fraction of a second to set in and may last for many,\nmany seconds). From a computational point of view, these diffuse systems that target\nspecific conductances should be thought of as modifying the basic electrical properties\nof the processing circuits on a relatively global scale, possibly for controlling the level of\narousal or vigilance and for providing a single slow global feedback signal during learning.\n9.4 Other Currents\nBesides the handful of voltage-dependent membrane currents discussed so far, there exist\na plethora of other currents that are responsible, in isolation or in concert with others, for\nshaping various phases of cellular excitability (Llinas, 1988). They include /M, a slowly\nchanging voltage-dependent potassium current found in pyramidal cells, which contributes\nto the accommodation of action potential discharge (Halliwell and Adams, 1982; Brown,\n1988), as well as an entire family of inward rectifier potassium currents, which are activated\nupon hyperpolarization of the membrane potential (Hagiwara, 1983; Spain, Schwindt, and\nGrill, 1987).\nA smaller and less well understood family of channels is primarily permeable to chloride\nions, by far the most abundant anions in biological systems (Hille, 1992). In general, the\nreversal potential for Cl~ is within 10-15 mV of the resting potential of neurons, and\nactivation of a chloride current will repolarize the cell and prevent it from firing. The voltage\nand time dependencies of chloride conductances are only minor and slow. Therefore, they\nprobably contribute to the leak conductance that is usually assumed to be independent of\nthe applied membrane potential. There also exists a calcium-activated chloride conductance\nwith only a weak voltage dependency, which could play a similar role to calcium-dependent\npotassium conductances (Mayer, 1985).\nA possible important current is the sustained or persistent sodium current /Na,p- It has\nbeen clearly documented in cortical pyramidal cells (Stafstrom et al., 1985; French et al.,\n1990) due to the sensitivity to TTX that it shares with its fast sibling, 7Na. This current\nturns on within a few milliseconds upon depolarization, but either does not inactivate at all\nor only very slowly, thereby providing a steady inward current. Because 7Naip is activated\nnear the resting potential, it tends to amplify small EPSPs. The firing frequency of the cell\nwill be quite sensitive to the total amount of this current, since too much /Na,p will prevent\nrepolarization following action potentials.\n9.5 An Integrated View\nFor the most part, this chapter has described individual currents and their possible functional\nroles. Before we go on to other matters, we would like to give the reader a holistic view of\nhow these different currents interrelate and how they function within an entire system. As\nexample, we will discuss the simulacrum of type B bullfrog sympathetic ganglion cells of\nAdams et al., (1986) and Yamada, Koch, and Adams (1998). These neurons make only a\nlowly contribution to the overall scheme of things, enabling the animal to control its glands\nfor slime production. From an electrophysiological point of view, though, these cells are"}
{"text": "===== Page 16 =====\n9.5 An Integrated View\n227\nFig. 9.10 MODELING A BULLFROG SYMPATHETIC GANGLION CELL \n(A) Electrical circuit used\nin the Yamada, Koch, and Adams (1998) simulacrum of type B bullfrog sympathetic ganglion\ncells. Given their spherical shape and the complete absence of any dendrites, a single lumped\ncompartment that includes seven time-, voltage-, and calcium-dependent membrane conductances\nis entirely sufficient to model their electrical behavior. (B) Underlying assumptions for modeling\nthe extracellular accumulation of potassium (in a single extracellular compartment) and the series of\nshells surrounding a well-mixed central core to account for Ca2+ diffusion inside the cell. A calcium\nbuffer is distributed throughout the cell. Not drawn to scale. Reprinted by permission from Yamada,\nKoch, and Adams (1998).\nideal since they are easy to cultivate, big (with a mean diameter of 35 fim), and devoid of\nany dendritic processes.\nThe electrical structure of the model, shown schematically in Fig. 9.10, includes the\nseven time-, voltage-, and calcium-dependent conductances found in these cells, in addition\nto a passive RC contribution: the fast sodium 7Na and the delayed rectifier potassium\n/DR currents underlying spike production, an L type calcium current /ca, the two forms\nof calcium-dependent potassium currents discussed above, /c and JAHP, the transient\npotassium current /A, and a fifth potassium current, the small and slowly activating voltage-\ndependent M current. The description of these currents is derived from voltage-clamp data\nand is based on the conventional Hodgkin-Huxley rate formalism and the use of Ohm's\nlaw. (for the numerical values of all parameters, consult the appendix in Yamada, Koch,\nand Adams, 1998). Because of the spherical geometry of these cells, they are electrically\ncompact and can be modeled using a single compartment.\nSympathetic ganglion cells are not compact from the point of view of the spatio-temporal\ndistribution of the Ca2+ ions entering the cell via calcium channels. As treated in detail in"}
{"text": "===== Page 17 =====\n228\nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nChap. 11, the diffusion equation is solved in spherical coordinates, using a series of thin\nshells surrounding a well-stirred, central core. Ca2+ ions can bind to a stationary buffer\ndistributed throughout the cytoplasm using a second-order reaction (see Sec. 11.4.1). In the\noutermost compartment just below the membrane, calcium ions are extruded from the cell\nvia a first-order calcium pump. Finally, in order to account for the accumulation of potassium\nions in the pericellular space just outside the neuronal membrane—leading to a significant\nreduction of the potassium battery EK following spike activity—the cell is assumed to\nbe surrounded by a single compartment, whose concentration of K+ ions is determined\nby a simple uptake mechanism. We leave the mathematics of solving two coupled sets of\nnonlinear equations for Vm(t) and [Ca2+](x, t) to Appendix C.\nFigures 9.11 and 9.12 illustrate the basic performance of the model in response to a short\ncurrent pulse that triggers an action potential. The spike itself (Fig. 9.8) is similar to the\nsquid axon spike (Fig. 6.6) with the noticable exception of the second, long-lasting phase\nof the afterhyperpolarization, mediated by /AHP- Moreover, in these cells /c, rather than\n/DR, is largely responsible for repolarizing the potential following the excursion of Vm to\n0 mV and beyond (as can be ascertained by comparing the peak amplitudes of /c and /DR).\nFig. 9.11 RESPONSE or THE SYMPATHETIC GANGLION CELL MODEL: MAJOR CURRENTS \nRe-\nsponse of the bullfrog sympathetic ganglion cell model (see Fig. 9.10) at rest to a 2-msec-long current\npulse of 1.75-nA amplitude (indicated by black bars). This panorama depicts the major currents\ninvolved in spike initiation and repolarization seen in (A) (identical to the spike in Fig. 9.8). Notice\nthe fast and slow components of the afterhyperpolarization. The two inward currents /Na and /ca are\nplotted in (B). Only 9% of the incoming charge is carried by Ca2+ ions, emphasizing the fact that the\nprimary function of calcium ions is to shape neuronal excitability, rather than to be directly involved in\nspike production. (C) Fast calcium-dependent potassium current /c that contributes substantially more\nto spike repolarization than the delayed rectifier current /DR, shown in (D). Reprinted by permission\nfrom Yamada, Koch, and Adams (1998)."}
{"text": "===== Page 18 =====\n9.5 An Integrated View\n229\nThe amplitude of /AHP is minute compared to /c; yet, because it is maintained, it has a\ncrucial role to play in determining spike accommodation (Adams et al., 1986). Changes in\nthe driving potential can be quite significant, as seen in the transient 50 mV reduction in\nECS (Fig. 9.12B) caused by the very brief elevation of [Ca2+], just below the membrane to\nmicromolar values.\nThe effect of blocking either /M or /AHP or both on the average firing frequency is the sub-\nject of Fig. 9.13. Blocking /AHP leaves the current threshold for spike initiation unchanged\nbut does increase the gain of the /-/ \ncurve by about a factor of 2. Exactly the same result\nis observed in a quite different system, vestibular neurons in the brainstem (du Lac, 1996;\nFig. 21.3) where pharmacological elimination of the slow calcium-dependent potassium\ncurrent increases the gain of the /-/ \ncurve by a factor varying between 1.38 and 2.28.\nBlocking but /M in the bullfrog leaves the slope of the discharge curve relatively\nunchanged while reducing I& to 5% of its original value. Removing both conductances\nFig. 9.12 RESPONSE OF THE SYMPATHETIC GANGLION CELL MODEL: MINOR CURRENTS AND\nCALCIUM CONCENTRATION Response of the bullfrog sympathetic ganglion cell to the same\nstimulus as in Fig. 9.11. (A) Three smaller potassium currents /M, /AHP, and /A- The latter does\nnot appear to play any significant role in these cells. /M is active at rest. The primary effect of\nblockage of /M is a reduction in current threshold /th, while blockage of /AHP leads to a partial loss\nof spike firing adaptation. Blockage of both currents occurs in vivo during stimulation of cholinergic\ninput to the ganglia. The changes in the Nernst potentials for E^a and EK are shown in (B). The\nprimary rapid and the secondary slow components in the change in £ca reflect the rapid increase in\nfree calcium concentration in the outermost shell just below the membrane plotted in (C). Activation\nof /ca leads to a rapid influx of Ca2+ ions that quickly bind to the buffer. The slow decay results from\ncalcium loss via the pump as well as diffusion toward the core of the cell. [Ca2+],; for the tenth shell\nbelow the membrane (dotted line) is also illustrated in (C). The calcium concentration in the central\ncore is shown in (D). Reprinted by permission from Yamada, Koch, and Adams (1998)."}
{"text": "===== Page 19 =====\n230\nCALCIUM AND CALCIUM-DEPENDENT POTASSIUM CURRENTS\nFig. 9.13 CHANGING THE GAIN OF THE DISCHARGE CURVE \nEffect of blocking either /M or /AHP\nor both on the /-/ curve of the model bullfrog sympathetic ganglion cell. Mean firing rate, averaged\nover 200 msec, following current steps of variable amplitude. Removing /M primarily affects /th but\nleaves the slope intact, while eliminating /AHP increases the gain of the discharge curve by a factor of\n2. Blocking the two simultaneously leads to the summation of both effects. The control of the gain of\na neuron via control of the amount of the slow calcium-dependent potassium current present might\nbe a crucial biophysical operation underlying different computations. Reprinted by permission from\nYamada, Koch, and Adams (1998).\nlowers the current threshold but increases the slope, leading to the dramatic removal of\nspike adaptation observed experimentally (Adams et al., 1986).\nIf ^AHP can be controlled in individual neurons, rather than by a globally acting neuro-\ntransmitter such as noradrenaline, as introduced above, it offers the possibility of selectively\nup or down regulating the gain of a neuron without affecting the spike generation mechanism\nitself. Given the importance of receptive field models that incorporate a multiplicative effect\non the gain of individual neurons (leading to so-called gain fields; Zipser and Andersen,\n1988; but see Pouget and Sejnowski, 1997), this might be a crucial canonical biophysical\noperation.\n9.6 Recapitulation\nIn this chapter, we summarized some of what is known about ionic currents populating the\nnervous system other than /Na and 7DR of squid axon fame. In particular, we introduced\ncalcium currents. Ca2+ ions are crucial for the life and death of neurons; they link rapid\nchanges in the membrane potential, used for computation, with action, such as neurotrans-\nmitter release at a synapse, muscle contraction, or biochemical changes underlying cellular\nplasticity. Three broad classes of calcium currents are known: L, T, and N currents, although\nit is unclear whether these are just three samples along a continuum of such currents. Because\nof the substantial imbalance between the concentration of Ca2+ inside and outside the cell,\nelectrical rectification needs to be taken account of. Typically, this requires the use of the\nGoldman-Hodgkin-Katz equation to describe calcium currents, rather than the linear Ohm's\nlaw. The accumulation of calcium ions following action potential activity has the useful\nfunction that the intracellular calcium concentration in the cell represents an index of the"}
{"text": "===== Page 20 =====\n9.6 Recapitulation \n• \n231\nrecent spiking history of the cell. Various proteins and enzymes, such as calcium-dependent\npotassium channels, might be able to read out this variable and use it to normalize excitability\nor for gain control.\nWhile calcium ions themselves play almost no role in generating conventional action\npotentials, calcium currents can support a slower all-or-none electrical event, the LT spike,\nleading to a burst of fast sodium spikes. It may be possible that these and other forms of\nbursts represent events of special significance in the nervous system.\nWe introduced some of the many potassium currents that have been identified, focusing\non the transient inactivating potassium current /A. It serves to linearize the discharge curve\naround threshold and implements a delay element between the onset of depolarizing input\nand spike initiation. It does so via a voltage-dependent current whose time constant is\nconsiderably faster than the duration of the delay.\nAlso important are the calcium-dependent potassium currents, collectively referred to as\n/K(Ca)- Calcium ions that move into the cell during the depolarizing phase of the spike turn\non these hyperpolarizing outward currents, rendering subsequent spiking that much more\ndifficult. That this might have profound significance for the nervous system can be observed\nin hippocampus, neocortex, and elsewhere. The release of noradrenaline by brainstem fibers\nabolishes spike frequency accommodation, greatly increasing the excitability of these cells.\nInterestingly, the effect of blocking only /AHP appears to be specific to increasing the slope\nof the cell's /-/ curve, that is, increasing its gain.\nWe keep on drawing analogies between the operations the nervous system carries out\nto process information and digital computers. There are, of course, very deep, conceptual\ndifferences between the two. One is the amazing diversity seen in biology. An /A current\nin a neocortical pyramidal cell is not the same as the /A current in hippocampal pyramid\ncells or in thalamic relay cells. Each has its own unique activation and inactivation range,\nkinetics, and pharmacology, optimized by evolution for its particular role in the survival of\nthe organism. A case in point appears to be calcium channels. So far, molecular biological\ntechniques have revealed about a dozen different genes coding for two of the five subunits\nof the calcium channel, a number that is likely to increase over time. This provides the\nnervous system with the wherewithal to generate a very large number of calcium channels\nwith different functional properties (Hofmann, Biel, and Flockerzi, 1994). This tremendous\ndiversity could be necessary to allow each organism to optimize its complement of ionic\ncurrents for its particular operations depending on its particular and unique developmental\nhistory. This is, indeed, very far removed from the way we design and build digital integrated\ncircuits using a very small library of canonical circuit elements."}
